{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [
        "Lzt1SMEoSshL",
        "EgfaelmnXe04",
        "m-Hm3lsWXkbE",
        "Na73oRb9Xq69",
        "DBnTIBCDGX94",
        "mLO6GGJ5Nn9w",
        "2aZgEs15Kln5"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1.bandpass"
      ],
      "metadata": {
        "id": "WuakxNwvvczU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "cXWIFrrheAli",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "178d862a-0852-4ab4-8a5a-f042320925e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip '/content/drive/MyDrive/model_line.zip' -d '/content/drive/MyDrive/model_line'"
      ],
      "metadata": {
        "id": "IMsCkbbpeDvJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c6ba509-4e6d-42b3-8562-1567bcc35d70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/model_line.zip\n",
            "   creating: /content/drive/MyDrive/model_line/model_line/\n",
            "   creating: /content/drive/MyDrive/model_line/model_line/band/\n",
            "  inflating: /content/drive/MyDrive/model_line/model_line/band/test_band_line.csv  \n",
            "  inflating: /content/drive/MyDrive/model_line/model_line/band/train_band_line.csv  \n",
            "  inflating: /content/drive/MyDrive/model_line/model_line/band/valid_band_line.csv  \n",
            "   creating: /content/drive/MyDrive/model_line/model_line/high/\n",
            "  inflating: /content/drive/MyDrive/model_line/model_line/high/test_high_line.csv  \n",
            "  inflating: /content/drive/MyDrive/model_line/model_line/high/train_high_line.csv  \n",
            "  inflating: /content/drive/MyDrive/model_line/model_line/high/valid_high_line.csv  \n",
            "   creating: /content/drive/MyDrive/model_line/model_line/low/\n",
            "  inflating: /content/drive/MyDrive/model_line/model_line/low/test_low_line.csv  \n",
            "  inflating: /content/drive/MyDrive/model_line/model_line/low/train_low_line.csv  \n",
            "  inflating: /content/drive/MyDrive/model_line/model_line/low/valid_low_line.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nhn3soxJQYzl"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import itertools\n",
        "import time\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import (CosineAnnealingLR,\n",
        "                                      CosineAnnealingWarmRestarts,\n",
        "                                      StepLR,\n",
        "                                      ExponentialLR)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import accuracy_score, auc, f1_score, precision_score, recall_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm, re, sys\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "from keras.layers import Conv2D, Conv1D, MaxPooling2D, MaxPooling1D, Flatten, BatchNormalization, Dense\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.callbacks import CSVLogger, ModelCheckpoint"
      ],
      "metadata": {
        "id": "88FIZSPSbA_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train = pd.read_csv('/content/drive/MyDrive/model_line/model_line/band/train_band_line.csv', header=None)\n",
        "data_test = pd.read_csv('/content/drive/MyDrive/model_line/model_line/band/test_band_line.csv', header=None)\n",
        "data_val= pd.read_csv('/content/drive/MyDrive/model_line/model_line/band/valid_band_line.csv', header=None)"
      ],
      "metadata": {
        "id": "Fo2pj1pSfN4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "YXRC4G52hrnx",
        "outputId": "dadbc748-fb3e-4235-bae0-08f712534c63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           0         1         2         3         4         5         6    \\\n",
              "0    -0.013974 -0.011168 -0.009118 -0.008067 -0.008125 -0.009210 -0.011005   \n",
              "1    -0.168748 -0.165213 -0.162149 -0.159616 -0.157617 -0.156088 -0.154895   \n",
              "2    -0.065523 -0.063387 -0.062618 -0.063184 -0.064857 -0.067233 -0.069777   \n",
              "3     0.517435  0.633848  0.709706  0.733996  0.703265  0.622346  0.503432   \n",
              "4    -0.069052 -0.066942 -0.064951 -0.062974 -0.060851 -0.058435 -0.055658   \n",
              "...        ...       ...       ...       ...       ...       ...       ...   \n",
              "5995  0.004594  0.009590  0.017135  0.025880  0.034416  0.041620  0.046867   \n",
              "5996  0.017135  0.017694  0.019432  0.022356  0.026302  0.030971  0.035976   \n",
              "5997 -0.061678 -0.060521 -0.059365 -0.058206 -0.057080 -0.056067 -0.055283   \n",
              "5998 -0.039287 -0.039374 -0.039054 -0.038510 -0.037933 -0.037494 -0.037327   \n",
              "5999  0.030423 -0.043405 -0.110988 -0.163599 -0.195505 -0.204771 -0.193219   \n",
              "\n",
              "           7         8         9    ...       246       247       248  \\\n",
              "0    -0.012983 -0.014471 -0.014771  ...  0.006733  0.009824  0.013030   \n",
              "1    -0.153851 -0.152741 -0.151345  ... -0.112212 -0.109266 -0.106187   \n",
              "2    -0.071909 -0.073104 -0.072994  ... -0.108012 -0.104863 -0.102909   \n",
              "3     0.363695  0.221988  0.095392  ... -0.079957 -0.067902 -0.046360   \n",
              "4    -0.052576 -0.049378 -0.046363  ... -0.134398 -0.135559 -0.138471   \n",
              "...        ...       ...       ...  ...       ...       ...       ...   \n",
              "5995  0.050103  0.051777  0.052674  ...  0.045016  0.016083 -0.009705   \n",
              "5996  0.040891  0.045291  0.048799  ...  0.005152  0.044268  0.089211   \n",
              "5997 -0.054843 -0.054816 -0.055175  ... -0.046773 -0.050592 -0.052317   \n",
              "5998 -0.037532 -0.038153 -0.039167  ... -0.058137 -0.060069 -0.061489   \n",
              "5999 -0.165648 -0.128563 -0.088765  ...  0.125802  0.136895  0.147996   \n",
              "\n",
              "           249       250       251       252       253       254       255  \n",
              "0     0.016239  0.019380  0.022418  0.025327  0.028062  0.030516  0.032514  \n",
              "1    -0.103433 -0.101427 -0.100452 -0.100579 -0.101653 -0.103329 -0.105151  \n",
              "2    -0.101926 -0.101481 -0.101056 -0.100197 -0.098638 -0.096375 -0.093669  \n",
              "3    -0.023911 -0.006614  0.002560  0.003524 -0.001689 -0.009950 -0.018044  \n",
              "4    -0.142558 -0.147015 -0.150950 -0.153535 -0.154163 -0.152557 -0.148830  \n",
              "...        ...       ...       ...       ...       ...       ...       ...  \n",
              "5995 -0.032355 -0.052129 -0.069386 -0.084460 -0.097612 -0.109031 -0.118868  \n",
              "5996  0.134152  0.171846  0.194955  0.197629  0.176948  0.133840  0.073184  \n",
              "5997 -0.052018 -0.049965 -0.046587 -0.042433 -0.038139 -0.034393 -0.031913  \n",
              "5998 -0.061986 -0.061385 -0.059834 -0.057803 -0.055972 -0.055062 -0.055631  \n",
              "5999  0.158825  0.169190  0.179084  0.188748  0.198658  0.209439  0.221705  \n",
              "\n",
              "[6000 rows x 256 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-758fe6fd-30b8-4756-a7d0-08b3445028d5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>246</th>\n",
              "      <th>247</th>\n",
              "      <th>248</th>\n",
              "      <th>249</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.013974</td>\n",
              "      <td>-0.011168</td>\n",
              "      <td>-0.009118</td>\n",
              "      <td>-0.008067</td>\n",
              "      <td>-0.008125</td>\n",
              "      <td>-0.009210</td>\n",
              "      <td>-0.011005</td>\n",
              "      <td>-0.012983</td>\n",
              "      <td>-0.014471</td>\n",
              "      <td>-0.014771</td>\n",
              "      <td>...</td>\n",
              "      <td>0.006733</td>\n",
              "      <td>0.009824</td>\n",
              "      <td>0.013030</td>\n",
              "      <td>0.016239</td>\n",
              "      <td>0.019380</td>\n",
              "      <td>0.022418</td>\n",
              "      <td>0.025327</td>\n",
              "      <td>0.028062</td>\n",
              "      <td>0.030516</td>\n",
              "      <td>0.032514</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.168748</td>\n",
              "      <td>-0.165213</td>\n",
              "      <td>-0.162149</td>\n",
              "      <td>-0.159616</td>\n",
              "      <td>-0.157617</td>\n",
              "      <td>-0.156088</td>\n",
              "      <td>-0.154895</td>\n",
              "      <td>-0.153851</td>\n",
              "      <td>-0.152741</td>\n",
              "      <td>-0.151345</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.112212</td>\n",
              "      <td>-0.109266</td>\n",
              "      <td>-0.106187</td>\n",
              "      <td>-0.103433</td>\n",
              "      <td>-0.101427</td>\n",
              "      <td>-0.100452</td>\n",
              "      <td>-0.100579</td>\n",
              "      <td>-0.101653</td>\n",
              "      <td>-0.103329</td>\n",
              "      <td>-0.105151</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.065523</td>\n",
              "      <td>-0.063387</td>\n",
              "      <td>-0.062618</td>\n",
              "      <td>-0.063184</td>\n",
              "      <td>-0.064857</td>\n",
              "      <td>-0.067233</td>\n",
              "      <td>-0.069777</td>\n",
              "      <td>-0.071909</td>\n",
              "      <td>-0.073104</td>\n",
              "      <td>-0.072994</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.108012</td>\n",
              "      <td>-0.104863</td>\n",
              "      <td>-0.102909</td>\n",
              "      <td>-0.101926</td>\n",
              "      <td>-0.101481</td>\n",
              "      <td>-0.101056</td>\n",
              "      <td>-0.100197</td>\n",
              "      <td>-0.098638</td>\n",
              "      <td>-0.096375</td>\n",
              "      <td>-0.093669</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.517435</td>\n",
              "      <td>0.633848</td>\n",
              "      <td>0.709706</td>\n",
              "      <td>0.733996</td>\n",
              "      <td>0.703265</td>\n",
              "      <td>0.622346</td>\n",
              "      <td>0.503432</td>\n",
              "      <td>0.363695</td>\n",
              "      <td>0.221988</td>\n",
              "      <td>0.095392</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.079957</td>\n",
              "      <td>-0.067902</td>\n",
              "      <td>-0.046360</td>\n",
              "      <td>-0.023911</td>\n",
              "      <td>-0.006614</td>\n",
              "      <td>0.002560</td>\n",
              "      <td>0.003524</td>\n",
              "      <td>-0.001689</td>\n",
              "      <td>-0.009950</td>\n",
              "      <td>-0.018044</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.069052</td>\n",
              "      <td>-0.066942</td>\n",
              "      <td>-0.064951</td>\n",
              "      <td>-0.062974</td>\n",
              "      <td>-0.060851</td>\n",
              "      <td>-0.058435</td>\n",
              "      <td>-0.055658</td>\n",
              "      <td>-0.052576</td>\n",
              "      <td>-0.049378</td>\n",
              "      <td>-0.046363</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.134398</td>\n",
              "      <td>-0.135559</td>\n",
              "      <td>-0.138471</td>\n",
              "      <td>-0.142558</td>\n",
              "      <td>-0.147015</td>\n",
              "      <td>-0.150950</td>\n",
              "      <td>-0.153535</td>\n",
              "      <td>-0.154163</td>\n",
              "      <td>-0.152557</td>\n",
              "      <td>-0.148830</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5995</th>\n",
              "      <td>0.004594</td>\n",
              "      <td>0.009590</td>\n",
              "      <td>0.017135</td>\n",
              "      <td>0.025880</td>\n",
              "      <td>0.034416</td>\n",
              "      <td>0.041620</td>\n",
              "      <td>0.046867</td>\n",
              "      <td>0.050103</td>\n",
              "      <td>0.051777</td>\n",
              "      <td>0.052674</td>\n",
              "      <td>...</td>\n",
              "      <td>0.045016</td>\n",
              "      <td>0.016083</td>\n",
              "      <td>-0.009705</td>\n",
              "      <td>-0.032355</td>\n",
              "      <td>-0.052129</td>\n",
              "      <td>-0.069386</td>\n",
              "      <td>-0.084460</td>\n",
              "      <td>-0.097612</td>\n",
              "      <td>-0.109031</td>\n",
              "      <td>-0.118868</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5996</th>\n",
              "      <td>0.017135</td>\n",
              "      <td>0.017694</td>\n",
              "      <td>0.019432</td>\n",
              "      <td>0.022356</td>\n",
              "      <td>0.026302</td>\n",
              "      <td>0.030971</td>\n",
              "      <td>0.035976</td>\n",
              "      <td>0.040891</td>\n",
              "      <td>0.045291</td>\n",
              "      <td>0.048799</td>\n",
              "      <td>...</td>\n",
              "      <td>0.005152</td>\n",
              "      <td>0.044268</td>\n",
              "      <td>0.089211</td>\n",
              "      <td>0.134152</td>\n",
              "      <td>0.171846</td>\n",
              "      <td>0.194955</td>\n",
              "      <td>0.197629</td>\n",
              "      <td>0.176948</td>\n",
              "      <td>0.133840</td>\n",
              "      <td>0.073184</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5997</th>\n",
              "      <td>-0.061678</td>\n",
              "      <td>-0.060521</td>\n",
              "      <td>-0.059365</td>\n",
              "      <td>-0.058206</td>\n",
              "      <td>-0.057080</td>\n",
              "      <td>-0.056067</td>\n",
              "      <td>-0.055283</td>\n",
              "      <td>-0.054843</td>\n",
              "      <td>-0.054816</td>\n",
              "      <td>-0.055175</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.046773</td>\n",
              "      <td>-0.050592</td>\n",
              "      <td>-0.052317</td>\n",
              "      <td>-0.052018</td>\n",
              "      <td>-0.049965</td>\n",
              "      <td>-0.046587</td>\n",
              "      <td>-0.042433</td>\n",
              "      <td>-0.038139</td>\n",
              "      <td>-0.034393</td>\n",
              "      <td>-0.031913</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5998</th>\n",
              "      <td>-0.039287</td>\n",
              "      <td>-0.039374</td>\n",
              "      <td>-0.039054</td>\n",
              "      <td>-0.038510</td>\n",
              "      <td>-0.037933</td>\n",
              "      <td>-0.037494</td>\n",
              "      <td>-0.037327</td>\n",
              "      <td>-0.037532</td>\n",
              "      <td>-0.038153</td>\n",
              "      <td>-0.039167</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.058137</td>\n",
              "      <td>-0.060069</td>\n",
              "      <td>-0.061489</td>\n",
              "      <td>-0.061986</td>\n",
              "      <td>-0.061385</td>\n",
              "      <td>-0.059834</td>\n",
              "      <td>-0.057803</td>\n",
              "      <td>-0.055972</td>\n",
              "      <td>-0.055062</td>\n",
              "      <td>-0.055631</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5999</th>\n",
              "      <td>0.030423</td>\n",
              "      <td>-0.043405</td>\n",
              "      <td>-0.110988</td>\n",
              "      <td>-0.163599</td>\n",
              "      <td>-0.195505</td>\n",
              "      <td>-0.204771</td>\n",
              "      <td>-0.193219</td>\n",
              "      <td>-0.165648</td>\n",
              "      <td>-0.128563</td>\n",
              "      <td>-0.088765</td>\n",
              "      <td>...</td>\n",
              "      <td>0.125802</td>\n",
              "      <td>0.136895</td>\n",
              "      <td>0.147996</td>\n",
              "      <td>0.158825</td>\n",
              "      <td>0.169190</td>\n",
              "      <td>0.179084</td>\n",
              "      <td>0.188748</td>\n",
              "      <td>0.198658</td>\n",
              "      <td>0.209439</td>\n",
              "      <td>0.221705</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6000 rows × 256 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-758fe6fd-30b8-4756-a7d0-08b3445028d5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-758fe6fd-30b8-4756-a7d0-08b3445028d5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-758fe6fd-30b8-4756-a7d0-08b3445028d5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CiFAK_crtJzg",
        "outputId": "638f2750-132e-4829-c132-4b3a76869ec8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(24000, 256)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 라벨생성"
      ],
      "metadata": {
        "id": "jaOKRwI5j8AT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "arr = np.array([])\n",
        "\n",
        "for i in range(1,101):\n",
        "  for j in range(240):\n",
        "    arr = np.append(arr, np.array([i]))"
      ],
      "metadata": {
        "id": "fGX-38SD_5Ec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train[256]=arr"
      ],
      "metadata": {
        "id": "j9sXeCZSMUpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "a2aUe3Cpex1M",
        "outputId": "f55028db-471f-4e3b-a102-0b40e886d652"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            0         1         2         3         4         5         6    \\\n",
              "0     -0.031820 -0.015874 -0.001172  0.011249  0.020713  0.026974  0.030190   \n",
              "1     -0.019872 -0.020594 -0.021637 -0.022515 -0.022798 -0.022202 -0.020644   \n",
              "2     -0.057454 -0.056822 -0.055984 -0.054985 -0.053945 -0.052999 -0.052250   \n",
              "3     -0.056099 -0.057035 -0.058395 -0.060043 -0.061694 -0.062967 -0.063465   \n",
              "4      0.053295  0.053439  0.055066  0.059409  0.067278  0.078961  0.094216   \n",
              "...         ...       ...       ...       ...       ...       ...       ...   \n",
              "23995  0.043512  0.020932  0.001382 -0.014947 -0.027957 -0.037670 -0.044270   \n",
              "23996 -0.076867 -0.083002 -0.088128 -0.091454 -0.092582 -0.091623 -0.089189   \n",
              "23997  0.022090  0.024929  0.027786  0.031100  0.035075  0.039622  0.044402   \n",
              "23998 -0.079288 -0.083358 -0.086464 -0.087995 -0.087513 -0.084880 -0.080336   \n",
              "23999 -0.076782 -0.072269 -0.066348 -0.059776 -0.053461 -0.048271 -0.044867   \n",
              "\n",
              "            7         8         9    ...       247       248       249  \\\n",
              "0      0.030831  0.029559  0.027102  ... -0.035003 -0.034647 -0.033418   \n",
              "1     -0.018236 -0.015233 -0.011971  ... -0.072854 -0.070401 -0.067539   \n",
              "2     -0.051733 -0.051414 -0.051213  ... -0.059447 -0.057278 -0.056011   \n",
              "3     -0.062887 -0.061116 -0.058272  ... -0.071806 -0.071741 -0.071302   \n",
              "4      0.112361  0.132422  0.153321  ...  0.150072  0.164082  0.180535   \n",
              "...         ...       ...       ...  ...       ...       ...       ...   \n",
              "23995 -0.048123 -0.049775 -0.049906  ... -0.020100 -0.018824 -0.018132   \n",
              "23996 -0.086281 -0.084051 -0.083490  ... -0.019352 -0.019054 -0.016308   \n",
              "23997  0.048933  0.052740  0.055510  ... -0.053679 -0.057318 -0.059515   \n",
              "23998 -0.074488 -0.068211 -0.062467  ... -0.066882 -0.067113 -0.068424   \n",
              "23999 -0.043568 -0.044302 -0.046625  ... -0.048461 -0.044455 -0.041670   \n",
              "\n",
              "            250       251       252       253       254       255    256  \n",
              "0     -0.031305 -0.028545 -0.025565 -0.022868 -0.020891 -0.019891    1.0  \n",
              "1     -0.064716 -0.062285 -0.060443 -0.059208 -0.058448 -0.057940    1.0  \n",
              "2     -0.055424 -0.055246 -0.055239 -0.055275 -0.055352 -0.055577    1.0  \n",
              "3     -0.070367 -0.068861 -0.066758 -0.064085 -0.060936 -0.057487    1.0  \n",
              "4      0.199168  0.219491  0.240843  0.262473  0.283635  0.303673    1.0  \n",
              "...         ...       ...       ...       ...       ...       ...    ...  \n",
              "23995 -0.018247 -0.019303 -0.021303 -0.024082 -0.027314 -0.030559  100.0  \n",
              "23996 -0.011295 -0.004783  0.002224  0.008807  0.014368  0.018726  100.0  \n",
              "23997 -0.061043 -0.062554 -0.064528 -0.067243 -0.070758 -0.074902  100.0  \n",
              "23998 -0.070704 -0.073600 -0.076563 -0.078930 -0.080050 -0.079413  100.0  \n",
              "23999 -0.040185 -0.039799 -0.040120 -0.040694 -0.041169 -0.041409  100.0  \n",
              "\n",
              "[24000 rows x 257 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e203f5f5-1253-49c8-bb70-6a5e8af7685c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>247</th>\n",
              "      <th>248</th>\n",
              "      <th>249</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "      <th>256</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.031820</td>\n",
              "      <td>-0.015874</td>\n",
              "      <td>-0.001172</td>\n",
              "      <td>0.011249</td>\n",
              "      <td>0.020713</td>\n",
              "      <td>0.026974</td>\n",
              "      <td>0.030190</td>\n",
              "      <td>0.030831</td>\n",
              "      <td>0.029559</td>\n",
              "      <td>0.027102</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.035003</td>\n",
              "      <td>-0.034647</td>\n",
              "      <td>-0.033418</td>\n",
              "      <td>-0.031305</td>\n",
              "      <td>-0.028545</td>\n",
              "      <td>-0.025565</td>\n",
              "      <td>-0.022868</td>\n",
              "      <td>-0.020891</td>\n",
              "      <td>-0.019891</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.019872</td>\n",
              "      <td>-0.020594</td>\n",
              "      <td>-0.021637</td>\n",
              "      <td>-0.022515</td>\n",
              "      <td>-0.022798</td>\n",
              "      <td>-0.022202</td>\n",
              "      <td>-0.020644</td>\n",
              "      <td>-0.018236</td>\n",
              "      <td>-0.015233</td>\n",
              "      <td>-0.011971</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.072854</td>\n",
              "      <td>-0.070401</td>\n",
              "      <td>-0.067539</td>\n",
              "      <td>-0.064716</td>\n",
              "      <td>-0.062285</td>\n",
              "      <td>-0.060443</td>\n",
              "      <td>-0.059208</td>\n",
              "      <td>-0.058448</td>\n",
              "      <td>-0.057940</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.057454</td>\n",
              "      <td>-0.056822</td>\n",
              "      <td>-0.055984</td>\n",
              "      <td>-0.054985</td>\n",
              "      <td>-0.053945</td>\n",
              "      <td>-0.052999</td>\n",
              "      <td>-0.052250</td>\n",
              "      <td>-0.051733</td>\n",
              "      <td>-0.051414</td>\n",
              "      <td>-0.051213</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.059447</td>\n",
              "      <td>-0.057278</td>\n",
              "      <td>-0.056011</td>\n",
              "      <td>-0.055424</td>\n",
              "      <td>-0.055246</td>\n",
              "      <td>-0.055239</td>\n",
              "      <td>-0.055275</td>\n",
              "      <td>-0.055352</td>\n",
              "      <td>-0.055577</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.056099</td>\n",
              "      <td>-0.057035</td>\n",
              "      <td>-0.058395</td>\n",
              "      <td>-0.060043</td>\n",
              "      <td>-0.061694</td>\n",
              "      <td>-0.062967</td>\n",
              "      <td>-0.063465</td>\n",
              "      <td>-0.062887</td>\n",
              "      <td>-0.061116</td>\n",
              "      <td>-0.058272</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.071806</td>\n",
              "      <td>-0.071741</td>\n",
              "      <td>-0.071302</td>\n",
              "      <td>-0.070367</td>\n",
              "      <td>-0.068861</td>\n",
              "      <td>-0.066758</td>\n",
              "      <td>-0.064085</td>\n",
              "      <td>-0.060936</td>\n",
              "      <td>-0.057487</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.053295</td>\n",
              "      <td>0.053439</td>\n",
              "      <td>0.055066</td>\n",
              "      <td>0.059409</td>\n",
              "      <td>0.067278</td>\n",
              "      <td>0.078961</td>\n",
              "      <td>0.094216</td>\n",
              "      <td>0.112361</td>\n",
              "      <td>0.132422</td>\n",
              "      <td>0.153321</td>\n",
              "      <td>...</td>\n",
              "      <td>0.150072</td>\n",
              "      <td>0.164082</td>\n",
              "      <td>0.180535</td>\n",
              "      <td>0.199168</td>\n",
              "      <td>0.219491</td>\n",
              "      <td>0.240843</td>\n",
              "      <td>0.262473</td>\n",
              "      <td>0.283635</td>\n",
              "      <td>0.303673</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23995</th>\n",
              "      <td>0.043512</td>\n",
              "      <td>0.020932</td>\n",
              "      <td>0.001382</td>\n",
              "      <td>-0.014947</td>\n",
              "      <td>-0.027957</td>\n",
              "      <td>-0.037670</td>\n",
              "      <td>-0.044270</td>\n",
              "      <td>-0.048123</td>\n",
              "      <td>-0.049775</td>\n",
              "      <td>-0.049906</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.020100</td>\n",
              "      <td>-0.018824</td>\n",
              "      <td>-0.018132</td>\n",
              "      <td>-0.018247</td>\n",
              "      <td>-0.019303</td>\n",
              "      <td>-0.021303</td>\n",
              "      <td>-0.024082</td>\n",
              "      <td>-0.027314</td>\n",
              "      <td>-0.030559</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23996</th>\n",
              "      <td>-0.076867</td>\n",
              "      <td>-0.083002</td>\n",
              "      <td>-0.088128</td>\n",
              "      <td>-0.091454</td>\n",
              "      <td>-0.092582</td>\n",
              "      <td>-0.091623</td>\n",
              "      <td>-0.089189</td>\n",
              "      <td>-0.086281</td>\n",
              "      <td>-0.084051</td>\n",
              "      <td>-0.083490</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.019352</td>\n",
              "      <td>-0.019054</td>\n",
              "      <td>-0.016308</td>\n",
              "      <td>-0.011295</td>\n",
              "      <td>-0.004783</td>\n",
              "      <td>0.002224</td>\n",
              "      <td>0.008807</td>\n",
              "      <td>0.014368</td>\n",
              "      <td>0.018726</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23997</th>\n",
              "      <td>0.022090</td>\n",
              "      <td>0.024929</td>\n",
              "      <td>0.027786</td>\n",
              "      <td>0.031100</td>\n",
              "      <td>0.035075</td>\n",
              "      <td>0.039622</td>\n",
              "      <td>0.044402</td>\n",
              "      <td>0.048933</td>\n",
              "      <td>0.052740</td>\n",
              "      <td>0.055510</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.053679</td>\n",
              "      <td>-0.057318</td>\n",
              "      <td>-0.059515</td>\n",
              "      <td>-0.061043</td>\n",
              "      <td>-0.062554</td>\n",
              "      <td>-0.064528</td>\n",
              "      <td>-0.067243</td>\n",
              "      <td>-0.070758</td>\n",
              "      <td>-0.074902</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23998</th>\n",
              "      <td>-0.079288</td>\n",
              "      <td>-0.083358</td>\n",
              "      <td>-0.086464</td>\n",
              "      <td>-0.087995</td>\n",
              "      <td>-0.087513</td>\n",
              "      <td>-0.084880</td>\n",
              "      <td>-0.080336</td>\n",
              "      <td>-0.074488</td>\n",
              "      <td>-0.068211</td>\n",
              "      <td>-0.062467</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.066882</td>\n",
              "      <td>-0.067113</td>\n",
              "      <td>-0.068424</td>\n",
              "      <td>-0.070704</td>\n",
              "      <td>-0.073600</td>\n",
              "      <td>-0.076563</td>\n",
              "      <td>-0.078930</td>\n",
              "      <td>-0.080050</td>\n",
              "      <td>-0.079413</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23999</th>\n",
              "      <td>-0.076782</td>\n",
              "      <td>-0.072269</td>\n",
              "      <td>-0.066348</td>\n",
              "      <td>-0.059776</td>\n",
              "      <td>-0.053461</td>\n",
              "      <td>-0.048271</td>\n",
              "      <td>-0.044867</td>\n",
              "      <td>-0.043568</td>\n",
              "      <td>-0.044302</td>\n",
              "      <td>-0.046625</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.048461</td>\n",
              "      <td>-0.044455</td>\n",
              "      <td>-0.041670</td>\n",
              "      <td>-0.040185</td>\n",
              "      <td>-0.039799</td>\n",
              "      <td>-0.040120</td>\n",
              "      <td>-0.040694</td>\n",
              "      <td>-0.041169</td>\n",
              "      <td>-0.041409</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>24000 rows × 257 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e203f5f5-1253-49c8-bb70-6a5e8af7685c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e203f5f5-1253-49c8-bb70-6a5e8af7685c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e203f5f5-1253-49c8-bb70-6a5e8af7685c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "arr_t = np.array([])\n",
        "\n",
        "for i in range(1,101):\n",
        "  for j in range(60):\n",
        "    arr_t = np.append(arr_t, np.array([i]))"
      ],
      "metadata": {
        "id": "flq_Q9uOe5fd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arr_v = np.array([])\n",
        "\n",
        "for i in range(1,101):\n",
        "  for j in range(60):\n",
        "    arr_v = np.append(arr_v, np.array([i]))"
      ],
      "metadata": {
        "id": "ddgT-ntvfAbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_test[256]=arr_t\n",
        "data_val[256]=arr_v\n",
        "data_val"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "daFrtTuHfD96",
        "outputId": "6843964c-ab16-4fb6-e479-0d0639b4d757"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           0         1         2         3         4         5         6    \\\n",
              "0    -0.054004 -0.050825 -0.048322 -0.046824 -0.046532 -0.047445 -0.049330   \n",
              "1    -0.209505 -0.208403 -0.206688 -0.204586 -0.202338 -0.200196 -0.198379   \n",
              "2    -0.067735 -0.065806 -0.063513 -0.060990 -0.058453 -0.056174 -0.054439   \n",
              "3    -0.002916 -0.007004 -0.012949 -0.020332 -0.028344 -0.035829 -0.041422   \n",
              "4    -0.028844 -0.028027 -0.028309 -0.029364 -0.030767 -0.032072 -0.032878   \n",
              "...        ...       ...       ...       ...       ...       ...       ...   \n",
              "5995  0.015685  0.016545  0.014765  0.010665  0.004884 -0.001771 -0.008494   \n",
              "5996 -0.171624 -0.175496 -0.177258 -0.176626 -0.173694 -0.168915 -0.163017   \n",
              "5997  0.000168 -0.001110 -0.003542 -0.006028 -0.007548 -0.007396 -0.005302   \n",
              "5998 -0.033347 -0.035287 -0.036169 -0.036014 -0.035070 -0.033744 -0.032503   \n",
              "5999 -0.041533 -0.041870 -0.042836 -0.044782 -0.047871 -0.052004 -0.056828   \n",
              "\n",
              "           7         8         9    ...       247       248       249  \\\n",
              "0    -0.051735 -0.054058 -0.055645  ... -0.028530 -0.029492 -0.029945   \n",
              "1    -0.197017 -0.196097 -0.195447  ... -0.205948 -0.202740 -0.198907   \n",
              "2    -0.053477 -0.053402 -0.054158  ... -0.093394 -0.093535 -0.092581   \n",
              "3    -0.043775 -0.041865 -0.035337  ... -0.097242 -0.124400 -0.137850   \n",
              "4    -0.032899 -0.032005 -0.030247  ... -0.074225 -0.075851 -0.077197   \n",
              "...        ...       ...       ...  ...       ...       ...       ...   \n",
              "5995 -0.014652 -0.019878 -0.024092  ... -0.052522 -0.020671  0.001335   \n",
              "5996 -0.156862 -0.151287 -0.146943  ...  0.013368  0.016800  0.019352   \n",
              "5997 -0.001433  0.003712  0.009473  ... -0.072272 -0.071268 -0.070220   \n",
              "5998 -0.031760 -0.031802 -0.032746  ... -0.036757 -0.033683 -0.032034   \n",
              "5999 -0.061806 -0.066332 -0.069845  ...  0.007247  0.056263  0.107776   \n",
              "\n",
              "           250       251       252       253       254       255    256  \n",
              "0    -0.029648 -0.028487 -0.026485 -0.023782 -0.020603 -0.017228    1.0  \n",
              "1    -0.194673 -0.190228 -0.185713 -0.181227 -0.176850 -0.172660    1.0  \n",
              "2    -0.090373 -0.086983 -0.082689 -0.077914 -0.073148 -0.068878    1.0  \n",
              "3    -0.128147 -0.087559 -0.012153  0.096530  0.230711  0.376848    1.0  \n",
              "4    -0.077950 -0.077920 -0.077074 -0.075531 -0.073514 -0.071283    1.0  \n",
              "...        ...       ...       ...       ...       ...       ...    ...  \n",
              "5995  0.013226  0.016606  0.014270  0.009428  0.004988  0.003041  100.0  \n",
              "5996  0.020889  0.021392  0.020985  0.019940  0.018650  0.017569  100.0  \n",
              "5997 -0.069094 -0.067885 -0.066618 -0.065335 -0.064074 -0.062856  100.0  \n",
              "5998 -0.031757 -0.032584 -0.034107 -0.035874 -0.037486 -0.038665  100.0  \n",
              "5999  0.154395  0.188125  0.201986  0.191623  0.156499  0.100301  100.0  \n",
              "\n",
              "[6000 rows x 257 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-aea00b35-653a-4d36-8899-064b57bc2291\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>247</th>\n",
              "      <th>248</th>\n",
              "      <th>249</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "      <th>256</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.054004</td>\n",
              "      <td>-0.050825</td>\n",
              "      <td>-0.048322</td>\n",
              "      <td>-0.046824</td>\n",
              "      <td>-0.046532</td>\n",
              "      <td>-0.047445</td>\n",
              "      <td>-0.049330</td>\n",
              "      <td>-0.051735</td>\n",
              "      <td>-0.054058</td>\n",
              "      <td>-0.055645</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.028530</td>\n",
              "      <td>-0.029492</td>\n",
              "      <td>-0.029945</td>\n",
              "      <td>-0.029648</td>\n",
              "      <td>-0.028487</td>\n",
              "      <td>-0.026485</td>\n",
              "      <td>-0.023782</td>\n",
              "      <td>-0.020603</td>\n",
              "      <td>-0.017228</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.209505</td>\n",
              "      <td>-0.208403</td>\n",
              "      <td>-0.206688</td>\n",
              "      <td>-0.204586</td>\n",
              "      <td>-0.202338</td>\n",
              "      <td>-0.200196</td>\n",
              "      <td>-0.198379</td>\n",
              "      <td>-0.197017</td>\n",
              "      <td>-0.196097</td>\n",
              "      <td>-0.195447</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.205948</td>\n",
              "      <td>-0.202740</td>\n",
              "      <td>-0.198907</td>\n",
              "      <td>-0.194673</td>\n",
              "      <td>-0.190228</td>\n",
              "      <td>-0.185713</td>\n",
              "      <td>-0.181227</td>\n",
              "      <td>-0.176850</td>\n",
              "      <td>-0.172660</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.067735</td>\n",
              "      <td>-0.065806</td>\n",
              "      <td>-0.063513</td>\n",
              "      <td>-0.060990</td>\n",
              "      <td>-0.058453</td>\n",
              "      <td>-0.056174</td>\n",
              "      <td>-0.054439</td>\n",
              "      <td>-0.053477</td>\n",
              "      <td>-0.053402</td>\n",
              "      <td>-0.054158</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.093394</td>\n",
              "      <td>-0.093535</td>\n",
              "      <td>-0.092581</td>\n",
              "      <td>-0.090373</td>\n",
              "      <td>-0.086983</td>\n",
              "      <td>-0.082689</td>\n",
              "      <td>-0.077914</td>\n",
              "      <td>-0.073148</td>\n",
              "      <td>-0.068878</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.002916</td>\n",
              "      <td>-0.007004</td>\n",
              "      <td>-0.012949</td>\n",
              "      <td>-0.020332</td>\n",
              "      <td>-0.028344</td>\n",
              "      <td>-0.035829</td>\n",
              "      <td>-0.041422</td>\n",
              "      <td>-0.043775</td>\n",
              "      <td>-0.041865</td>\n",
              "      <td>-0.035337</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.097242</td>\n",
              "      <td>-0.124400</td>\n",
              "      <td>-0.137850</td>\n",
              "      <td>-0.128147</td>\n",
              "      <td>-0.087559</td>\n",
              "      <td>-0.012153</td>\n",
              "      <td>0.096530</td>\n",
              "      <td>0.230711</td>\n",
              "      <td>0.376848</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.028844</td>\n",
              "      <td>-0.028027</td>\n",
              "      <td>-0.028309</td>\n",
              "      <td>-0.029364</td>\n",
              "      <td>-0.030767</td>\n",
              "      <td>-0.032072</td>\n",
              "      <td>-0.032878</td>\n",
              "      <td>-0.032899</td>\n",
              "      <td>-0.032005</td>\n",
              "      <td>-0.030247</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.074225</td>\n",
              "      <td>-0.075851</td>\n",
              "      <td>-0.077197</td>\n",
              "      <td>-0.077950</td>\n",
              "      <td>-0.077920</td>\n",
              "      <td>-0.077074</td>\n",
              "      <td>-0.075531</td>\n",
              "      <td>-0.073514</td>\n",
              "      <td>-0.071283</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5995</th>\n",
              "      <td>0.015685</td>\n",
              "      <td>0.016545</td>\n",
              "      <td>0.014765</td>\n",
              "      <td>0.010665</td>\n",
              "      <td>0.004884</td>\n",
              "      <td>-0.001771</td>\n",
              "      <td>-0.008494</td>\n",
              "      <td>-0.014652</td>\n",
              "      <td>-0.019878</td>\n",
              "      <td>-0.024092</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.052522</td>\n",
              "      <td>-0.020671</td>\n",
              "      <td>0.001335</td>\n",
              "      <td>0.013226</td>\n",
              "      <td>0.016606</td>\n",
              "      <td>0.014270</td>\n",
              "      <td>0.009428</td>\n",
              "      <td>0.004988</td>\n",
              "      <td>0.003041</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5996</th>\n",
              "      <td>-0.171624</td>\n",
              "      <td>-0.175496</td>\n",
              "      <td>-0.177258</td>\n",
              "      <td>-0.176626</td>\n",
              "      <td>-0.173694</td>\n",
              "      <td>-0.168915</td>\n",
              "      <td>-0.163017</td>\n",
              "      <td>-0.156862</td>\n",
              "      <td>-0.151287</td>\n",
              "      <td>-0.146943</td>\n",
              "      <td>...</td>\n",
              "      <td>0.013368</td>\n",
              "      <td>0.016800</td>\n",
              "      <td>0.019352</td>\n",
              "      <td>0.020889</td>\n",
              "      <td>0.021392</td>\n",
              "      <td>0.020985</td>\n",
              "      <td>0.019940</td>\n",
              "      <td>0.018650</td>\n",
              "      <td>0.017569</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5997</th>\n",
              "      <td>0.000168</td>\n",
              "      <td>-0.001110</td>\n",
              "      <td>-0.003542</td>\n",
              "      <td>-0.006028</td>\n",
              "      <td>-0.007548</td>\n",
              "      <td>-0.007396</td>\n",
              "      <td>-0.005302</td>\n",
              "      <td>-0.001433</td>\n",
              "      <td>0.003712</td>\n",
              "      <td>0.009473</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.072272</td>\n",
              "      <td>-0.071268</td>\n",
              "      <td>-0.070220</td>\n",
              "      <td>-0.069094</td>\n",
              "      <td>-0.067885</td>\n",
              "      <td>-0.066618</td>\n",
              "      <td>-0.065335</td>\n",
              "      <td>-0.064074</td>\n",
              "      <td>-0.062856</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5998</th>\n",
              "      <td>-0.033347</td>\n",
              "      <td>-0.035287</td>\n",
              "      <td>-0.036169</td>\n",
              "      <td>-0.036014</td>\n",
              "      <td>-0.035070</td>\n",
              "      <td>-0.033744</td>\n",
              "      <td>-0.032503</td>\n",
              "      <td>-0.031760</td>\n",
              "      <td>-0.031802</td>\n",
              "      <td>-0.032746</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.036757</td>\n",
              "      <td>-0.033683</td>\n",
              "      <td>-0.032034</td>\n",
              "      <td>-0.031757</td>\n",
              "      <td>-0.032584</td>\n",
              "      <td>-0.034107</td>\n",
              "      <td>-0.035874</td>\n",
              "      <td>-0.037486</td>\n",
              "      <td>-0.038665</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5999</th>\n",
              "      <td>-0.041533</td>\n",
              "      <td>-0.041870</td>\n",
              "      <td>-0.042836</td>\n",
              "      <td>-0.044782</td>\n",
              "      <td>-0.047871</td>\n",
              "      <td>-0.052004</td>\n",
              "      <td>-0.056828</td>\n",
              "      <td>-0.061806</td>\n",
              "      <td>-0.066332</td>\n",
              "      <td>-0.069845</td>\n",
              "      <td>...</td>\n",
              "      <td>0.007247</td>\n",
              "      <td>0.056263</td>\n",
              "      <td>0.107776</td>\n",
              "      <td>0.154395</td>\n",
              "      <td>0.188125</td>\n",
              "      <td>0.201986</td>\n",
              "      <td>0.191623</td>\n",
              "      <td>0.156499</td>\n",
              "      <td>0.100301</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6000 rows × 257 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-aea00b35-653a-4d36-8899-064b57bc2291')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-aea00b35-653a-4d36-8899-064b57bc2291 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-aea00b35-653a-4d36-8899-064b57bc2291');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# y_train = arr\n",
        "# y_test = arr_t\n",
        "# y_val = arr_v"
      ],
      "metadata": {
        "id": "AtpvllHJRIr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터 전체 섞기"
      ],
      "metadata": {
        "id": "I7BNW6vLSn40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_total=pd.concat([data_train, data_test, data_val])\n",
        "data_total"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "Tb9_tMy0SUkK",
        "outputId": "5c1f52da-6fc3-4d28-d571-410931b386f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           0         1         2         3         4         5         6    \\\n",
              "0    -0.031820 -0.015874 -0.001172  0.011249  0.020713  0.026974  0.030190   \n",
              "1    -0.019872 -0.020594 -0.021637 -0.022515 -0.022798 -0.022202 -0.020644   \n",
              "2    -0.057454 -0.056822 -0.055984 -0.054985 -0.053945 -0.052999 -0.052250   \n",
              "3    -0.056099 -0.057035 -0.058395 -0.060043 -0.061694 -0.062967 -0.063465   \n",
              "4     0.053295  0.053439  0.055066  0.059409  0.067278  0.078961  0.094216   \n",
              "...        ...       ...       ...       ...       ...       ...       ...   \n",
              "5995  0.015685  0.016545  0.014765  0.010665  0.004884 -0.001771 -0.008494   \n",
              "5996 -0.171624 -0.175496 -0.177258 -0.176626 -0.173694 -0.168915 -0.163017   \n",
              "5997  0.000168 -0.001110 -0.003542 -0.006028 -0.007548 -0.007396 -0.005302   \n",
              "5998 -0.033347 -0.035287 -0.036169 -0.036014 -0.035070 -0.033744 -0.032503   \n",
              "5999 -0.041533 -0.041870 -0.042836 -0.044782 -0.047871 -0.052004 -0.056828   \n",
              "\n",
              "           7         8         9    ...       247       248       249  \\\n",
              "0     0.030831  0.029559  0.027102  ... -0.035003 -0.034647 -0.033418   \n",
              "1    -0.018236 -0.015233 -0.011971  ... -0.072854 -0.070401 -0.067539   \n",
              "2    -0.051733 -0.051414 -0.051213  ... -0.059447 -0.057278 -0.056011   \n",
              "3    -0.062887 -0.061116 -0.058272  ... -0.071806 -0.071741 -0.071302   \n",
              "4     0.112361  0.132422  0.153321  ...  0.150072  0.164082  0.180535   \n",
              "...        ...       ...       ...  ...       ...       ...       ...   \n",
              "5995 -0.014652 -0.019878 -0.024092  ... -0.052522 -0.020671  0.001335   \n",
              "5996 -0.156862 -0.151287 -0.146943  ...  0.013368  0.016800  0.019352   \n",
              "5997 -0.001433  0.003712  0.009473  ... -0.072272 -0.071268 -0.070220   \n",
              "5998 -0.031760 -0.031802 -0.032746  ... -0.036757 -0.033683 -0.032034   \n",
              "5999 -0.061806 -0.066332 -0.069845  ...  0.007247  0.056263  0.107776   \n",
              "\n",
              "           250       251       252       253       254       255    256  \n",
              "0    -0.031305 -0.028545 -0.025565 -0.022868 -0.020891 -0.019891    1.0  \n",
              "1    -0.064716 -0.062285 -0.060443 -0.059208 -0.058448 -0.057940    1.0  \n",
              "2    -0.055424 -0.055246 -0.055239 -0.055275 -0.055352 -0.055577    1.0  \n",
              "3    -0.070367 -0.068861 -0.066758 -0.064085 -0.060936 -0.057487    1.0  \n",
              "4     0.199168  0.219491  0.240843  0.262473  0.283635  0.303673    1.0  \n",
              "...        ...       ...       ...       ...       ...       ...    ...  \n",
              "5995  0.013226  0.016606  0.014270  0.009428  0.004988  0.003041  100.0  \n",
              "5996  0.020889  0.021392  0.020985  0.019940  0.018650  0.017569  100.0  \n",
              "5997 -0.069094 -0.067885 -0.066618 -0.065335 -0.064074 -0.062856  100.0  \n",
              "5998 -0.031757 -0.032584 -0.034107 -0.035874 -0.037486 -0.038665  100.0  \n",
              "5999  0.154395  0.188125  0.201986  0.191623  0.156499  0.100301  100.0  \n",
              "\n",
              "[36000 rows x 257 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-03ff9594-4042-4ba3-918d-6d968dddac33\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>247</th>\n",
              "      <th>248</th>\n",
              "      <th>249</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "      <th>256</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.031820</td>\n",
              "      <td>-0.015874</td>\n",
              "      <td>-0.001172</td>\n",
              "      <td>0.011249</td>\n",
              "      <td>0.020713</td>\n",
              "      <td>0.026974</td>\n",
              "      <td>0.030190</td>\n",
              "      <td>0.030831</td>\n",
              "      <td>0.029559</td>\n",
              "      <td>0.027102</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.035003</td>\n",
              "      <td>-0.034647</td>\n",
              "      <td>-0.033418</td>\n",
              "      <td>-0.031305</td>\n",
              "      <td>-0.028545</td>\n",
              "      <td>-0.025565</td>\n",
              "      <td>-0.022868</td>\n",
              "      <td>-0.020891</td>\n",
              "      <td>-0.019891</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.019872</td>\n",
              "      <td>-0.020594</td>\n",
              "      <td>-0.021637</td>\n",
              "      <td>-0.022515</td>\n",
              "      <td>-0.022798</td>\n",
              "      <td>-0.022202</td>\n",
              "      <td>-0.020644</td>\n",
              "      <td>-0.018236</td>\n",
              "      <td>-0.015233</td>\n",
              "      <td>-0.011971</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.072854</td>\n",
              "      <td>-0.070401</td>\n",
              "      <td>-0.067539</td>\n",
              "      <td>-0.064716</td>\n",
              "      <td>-0.062285</td>\n",
              "      <td>-0.060443</td>\n",
              "      <td>-0.059208</td>\n",
              "      <td>-0.058448</td>\n",
              "      <td>-0.057940</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.057454</td>\n",
              "      <td>-0.056822</td>\n",
              "      <td>-0.055984</td>\n",
              "      <td>-0.054985</td>\n",
              "      <td>-0.053945</td>\n",
              "      <td>-0.052999</td>\n",
              "      <td>-0.052250</td>\n",
              "      <td>-0.051733</td>\n",
              "      <td>-0.051414</td>\n",
              "      <td>-0.051213</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.059447</td>\n",
              "      <td>-0.057278</td>\n",
              "      <td>-0.056011</td>\n",
              "      <td>-0.055424</td>\n",
              "      <td>-0.055246</td>\n",
              "      <td>-0.055239</td>\n",
              "      <td>-0.055275</td>\n",
              "      <td>-0.055352</td>\n",
              "      <td>-0.055577</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.056099</td>\n",
              "      <td>-0.057035</td>\n",
              "      <td>-0.058395</td>\n",
              "      <td>-0.060043</td>\n",
              "      <td>-0.061694</td>\n",
              "      <td>-0.062967</td>\n",
              "      <td>-0.063465</td>\n",
              "      <td>-0.062887</td>\n",
              "      <td>-0.061116</td>\n",
              "      <td>-0.058272</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.071806</td>\n",
              "      <td>-0.071741</td>\n",
              "      <td>-0.071302</td>\n",
              "      <td>-0.070367</td>\n",
              "      <td>-0.068861</td>\n",
              "      <td>-0.066758</td>\n",
              "      <td>-0.064085</td>\n",
              "      <td>-0.060936</td>\n",
              "      <td>-0.057487</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.053295</td>\n",
              "      <td>0.053439</td>\n",
              "      <td>0.055066</td>\n",
              "      <td>0.059409</td>\n",
              "      <td>0.067278</td>\n",
              "      <td>0.078961</td>\n",
              "      <td>0.094216</td>\n",
              "      <td>0.112361</td>\n",
              "      <td>0.132422</td>\n",
              "      <td>0.153321</td>\n",
              "      <td>...</td>\n",
              "      <td>0.150072</td>\n",
              "      <td>0.164082</td>\n",
              "      <td>0.180535</td>\n",
              "      <td>0.199168</td>\n",
              "      <td>0.219491</td>\n",
              "      <td>0.240843</td>\n",
              "      <td>0.262473</td>\n",
              "      <td>0.283635</td>\n",
              "      <td>0.303673</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5995</th>\n",
              "      <td>0.015685</td>\n",
              "      <td>0.016545</td>\n",
              "      <td>0.014765</td>\n",
              "      <td>0.010665</td>\n",
              "      <td>0.004884</td>\n",
              "      <td>-0.001771</td>\n",
              "      <td>-0.008494</td>\n",
              "      <td>-0.014652</td>\n",
              "      <td>-0.019878</td>\n",
              "      <td>-0.024092</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.052522</td>\n",
              "      <td>-0.020671</td>\n",
              "      <td>0.001335</td>\n",
              "      <td>0.013226</td>\n",
              "      <td>0.016606</td>\n",
              "      <td>0.014270</td>\n",
              "      <td>0.009428</td>\n",
              "      <td>0.004988</td>\n",
              "      <td>0.003041</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5996</th>\n",
              "      <td>-0.171624</td>\n",
              "      <td>-0.175496</td>\n",
              "      <td>-0.177258</td>\n",
              "      <td>-0.176626</td>\n",
              "      <td>-0.173694</td>\n",
              "      <td>-0.168915</td>\n",
              "      <td>-0.163017</td>\n",
              "      <td>-0.156862</td>\n",
              "      <td>-0.151287</td>\n",
              "      <td>-0.146943</td>\n",
              "      <td>...</td>\n",
              "      <td>0.013368</td>\n",
              "      <td>0.016800</td>\n",
              "      <td>0.019352</td>\n",
              "      <td>0.020889</td>\n",
              "      <td>0.021392</td>\n",
              "      <td>0.020985</td>\n",
              "      <td>0.019940</td>\n",
              "      <td>0.018650</td>\n",
              "      <td>0.017569</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5997</th>\n",
              "      <td>0.000168</td>\n",
              "      <td>-0.001110</td>\n",
              "      <td>-0.003542</td>\n",
              "      <td>-0.006028</td>\n",
              "      <td>-0.007548</td>\n",
              "      <td>-0.007396</td>\n",
              "      <td>-0.005302</td>\n",
              "      <td>-0.001433</td>\n",
              "      <td>0.003712</td>\n",
              "      <td>0.009473</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.072272</td>\n",
              "      <td>-0.071268</td>\n",
              "      <td>-0.070220</td>\n",
              "      <td>-0.069094</td>\n",
              "      <td>-0.067885</td>\n",
              "      <td>-0.066618</td>\n",
              "      <td>-0.065335</td>\n",
              "      <td>-0.064074</td>\n",
              "      <td>-0.062856</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5998</th>\n",
              "      <td>-0.033347</td>\n",
              "      <td>-0.035287</td>\n",
              "      <td>-0.036169</td>\n",
              "      <td>-0.036014</td>\n",
              "      <td>-0.035070</td>\n",
              "      <td>-0.033744</td>\n",
              "      <td>-0.032503</td>\n",
              "      <td>-0.031760</td>\n",
              "      <td>-0.031802</td>\n",
              "      <td>-0.032746</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.036757</td>\n",
              "      <td>-0.033683</td>\n",
              "      <td>-0.032034</td>\n",
              "      <td>-0.031757</td>\n",
              "      <td>-0.032584</td>\n",
              "      <td>-0.034107</td>\n",
              "      <td>-0.035874</td>\n",
              "      <td>-0.037486</td>\n",
              "      <td>-0.038665</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5999</th>\n",
              "      <td>-0.041533</td>\n",
              "      <td>-0.041870</td>\n",
              "      <td>-0.042836</td>\n",
              "      <td>-0.044782</td>\n",
              "      <td>-0.047871</td>\n",
              "      <td>-0.052004</td>\n",
              "      <td>-0.056828</td>\n",
              "      <td>-0.061806</td>\n",
              "      <td>-0.066332</td>\n",
              "      <td>-0.069845</td>\n",
              "      <td>...</td>\n",
              "      <td>0.007247</td>\n",
              "      <td>0.056263</td>\n",
              "      <td>0.107776</td>\n",
              "      <td>0.154395</td>\n",
              "      <td>0.188125</td>\n",
              "      <td>0.201986</td>\n",
              "      <td>0.191623</td>\n",
              "      <td>0.156499</td>\n",
              "      <td>0.100301</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>36000 rows × 257 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-03ff9594-4042-4ba3-918d-6d968dddac33')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-03ff9594-4042-4ba3-918d-6d968dddac33 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-03ff9594-4042-4ba3-918d-6d968dddac33');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_total=data_total.sample(frac=1)"
      ],
      "metadata": {
        "id": "zuAljzhlSxO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train=data_total.iloc[0:24000, :]\n",
        "data_val=data_total.iloc[24000:30000, :]\n",
        "data_test=data_total.iloc[30000:36000,:]"
      ],
      "metadata": {
        "id": "dAO2XTRKS938"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "R03ForgAUEFw",
        "outputId": "2abdc97d-0945-456d-90e4-4ec674f51cb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            0         1         2         3         4         5         6    \\\n",
              "2931   0.093075  0.083780  0.074598  0.066240  0.059178  0.053574  0.049267   \n",
              "9134   0.062594  0.043542  0.026331  0.011100 -0.002136 -0.013390 -0.022658   \n",
              "16633 -0.052265 -0.058428 -0.064190 -0.069700 -0.074866 -0.079454 -0.083214   \n",
              "21085 -0.031926 -0.029382 -0.027998 -0.028225 -0.030224 -0.033753 -0.038146   \n",
              "3939  -0.040913 -0.048534 -0.055368 -0.060233 -0.062451 -0.062108 -0.060157   \n",
              "...         ...       ...       ...       ...       ...       ...       ...   \n",
              "1950   0.101418  0.104209  0.107024  0.110509  0.115404  0.122480  0.132406   \n",
              "2559   0.006331  0.001729 -0.003743 -0.009459 -0.014882 -0.019655 -0.023610   \n",
              "14305 -0.073399 -0.073273 -0.074355 -0.076096 -0.077895 -0.079249 -0.079873   \n",
              "7228   0.014458  0.025175  0.034781  0.042367  0.047363  0.049621  0.049390   \n",
              "5471   0.079789 -0.002687 -0.092140 -0.176731 -0.245732 -0.291428 -0.310250   \n",
              "\n",
              "            7         8         9    ...       247       248       249  \\\n",
              "2931   0.045813  0.042554  0.038727  ... -0.021742 -0.018312 -0.014072   \n",
              "9134  -0.029900 -0.035072 -0.038172  ... -0.008176 -0.011257 -0.017116   \n",
              "16633 -0.085939 -0.087489 -0.087764  ... -0.029825 -0.026519 -0.024759   \n",
              "21085 -0.042391 -0.045306 -0.045782  ...  0.281163  0.308788  0.306347   \n",
              "3939  -0.058294 -0.058604 -0.063013  ...  0.009137  0.017063  0.027065   \n",
              "...         ...       ...       ...  ...       ...       ...       ...   \n",
              "1950   0.145571  0.161941  0.180962  ... -0.144181 -0.149649 -0.154869   \n",
              "2559  -0.026728 -0.029039 -0.030529  ... -0.000696 -0.001551 -0.002893   \n",
              "14305 -0.079751 -0.079111 -0.078337  ... -0.068108 -0.072968 -0.076441   \n",
              "7228   0.047233  0.043879  0.040069  ... -0.198819 -0.186027 -0.156859   \n",
              "5471  -0.302957 -0.273942 -0.229944  ...  0.177536  0.186216  0.195187   \n",
              "\n",
              "            250       251       252       253       254       255   256  \n",
              "2931  -0.008702 -0.002122  0.005451  0.013496  0.021243  0.027773  49.0  \n",
              "9134  -0.025439 -0.035572 -0.046571 -0.057305 -0.066604 -0.073445  39.0  \n",
              "16633 -0.026030 -0.031248 -0.040349 -0.052036 -0.063769 -0.072071  70.0  \n",
              "21085  0.273150  0.213806  0.137352  0.055393 -0.020318 -0.080210  88.0  \n",
              "3939   0.038950  0.052250  0.066308  0.080408  0.093909  0.106350  17.0  \n",
              "...         ...       ...       ...       ...       ...       ...   ...  \n",
              "1950  -0.158562 -0.159769 -0.158063 -0.153639 -0.147231 -0.139908  33.0  \n",
              "2559  -0.004604 -0.006442 -0.008118 -0.009409 -0.010245 -0.010763  43.0  \n",
              "14305 -0.078125 -0.077893 -0.075899 -0.072532 -0.068328 -0.063859  60.0  \n",
              "7228  -0.117200 -0.073893 -0.033323 -0.000290  0.022628  0.035142  31.0  \n",
              "5471   0.204468  0.213888  0.223064  0.231446  0.238390  0.243273  92.0  \n",
              "\n",
              "[24000 rows x 257 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c9214758-8f2e-499b-887c-3309fa6ea709\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>247</th>\n",
              "      <th>248</th>\n",
              "      <th>249</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "      <th>256</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2931</th>\n",
              "      <td>0.093075</td>\n",
              "      <td>0.083780</td>\n",
              "      <td>0.074598</td>\n",
              "      <td>0.066240</td>\n",
              "      <td>0.059178</td>\n",
              "      <td>0.053574</td>\n",
              "      <td>0.049267</td>\n",
              "      <td>0.045813</td>\n",
              "      <td>0.042554</td>\n",
              "      <td>0.038727</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.021742</td>\n",
              "      <td>-0.018312</td>\n",
              "      <td>-0.014072</td>\n",
              "      <td>-0.008702</td>\n",
              "      <td>-0.002122</td>\n",
              "      <td>0.005451</td>\n",
              "      <td>0.013496</td>\n",
              "      <td>0.021243</td>\n",
              "      <td>0.027773</td>\n",
              "      <td>49.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9134</th>\n",
              "      <td>0.062594</td>\n",
              "      <td>0.043542</td>\n",
              "      <td>0.026331</td>\n",
              "      <td>0.011100</td>\n",
              "      <td>-0.002136</td>\n",
              "      <td>-0.013390</td>\n",
              "      <td>-0.022658</td>\n",
              "      <td>-0.029900</td>\n",
              "      <td>-0.035072</td>\n",
              "      <td>-0.038172</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.008176</td>\n",
              "      <td>-0.011257</td>\n",
              "      <td>-0.017116</td>\n",
              "      <td>-0.025439</td>\n",
              "      <td>-0.035572</td>\n",
              "      <td>-0.046571</td>\n",
              "      <td>-0.057305</td>\n",
              "      <td>-0.066604</td>\n",
              "      <td>-0.073445</td>\n",
              "      <td>39.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16633</th>\n",
              "      <td>-0.052265</td>\n",
              "      <td>-0.058428</td>\n",
              "      <td>-0.064190</td>\n",
              "      <td>-0.069700</td>\n",
              "      <td>-0.074866</td>\n",
              "      <td>-0.079454</td>\n",
              "      <td>-0.083214</td>\n",
              "      <td>-0.085939</td>\n",
              "      <td>-0.087489</td>\n",
              "      <td>-0.087764</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.029825</td>\n",
              "      <td>-0.026519</td>\n",
              "      <td>-0.024759</td>\n",
              "      <td>-0.026030</td>\n",
              "      <td>-0.031248</td>\n",
              "      <td>-0.040349</td>\n",
              "      <td>-0.052036</td>\n",
              "      <td>-0.063769</td>\n",
              "      <td>-0.072071</td>\n",
              "      <td>70.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21085</th>\n",
              "      <td>-0.031926</td>\n",
              "      <td>-0.029382</td>\n",
              "      <td>-0.027998</td>\n",
              "      <td>-0.028225</td>\n",
              "      <td>-0.030224</td>\n",
              "      <td>-0.033753</td>\n",
              "      <td>-0.038146</td>\n",
              "      <td>-0.042391</td>\n",
              "      <td>-0.045306</td>\n",
              "      <td>-0.045782</td>\n",
              "      <td>...</td>\n",
              "      <td>0.281163</td>\n",
              "      <td>0.308788</td>\n",
              "      <td>0.306347</td>\n",
              "      <td>0.273150</td>\n",
              "      <td>0.213806</td>\n",
              "      <td>0.137352</td>\n",
              "      <td>0.055393</td>\n",
              "      <td>-0.020318</td>\n",
              "      <td>-0.080210</td>\n",
              "      <td>88.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3939</th>\n",
              "      <td>-0.040913</td>\n",
              "      <td>-0.048534</td>\n",
              "      <td>-0.055368</td>\n",
              "      <td>-0.060233</td>\n",
              "      <td>-0.062451</td>\n",
              "      <td>-0.062108</td>\n",
              "      <td>-0.060157</td>\n",
              "      <td>-0.058294</td>\n",
              "      <td>-0.058604</td>\n",
              "      <td>-0.063013</td>\n",
              "      <td>...</td>\n",
              "      <td>0.009137</td>\n",
              "      <td>0.017063</td>\n",
              "      <td>0.027065</td>\n",
              "      <td>0.038950</td>\n",
              "      <td>0.052250</td>\n",
              "      <td>0.066308</td>\n",
              "      <td>0.080408</td>\n",
              "      <td>0.093909</td>\n",
              "      <td>0.106350</td>\n",
              "      <td>17.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1950</th>\n",
              "      <td>0.101418</td>\n",
              "      <td>0.104209</td>\n",
              "      <td>0.107024</td>\n",
              "      <td>0.110509</td>\n",
              "      <td>0.115404</td>\n",
              "      <td>0.122480</td>\n",
              "      <td>0.132406</td>\n",
              "      <td>0.145571</td>\n",
              "      <td>0.161941</td>\n",
              "      <td>0.180962</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.144181</td>\n",
              "      <td>-0.149649</td>\n",
              "      <td>-0.154869</td>\n",
              "      <td>-0.158562</td>\n",
              "      <td>-0.159769</td>\n",
              "      <td>-0.158063</td>\n",
              "      <td>-0.153639</td>\n",
              "      <td>-0.147231</td>\n",
              "      <td>-0.139908</td>\n",
              "      <td>33.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2559</th>\n",
              "      <td>0.006331</td>\n",
              "      <td>0.001729</td>\n",
              "      <td>-0.003743</td>\n",
              "      <td>-0.009459</td>\n",
              "      <td>-0.014882</td>\n",
              "      <td>-0.019655</td>\n",
              "      <td>-0.023610</td>\n",
              "      <td>-0.026728</td>\n",
              "      <td>-0.029039</td>\n",
              "      <td>-0.030529</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.000696</td>\n",
              "      <td>-0.001551</td>\n",
              "      <td>-0.002893</td>\n",
              "      <td>-0.004604</td>\n",
              "      <td>-0.006442</td>\n",
              "      <td>-0.008118</td>\n",
              "      <td>-0.009409</td>\n",
              "      <td>-0.010245</td>\n",
              "      <td>-0.010763</td>\n",
              "      <td>43.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14305</th>\n",
              "      <td>-0.073399</td>\n",
              "      <td>-0.073273</td>\n",
              "      <td>-0.074355</td>\n",
              "      <td>-0.076096</td>\n",
              "      <td>-0.077895</td>\n",
              "      <td>-0.079249</td>\n",
              "      <td>-0.079873</td>\n",
              "      <td>-0.079751</td>\n",
              "      <td>-0.079111</td>\n",
              "      <td>-0.078337</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.068108</td>\n",
              "      <td>-0.072968</td>\n",
              "      <td>-0.076441</td>\n",
              "      <td>-0.078125</td>\n",
              "      <td>-0.077893</td>\n",
              "      <td>-0.075899</td>\n",
              "      <td>-0.072532</td>\n",
              "      <td>-0.068328</td>\n",
              "      <td>-0.063859</td>\n",
              "      <td>60.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7228</th>\n",
              "      <td>0.014458</td>\n",
              "      <td>0.025175</td>\n",
              "      <td>0.034781</td>\n",
              "      <td>0.042367</td>\n",
              "      <td>0.047363</td>\n",
              "      <td>0.049621</td>\n",
              "      <td>0.049390</td>\n",
              "      <td>0.047233</td>\n",
              "      <td>0.043879</td>\n",
              "      <td>0.040069</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.198819</td>\n",
              "      <td>-0.186027</td>\n",
              "      <td>-0.156859</td>\n",
              "      <td>-0.117200</td>\n",
              "      <td>-0.073893</td>\n",
              "      <td>-0.033323</td>\n",
              "      <td>-0.000290</td>\n",
              "      <td>0.022628</td>\n",
              "      <td>0.035142</td>\n",
              "      <td>31.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5471</th>\n",
              "      <td>0.079789</td>\n",
              "      <td>-0.002687</td>\n",
              "      <td>-0.092140</td>\n",
              "      <td>-0.176731</td>\n",
              "      <td>-0.245732</td>\n",
              "      <td>-0.291428</td>\n",
              "      <td>-0.310250</td>\n",
              "      <td>-0.302957</td>\n",
              "      <td>-0.273942</td>\n",
              "      <td>-0.229944</td>\n",
              "      <td>...</td>\n",
              "      <td>0.177536</td>\n",
              "      <td>0.186216</td>\n",
              "      <td>0.195187</td>\n",
              "      <td>0.204468</td>\n",
              "      <td>0.213888</td>\n",
              "      <td>0.223064</td>\n",
              "      <td>0.231446</td>\n",
              "      <td>0.238390</td>\n",
              "      <td>0.243273</td>\n",
              "      <td>92.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>24000 rows × 257 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c9214758-8f2e-499b-887c-3309fa6ea709')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c9214758-8f2e-499b-887c-3309fa6ea709 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c9214758-8f2e-499b-887c-3309fa6ea709');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터 따로 섞기"
      ],
      "metadata": {
        "id": "Lzt1SMEoSshL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_train = data_train.sample(frac=1).reset_index(drop=True)\n",
        "data_test = data_test.sample(frac=1).reset_index(drop=True)\n",
        "data_val = data_val.sample(frac=1).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "rpmNEC3Z0gtl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "ovruSUp41EA8",
        "outputId": "e071d1b2-f4ad-4b00-cf90-3a013213ee23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            0         1         2         3         4         5         6    \\\n",
              "0      0.013167  0.017490  0.018646  0.016446  0.011114  0.003245 -0.006287   \n",
              "1     -0.115512 -0.115289 -0.113396 -0.110355 -0.106877 -0.103780 -0.101890   \n",
              "2     -0.009042 -0.008103 -0.008920 -0.010897 -0.013278 -0.015248 -0.016029   \n",
              "3      0.402142  0.302911  0.185292  0.066715 -0.037182 -0.115259 -0.162345   \n",
              "4     -0.007963  0.005535  0.017452  0.026380  0.031221  0.031279  0.026293   \n",
              "...         ...       ...       ...       ...       ...       ...       ...   \n",
              "23995 -0.109838 -0.112002 -0.113641 -0.114621 -0.114863 -0.114353 -0.113177   \n",
              "23996 -0.040675 -0.054378 -0.068501 -0.081380 -0.091338 -0.096999 -0.097585   \n",
              "23997 -0.019681 -0.041208 -0.061138 -0.078153 -0.091414 -0.100656 -0.106163   \n",
              "23998  0.003074 -0.012424 -0.024224 -0.032849 -0.039069 -0.043762 -0.047765   \n",
              "23999 -0.016414 -0.019264 -0.023513 -0.029241 -0.036146 -0.043594 -0.050755   \n",
              "\n",
              "            7         8         9    ...       247       248       249  \\\n",
              "0     -0.016464 -0.026283 -0.034921  ... -0.004174 -0.002091  0.000968   \n",
              "1     -0.101913 -0.104281 -0.108999  ...  0.008242  0.022894  0.039154   \n",
              "2     -0.014967 -0.011609 -0.005777  ... -0.041403 -0.040630 -0.041345   \n",
              "3     -0.179306 -0.171868 -0.148656  ... -0.079294 -0.081055 -0.082744   \n",
              "4      0.016416  0.002152 -0.015730  ...  0.008343 -0.091526 -0.188443   \n",
              "...         ...       ...       ...  ...       ...       ...       ...   \n",
              "23995 -0.111554 -0.109847 -0.108525  ... -0.108024 -0.108699 -0.108706   \n",
              "23996 -0.093149 -0.084687 -0.074089  ...  0.001157  0.069442  0.143595   \n",
              "23997 -0.108632 -0.108992 -0.108220  ... -0.045588 -0.048637 -0.051353   \n",
              "23998 -0.051742 -0.056075 -0.060821  ... -0.119279 -0.118505 -0.116912   \n",
              "23999 -0.056785 -0.061007 -0.063049  ... -0.090826 -0.090601 -0.087764   \n",
              "\n",
              "            250       251       252       253       254       255   256  \n",
              "0      0.004741  0.008918  0.013208  0.017364  0.021204  0.024627  72.0  \n",
              "1      0.056616  0.074825  0.093340  0.111823  0.130108  0.148236  54.0  \n",
              "2     -0.043381 -0.046298 -0.049451 -0.052130 -0.053720 -0.053853  61.0  \n",
              "3     -0.083994 -0.084521 -0.084185 -0.083029 -0.081281 -0.079306  40.0  \n",
              "4     -0.269607 -0.325108 -0.349555 -0.342722 -0.309150 -0.256869   9.0  \n",
              "...         ...       ...       ...       ...       ...       ...   ...  \n",
              "23995 -0.108044 -0.106867 -0.105405 -0.103864 -0.102335 -0.100754  62.0  \n",
              "23996  0.214329  0.271583  0.306394  0.312725  0.288831  0.237771  41.0  \n",
              "23997 -0.053427 -0.054666 -0.054988 -0.054436 -0.053175 -0.051491  35.0  \n",
              "23998 -0.114928 -0.113008 -0.111522 -0.110699 -0.110611 -0.111212  98.0  \n",
              "23999 -0.082253 -0.074366 -0.064672 -0.053875 -0.042684 -0.031708  81.0  \n",
              "\n",
              "[24000 rows x 257 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-20518848-deb8-470c-bef6-2095db7f43fb\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>247</th>\n",
              "      <th>248</th>\n",
              "      <th>249</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "      <th>256</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.013167</td>\n",
              "      <td>0.017490</td>\n",
              "      <td>0.018646</td>\n",
              "      <td>0.016446</td>\n",
              "      <td>0.011114</td>\n",
              "      <td>0.003245</td>\n",
              "      <td>-0.006287</td>\n",
              "      <td>-0.016464</td>\n",
              "      <td>-0.026283</td>\n",
              "      <td>-0.034921</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.004174</td>\n",
              "      <td>-0.002091</td>\n",
              "      <td>0.000968</td>\n",
              "      <td>0.004741</td>\n",
              "      <td>0.008918</td>\n",
              "      <td>0.013208</td>\n",
              "      <td>0.017364</td>\n",
              "      <td>0.021204</td>\n",
              "      <td>0.024627</td>\n",
              "      <td>72.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.115512</td>\n",
              "      <td>-0.115289</td>\n",
              "      <td>-0.113396</td>\n",
              "      <td>-0.110355</td>\n",
              "      <td>-0.106877</td>\n",
              "      <td>-0.103780</td>\n",
              "      <td>-0.101890</td>\n",
              "      <td>-0.101913</td>\n",
              "      <td>-0.104281</td>\n",
              "      <td>-0.108999</td>\n",
              "      <td>...</td>\n",
              "      <td>0.008242</td>\n",
              "      <td>0.022894</td>\n",
              "      <td>0.039154</td>\n",
              "      <td>0.056616</td>\n",
              "      <td>0.074825</td>\n",
              "      <td>0.093340</td>\n",
              "      <td>0.111823</td>\n",
              "      <td>0.130108</td>\n",
              "      <td>0.148236</td>\n",
              "      <td>54.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.009042</td>\n",
              "      <td>-0.008103</td>\n",
              "      <td>-0.008920</td>\n",
              "      <td>-0.010897</td>\n",
              "      <td>-0.013278</td>\n",
              "      <td>-0.015248</td>\n",
              "      <td>-0.016029</td>\n",
              "      <td>-0.014967</td>\n",
              "      <td>-0.011609</td>\n",
              "      <td>-0.005777</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.041403</td>\n",
              "      <td>-0.040630</td>\n",
              "      <td>-0.041345</td>\n",
              "      <td>-0.043381</td>\n",
              "      <td>-0.046298</td>\n",
              "      <td>-0.049451</td>\n",
              "      <td>-0.052130</td>\n",
              "      <td>-0.053720</td>\n",
              "      <td>-0.053853</td>\n",
              "      <td>61.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.402142</td>\n",
              "      <td>0.302911</td>\n",
              "      <td>0.185292</td>\n",
              "      <td>0.066715</td>\n",
              "      <td>-0.037182</td>\n",
              "      <td>-0.115259</td>\n",
              "      <td>-0.162345</td>\n",
              "      <td>-0.179306</td>\n",
              "      <td>-0.171868</td>\n",
              "      <td>-0.148656</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.079294</td>\n",
              "      <td>-0.081055</td>\n",
              "      <td>-0.082744</td>\n",
              "      <td>-0.083994</td>\n",
              "      <td>-0.084521</td>\n",
              "      <td>-0.084185</td>\n",
              "      <td>-0.083029</td>\n",
              "      <td>-0.081281</td>\n",
              "      <td>-0.079306</td>\n",
              "      <td>40.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.007963</td>\n",
              "      <td>0.005535</td>\n",
              "      <td>0.017452</td>\n",
              "      <td>0.026380</td>\n",
              "      <td>0.031221</td>\n",
              "      <td>0.031279</td>\n",
              "      <td>0.026293</td>\n",
              "      <td>0.016416</td>\n",
              "      <td>0.002152</td>\n",
              "      <td>-0.015730</td>\n",
              "      <td>...</td>\n",
              "      <td>0.008343</td>\n",
              "      <td>-0.091526</td>\n",
              "      <td>-0.188443</td>\n",
              "      <td>-0.269607</td>\n",
              "      <td>-0.325108</td>\n",
              "      <td>-0.349555</td>\n",
              "      <td>-0.342722</td>\n",
              "      <td>-0.309150</td>\n",
              "      <td>-0.256869</td>\n",
              "      <td>9.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23995</th>\n",
              "      <td>-0.109838</td>\n",
              "      <td>-0.112002</td>\n",
              "      <td>-0.113641</td>\n",
              "      <td>-0.114621</td>\n",
              "      <td>-0.114863</td>\n",
              "      <td>-0.114353</td>\n",
              "      <td>-0.113177</td>\n",
              "      <td>-0.111554</td>\n",
              "      <td>-0.109847</td>\n",
              "      <td>-0.108525</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.108024</td>\n",
              "      <td>-0.108699</td>\n",
              "      <td>-0.108706</td>\n",
              "      <td>-0.108044</td>\n",
              "      <td>-0.106867</td>\n",
              "      <td>-0.105405</td>\n",
              "      <td>-0.103864</td>\n",
              "      <td>-0.102335</td>\n",
              "      <td>-0.100754</td>\n",
              "      <td>62.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23996</th>\n",
              "      <td>-0.040675</td>\n",
              "      <td>-0.054378</td>\n",
              "      <td>-0.068501</td>\n",
              "      <td>-0.081380</td>\n",
              "      <td>-0.091338</td>\n",
              "      <td>-0.096999</td>\n",
              "      <td>-0.097585</td>\n",
              "      <td>-0.093149</td>\n",
              "      <td>-0.084687</td>\n",
              "      <td>-0.074089</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001157</td>\n",
              "      <td>0.069442</td>\n",
              "      <td>0.143595</td>\n",
              "      <td>0.214329</td>\n",
              "      <td>0.271583</td>\n",
              "      <td>0.306394</td>\n",
              "      <td>0.312725</td>\n",
              "      <td>0.288831</td>\n",
              "      <td>0.237771</td>\n",
              "      <td>41.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23997</th>\n",
              "      <td>-0.019681</td>\n",
              "      <td>-0.041208</td>\n",
              "      <td>-0.061138</td>\n",
              "      <td>-0.078153</td>\n",
              "      <td>-0.091414</td>\n",
              "      <td>-0.100656</td>\n",
              "      <td>-0.106163</td>\n",
              "      <td>-0.108632</td>\n",
              "      <td>-0.108992</td>\n",
              "      <td>-0.108220</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.045588</td>\n",
              "      <td>-0.048637</td>\n",
              "      <td>-0.051353</td>\n",
              "      <td>-0.053427</td>\n",
              "      <td>-0.054666</td>\n",
              "      <td>-0.054988</td>\n",
              "      <td>-0.054436</td>\n",
              "      <td>-0.053175</td>\n",
              "      <td>-0.051491</td>\n",
              "      <td>35.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23998</th>\n",
              "      <td>0.003074</td>\n",
              "      <td>-0.012424</td>\n",
              "      <td>-0.024224</td>\n",
              "      <td>-0.032849</td>\n",
              "      <td>-0.039069</td>\n",
              "      <td>-0.043762</td>\n",
              "      <td>-0.047765</td>\n",
              "      <td>-0.051742</td>\n",
              "      <td>-0.056075</td>\n",
              "      <td>-0.060821</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.119279</td>\n",
              "      <td>-0.118505</td>\n",
              "      <td>-0.116912</td>\n",
              "      <td>-0.114928</td>\n",
              "      <td>-0.113008</td>\n",
              "      <td>-0.111522</td>\n",
              "      <td>-0.110699</td>\n",
              "      <td>-0.110611</td>\n",
              "      <td>-0.111212</td>\n",
              "      <td>98.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23999</th>\n",
              "      <td>-0.016414</td>\n",
              "      <td>-0.019264</td>\n",
              "      <td>-0.023513</td>\n",
              "      <td>-0.029241</td>\n",
              "      <td>-0.036146</td>\n",
              "      <td>-0.043594</td>\n",
              "      <td>-0.050755</td>\n",
              "      <td>-0.056785</td>\n",
              "      <td>-0.061007</td>\n",
              "      <td>-0.063049</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.090826</td>\n",
              "      <td>-0.090601</td>\n",
              "      <td>-0.087764</td>\n",
              "      <td>-0.082253</td>\n",
              "      <td>-0.074366</td>\n",
              "      <td>-0.064672</td>\n",
              "      <td>-0.053875</td>\n",
              "      <td>-0.042684</td>\n",
              "      <td>-0.031708</td>\n",
              "      <td>81.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>24000 rows × 257 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-20518848-deb8-470c-bef6-2095db7f43fb')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-20518848-deb8-470c-bef6-2095db7f43fb button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-20518848-deb8-470c-bef6-2095db7f43fb');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 전처리"
      ],
      "metadata": {
        "id": "JQBBj0ypT8T-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = to_categorical(data_train[256])\n",
        "y_test = to_categorical(data_test[256])\n",
        "y_val = to_categorical(data_val[256])\n",
        "# y_train = to_categorical(arr)\n",
        "# y_test = to_categorical(arr_t)\n",
        "# y_val = to_categorical(arr_v)\n",
        "y_train[23999]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "koQxh6-XfrG1",
        "outputId": "c926800e-4613-4bf9-a351-55708a5e33f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_train.drop([256], axis=1, inplace=True)\n",
        "data_test.drop([256], axis=1, inplace=True)\n",
        "data_val.drop([256], axis=1, inplace=True)\n",
        "data_train"
      ],
      "metadata": {
        "id": "QJUFXwCRkGhC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "outputId": "6a0f6519-a1bf-475a-eeab-c302aed980dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py:4906: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  return super().drop(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            0         1         2         3         4         5         6    \\\n",
              "2931   0.093075  0.083780  0.074598  0.066240  0.059178  0.053574  0.049267   \n",
              "9134   0.062594  0.043542  0.026331  0.011100 -0.002136 -0.013390 -0.022658   \n",
              "16633 -0.052265 -0.058428 -0.064190 -0.069700 -0.074866 -0.079454 -0.083214   \n",
              "21085 -0.031926 -0.029382 -0.027998 -0.028225 -0.030224 -0.033753 -0.038146   \n",
              "3939  -0.040913 -0.048534 -0.055368 -0.060233 -0.062451 -0.062108 -0.060157   \n",
              "...         ...       ...       ...       ...       ...       ...       ...   \n",
              "1950   0.101418  0.104209  0.107024  0.110509  0.115404  0.122480  0.132406   \n",
              "2559   0.006331  0.001729 -0.003743 -0.009459 -0.014882 -0.019655 -0.023610   \n",
              "14305 -0.073399 -0.073273 -0.074355 -0.076096 -0.077895 -0.079249 -0.079873   \n",
              "7228   0.014458  0.025175  0.034781  0.042367  0.047363  0.049621  0.049390   \n",
              "5471   0.079789 -0.002687 -0.092140 -0.176731 -0.245732 -0.291428 -0.310250   \n",
              "\n",
              "            7         8         9    ...       246       247       248  \\\n",
              "2931   0.045813  0.042554  0.038727  ... -0.024828 -0.021742 -0.018312   \n",
              "9134  -0.029900 -0.035072 -0.038172  ... -0.007846 -0.008176 -0.011257   \n",
              "16633 -0.085939 -0.087489 -0.087764  ... -0.033089 -0.029825 -0.026519   \n",
              "21085 -0.042391 -0.045306 -0.045782  ...  0.229120  0.281163  0.308788   \n",
              "3939  -0.058294 -0.058604 -0.063013  ...  0.003183  0.009137  0.017063   \n",
              "...         ...       ...       ...  ...       ...       ...       ...   \n",
              "1950   0.145571  0.161941  0.180962  ... -0.139767 -0.144181 -0.149649   \n",
              "2559  -0.026728 -0.029039 -0.030529  ... -0.000266 -0.000696 -0.001551   \n",
              "14305 -0.079751 -0.079111 -0.078337  ... -0.062467 -0.068108 -0.072968   \n",
              "7228   0.047233  0.043879  0.040069  ... -0.191666 -0.198819 -0.186027   \n",
              "5471  -0.302957 -0.273942 -0.229944  ...  0.168975  0.177536  0.186216   \n",
              "\n",
              "            249       250       251       252       253       254       255  \n",
              "2931  -0.014072 -0.008702 -0.002122  0.005451  0.013496  0.021243  0.027773  \n",
              "9134  -0.017116 -0.025439 -0.035572 -0.046571 -0.057305 -0.066604 -0.073445  \n",
              "16633 -0.024759 -0.026030 -0.031248 -0.040349 -0.052036 -0.063769 -0.072071  \n",
              "21085  0.306347  0.273150  0.213806  0.137352  0.055393 -0.020318 -0.080210  \n",
              "3939   0.027065  0.038950  0.052250  0.066308  0.080408  0.093909  0.106350  \n",
              "...         ...       ...       ...       ...       ...       ...       ...  \n",
              "1950  -0.154869 -0.158562 -0.159769 -0.158063 -0.153639 -0.147231 -0.139908  \n",
              "2559  -0.002893 -0.004604 -0.006442 -0.008118 -0.009409 -0.010245 -0.010763  \n",
              "14305 -0.076441 -0.078125 -0.077893 -0.075899 -0.072532 -0.068328 -0.063859  \n",
              "7228  -0.156859 -0.117200 -0.073893 -0.033323 -0.000290  0.022628  0.035142  \n",
              "5471   0.195187  0.204468  0.213888  0.223064  0.231446  0.238390  0.243273  \n",
              "\n",
              "[24000 rows x 256 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-80c02474-17ee-42d5-a2c0-a66740904fb7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>246</th>\n",
              "      <th>247</th>\n",
              "      <th>248</th>\n",
              "      <th>249</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2931</th>\n",
              "      <td>0.093075</td>\n",
              "      <td>0.083780</td>\n",
              "      <td>0.074598</td>\n",
              "      <td>0.066240</td>\n",
              "      <td>0.059178</td>\n",
              "      <td>0.053574</td>\n",
              "      <td>0.049267</td>\n",
              "      <td>0.045813</td>\n",
              "      <td>0.042554</td>\n",
              "      <td>0.038727</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.024828</td>\n",
              "      <td>-0.021742</td>\n",
              "      <td>-0.018312</td>\n",
              "      <td>-0.014072</td>\n",
              "      <td>-0.008702</td>\n",
              "      <td>-0.002122</td>\n",
              "      <td>0.005451</td>\n",
              "      <td>0.013496</td>\n",
              "      <td>0.021243</td>\n",
              "      <td>0.027773</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9134</th>\n",
              "      <td>0.062594</td>\n",
              "      <td>0.043542</td>\n",
              "      <td>0.026331</td>\n",
              "      <td>0.011100</td>\n",
              "      <td>-0.002136</td>\n",
              "      <td>-0.013390</td>\n",
              "      <td>-0.022658</td>\n",
              "      <td>-0.029900</td>\n",
              "      <td>-0.035072</td>\n",
              "      <td>-0.038172</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.007846</td>\n",
              "      <td>-0.008176</td>\n",
              "      <td>-0.011257</td>\n",
              "      <td>-0.017116</td>\n",
              "      <td>-0.025439</td>\n",
              "      <td>-0.035572</td>\n",
              "      <td>-0.046571</td>\n",
              "      <td>-0.057305</td>\n",
              "      <td>-0.066604</td>\n",
              "      <td>-0.073445</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16633</th>\n",
              "      <td>-0.052265</td>\n",
              "      <td>-0.058428</td>\n",
              "      <td>-0.064190</td>\n",
              "      <td>-0.069700</td>\n",
              "      <td>-0.074866</td>\n",
              "      <td>-0.079454</td>\n",
              "      <td>-0.083214</td>\n",
              "      <td>-0.085939</td>\n",
              "      <td>-0.087489</td>\n",
              "      <td>-0.087764</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.033089</td>\n",
              "      <td>-0.029825</td>\n",
              "      <td>-0.026519</td>\n",
              "      <td>-0.024759</td>\n",
              "      <td>-0.026030</td>\n",
              "      <td>-0.031248</td>\n",
              "      <td>-0.040349</td>\n",
              "      <td>-0.052036</td>\n",
              "      <td>-0.063769</td>\n",
              "      <td>-0.072071</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21085</th>\n",
              "      <td>-0.031926</td>\n",
              "      <td>-0.029382</td>\n",
              "      <td>-0.027998</td>\n",
              "      <td>-0.028225</td>\n",
              "      <td>-0.030224</td>\n",
              "      <td>-0.033753</td>\n",
              "      <td>-0.038146</td>\n",
              "      <td>-0.042391</td>\n",
              "      <td>-0.045306</td>\n",
              "      <td>-0.045782</td>\n",
              "      <td>...</td>\n",
              "      <td>0.229120</td>\n",
              "      <td>0.281163</td>\n",
              "      <td>0.308788</td>\n",
              "      <td>0.306347</td>\n",
              "      <td>0.273150</td>\n",
              "      <td>0.213806</td>\n",
              "      <td>0.137352</td>\n",
              "      <td>0.055393</td>\n",
              "      <td>-0.020318</td>\n",
              "      <td>-0.080210</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3939</th>\n",
              "      <td>-0.040913</td>\n",
              "      <td>-0.048534</td>\n",
              "      <td>-0.055368</td>\n",
              "      <td>-0.060233</td>\n",
              "      <td>-0.062451</td>\n",
              "      <td>-0.062108</td>\n",
              "      <td>-0.060157</td>\n",
              "      <td>-0.058294</td>\n",
              "      <td>-0.058604</td>\n",
              "      <td>-0.063013</td>\n",
              "      <td>...</td>\n",
              "      <td>0.003183</td>\n",
              "      <td>0.009137</td>\n",
              "      <td>0.017063</td>\n",
              "      <td>0.027065</td>\n",
              "      <td>0.038950</td>\n",
              "      <td>0.052250</td>\n",
              "      <td>0.066308</td>\n",
              "      <td>0.080408</td>\n",
              "      <td>0.093909</td>\n",
              "      <td>0.106350</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1950</th>\n",
              "      <td>0.101418</td>\n",
              "      <td>0.104209</td>\n",
              "      <td>0.107024</td>\n",
              "      <td>0.110509</td>\n",
              "      <td>0.115404</td>\n",
              "      <td>0.122480</td>\n",
              "      <td>0.132406</td>\n",
              "      <td>0.145571</td>\n",
              "      <td>0.161941</td>\n",
              "      <td>0.180962</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.139767</td>\n",
              "      <td>-0.144181</td>\n",
              "      <td>-0.149649</td>\n",
              "      <td>-0.154869</td>\n",
              "      <td>-0.158562</td>\n",
              "      <td>-0.159769</td>\n",
              "      <td>-0.158063</td>\n",
              "      <td>-0.153639</td>\n",
              "      <td>-0.147231</td>\n",
              "      <td>-0.139908</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2559</th>\n",
              "      <td>0.006331</td>\n",
              "      <td>0.001729</td>\n",
              "      <td>-0.003743</td>\n",
              "      <td>-0.009459</td>\n",
              "      <td>-0.014882</td>\n",
              "      <td>-0.019655</td>\n",
              "      <td>-0.023610</td>\n",
              "      <td>-0.026728</td>\n",
              "      <td>-0.029039</td>\n",
              "      <td>-0.030529</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.000266</td>\n",
              "      <td>-0.000696</td>\n",
              "      <td>-0.001551</td>\n",
              "      <td>-0.002893</td>\n",
              "      <td>-0.004604</td>\n",
              "      <td>-0.006442</td>\n",
              "      <td>-0.008118</td>\n",
              "      <td>-0.009409</td>\n",
              "      <td>-0.010245</td>\n",
              "      <td>-0.010763</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14305</th>\n",
              "      <td>-0.073399</td>\n",
              "      <td>-0.073273</td>\n",
              "      <td>-0.074355</td>\n",
              "      <td>-0.076096</td>\n",
              "      <td>-0.077895</td>\n",
              "      <td>-0.079249</td>\n",
              "      <td>-0.079873</td>\n",
              "      <td>-0.079751</td>\n",
              "      <td>-0.079111</td>\n",
              "      <td>-0.078337</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.062467</td>\n",
              "      <td>-0.068108</td>\n",
              "      <td>-0.072968</td>\n",
              "      <td>-0.076441</td>\n",
              "      <td>-0.078125</td>\n",
              "      <td>-0.077893</td>\n",
              "      <td>-0.075899</td>\n",
              "      <td>-0.072532</td>\n",
              "      <td>-0.068328</td>\n",
              "      <td>-0.063859</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7228</th>\n",
              "      <td>0.014458</td>\n",
              "      <td>0.025175</td>\n",
              "      <td>0.034781</td>\n",
              "      <td>0.042367</td>\n",
              "      <td>0.047363</td>\n",
              "      <td>0.049621</td>\n",
              "      <td>0.049390</td>\n",
              "      <td>0.047233</td>\n",
              "      <td>0.043879</td>\n",
              "      <td>0.040069</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.191666</td>\n",
              "      <td>-0.198819</td>\n",
              "      <td>-0.186027</td>\n",
              "      <td>-0.156859</td>\n",
              "      <td>-0.117200</td>\n",
              "      <td>-0.073893</td>\n",
              "      <td>-0.033323</td>\n",
              "      <td>-0.000290</td>\n",
              "      <td>0.022628</td>\n",
              "      <td>0.035142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5471</th>\n",
              "      <td>0.079789</td>\n",
              "      <td>-0.002687</td>\n",
              "      <td>-0.092140</td>\n",
              "      <td>-0.176731</td>\n",
              "      <td>-0.245732</td>\n",
              "      <td>-0.291428</td>\n",
              "      <td>-0.310250</td>\n",
              "      <td>-0.302957</td>\n",
              "      <td>-0.273942</td>\n",
              "      <td>-0.229944</td>\n",
              "      <td>...</td>\n",
              "      <td>0.168975</td>\n",
              "      <td>0.177536</td>\n",
              "      <td>0.186216</td>\n",
              "      <td>0.195187</td>\n",
              "      <td>0.204468</td>\n",
              "      <td>0.213888</td>\n",
              "      <td>0.223064</td>\n",
              "      <td>0.231446</td>\n",
              "      <td>0.238390</td>\n",
              "      <td>0.243273</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>24000 rows × 256 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-80c02474-17ee-42d5-a2c0-a66740904fb7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-80c02474-17ee-42d5-a2c0-a66740904fb7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-80c02474-17ee-42d5-a2c0-a66740904fb7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train=data_train.values\n",
        "X_test=data_test.values\n",
        "X_val=data_val.values"
      ],
      "metadata": {
        "id": "EK7Q96RZtwZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(X_val.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)\n",
        "print(y_val.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwfh9AwhjkMM",
        "outputId": "0b5a251d-4ab7-47cb-cadf-05a820153638"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(24000, 256)\n",
            "(6000, 256)\n",
            "(6000, 256)\n",
            "(24000, 101)\n",
            "(6000, 101)\n",
            "(6000, 101)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# X_train = X_train.reshape(1, 256, 24000)\n",
        "# X_test = X_test.reshape(1, 256, 6000)\n",
        "# X_val = X_val.reshape(1, 256, 6000)\n",
        "X_train = X_train.reshape(24000, 256, 1)\n",
        "X_test = X_test.reshape(6000, 256, 1)\n",
        "X_val = X_val.reshape(6000, 256, 1)\n",
        "X_train.shape, X_test.shape, X_val.shape"
      ],
      "metadata": {
        "id": "50YyZlL9htW3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42dc7c9d-ba73-49af-dbb8-5e8203e7418c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((24000, 256, 1), (6000, 256, 1), (6000, 256, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## cnn-lstm 모델"
      ],
      "metadata": {
        "id": "EgfaelmnXe04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model():\n",
        "    model = Sequential()\n",
        "    model.add(Conv1D(filters=64, kernel_size=6, activation='relu', \n",
        "                    padding='same', input_shape=(256, 1)))\n",
        "    model.add(BatchNormalization())\n",
        "    \n",
        "    # adding a pooling layer\n",
        "    model.add(MaxPooling1D(pool_size=(3), strides=2, padding='same'))\n",
        "    \n",
        "    model.add(Conv1D(filters=64, kernel_size=6, activation='relu', \n",
        "                    padding='same', input_shape=(256, 1)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling1D(pool_size=(3), strides=2, padding='same'))\n",
        "    \n",
        "    model.add(Conv1D(filters=64, kernel_size=6, activation='relu', \n",
        "                    padding='same', input_shape=(256, 1)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling1D(pool_size=(3), strides=2, padding='same'))\n",
        "    \n",
        "    model.add(Flatten())\n",
        "    #model.add(Dense(64, activation='relu'))\n",
        "    #model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(101, activation='softmax'))\n",
        "    \n",
        "    model.compile(optimizer='adam', loss='mse',metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "DqFKm7PSQS3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model()\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "wum-FyK7QVUe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2397946-5a7a-4f1f-e09b-f35229db7ab7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_18\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_17 (Conv1D)          (None, 256, 64)           448       \n",
            "                                                                 \n",
            " batch_normalization_12 (Bat  (None, 256, 64)          256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling1d_17 (MaxPoolin  (None, 128, 64)          0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_18 (Conv1D)          (None, 128, 64)           24640     \n",
            "                                                                 \n",
            " batch_normalization_13 (Bat  (None, 128, 64)          256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling1d_18 (MaxPoolin  (None, 64, 64)           0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_19 (Conv1D)          (None, 64, 64)            24640     \n",
            "                                                                 \n",
            " batch_normalization_14 (Bat  (None, 64, 64)           256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling1d_19 (MaxPoolin  (None, 32, 64)           0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " flatten_13 (Flatten)        (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_24 (Dense)            (None, 100)               204900    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 255,396\n",
            "Trainable params: 255,012\n",
            "Non-trainable params: 384\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## cnn-gru 모델"
      ],
      "metadata": {
        "id": "m-Hm3lsWXkbE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Input, Embedding, GRU, LSTM, MaxPooling1D, GlobalMaxPool1D, CuDNNGRU\n",
        "from keras.layers import Dropout, Dense, Activation, Flatten, Conv1D, SpatialDropout1D\n",
        "from keras.models import Sequential\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv1D(filters=64, kernel_size=6, activation='relu', padding='same', input_shape=(256, 1)))\n",
        "model.add(SpatialDropout1D(0.2))\n",
        "model.add(MaxPooling1D(4))\n",
        "model.add(CuDNNGRU(64))\n",
        "\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Dense(101, activation='softmax'))\n",
        "model.compile(optimizer='adam', loss='mse',metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXzfgBF75uk-",
        "outputId": "33f77877-e599-4ded-8400-373f5cec14bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_13 (Conv1D)          (None, 256, 64)           448       \n",
            "                                                                 \n",
            " spatial_dropout1d_4 (Spatia  (None, 256, 64)          0         \n",
            " lDropout1D)                                                     \n",
            "                                                                 \n",
            " max_pooling1d_13 (MaxPoolin  (None, 64, 64)           0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " cu_dnngru_4 (CuDNNGRU)      (None, 64)                24960     \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 101)               6565      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 31,973\n",
            "Trainable params: 31,973\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## gru모델"
      ],
      "metadata": {
        "id": "Na73oRb9Xq69"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Input, Embedding, GRU, LSTM, SimpleRNN, MaxPooling1D, GlobalMaxPool1D, CuDNNGRU\n",
        "from keras.layers import Dropout, Dense, Activation, Flatten, Conv1D, SpatialDropout1D\n",
        "from keras.models import Sequential\n",
        "\n",
        "model = Sequential()\n",
        "model.add(GRU(units=100, return_sequences=True, input_shape=(256,1))) #shape[1]=열\n",
        "model.add(GRU(units=50, return_sequences=True))\n",
        "model.add(GRU(units=50, return_sequences=True))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Dense(101, activation='softmax'))\n",
        "model.compile(optimizer='adam', loss='mse',metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWyVmfGGXar5",
        "outputId": "88431697-9307-483c-a3df-fdb26de53d7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " gru_22 (GRU)                (None, 256, 100)          30900     \n",
            "                                                                 \n",
            " gru_23 (GRU)                (None, 256, 50)           22800     \n",
            "                                                                 \n",
            " gru_24 (GRU)                (None, 256, 50)           15300     \n",
            "                                                                 \n",
            " flatten_3 (Flatten)         (None, 12800)             0         \n",
            "                                                                 \n",
            " dropout_11 (Dropout)        (None, 12800)             0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 101)               1292901   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,361,901\n",
            "Trainable params: 1,361,901\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSTM"
      ],
      "metadata": {
        "id": "DBnTIBCDGX94"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(units=100, return_sequences=True, input_shape=(256,1))) #shape[1]=열\n",
        "model.add(LSTM(units=50, return_sequences=True))\n",
        "model.add(LSTM(units=50, return_sequences=True))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Dense(101, activation='softmax'))\n",
        "model.compile(optimizer='adam', loss='mse',metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jbh5mP0GWh5",
        "outputId": "8055881e-f18e-4941-e7d3-1d853b39d2e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_3 (LSTM)               (None, 256, 100)          40800     \n",
            "                                                                 \n",
            " lstm_4 (LSTM)               (None, 256, 50)           30200     \n",
            "                                                                 \n",
            " lstm_5 (LSTM)               (None, 256, 50)           20200     \n",
            "                                                                 \n",
            " flatten_6 (Flatten)         (None, 12800)             0         \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 12800)             0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 101)               1292901   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,384,101\n",
            "Trainable params: 1,384,101\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RNN모델"
      ],
      "metadata": {
        "id": "mLO6GGJ5Nn9w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Input, Embedding, GRU, LSTM, SimpleRNN, MaxPooling1D, GlobalMaxPool1D, CuDNNGRU\n",
        "from keras.layers import Dropout, Dense, Activation, Flatten, Conv1D, SpatialDropout1D\n",
        "from keras.models import Sequential\n",
        "\n",
        "model = Sequential()\n",
        "model.add(SimpleRNN(units=100, return_sequences=True, input_shape=(256,1))) #shape[1]=열\n",
        "model.add(SimpleRNN(units=50, return_sequences=True))\n",
        "model.add(SimpleRNN(units=50, return_sequences=True))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(101, activation='softmax'))\n",
        "model.compile(optimizer='adam', loss='mse',metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHOArXJVNnd0",
        "outputId": "4f56aa04-4c34-4860-fc44-299596d4f96f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " simple_rnn_12 (SimpleRNN)   (None, 256, 100)          10200     \n",
            "                                                                 \n",
            " simple_rnn_13 (SimpleRNN)   (None, 256, 50)           7550      \n",
            "                                                                 \n",
            " simple_rnn_14 (SimpleRNN)   (None, 256, 50)           5050      \n",
            "                                                                 \n",
            " flatten_4 (Flatten)         (None, 12800)             0         \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 12800)             0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 101)               1292901   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,315,701\n",
            "Trainable params: 1,315,701\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNN"
      ],
      "metadata": {
        "id": "rLUIZ_F6tj6J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model():\n",
        "  model = Sequential()\n",
        "  model.add(Conv1D(256, 8, padding='same', activation='relu', input_shape= (256,1)))\n",
        "  model.add(MaxPooling1D())\n",
        "  model.add(Conv1D(256, 8, padding='same', activation='relu'))\n",
        "  model.add(MaxPooling1D())\n",
        "  model.add(Conv1D(64, 8, padding='same',activation='relu'))\n",
        "  model.add(MaxPooling1D())\n",
        "  model.add(Conv1D(64, 8, padding='same', activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(101, activation='softmax')) # activation='softmax'\n",
        "\n",
        "  model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
        "  model.summary()\n",
        "  return model"
      ],
      "metadata": {
        "id": "wWy9tN9ttlRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKFiYvIRtoXu",
        "outputId": "6fc01b9a-a5a8-4976-c711-e65c2587ea5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_4 (Conv1D)           (None, 256, 256)          2304      \n",
            "                                                                 \n",
            " max_pooling1d_3 (MaxPooling  (None, 128, 256)         0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " conv1d_5 (Conv1D)           (None, 128, 256)          524544    \n",
            "                                                                 \n",
            " max_pooling1d_4 (MaxPooling  (None, 64, 256)          0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " conv1d_6 (Conv1D)           (None, 64, 64)            131136    \n",
            "                                                                 \n",
            " max_pooling1d_5 (MaxPooling  (None, 32, 64)           0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " conv1d_7 (Conv1D)           (None, 32, 64)            32832     \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 32, 64)           256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 101)               206949    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 898,021\n",
            "Trainable params: 897,893\n",
            "Non-trainable params: 128\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델학습/평가"
      ],
      "metadata": {
        "id": "bWixFuhvKbcz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train, y_train, batch_size = 1024, epochs = 1000, verbose = 1, validation_data=(X_val, y_val))"
      ],
      "metadata": {
        "id": "t-UtiKqRQYVv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ed4ca2c-9eaf-4ddb-c06c-f11ec5b667b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "24/24 [==============================] - 8s 167ms/step - loss: 0.0098 - accuracy: 0.0075 - val_loss: 0.0098 - val_accuracy: 0.0063\n",
            "Epoch 2/1000\n",
            "24/24 [==============================] - 3s 129ms/step - loss: 0.0098 - accuracy: 0.0096 - val_loss: 0.0098 - val_accuracy: 0.0093\n",
            "Epoch 3/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 0.0098 - accuracy: 0.0093 - val_loss: 0.0098 - val_accuracy: 0.0073\n",
            "Epoch 4/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 0.0098 - accuracy: 0.0117 - val_loss: 0.0098 - val_accuracy: 0.0077\n",
            "Epoch 5/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 0.0098 - accuracy: 0.0121 - val_loss: 0.0098 - val_accuracy: 0.0058\n",
            "Epoch 6/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 0.0098 - accuracy: 0.0122 - val_loss: 0.0098 - val_accuracy: 0.0097\n",
            "Epoch 7/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 0.0098 - accuracy: 0.0150 - val_loss: 0.0098 - val_accuracy: 0.0122\n",
            "Epoch 8/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 0.0098 - accuracy: 0.0182 - val_loss: 0.0098 - val_accuracy: 0.0130\n",
            "Epoch 9/1000\n",
            "24/24 [==============================] - 3s 132ms/step - loss: 0.0098 - accuracy: 0.0222 - val_loss: 0.0098 - val_accuracy: 0.0142\n",
            "Epoch 10/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 0.0098 - accuracy: 0.0254 - val_loss: 0.0098 - val_accuracy: 0.0338\n",
            "Epoch 11/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 0.0097 - accuracy: 0.0389 - val_loss: 0.0098 - val_accuracy: 0.0347\n",
            "Epoch 12/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 0.0097 - accuracy: 0.0508 - val_loss: 0.0098 - val_accuracy: 0.0438\n",
            "Epoch 13/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 0.0097 - accuracy: 0.0616 - val_loss: 0.0098 - val_accuracy: 0.0473\n",
            "Epoch 14/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 0.0097 - accuracy: 0.0745 - val_loss: 0.0098 - val_accuracy: 0.0605\n",
            "Epoch 15/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 0.0096 - accuracy: 0.0857 - val_loss: 0.0097 - val_accuracy: 0.0640\n",
            "Epoch 16/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 0.0096 - accuracy: 0.0978 - val_loss: 0.0097 - val_accuracy: 0.0778\n",
            "Epoch 17/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 0.0095 - accuracy: 0.1135 - val_loss: 0.0097 - val_accuracy: 0.1017\n",
            "Epoch 18/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 0.0095 - accuracy: 0.1271 - val_loss: 0.0096 - val_accuracy: 0.1068\n",
            "Epoch 19/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 0.0094 - accuracy: 0.1423 - val_loss: 0.0096 - val_accuracy: 0.1118\n",
            "Epoch 20/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 0.0093 - accuracy: 0.1605 - val_loss: 0.0095 - val_accuracy: 0.1262\n",
            "Epoch 21/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 0.0092 - accuracy: 0.1777 - val_loss: 0.0095 - val_accuracy: 0.1375\n",
            "Epoch 22/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 0.0091 - accuracy: 0.1924 - val_loss: 0.0095 - val_accuracy: 0.1460\n",
            "Epoch 23/1000\n",
            "24/24 [==============================] - 3s 129ms/step - loss: 0.0090 - accuracy: 0.2057 - val_loss: 0.0093 - val_accuracy: 0.1637\n",
            "Epoch 24/1000\n",
            "24/24 [==============================] - 3s 129ms/step - loss: 0.0089 - accuracy: 0.2253 - val_loss: 0.0093 - val_accuracy: 0.1697\n",
            "Epoch 25/1000\n",
            "24/24 [==============================] - 3s 129ms/step - loss: 0.0088 - accuracy: 0.2401 - val_loss: 0.0092 - val_accuracy: 0.1863\n",
            "Epoch 26/1000\n",
            "24/24 [==============================] - 3s 129ms/step - loss: 0.0086 - accuracy: 0.2581 - val_loss: 0.0092 - val_accuracy: 0.1975\n",
            "Epoch 27/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 0.0085 - accuracy: 0.2801 - val_loss: 0.0091 - val_accuracy: 0.2093\n",
            "Epoch 28/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 0.0083 - accuracy: 0.2975 - val_loss: 0.0090 - val_accuracy: 0.2212\n",
            "Epoch 29/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 0.0081 - accuracy: 0.3252 - val_loss: 0.0088 - val_accuracy: 0.2527\n",
            "Epoch 30/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 0.0079 - accuracy: 0.3512 - val_loss: 0.0086 - val_accuracy: 0.2777\n",
            "Epoch 31/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 0.0077 - accuracy: 0.3730 - val_loss: 0.0085 - val_accuracy: 0.2907\n",
            "Epoch 32/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 0.0075 - accuracy: 0.3962 - val_loss: 0.0084 - val_accuracy: 0.3062\n",
            "Epoch 33/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 0.0072 - accuracy: 0.4203 - val_loss: 0.0082 - val_accuracy: 0.3312\n",
            "Epoch 34/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 0.0070 - accuracy: 0.4457 - val_loss: 0.0081 - val_accuracy: 0.3420\n",
            "Epoch 35/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 0.0068 - accuracy: 0.4671 - val_loss: 0.0078 - val_accuracy: 0.3757\n",
            "Epoch 36/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 0.0065 - accuracy: 0.4870 - val_loss: 0.0078 - val_accuracy: 0.3817\n",
            "Epoch 37/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 0.0064 - accuracy: 0.5040 - val_loss: 0.0077 - val_accuracy: 0.3935\n",
            "Epoch 38/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 0.0062 - accuracy: 0.5221 - val_loss: 0.0075 - val_accuracy: 0.4120\n",
            "Epoch 39/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 0.0059 - accuracy: 0.5399 - val_loss: 0.0074 - val_accuracy: 0.4263\n",
            "Epoch 40/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 0.0058 - accuracy: 0.5550 - val_loss: 0.0071 - val_accuracy: 0.4483\n",
            "Epoch 41/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 0.0056 - accuracy: 0.5747 - val_loss: 0.0070 - val_accuracy: 0.4612\n",
            "Epoch 42/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 0.0053 - accuracy: 0.5954 - val_loss: 0.0067 - val_accuracy: 0.4898\n",
            "Epoch 43/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 0.0051 - accuracy: 0.6137 - val_loss: 0.0067 - val_accuracy: 0.4912\n",
            "Epoch 44/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 0.0049 - accuracy: 0.6332 - val_loss: 0.0063 - val_accuracy: 0.5240\n",
            "Epoch 45/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 0.0047 - accuracy: 0.6478 - val_loss: 0.0063 - val_accuracy: 0.5318\n",
            "Epoch 46/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 0.0044 - accuracy: 0.6725 - val_loss: 0.0060 - val_accuracy: 0.5573\n",
            "Epoch 47/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 0.0042 - accuracy: 0.6868 - val_loss: 0.0058 - val_accuracy: 0.5678\n",
            "Epoch 48/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 0.0040 - accuracy: 0.7013 - val_loss: 0.0057 - val_accuracy: 0.5855\n",
            "Epoch 49/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 0.0038 - accuracy: 0.7187 - val_loss: 0.0054 - val_accuracy: 0.6060\n",
            "Epoch 50/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 0.0036 - accuracy: 0.7365 - val_loss: 0.0052 - val_accuracy: 0.6210\n",
            "Epoch 51/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 0.0034 - accuracy: 0.7488 - val_loss: 0.0050 - val_accuracy: 0.6402\n",
            "Epoch 52/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 0.0033 - accuracy: 0.7629 - val_loss: 0.0050 - val_accuracy: 0.6368\n",
            "Epoch 53/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 0.0031 - accuracy: 0.7723 - val_loss: 0.0048 - val_accuracy: 0.6578\n",
            "Epoch 54/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 0.0030 - accuracy: 0.7833 - val_loss: 0.0046 - val_accuracy: 0.6655\n",
            "Epoch 55/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 0.0029 - accuracy: 0.7931 - val_loss: 0.0046 - val_accuracy: 0.6710\n",
            "Epoch 56/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 0.0027 - accuracy: 0.8014 - val_loss: 0.0044 - val_accuracy: 0.6843\n",
            "Epoch 57/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 0.0026 - accuracy: 0.8124 - val_loss: 0.0043 - val_accuracy: 0.6933\n",
            "Epoch 58/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 0.0025 - accuracy: 0.8191 - val_loss: 0.0044 - val_accuracy: 0.6867\n",
            "Epoch 59/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 0.0024 - accuracy: 0.8260 - val_loss: 0.0043 - val_accuracy: 0.6945\n",
            "Epoch 60/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 0.0023 - accuracy: 0.8357 - val_loss: 0.0041 - val_accuracy: 0.7093\n",
            "Epoch 61/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 0.0022 - accuracy: 0.8410 - val_loss: 0.0041 - val_accuracy: 0.7058\n",
            "Epoch 62/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 0.0021 - accuracy: 0.8463 - val_loss: 0.0040 - val_accuracy: 0.7187\n",
            "Epoch 63/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 0.0020 - accuracy: 0.8511 - val_loss: 0.0039 - val_accuracy: 0.7230\n",
            "Epoch 64/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 0.0020 - accuracy: 0.8545 - val_loss: 0.0039 - val_accuracy: 0.7242\n",
            "Epoch 65/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 0.0019 - accuracy: 0.8586 - val_loss: 0.0039 - val_accuracy: 0.7280\n",
            "Epoch 66/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 0.0019 - accuracy: 0.8619 - val_loss: 0.0038 - val_accuracy: 0.7325\n",
            "Epoch 67/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 0.0019 - accuracy: 0.8652 - val_loss: 0.0038 - val_accuracy: 0.7338\n",
            "Epoch 68/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 0.0017 - accuracy: 0.8740 - val_loss: 0.0037 - val_accuracy: 0.7450\n",
            "Epoch 69/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 0.0017 - accuracy: 0.8770 - val_loss: 0.0035 - val_accuracy: 0.7542\n",
            "Epoch 70/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 0.0016 - accuracy: 0.8825 - val_loss: 0.0035 - val_accuracy: 0.7580\n",
            "Epoch 71/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 0.0015 - accuracy: 0.8860 - val_loss: 0.0034 - val_accuracy: 0.7635\n",
            "Epoch 72/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 0.0015 - accuracy: 0.8890 - val_loss: 0.0033 - val_accuracy: 0.7663\n",
            "Epoch 73/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 0.0014 - accuracy: 0.8925 - val_loss: 0.0034 - val_accuracy: 0.7665\n",
            "Epoch 74/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 0.0014 - accuracy: 0.8943 - val_loss: 0.0033 - val_accuracy: 0.7735\n",
            "Epoch 75/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 0.0014 - accuracy: 0.8982 - val_loss: 0.0032 - val_accuracy: 0.7728\n",
            "Epoch 76/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 0.0013 - accuracy: 0.9018 - val_loss: 0.0033 - val_accuracy: 0.7773\n",
            "Epoch 77/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 0.0013 - accuracy: 0.9049 - val_loss: 0.0033 - val_accuracy: 0.7717\n",
            "Epoch 78/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 0.0013 - accuracy: 0.9085 - val_loss: 0.0031 - val_accuracy: 0.7825\n",
            "Epoch 79/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 0.0012 - accuracy: 0.9117 - val_loss: 0.0031 - val_accuracy: 0.7813\n",
            "Epoch 80/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 0.0012 - accuracy: 0.9148 - val_loss: 0.0031 - val_accuracy: 0.7895\n",
            "Epoch 81/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 0.0011 - accuracy: 0.9174 - val_loss: 0.0030 - val_accuracy: 0.7880\n",
            "Epoch 82/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 0.0011 - accuracy: 0.9212 - val_loss: 0.0030 - val_accuracy: 0.7930\n",
            "Epoch 83/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 0.0010 - accuracy: 0.9234 - val_loss: 0.0029 - val_accuracy: 0.8002\n",
            "Epoch 84/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 0.0010 - accuracy: 0.9249 - val_loss: 0.0030 - val_accuracy: 0.7960\n",
            "Epoch 85/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 0.0010 - accuracy: 0.9252 - val_loss: 0.0028 - val_accuracy: 0.8068\n",
            "Epoch 86/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 9.7027e-04 - accuracy: 0.9272 - val_loss: 0.0029 - val_accuracy: 0.7972\n",
            "Epoch 87/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 9.4202e-04 - accuracy: 0.9288 - val_loss: 0.0028 - val_accuracy: 0.8103\n",
            "Epoch 88/1000\n",
            "24/24 [==============================] - 3s 132ms/step - loss: 9.1531e-04 - accuracy: 0.9303 - val_loss: 0.0028 - val_accuracy: 0.8092\n",
            "Epoch 89/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 8.9149e-04 - accuracy: 0.9323 - val_loss: 0.0028 - val_accuracy: 0.8050\n",
            "Epoch 90/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 8.8153e-04 - accuracy: 0.9332 - val_loss: 0.0028 - val_accuracy: 0.8092\n",
            "Epoch 91/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 8.6036e-04 - accuracy: 0.9336 - val_loss: 0.0027 - val_accuracy: 0.8135\n",
            "Epoch 92/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 8.5166e-04 - accuracy: 0.9351 - val_loss: 0.0027 - val_accuracy: 0.8135\n",
            "Epoch 93/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 8.3758e-04 - accuracy: 0.9357 - val_loss: 0.0028 - val_accuracy: 0.8078\n",
            "Epoch 94/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 8.3294e-04 - accuracy: 0.9366 - val_loss: 0.0027 - val_accuracy: 0.8150\n",
            "Epoch 95/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 8.1560e-04 - accuracy: 0.9372 - val_loss: 0.0027 - val_accuracy: 0.8167\n",
            "Epoch 96/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 7.9469e-04 - accuracy: 0.9386 - val_loss: 0.0026 - val_accuracy: 0.8237\n",
            "Epoch 97/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 7.8730e-04 - accuracy: 0.9388 - val_loss: 0.0026 - val_accuracy: 0.8172\n",
            "Epoch 98/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 7.6989e-04 - accuracy: 0.9405 - val_loss: 0.0026 - val_accuracy: 0.8225\n",
            "Epoch 99/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 7.6169e-04 - accuracy: 0.9413 - val_loss: 0.0027 - val_accuracy: 0.8160\n",
            "Epoch 100/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 7.4987e-04 - accuracy: 0.9423 - val_loss: 0.0026 - val_accuracy: 0.8200\n",
            "Epoch 101/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 7.3807e-04 - accuracy: 0.9432 - val_loss: 0.0026 - val_accuracy: 0.8195\n",
            "Epoch 102/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 7.2834e-04 - accuracy: 0.9446 - val_loss: 0.0026 - val_accuracy: 0.8218\n",
            "Epoch 103/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 7.0072e-04 - accuracy: 0.9461 - val_loss: 0.0025 - val_accuracy: 0.8257\n",
            "Epoch 104/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 6.9175e-04 - accuracy: 0.9469 - val_loss: 0.0025 - val_accuracy: 0.8282\n",
            "Epoch 105/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 6.7817e-04 - accuracy: 0.9476 - val_loss: 0.0025 - val_accuracy: 0.8288\n",
            "Epoch 106/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 6.7598e-04 - accuracy: 0.9482 - val_loss: 0.0025 - val_accuracy: 0.8253\n",
            "Epoch 107/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 6.8446e-04 - accuracy: 0.9483 - val_loss: 0.0025 - val_accuracy: 0.8272\n",
            "Epoch 108/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 6.5655e-04 - accuracy: 0.9495 - val_loss: 0.0025 - val_accuracy: 0.8258\n",
            "Epoch 109/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 6.3803e-04 - accuracy: 0.9509 - val_loss: 0.0025 - val_accuracy: 0.8302\n",
            "Epoch 110/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 6.2387e-04 - accuracy: 0.9514 - val_loss: 0.0025 - val_accuracy: 0.8288\n",
            "Epoch 111/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 6.1872e-04 - accuracy: 0.9521 - val_loss: 0.0025 - val_accuracy: 0.8270\n",
            "Epoch 112/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 6.1225e-04 - accuracy: 0.9523 - val_loss: 0.0025 - val_accuracy: 0.8273\n",
            "Epoch 113/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 6.0618e-04 - accuracy: 0.9525 - val_loss: 0.0024 - val_accuracy: 0.8325\n",
            "Epoch 114/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 5.9301e-04 - accuracy: 0.9531 - val_loss: 0.0024 - val_accuracy: 0.8348\n",
            "Epoch 115/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 5.9891e-04 - accuracy: 0.9532 - val_loss: 0.0024 - val_accuracy: 0.8332\n",
            "Epoch 116/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 5.8547e-04 - accuracy: 0.9538 - val_loss: 0.0024 - val_accuracy: 0.8393\n",
            "Epoch 117/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 6.0886e-04 - accuracy: 0.9526 - val_loss: 0.0026 - val_accuracy: 0.8242\n",
            "Epoch 118/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 6.0178e-04 - accuracy: 0.9532 - val_loss: 0.0024 - val_accuracy: 0.8385\n",
            "Epoch 119/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 5.8404e-04 - accuracy: 0.9539 - val_loss: 0.0024 - val_accuracy: 0.8380\n",
            "Epoch 120/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 5.7808e-04 - accuracy: 0.9541 - val_loss: 0.0024 - val_accuracy: 0.8357\n",
            "Epoch 121/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 5.7564e-04 - accuracy: 0.9544 - val_loss: 0.0024 - val_accuracy: 0.8363\n",
            "Epoch 122/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 5.6816e-04 - accuracy: 0.9549 - val_loss: 0.0024 - val_accuracy: 0.8375\n",
            "Epoch 123/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 5.7019e-04 - accuracy: 0.9547 - val_loss: 0.0024 - val_accuracy: 0.8333\n",
            "Epoch 124/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 5.6462e-04 - accuracy: 0.9551 - val_loss: 0.0024 - val_accuracy: 0.8402\n",
            "Epoch 125/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 5.5903e-04 - accuracy: 0.9557 - val_loss: 0.0024 - val_accuracy: 0.8385\n",
            "Epoch 126/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 5.7384e-04 - accuracy: 0.9557 - val_loss: 0.0024 - val_accuracy: 0.8380\n",
            "Epoch 127/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 5.6886e-04 - accuracy: 0.9562 - val_loss: 0.0024 - val_accuracy: 0.8405\n",
            "Epoch 128/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 5.9662e-04 - accuracy: 0.9558 - val_loss: 0.0025 - val_accuracy: 0.8310\n",
            "Epoch 129/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 6.0863e-04 - accuracy: 0.9549 - val_loss: 0.0025 - val_accuracy: 0.8312\n",
            "Epoch 130/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 6.3883e-04 - accuracy: 0.9531 - val_loss: 0.0025 - val_accuracy: 0.8305\n",
            "Epoch 131/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 6.5664e-04 - accuracy: 0.9523 - val_loss: 0.0025 - val_accuracy: 0.8293\n",
            "Epoch 132/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 6.3602e-04 - accuracy: 0.9539 - val_loss: 0.0025 - val_accuracy: 0.8287\n",
            "Epoch 133/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 6.4747e-04 - accuracy: 0.9537 - val_loss: 0.0025 - val_accuracy: 0.8320\n",
            "Epoch 134/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 5.9775e-04 - accuracy: 0.9557 - val_loss: 0.0024 - val_accuracy: 0.8388\n",
            "Epoch 135/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 5.5946e-04 - accuracy: 0.9582 - val_loss: 0.0023 - val_accuracy: 0.8428\n",
            "Epoch 136/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 5.4366e-04 - accuracy: 0.9587 - val_loss: 0.0024 - val_accuracy: 0.8372\n",
            "Epoch 137/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 5.4733e-04 - accuracy: 0.9588 - val_loss: 0.0023 - val_accuracy: 0.8453\n",
            "Epoch 138/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 5.2274e-04 - accuracy: 0.9598 - val_loss: 0.0023 - val_accuracy: 0.8465\n",
            "Epoch 139/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 5.1355e-04 - accuracy: 0.9600 - val_loss: 0.0023 - val_accuracy: 0.8478\n",
            "Epoch 140/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 4.9498e-04 - accuracy: 0.9611 - val_loss: 0.0023 - val_accuracy: 0.8503\n",
            "Epoch 141/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 4.8114e-04 - accuracy: 0.9620 - val_loss: 0.0022 - val_accuracy: 0.8517\n",
            "Epoch 142/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 4.7357e-04 - accuracy: 0.9632 - val_loss: 0.0022 - val_accuracy: 0.8523\n",
            "Epoch 143/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 4.5709e-04 - accuracy: 0.9640 - val_loss: 0.0022 - val_accuracy: 0.8527\n",
            "Epoch 144/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 4.5195e-04 - accuracy: 0.9644 - val_loss: 0.0022 - val_accuracy: 0.8518\n",
            "Epoch 145/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 4.5528e-04 - accuracy: 0.9646 - val_loss: 0.0022 - val_accuracy: 0.8508\n",
            "Epoch 146/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 4.5072e-04 - accuracy: 0.9648 - val_loss: 0.0022 - val_accuracy: 0.8518\n",
            "Epoch 147/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 4.3816e-04 - accuracy: 0.9657 - val_loss: 0.0022 - val_accuracy: 0.8513\n",
            "Epoch 148/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 4.2891e-04 - accuracy: 0.9660 - val_loss: 0.0022 - val_accuracy: 0.8530\n",
            "Epoch 149/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 4.2949e-04 - accuracy: 0.9664 - val_loss: 0.0022 - val_accuracy: 0.8510\n",
            "Epoch 150/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 4.1681e-04 - accuracy: 0.9670 - val_loss: 0.0022 - val_accuracy: 0.8550\n",
            "Epoch 151/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 4.0525e-04 - accuracy: 0.9674 - val_loss: 0.0021 - val_accuracy: 0.8575\n",
            "Epoch 152/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 4.0503e-04 - accuracy: 0.9675 - val_loss: 0.0022 - val_accuracy: 0.8553\n",
            "Epoch 153/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 4.0446e-04 - accuracy: 0.9676 - val_loss: 0.0021 - val_accuracy: 0.8592\n",
            "Epoch 154/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 4.0579e-04 - accuracy: 0.9676 - val_loss: 0.0021 - val_accuracy: 0.8573\n",
            "Epoch 155/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 4.0314e-04 - accuracy: 0.9678 - val_loss: 0.0021 - val_accuracy: 0.8595\n",
            "Epoch 156/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.9551e-04 - accuracy: 0.9678 - val_loss: 0.0021 - val_accuracy: 0.8595\n",
            "Epoch 157/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.9576e-04 - accuracy: 0.9679 - val_loss: 0.0021 - val_accuracy: 0.8630\n",
            "Epoch 158/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.9569e-04 - accuracy: 0.9680 - val_loss: 0.0020 - val_accuracy: 0.8618\n",
            "Epoch 159/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.8795e-04 - accuracy: 0.9682 - val_loss: 0.0021 - val_accuracy: 0.8585\n",
            "Epoch 160/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.8689e-04 - accuracy: 0.9685 - val_loss: 0.0021 - val_accuracy: 0.8575\n",
            "Epoch 161/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.8897e-04 - accuracy: 0.9685 - val_loss: 0.0021 - val_accuracy: 0.8588\n",
            "Epoch 162/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.9154e-04 - accuracy: 0.9683 - val_loss: 0.0022 - val_accuracy: 0.8550\n",
            "Epoch 163/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.8948e-04 - accuracy: 0.9687 - val_loss: 0.0020 - val_accuracy: 0.8633\n",
            "Epoch 164/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.9139e-04 - accuracy: 0.9688 - val_loss: 0.0021 - val_accuracy: 0.8642\n",
            "Epoch 165/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.8367e-04 - accuracy: 0.9692 - val_loss: 0.0021 - val_accuracy: 0.8590\n",
            "Epoch 166/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.8979e-04 - accuracy: 0.9690 - val_loss: 0.0021 - val_accuracy: 0.8603\n",
            "Epoch 167/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.8766e-04 - accuracy: 0.9694 - val_loss: 0.0022 - val_accuracy: 0.8568\n",
            "Epoch 168/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.7871e-04 - accuracy: 0.9698 - val_loss: 0.0021 - val_accuracy: 0.8602\n",
            "Epoch 169/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.8464e-04 - accuracy: 0.9697 - val_loss: 0.0022 - val_accuracy: 0.8517\n",
            "Epoch 170/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.7265e-04 - accuracy: 0.9706 - val_loss: 0.0021 - val_accuracy: 0.8585\n",
            "Epoch 171/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.6045e-04 - accuracy: 0.9713 - val_loss: 0.0021 - val_accuracy: 0.8577\n",
            "Epoch 172/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.6307e-04 - accuracy: 0.9715 - val_loss: 0.0021 - val_accuracy: 0.8615\n",
            "Epoch 173/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.8149e-04 - accuracy: 0.9709 - val_loss: 0.0022 - val_accuracy: 0.8507\n",
            "Epoch 174/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 4.0059e-04 - accuracy: 0.9700 - val_loss: 0.0022 - val_accuracy: 0.8565\n",
            "Epoch 175/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 4.0321e-04 - accuracy: 0.9703 - val_loss: 0.0023 - val_accuracy: 0.8527\n",
            "Epoch 176/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 4.3101e-04 - accuracy: 0.9682 - val_loss: 0.0023 - val_accuracy: 0.8457\n",
            "Epoch 177/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 4.4747e-04 - accuracy: 0.9671 - val_loss: 0.0023 - val_accuracy: 0.8495\n",
            "Epoch 178/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 4.8301e-04 - accuracy: 0.9653 - val_loss: 0.0023 - val_accuracy: 0.8505\n",
            "Epoch 179/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 5.2004e-04 - accuracy: 0.9634 - val_loss: 0.0023 - val_accuracy: 0.8485\n",
            "Epoch 180/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 5.1266e-04 - accuracy: 0.9639 - val_loss: 0.0024 - val_accuracy: 0.8460\n",
            "Epoch 181/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 4.6769e-04 - accuracy: 0.9671 - val_loss: 0.0023 - val_accuracy: 0.8545\n",
            "Epoch 182/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 4.4030e-04 - accuracy: 0.9681 - val_loss: 0.0023 - val_accuracy: 0.8525\n",
            "Epoch 183/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 4.4912e-04 - accuracy: 0.9679 - val_loss: 0.0022 - val_accuracy: 0.8580\n",
            "Epoch 184/1000\n",
            "24/24 [==============================] - 3s 133ms/step - loss: 4.6229e-04 - accuracy: 0.9668 - val_loss: 0.0023 - val_accuracy: 0.8482\n",
            "Epoch 185/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 4.9334e-04 - accuracy: 0.9652 - val_loss: 0.0023 - val_accuracy: 0.8502\n",
            "Epoch 186/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 4.7780e-04 - accuracy: 0.9661 - val_loss: 0.0022 - val_accuracy: 0.8555\n",
            "Epoch 187/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 4.5833e-04 - accuracy: 0.9674 - val_loss: 0.0024 - val_accuracy: 0.8450\n",
            "Epoch 188/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 4.1229e-04 - accuracy: 0.9700 - val_loss: 0.0021 - val_accuracy: 0.8623\n",
            "Epoch 189/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.9557e-04 - accuracy: 0.9711 - val_loss: 0.0022 - val_accuracy: 0.8587\n",
            "Epoch 190/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.7570e-04 - accuracy: 0.9721 - val_loss: 0.0021 - val_accuracy: 0.8640\n",
            "Epoch 191/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.8066e-04 - accuracy: 0.9714 - val_loss: 0.0022 - val_accuracy: 0.8605\n",
            "Epoch 192/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.8528e-04 - accuracy: 0.9715 - val_loss: 0.0021 - val_accuracy: 0.8670\n",
            "Epoch 193/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.6723e-04 - accuracy: 0.9724 - val_loss: 0.0021 - val_accuracy: 0.8652\n",
            "Epoch 194/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.6472e-04 - accuracy: 0.9725 - val_loss: 0.0021 - val_accuracy: 0.8650\n",
            "Epoch 195/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.5271e-04 - accuracy: 0.9729 - val_loss: 0.0020 - val_accuracy: 0.8695\n",
            "Epoch 196/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.4345e-04 - accuracy: 0.9732 - val_loss: 0.0019 - val_accuracy: 0.8747\n",
            "Epoch 197/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.4196e-04 - accuracy: 0.9734 - val_loss: 0.0019 - val_accuracy: 0.8747\n",
            "Epoch 198/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.4249e-04 - accuracy: 0.9733 - val_loss: 0.0020 - val_accuracy: 0.8680\n",
            "Epoch 199/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.3928e-04 - accuracy: 0.9734 - val_loss: 0.0020 - val_accuracy: 0.8662\n",
            "Epoch 200/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.3888e-04 - accuracy: 0.9732 - val_loss: 0.0019 - val_accuracy: 0.8712\n",
            "Epoch 201/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.3578e-04 - accuracy: 0.9735 - val_loss: 0.0020 - val_accuracy: 0.8712\n",
            "Epoch 202/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.3149e-04 - accuracy: 0.9737 - val_loss: 0.0019 - val_accuracy: 0.8715\n",
            "Epoch 203/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.3038e-04 - accuracy: 0.9736 - val_loss: 0.0020 - val_accuracy: 0.8693\n",
            "Epoch 204/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 3.2675e-04 - accuracy: 0.9737 - val_loss: 0.0019 - val_accuracy: 0.8750\n",
            "Epoch 205/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.2398e-04 - accuracy: 0.9738 - val_loss: 0.0019 - val_accuracy: 0.8747\n",
            "Epoch 206/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.2245e-04 - accuracy: 0.9739 - val_loss: 0.0019 - val_accuracy: 0.8727\n",
            "Epoch 207/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.2040e-04 - accuracy: 0.9739 - val_loss: 0.0019 - val_accuracy: 0.8745\n",
            "Epoch 208/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.1994e-04 - accuracy: 0.9740 - val_loss: 0.0019 - val_accuracy: 0.8740\n",
            "Epoch 209/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.1786e-04 - accuracy: 0.9740 - val_loss: 0.0019 - val_accuracy: 0.8750\n",
            "Epoch 210/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.1932e-04 - accuracy: 0.9740 - val_loss: 0.0019 - val_accuracy: 0.8742\n",
            "Epoch 211/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.1558e-04 - accuracy: 0.9741 - val_loss: 0.0019 - val_accuracy: 0.8715\n",
            "Epoch 212/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.1653e-04 - accuracy: 0.9740 - val_loss: 0.0019 - val_accuracy: 0.8728\n",
            "Epoch 213/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.1664e-04 - accuracy: 0.9741 - val_loss: 0.0019 - val_accuracy: 0.8773\n",
            "Epoch 214/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.1390e-04 - accuracy: 0.9742 - val_loss: 0.0019 - val_accuracy: 0.8723\n",
            "Epoch 215/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.1540e-04 - accuracy: 0.9742 - val_loss: 0.0019 - val_accuracy: 0.8753\n",
            "Epoch 216/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.1054e-04 - accuracy: 0.9743 - val_loss: 0.0019 - val_accuracy: 0.8727\n",
            "Epoch 217/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.1144e-04 - accuracy: 0.9743 - val_loss: 0.0019 - val_accuracy: 0.8793\n",
            "Epoch 218/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.0791e-04 - accuracy: 0.9745 - val_loss: 0.0019 - val_accuracy: 0.8737\n",
            "Epoch 219/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.0814e-04 - accuracy: 0.9745 - val_loss: 0.0019 - val_accuracy: 0.8773\n",
            "Epoch 220/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.0649e-04 - accuracy: 0.9747 - val_loss: 0.0019 - val_accuracy: 0.8715\n",
            "Epoch 221/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.0524e-04 - accuracy: 0.9749 - val_loss: 0.0019 - val_accuracy: 0.8747\n",
            "Epoch 222/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.0233e-04 - accuracy: 0.9750 - val_loss: 0.0019 - val_accuracy: 0.8717\n",
            "Epoch 223/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.0240e-04 - accuracy: 0.9750 - val_loss: 0.0018 - val_accuracy: 0.8780\n",
            "Epoch 224/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.9943e-04 - accuracy: 0.9752 - val_loss: 0.0018 - val_accuracy: 0.8797\n",
            "Epoch 225/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.0130e-04 - accuracy: 0.9751 - val_loss: 0.0019 - val_accuracy: 0.8748\n",
            "Epoch 226/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.0502e-04 - accuracy: 0.9750 - val_loss: 0.0019 - val_accuracy: 0.8747\n",
            "Epoch 227/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.0219e-04 - accuracy: 0.9751 - val_loss: 0.0019 - val_accuracy: 0.8752\n",
            "Epoch 228/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.0376e-04 - accuracy: 0.9753 - val_loss: 0.0019 - val_accuracy: 0.8775\n",
            "Epoch 229/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.0213e-04 - accuracy: 0.9753 - val_loss: 0.0018 - val_accuracy: 0.8795\n",
            "Epoch 230/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.0263e-04 - accuracy: 0.9752 - val_loss: 0.0019 - val_accuracy: 0.8762\n",
            "Epoch 231/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.0633e-04 - accuracy: 0.9751 - val_loss: 0.0019 - val_accuracy: 0.8768\n",
            "Epoch 232/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.3599e-04 - accuracy: 0.9737 - val_loss: 0.0021 - val_accuracy: 0.8657\n",
            "Epoch 233/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.5444e-04 - accuracy: 0.9728 - val_loss: 0.0022 - val_accuracy: 0.8553\n",
            "Epoch 234/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 0.0012 - accuracy: 0.9196 - val_loss: 0.0030 - val_accuracy: 0.8090\n",
            "Epoch 235/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 8.1692e-04 - accuracy: 0.9443 - val_loss: 0.0025 - val_accuracy: 0.8477\n",
            "Epoch 236/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 5.9714e-04 - accuracy: 0.9583 - val_loss: 0.0022 - val_accuracy: 0.8563\n",
            "Epoch 237/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 4.5486e-04 - accuracy: 0.9681 - val_loss: 0.0022 - val_accuracy: 0.8593\n",
            "Epoch 238/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 4.1245e-04 - accuracy: 0.9708 - val_loss: 0.0020 - val_accuracy: 0.8693\n",
            "Epoch 239/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.8876e-04 - accuracy: 0.9720 - val_loss: 0.0020 - val_accuracy: 0.8740\n",
            "Epoch 240/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.4652e-04 - accuracy: 0.9740 - val_loss: 0.0019 - val_accuracy: 0.8790\n",
            "Epoch 241/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.3492e-04 - accuracy: 0.9747 - val_loss: 0.0020 - val_accuracy: 0.8730\n",
            "Epoch 242/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.3242e-04 - accuracy: 0.9746 - val_loss: 0.0019 - val_accuracy: 0.8783\n",
            "Epoch 243/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.1887e-04 - accuracy: 0.9754 - val_loss: 0.0018 - val_accuracy: 0.8833\n",
            "Epoch 244/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.1522e-04 - accuracy: 0.9754 - val_loss: 0.0018 - val_accuracy: 0.8822\n",
            "Epoch 245/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.1398e-04 - accuracy: 0.9754 - val_loss: 0.0019 - val_accuracy: 0.8793\n",
            "Epoch 246/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.1302e-04 - accuracy: 0.9753 - val_loss: 0.0018 - val_accuracy: 0.8823\n",
            "Epoch 247/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.1013e-04 - accuracy: 0.9755 - val_loss: 0.0018 - val_accuracy: 0.8862\n",
            "Epoch 248/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.0031e-04 - accuracy: 0.9761 - val_loss: 0.0018 - val_accuracy: 0.8822\n",
            "Epoch 249/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.9669e-04 - accuracy: 0.9765 - val_loss: 0.0018 - val_accuracy: 0.8860\n",
            "Epoch 250/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 2.8933e-04 - accuracy: 0.9770 - val_loss: 0.0018 - val_accuracy: 0.8818\n",
            "Epoch 251/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.8600e-04 - accuracy: 0.9770 - val_loss: 0.0017 - val_accuracy: 0.8883\n",
            "Epoch 252/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.8523e-04 - accuracy: 0.9772 - val_loss: 0.0018 - val_accuracy: 0.8847\n",
            "Epoch 253/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.8783e-04 - accuracy: 0.9770 - val_loss: 0.0018 - val_accuracy: 0.8867\n",
            "Epoch 254/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.8440e-04 - accuracy: 0.9770 - val_loss: 0.0018 - val_accuracy: 0.8848\n",
            "Epoch 255/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.8315e-04 - accuracy: 0.9771 - val_loss: 0.0018 - val_accuracy: 0.8850\n",
            "Epoch 256/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.8116e-04 - accuracy: 0.9771 - val_loss: 0.0018 - val_accuracy: 0.8865\n",
            "Epoch 257/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.7533e-04 - accuracy: 0.9775 - val_loss: 0.0017 - val_accuracy: 0.8865\n",
            "Epoch 258/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.7277e-04 - accuracy: 0.9775 - val_loss: 0.0018 - val_accuracy: 0.8818\n",
            "Epoch 259/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.7316e-04 - accuracy: 0.9775 - val_loss: 0.0017 - val_accuracy: 0.8858\n",
            "Epoch 260/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.6985e-04 - accuracy: 0.9777 - val_loss: 0.0017 - val_accuracy: 0.8870\n",
            "Epoch 261/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.7094e-04 - accuracy: 0.9778 - val_loss: 0.0018 - val_accuracy: 0.8838\n",
            "Epoch 262/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.6894e-04 - accuracy: 0.9778 - val_loss: 0.0018 - val_accuracy: 0.8865\n",
            "Epoch 263/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.6610e-04 - accuracy: 0.9779 - val_loss: 0.0018 - val_accuracy: 0.8860\n",
            "Epoch 264/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 2.6386e-04 - accuracy: 0.9779 - val_loss: 0.0017 - val_accuracy: 0.8863\n",
            "Epoch 265/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.6523e-04 - accuracy: 0.9779 - val_loss: 0.0017 - val_accuracy: 0.8912\n",
            "Epoch 266/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.6558e-04 - accuracy: 0.9779 - val_loss: 0.0017 - val_accuracy: 0.8867\n",
            "Epoch 267/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.6357e-04 - accuracy: 0.9779 - val_loss: 0.0017 - val_accuracy: 0.8888\n",
            "Epoch 268/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.6343e-04 - accuracy: 0.9780 - val_loss: 0.0017 - val_accuracy: 0.8853\n",
            "Epoch 269/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.6383e-04 - accuracy: 0.9780 - val_loss: 0.0017 - val_accuracy: 0.8893\n",
            "Epoch 270/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.6232e-04 - accuracy: 0.9781 - val_loss: 0.0017 - val_accuracy: 0.8877\n",
            "Epoch 271/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.6485e-04 - accuracy: 0.9780 - val_loss: 0.0017 - val_accuracy: 0.8867\n",
            "Epoch 272/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.6339e-04 - accuracy: 0.9780 - val_loss: 0.0017 - val_accuracy: 0.8867\n",
            "Epoch 273/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 2.6826e-04 - accuracy: 0.9778 - val_loss: 0.0018 - val_accuracy: 0.8805\n",
            "Epoch 274/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.6795e-04 - accuracy: 0.9779 - val_loss: 0.0018 - val_accuracy: 0.8850\n",
            "Epoch 275/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.6684e-04 - accuracy: 0.9780 - val_loss: 0.0018 - val_accuracy: 0.8825\n",
            "Epoch 276/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.7045e-04 - accuracy: 0.9778 - val_loss: 0.0018 - val_accuracy: 0.8807\n",
            "Epoch 277/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.6580e-04 - accuracy: 0.9781 - val_loss: 0.0018 - val_accuracy: 0.8855\n",
            "Epoch 278/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.6386e-04 - accuracy: 0.9781 - val_loss: 0.0017 - val_accuracy: 0.8855\n",
            "Epoch 279/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.6310e-04 - accuracy: 0.9780 - val_loss: 0.0018 - val_accuracy: 0.8843\n",
            "Epoch 280/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 2.6564e-04 - accuracy: 0.9780 - val_loss: 0.0018 - val_accuracy: 0.8848\n",
            "Epoch 281/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.6353e-04 - accuracy: 0.9781 - val_loss: 0.0018 - val_accuracy: 0.8842\n",
            "Epoch 282/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.6463e-04 - accuracy: 0.9781 - val_loss: 0.0018 - val_accuracy: 0.8850\n",
            "Epoch 283/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.6481e-04 - accuracy: 0.9780 - val_loss: 0.0017 - val_accuracy: 0.8865\n",
            "Epoch 284/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.6224e-04 - accuracy: 0.9781 - val_loss: 0.0017 - val_accuracy: 0.8867\n",
            "Epoch 285/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.6329e-04 - accuracy: 0.9781 - val_loss: 0.0017 - val_accuracy: 0.8895\n",
            "Epoch 286/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.6759e-04 - accuracy: 0.9780 - val_loss: 0.0018 - val_accuracy: 0.8858\n",
            "Epoch 287/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.6938e-04 - accuracy: 0.9780 - val_loss: 0.0018 - val_accuracy: 0.8808\n",
            "Epoch 288/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.6816e-04 - accuracy: 0.9781 - val_loss: 0.0018 - val_accuracy: 0.8875\n",
            "Epoch 289/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 2.6777e-04 - accuracy: 0.9780 - val_loss: 0.0018 - val_accuracy: 0.8838\n",
            "Epoch 290/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.7586e-04 - accuracy: 0.9777 - val_loss: 0.0019 - val_accuracy: 0.8795\n",
            "Epoch 291/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.7992e-04 - accuracy: 0.9777 - val_loss: 0.0019 - val_accuracy: 0.8780\n",
            "Epoch 292/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.9285e-04 - accuracy: 0.9771 - val_loss: 0.0019 - val_accuracy: 0.8770\n",
            "Epoch 293/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.9571e-04 - accuracy: 0.9769 - val_loss: 0.0019 - val_accuracy: 0.8780\n",
            "Epoch 294/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.0676e-04 - accuracy: 0.9766 - val_loss: 0.0020 - val_accuracy: 0.8737\n",
            "Epoch 295/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.1827e-04 - accuracy: 0.9756 - val_loss: 0.0019 - val_accuracy: 0.8753\n",
            "Epoch 296/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.5053e-04 - accuracy: 0.9744 - val_loss: 0.0021 - val_accuracy: 0.8692\n",
            "Epoch 297/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.6082e-04 - accuracy: 0.9735 - val_loss: 0.0021 - val_accuracy: 0.8632\n",
            "Epoch 298/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.9181e-04 - accuracy: 0.9720 - val_loss: 0.0020 - val_accuracy: 0.8725\n",
            "Epoch 299/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.8021e-04 - accuracy: 0.9724 - val_loss: 0.0020 - val_accuracy: 0.8713\n",
            "Epoch 300/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.8508e-04 - accuracy: 0.9723 - val_loss: 0.0020 - val_accuracy: 0.8767\n",
            "Epoch 301/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.6561e-04 - accuracy: 0.9740 - val_loss: 0.0020 - val_accuracy: 0.8782\n",
            "Epoch 302/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.4972e-04 - accuracy: 0.9751 - val_loss: 0.0020 - val_accuracy: 0.8758\n",
            "Epoch 303/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.2614e-04 - accuracy: 0.9760 - val_loss: 0.0020 - val_accuracy: 0.8757\n",
            "Epoch 304/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.5923e-04 - accuracy: 0.9743 - val_loss: 0.0020 - val_accuracy: 0.8710\n",
            "Epoch 305/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.5599e-04 - accuracy: 0.9747 - val_loss: 0.0018 - val_accuracy: 0.8827\n",
            "Epoch 306/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.4093e-04 - accuracy: 0.9753 - val_loss: 0.0019 - val_accuracy: 0.8795\n",
            "Epoch 307/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.5507e-04 - accuracy: 0.9740 - val_loss: 0.0020 - val_accuracy: 0.8707\n",
            "Epoch 308/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 4.0144e-04 - accuracy: 0.9712 - val_loss: 0.0022 - val_accuracy: 0.8648\n",
            "Epoch 309/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 4.6743e-04 - accuracy: 0.9672 - val_loss: 0.0021 - val_accuracy: 0.8685\n",
            "Epoch 310/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 4.1910e-04 - accuracy: 0.9707 - val_loss: 0.0019 - val_accuracy: 0.8797\n",
            "Epoch 311/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.5589e-04 - accuracy: 0.9747 - val_loss: 0.0019 - val_accuracy: 0.8835\n",
            "Epoch 312/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.2794e-04 - accuracy: 0.9762 - val_loss: 0.0018 - val_accuracy: 0.8843\n",
            "Epoch 313/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.1117e-04 - accuracy: 0.9775 - val_loss: 0.0017 - val_accuracy: 0.8900\n",
            "Epoch 314/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.8831e-04 - accuracy: 0.9785 - val_loss: 0.0018 - val_accuracy: 0.8827\n",
            "Epoch 315/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.8312e-04 - accuracy: 0.9788 - val_loss: 0.0017 - val_accuracy: 0.8888\n",
            "Epoch 316/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 2.7132e-04 - accuracy: 0.9789 - val_loss: 0.0017 - val_accuracy: 0.8920\n",
            "Epoch 317/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 2.7658e-04 - accuracy: 0.9785 - val_loss: 0.0017 - val_accuracy: 0.8948\n",
            "Epoch 318/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.6419e-04 - accuracy: 0.9796 - val_loss: 0.0017 - val_accuracy: 0.8908\n",
            "Epoch 319/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.5540e-04 - accuracy: 0.9799 - val_loss: 0.0017 - val_accuracy: 0.8958\n",
            "Epoch 320/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.5484e-04 - accuracy: 0.9799 - val_loss: 0.0016 - val_accuracy: 0.8958\n",
            "Epoch 321/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.4862e-04 - accuracy: 0.9801 - val_loss: 0.0017 - val_accuracy: 0.8948\n",
            "Epoch 322/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.5135e-04 - accuracy: 0.9798 - val_loss: 0.0017 - val_accuracy: 0.8922\n",
            "Epoch 323/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.4763e-04 - accuracy: 0.9800 - val_loss: 0.0016 - val_accuracy: 0.8960\n",
            "Epoch 324/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.4410e-04 - accuracy: 0.9802 - val_loss: 0.0016 - val_accuracy: 0.8992\n",
            "Epoch 325/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.4092e-04 - accuracy: 0.9803 - val_loss: 0.0015 - val_accuracy: 0.9023\n",
            "Epoch 326/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.4340e-04 - accuracy: 0.9802 - val_loss: 0.0015 - val_accuracy: 0.9022\n",
            "Epoch 327/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.3905e-04 - accuracy: 0.9804 - val_loss: 0.0016 - val_accuracy: 0.8998\n",
            "Epoch 328/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.3729e-04 - accuracy: 0.9804 - val_loss: 0.0016 - val_accuracy: 0.8998\n",
            "Epoch 329/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.3797e-04 - accuracy: 0.9804 - val_loss: 0.0016 - val_accuracy: 0.9003\n",
            "Epoch 330/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.4165e-04 - accuracy: 0.9803 - val_loss: 0.0016 - val_accuracy: 0.8987\n",
            "Epoch 331/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.4153e-04 - accuracy: 0.9803 - val_loss: 0.0016 - val_accuracy: 0.9015\n",
            "Epoch 332/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.4261e-04 - accuracy: 0.9802 - val_loss: 0.0016 - val_accuracy: 0.8973\n",
            "Epoch 333/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.4395e-04 - accuracy: 0.9802 - val_loss: 0.0016 - val_accuracy: 0.8973\n",
            "Epoch 334/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.3995e-04 - accuracy: 0.9803 - val_loss: 0.0016 - val_accuracy: 0.8995\n",
            "Epoch 335/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.3648e-04 - accuracy: 0.9804 - val_loss: 0.0016 - val_accuracy: 0.8993\n",
            "Epoch 336/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.3576e-04 - accuracy: 0.9804 - val_loss: 0.0016 - val_accuracy: 0.8987\n",
            "Epoch 337/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.3728e-04 - accuracy: 0.9804 - val_loss: 0.0016 - val_accuracy: 0.8993\n",
            "Epoch 338/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.3703e-04 - accuracy: 0.9803 - val_loss: 0.0016 - val_accuracy: 0.8950\n",
            "Epoch 339/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.3829e-04 - accuracy: 0.9803 - val_loss: 0.0016 - val_accuracy: 0.8982\n",
            "Epoch 340/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.3817e-04 - accuracy: 0.9803 - val_loss: 0.0016 - val_accuracy: 0.8992\n",
            "Epoch 341/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.3987e-04 - accuracy: 0.9801 - val_loss: 0.0016 - val_accuracy: 0.8978\n",
            "Epoch 342/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.3527e-04 - accuracy: 0.9804 - val_loss: 0.0016 - val_accuracy: 0.8978\n",
            "Epoch 343/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.3623e-04 - accuracy: 0.9804 - val_loss: 0.0016 - val_accuracy: 0.8967\n",
            "Epoch 344/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.3258e-04 - accuracy: 0.9805 - val_loss: 0.0016 - val_accuracy: 0.8995\n",
            "Epoch 345/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.3181e-04 - accuracy: 0.9804 - val_loss: 0.0016 - val_accuracy: 0.8972\n",
            "Epoch 346/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.3254e-04 - accuracy: 0.9804 - val_loss: 0.0016 - val_accuracy: 0.8958\n",
            "Epoch 347/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.3180e-04 - accuracy: 0.9804 - val_loss: 0.0016 - val_accuracy: 0.8990\n",
            "Epoch 348/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.3450e-04 - accuracy: 0.9803 - val_loss: 0.0016 - val_accuracy: 0.8973\n",
            "Epoch 349/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.3276e-04 - accuracy: 0.9804 - val_loss: 0.0016 - val_accuracy: 0.8952\n",
            "Epoch 350/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.3390e-04 - accuracy: 0.9804 - val_loss: 0.0016 - val_accuracy: 0.8985\n",
            "Epoch 351/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.3209e-04 - accuracy: 0.9805 - val_loss: 0.0017 - val_accuracy: 0.8918\n",
            "Epoch 352/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.4150e-04 - accuracy: 0.9803 - val_loss: 0.0016 - val_accuracy: 0.8968\n",
            "Epoch 353/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.4844e-04 - accuracy: 0.9802 - val_loss: 0.0017 - val_accuracy: 0.8900\n",
            "Epoch 354/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.4146e-04 - accuracy: 0.9805 - val_loss: 0.0017 - val_accuracy: 0.8938\n",
            "Epoch 355/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.3764e-04 - accuracy: 0.9806 - val_loss: 0.0018 - val_accuracy: 0.8823\n",
            "Epoch 356/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.4749e-04 - accuracy: 0.9800 - val_loss: 0.0017 - val_accuracy: 0.8890\n",
            "Epoch 357/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.4422e-04 - accuracy: 0.9803 - val_loss: 0.0017 - val_accuracy: 0.8902\n",
            "Epoch 358/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 2.4139e-04 - accuracy: 0.9806 - val_loss: 0.0017 - val_accuracy: 0.8940\n",
            "Epoch 359/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.4069e-04 - accuracy: 0.9805 - val_loss: 0.0016 - val_accuracy: 0.8987\n",
            "Epoch 360/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.4140e-04 - accuracy: 0.9804 - val_loss: 0.0016 - val_accuracy: 0.8995\n",
            "Epoch 361/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.5643e-04 - accuracy: 0.9801 - val_loss: 0.0018 - val_accuracy: 0.8877\n",
            "Epoch 362/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.6340e-04 - accuracy: 0.9798 - val_loss: 0.0017 - val_accuracy: 0.8895\n",
            "Epoch 363/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.5881e-04 - accuracy: 0.9801 - val_loss: 0.0018 - val_accuracy: 0.8852\n",
            "Epoch 364/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.9620e-04 - accuracy: 0.9778 - val_loss: 0.0018 - val_accuracy: 0.8878\n",
            "Epoch 365/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.8627e-04 - accuracy: 0.9782 - val_loss: 0.0020 - val_accuracy: 0.8778\n",
            "Epoch 366/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.0671e-04 - accuracy: 0.9775 - val_loss: 0.0019 - val_accuracy: 0.8762\n",
            "Epoch 367/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.0805e-04 - accuracy: 0.9775 - val_loss: 0.0019 - val_accuracy: 0.8843\n",
            "Epoch 368/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.3941e-04 - accuracy: 0.9756 - val_loss: 0.0019 - val_accuracy: 0.8833\n",
            "Epoch 369/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.6180e-04 - accuracy: 0.9740 - val_loss: 0.0022 - val_accuracy: 0.8643\n",
            "Epoch 370/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.7582e-04 - accuracy: 0.9740 - val_loss: 0.0018 - val_accuracy: 0.8838\n",
            "Epoch 371/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 4.1377e-04 - accuracy: 0.9708 - val_loss: 0.0021 - val_accuracy: 0.8717\n",
            "Epoch 372/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.8776e-04 - accuracy: 0.9732 - val_loss: 0.0019 - val_accuracy: 0.8815\n",
            "Epoch 373/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.8680e-04 - accuracy: 0.9731 - val_loss: 0.0018 - val_accuracy: 0.8852\n",
            "Epoch 374/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.7727e-04 - accuracy: 0.9740 - val_loss: 0.0018 - val_accuracy: 0.8867\n",
            "Epoch 375/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.5024e-04 - accuracy: 0.9758 - val_loss: 0.0019 - val_accuracy: 0.8837\n",
            "Epoch 376/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.2307e-04 - accuracy: 0.9771 - val_loss: 0.0018 - val_accuracy: 0.8900\n",
            "Epoch 377/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.0432e-04 - accuracy: 0.9782 - val_loss: 0.0017 - val_accuracy: 0.8940\n",
            "Epoch 378/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.6973e-04 - accuracy: 0.9798 - val_loss: 0.0017 - val_accuracy: 0.8933\n",
            "Epoch 379/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.6384e-04 - accuracy: 0.9806 - val_loss: 0.0018 - val_accuracy: 0.8862\n",
            "Epoch 380/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.7182e-04 - accuracy: 0.9795 - val_loss: 0.0016 - val_accuracy: 0.9013\n",
            "Epoch 381/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.6234e-04 - accuracy: 0.9806 - val_loss: 0.0018 - val_accuracy: 0.8897\n",
            "Epoch 382/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.4141e-04 - accuracy: 0.9814 - val_loss: 0.0016 - val_accuracy: 0.9000\n",
            "Epoch 383/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.3804e-04 - accuracy: 0.9819 - val_loss: 0.0016 - val_accuracy: 0.8998\n",
            "Epoch 384/1000\n",
            "24/24 [==============================] - 3s 129ms/step - loss: 2.2203e-04 - accuracy: 0.9828 - val_loss: 0.0015 - val_accuracy: 0.9035\n",
            "Epoch 385/1000\n",
            "24/24 [==============================] - 3s 129ms/step - loss: 2.0968e-04 - accuracy: 0.9832 - val_loss: 0.0015 - val_accuracy: 0.9065\n",
            "Epoch 386/1000\n",
            "24/24 [==============================] - 3s 129ms/step - loss: 2.0873e-04 - accuracy: 0.9834 - val_loss: 0.0015 - val_accuracy: 0.9045\n",
            "Epoch 387/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.0567e-04 - accuracy: 0.9835 - val_loss: 0.0015 - val_accuracy: 0.9080\n",
            "Epoch 388/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.0678e-04 - accuracy: 0.9837 - val_loss: 0.0015 - val_accuracy: 0.9065\n",
            "Epoch 389/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.0314e-04 - accuracy: 0.9838 - val_loss: 0.0015 - val_accuracy: 0.9062\n",
            "Epoch 390/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.0606e-04 - accuracy: 0.9836 - val_loss: 0.0015 - val_accuracy: 0.9053\n",
            "Epoch 391/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.9902e-04 - accuracy: 0.9841 - val_loss: 0.0015 - val_accuracy: 0.9043\n",
            "Epoch 392/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.9373e-04 - accuracy: 0.9843 - val_loss: 0.0014 - val_accuracy: 0.9102\n",
            "Epoch 393/1000\n",
            "24/24 [==============================] - 3s 129ms/step - loss: 1.9003e-04 - accuracy: 0.9845 - val_loss: 0.0014 - val_accuracy: 0.9103\n",
            "Epoch 394/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.8232e-04 - accuracy: 0.9851 - val_loss: 0.0014 - val_accuracy: 0.9097\n",
            "Epoch 395/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.7976e-04 - accuracy: 0.9853 - val_loss: 0.0014 - val_accuracy: 0.9097\n",
            "Epoch 396/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.7937e-04 - accuracy: 0.9854 - val_loss: 0.0015 - val_accuracy: 0.9067\n",
            "Epoch 397/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.7921e-04 - accuracy: 0.9853 - val_loss: 0.0014 - val_accuracy: 0.9102\n",
            "Epoch 398/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.7922e-04 - accuracy: 0.9853 - val_loss: 0.0014 - val_accuracy: 0.9083\n",
            "Epoch 399/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.7702e-04 - accuracy: 0.9854 - val_loss: 0.0014 - val_accuracy: 0.9085\n",
            "Epoch 400/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.7564e-04 - accuracy: 0.9854 - val_loss: 0.0015 - val_accuracy: 0.9078\n",
            "Epoch 401/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.6712e-04 - accuracy: 0.9862 - val_loss: 0.0015 - val_accuracy: 0.9092\n",
            "Epoch 402/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.6226e-04 - accuracy: 0.9868 - val_loss: 0.0015 - val_accuracy: 0.9087\n",
            "Epoch 403/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.6207e-04 - accuracy: 0.9866 - val_loss: 0.0015 - val_accuracy: 0.9078\n",
            "Epoch 404/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.7389e-04 - accuracy: 0.9861 - val_loss: 0.0015 - val_accuracy: 0.9043\n",
            "Epoch 405/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.7274e-04 - accuracy: 0.9865 - val_loss: 0.0015 - val_accuracy: 0.9057\n",
            "Epoch 406/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.6791e-04 - accuracy: 0.9867 - val_loss: 0.0016 - val_accuracy: 0.9007\n",
            "Epoch 407/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.7036e-04 - accuracy: 0.9864 - val_loss: 0.0016 - val_accuracy: 0.9003\n",
            "Epoch 408/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.7956e-04 - accuracy: 0.9859 - val_loss: 0.0015 - val_accuracy: 0.9080\n",
            "Epoch 409/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.7101e-04 - accuracy: 0.9865 - val_loss: 0.0016 - val_accuracy: 0.8995\n",
            "Epoch 410/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.7291e-04 - accuracy: 0.9863 - val_loss: 0.0015 - val_accuracy: 0.9043\n",
            "Epoch 411/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.6502e-04 - accuracy: 0.9868 - val_loss: 0.0014 - val_accuracy: 0.9093\n",
            "Epoch 412/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.6731e-04 - accuracy: 0.9865 - val_loss: 0.0015 - val_accuracy: 0.9032\n",
            "Epoch 413/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.6364e-04 - accuracy: 0.9868 - val_loss: 0.0016 - val_accuracy: 0.9020\n",
            "Epoch 414/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.7111e-04 - accuracy: 0.9862 - val_loss: 0.0016 - val_accuracy: 0.9030\n",
            "Epoch 415/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.6298e-04 - accuracy: 0.9868 - val_loss: 0.0015 - val_accuracy: 0.9060\n",
            "Epoch 416/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5700e-04 - accuracy: 0.9871 - val_loss: 0.0014 - val_accuracy: 0.9117\n",
            "Epoch 417/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5716e-04 - accuracy: 0.9872 - val_loss: 0.0016 - val_accuracy: 0.9023\n",
            "Epoch 418/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5705e-04 - accuracy: 0.9871 - val_loss: 0.0014 - val_accuracy: 0.9093\n",
            "Epoch 419/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5392e-04 - accuracy: 0.9872 - val_loss: 0.0014 - val_accuracy: 0.9133\n",
            "Epoch 420/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5320e-04 - accuracy: 0.9872 - val_loss: 0.0015 - val_accuracy: 0.9038\n",
            "Epoch 421/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.6079e-04 - accuracy: 0.9871 - val_loss: 0.0014 - val_accuracy: 0.9082\n",
            "Epoch 422/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5698e-04 - accuracy: 0.9872 - val_loss: 0.0015 - val_accuracy: 0.9062\n",
            "Epoch 423/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 1.5791e-04 - accuracy: 0.9871 - val_loss: 0.0014 - val_accuracy: 0.9118\n",
            "Epoch 424/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5584e-04 - accuracy: 0.9871 - val_loss: 0.0015 - val_accuracy: 0.9072\n",
            "Epoch 425/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.6215e-04 - accuracy: 0.9867 - val_loss: 0.0015 - val_accuracy: 0.9065\n",
            "Epoch 426/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.6568e-04 - accuracy: 0.9865 - val_loss: 0.0016 - val_accuracy: 0.8973\n",
            "Epoch 427/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5996e-04 - accuracy: 0.9869 - val_loss: 0.0015 - val_accuracy: 0.9070\n",
            "Epoch 428/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 1.6190e-04 - accuracy: 0.9868 - val_loss: 0.0014 - val_accuracy: 0.9078\n",
            "Epoch 429/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5939e-04 - accuracy: 0.9870 - val_loss: 0.0015 - val_accuracy: 0.9060\n",
            "Epoch 430/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5827e-04 - accuracy: 0.9870 - val_loss: 0.0015 - val_accuracy: 0.9047\n",
            "Epoch 431/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.6014e-04 - accuracy: 0.9870 - val_loss: 0.0015 - val_accuracy: 0.9082\n",
            "Epoch 432/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.6330e-04 - accuracy: 0.9868 - val_loss: 0.0015 - val_accuracy: 0.9042\n",
            "Epoch 433/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 1.6066e-04 - accuracy: 0.9870 - val_loss: 0.0015 - val_accuracy: 0.9055\n",
            "Epoch 434/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5457e-04 - accuracy: 0.9873 - val_loss: 0.0014 - val_accuracy: 0.9110\n",
            "Epoch 435/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 1.5469e-04 - accuracy: 0.9872 - val_loss: 0.0014 - val_accuracy: 0.9115\n",
            "Epoch 436/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 1.5659e-04 - accuracy: 0.9872 - val_loss: 0.0015 - val_accuracy: 0.9087\n",
            "Epoch 437/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5414e-04 - accuracy: 0.9872 - val_loss: 0.0014 - val_accuracy: 0.9120\n",
            "Epoch 438/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5341e-04 - accuracy: 0.9872 - val_loss: 0.0014 - val_accuracy: 0.9125\n",
            "Epoch 439/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.6184e-04 - accuracy: 0.9868 - val_loss: 0.0015 - val_accuracy: 0.9058\n",
            "Epoch 440/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.7976e-04 - accuracy: 0.9856 - val_loss: 0.0015 - val_accuracy: 0.9033\n",
            "Epoch 441/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.9539e-04 - accuracy: 0.9854 - val_loss: 0.0017 - val_accuracy: 0.8975\n",
            "Epoch 442/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.8271e-04 - accuracy: 0.9859 - val_loss: 0.0016 - val_accuracy: 0.9042\n",
            "Epoch 443/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.8325e-04 - accuracy: 0.9862 - val_loss: 0.0015 - val_accuracy: 0.9063\n",
            "Epoch 444/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.9732e-04 - accuracy: 0.9855 - val_loss: 0.0015 - val_accuracy: 0.9055\n",
            "Epoch 445/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 2.0517e-04 - accuracy: 0.9847 - val_loss: 0.0017 - val_accuracy: 0.8960\n",
            "Epoch 446/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 2.1063e-04 - accuracy: 0.9845 - val_loss: 0.0018 - val_accuracy: 0.8902\n",
            "Epoch 447/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 2.4244e-04 - accuracy: 0.9826 - val_loss: 0.0019 - val_accuracy: 0.8875\n",
            "Epoch 448/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 2.5058e-04 - accuracy: 0.9816 - val_loss: 0.0019 - val_accuracy: 0.8852\n",
            "Epoch 449/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 2.4932e-04 - accuracy: 0.9822 - val_loss: 0.0016 - val_accuracy: 0.9015\n",
            "Epoch 450/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.4240e-04 - accuracy: 0.9826 - val_loss: 0.0017 - val_accuracy: 0.8978\n",
            "Epoch 451/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.1458e-04 - accuracy: 0.9847 - val_loss: 0.0016 - val_accuracy: 0.9007\n",
            "Epoch 452/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 2.2553e-04 - accuracy: 0.9840 - val_loss: 0.0017 - val_accuracy: 0.8978\n",
            "Epoch 453/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 2.2018e-04 - accuracy: 0.9842 - val_loss: 0.0017 - val_accuracy: 0.8978\n",
            "Epoch 454/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.0969e-04 - accuracy: 0.9846 - val_loss: 0.0015 - val_accuracy: 0.9058\n",
            "Epoch 455/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.0578e-04 - accuracy: 0.9851 - val_loss: 0.0016 - val_accuracy: 0.9025\n",
            "Epoch 456/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.8643e-04 - accuracy: 0.9860 - val_loss: 0.0017 - val_accuracy: 0.8948\n",
            "Epoch 457/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.9216e-04 - accuracy: 0.9855 - val_loss: 0.0015 - val_accuracy: 0.9063\n",
            "Epoch 458/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.8364e-04 - accuracy: 0.9860 - val_loss: 0.0015 - val_accuracy: 0.9080\n",
            "Epoch 459/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.7437e-04 - accuracy: 0.9868 - val_loss: 0.0015 - val_accuracy: 0.9098\n",
            "Epoch 460/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.8029e-04 - accuracy: 0.9862 - val_loss: 0.0014 - val_accuracy: 0.9125\n",
            "Epoch 461/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.0849e-04 - accuracy: 0.9843 - val_loss: 0.0019 - val_accuracy: 0.8838\n",
            "Epoch 462/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.8874e-04 - accuracy: 0.9799 - val_loss: 0.0017 - val_accuracy: 0.8987\n",
            "Epoch 463/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.2499e-04 - accuracy: 0.9836 - val_loss: 0.0017 - val_accuracy: 0.8980\n",
            "Epoch 464/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.2862e-04 - accuracy: 0.9833 - val_loss: 0.0016 - val_accuracy: 0.9043\n",
            "Epoch 465/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.2447e-04 - accuracy: 0.9833 - val_loss: 0.0016 - val_accuracy: 0.9020\n",
            "Epoch 466/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.0532e-04 - accuracy: 0.9846 - val_loss: 0.0016 - val_accuracy: 0.9010\n",
            "Epoch 467/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.9301e-04 - accuracy: 0.9854 - val_loss: 0.0015 - val_accuracy: 0.9112\n",
            "Epoch 468/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.7290e-04 - accuracy: 0.9865 - val_loss: 0.0015 - val_accuracy: 0.9102\n",
            "Epoch 469/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.6545e-04 - accuracy: 0.9870 - val_loss: 0.0014 - val_accuracy: 0.9157\n",
            "Epoch 470/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5859e-04 - accuracy: 0.9872 - val_loss: 0.0015 - val_accuracy: 0.9097\n",
            "Epoch 471/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.6373e-04 - accuracy: 0.9870 - val_loss: 0.0014 - val_accuracy: 0.9132\n",
            "Epoch 472/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5346e-04 - accuracy: 0.9875 - val_loss: 0.0014 - val_accuracy: 0.9127\n",
            "Epoch 473/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5905e-04 - accuracy: 0.9870 - val_loss: 0.0014 - val_accuracy: 0.9150\n",
            "Epoch 474/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5675e-04 - accuracy: 0.9872 - val_loss: 0.0015 - val_accuracy: 0.9087\n",
            "Epoch 475/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5924e-04 - accuracy: 0.9871 - val_loss: 0.0014 - val_accuracy: 0.9137\n",
            "Epoch 476/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5565e-04 - accuracy: 0.9873 - val_loss: 0.0014 - val_accuracy: 0.9130\n",
            "Epoch 477/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5520e-04 - accuracy: 0.9873 - val_loss: 0.0014 - val_accuracy: 0.9155\n",
            "Epoch 478/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5287e-04 - accuracy: 0.9874 - val_loss: 0.0014 - val_accuracy: 0.9152\n",
            "Epoch 479/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 1.5051e-04 - accuracy: 0.9874 - val_loss: 0.0014 - val_accuracy: 0.9140\n",
            "Epoch 480/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5428e-04 - accuracy: 0.9872 - val_loss: 0.0014 - val_accuracy: 0.9137\n",
            "Epoch 481/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5986e-04 - accuracy: 0.9867 - val_loss: 0.0014 - val_accuracy: 0.9130\n",
            "Epoch 482/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5920e-04 - accuracy: 0.9871 - val_loss: 0.0014 - val_accuracy: 0.9107\n",
            "Epoch 483/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5225e-04 - accuracy: 0.9874 - val_loss: 0.0013 - val_accuracy: 0.9188\n",
            "Epoch 484/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5203e-04 - accuracy: 0.9874 - val_loss: 0.0014 - val_accuracy: 0.9130\n",
            "Epoch 485/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5008e-04 - accuracy: 0.9875 - val_loss: 0.0014 - val_accuracy: 0.9123\n",
            "Epoch 486/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5082e-04 - accuracy: 0.9874 - val_loss: 0.0013 - val_accuracy: 0.9200\n",
            "Epoch 487/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5061e-04 - accuracy: 0.9873 - val_loss: 0.0013 - val_accuracy: 0.9180\n",
            "Epoch 488/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 1.4775e-04 - accuracy: 0.9875 - val_loss: 0.0013 - val_accuracy: 0.9213\n",
            "Epoch 489/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4808e-04 - accuracy: 0.9875 - val_loss: 0.0014 - val_accuracy: 0.9153\n",
            "Epoch 490/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5075e-04 - accuracy: 0.9873 - val_loss: 0.0015 - val_accuracy: 0.9052\n",
            "Epoch 491/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 1.5272e-04 - accuracy: 0.9874 - val_loss: 0.0014 - val_accuracy: 0.9140\n",
            "Epoch 492/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5236e-04 - accuracy: 0.9873 - val_loss: 0.0014 - val_accuracy: 0.9138\n",
            "Epoch 493/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5507e-04 - accuracy: 0.9871 - val_loss: 0.0014 - val_accuracy: 0.9130\n",
            "Epoch 494/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5649e-04 - accuracy: 0.9871 - val_loss: 0.0014 - val_accuracy: 0.9125\n",
            "Epoch 495/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.6641e-04 - accuracy: 0.9867 - val_loss: 0.0014 - val_accuracy: 0.9137\n",
            "Epoch 496/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.6810e-04 - accuracy: 0.9866 - val_loss: 0.0016 - val_accuracy: 0.9062\n",
            "Epoch 497/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 1.7300e-04 - accuracy: 0.9863 - val_loss: 0.0014 - val_accuracy: 0.9133\n",
            "Epoch 498/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 1.7014e-04 - accuracy: 0.9864 - val_loss: 0.0015 - val_accuracy: 0.9068\n",
            "Epoch 499/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.6965e-04 - accuracy: 0.9865 - val_loss: 0.0017 - val_accuracy: 0.8968\n",
            "Epoch 500/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.7215e-04 - accuracy: 0.9865 - val_loss: 0.0015 - val_accuracy: 0.9083\n",
            "Epoch 501/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.9420e-04 - accuracy: 0.9853 - val_loss: 0.0015 - val_accuracy: 0.9083\n",
            "Epoch 502/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.8437e-04 - accuracy: 0.9858 - val_loss: 0.0015 - val_accuracy: 0.9098\n",
            "Epoch 503/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.1109e-04 - accuracy: 0.9842 - val_loss: 0.0020 - val_accuracy: 0.8777\n",
            "Epoch 504/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.7440e-04 - accuracy: 0.9805 - val_loss: 0.0018 - val_accuracy: 0.8907\n",
            "Epoch 505/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.4255e-04 - accuracy: 0.9766 - val_loss: 0.0019 - val_accuracy: 0.8823\n",
            "Epoch 506/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 2.8886e-04 - accuracy: 0.9798 - val_loss: 0.0019 - val_accuracy: 0.8858\n",
            "Epoch 507/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.6322e-04 - accuracy: 0.9817 - val_loss: 0.0016 - val_accuracy: 0.9018\n",
            "Epoch 508/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.1490e-04 - accuracy: 0.9847 - val_loss: 0.0014 - val_accuracy: 0.9117\n",
            "Epoch 509/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 2.2135e-04 - accuracy: 0.9845 - val_loss: 0.0014 - val_accuracy: 0.9127\n",
            "Epoch 510/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 1.9058e-04 - accuracy: 0.9859 - val_loss: 0.0014 - val_accuracy: 0.9160\n",
            "Epoch 511/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.7257e-04 - accuracy: 0.9867 - val_loss: 0.0014 - val_accuracy: 0.9127\n",
            "Epoch 512/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 1.6163e-04 - accuracy: 0.9872 - val_loss: 0.0014 - val_accuracy: 0.9138\n",
            "Epoch 513/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 1.5811e-04 - accuracy: 0.9872 - val_loss: 0.0013 - val_accuracy: 0.9180\n",
            "Epoch 514/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 1.5573e-04 - accuracy: 0.9875 - val_loss: 0.0014 - val_accuracy: 0.9127\n",
            "Epoch 515/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5603e-04 - accuracy: 0.9874 - val_loss: 0.0013 - val_accuracy: 0.9188\n",
            "Epoch 516/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5437e-04 - accuracy: 0.9876 - val_loss: 0.0014 - val_accuracy: 0.9167\n",
            "Epoch 517/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 1.5191e-04 - accuracy: 0.9874 - val_loss: 0.0015 - val_accuracy: 0.9110\n",
            "Epoch 518/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 1.5423e-04 - accuracy: 0.9873 - val_loss: 0.0014 - val_accuracy: 0.9148\n",
            "Epoch 519/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5266e-04 - accuracy: 0.9874 - val_loss: 0.0014 - val_accuracy: 0.9173\n",
            "Epoch 520/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4969e-04 - accuracy: 0.9875 - val_loss: 0.0014 - val_accuracy: 0.9162\n",
            "Epoch 521/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4714e-04 - accuracy: 0.9876 - val_loss: 0.0014 - val_accuracy: 0.9115\n",
            "Epoch 522/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5014e-04 - accuracy: 0.9875 - val_loss: 0.0013 - val_accuracy: 0.9190\n",
            "Epoch 523/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4700e-04 - accuracy: 0.9876 - val_loss: 0.0013 - val_accuracy: 0.9198\n",
            "Epoch 524/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4760e-04 - accuracy: 0.9876 - val_loss: 0.0013 - val_accuracy: 0.9188\n",
            "Epoch 525/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5536e-04 - accuracy: 0.9873 - val_loss: 0.0013 - val_accuracy: 0.9202\n",
            "Epoch 526/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4745e-04 - accuracy: 0.9876 - val_loss: 0.0013 - val_accuracy: 0.9185\n",
            "Epoch 527/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4535e-04 - accuracy: 0.9877 - val_loss: 0.0013 - val_accuracy: 0.9190\n",
            "Epoch 528/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4527e-04 - accuracy: 0.9877 - val_loss: 0.0013 - val_accuracy: 0.9223\n",
            "Epoch 529/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4567e-04 - accuracy: 0.9877 - val_loss: 0.0013 - val_accuracy: 0.9192\n",
            "Epoch 530/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4646e-04 - accuracy: 0.9877 - val_loss: 0.0013 - val_accuracy: 0.9217\n",
            "Epoch 531/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4604e-04 - accuracy: 0.9877 - val_loss: 0.0013 - val_accuracy: 0.9200\n",
            "Epoch 532/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4496e-04 - accuracy: 0.9877 - val_loss: 0.0013 - val_accuracy: 0.9188\n",
            "Epoch 533/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4540e-04 - accuracy: 0.9876 - val_loss: 0.0013 - val_accuracy: 0.9200\n",
            "Epoch 534/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4530e-04 - accuracy: 0.9876 - val_loss: 0.0013 - val_accuracy: 0.9202\n",
            "Epoch 535/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4473e-04 - accuracy: 0.9877 - val_loss: 0.0013 - val_accuracy: 0.9200\n",
            "Epoch 536/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4540e-04 - accuracy: 0.9877 - val_loss: 0.0013 - val_accuracy: 0.9195\n",
            "Epoch 537/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4590e-04 - accuracy: 0.9876 - val_loss: 0.0013 - val_accuracy: 0.9188\n",
            "Epoch 538/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4587e-04 - accuracy: 0.9875 - val_loss: 0.0014 - val_accuracy: 0.9162\n",
            "Epoch 539/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4740e-04 - accuracy: 0.9875 - val_loss: 0.0013 - val_accuracy: 0.9173\n",
            "Epoch 540/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4582e-04 - accuracy: 0.9876 - val_loss: 0.0013 - val_accuracy: 0.9195\n",
            "Epoch 541/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4519e-04 - accuracy: 0.9877 - val_loss: 0.0013 - val_accuracy: 0.9190\n",
            "Epoch 542/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4492e-04 - accuracy: 0.9877 - val_loss: 0.0013 - val_accuracy: 0.9193\n",
            "Epoch 543/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4485e-04 - accuracy: 0.9877 - val_loss: 0.0014 - val_accuracy: 0.9170\n",
            "Epoch 544/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5166e-04 - accuracy: 0.9873 - val_loss: 0.0013 - val_accuracy: 0.9188\n",
            "Epoch 545/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5179e-04 - accuracy: 0.9873 - val_loss: 0.0013 - val_accuracy: 0.9183\n",
            "Epoch 546/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4865e-04 - accuracy: 0.9876 - val_loss: 0.0013 - val_accuracy: 0.9223\n",
            "Epoch 547/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4720e-04 - accuracy: 0.9876 - val_loss: 0.0013 - val_accuracy: 0.9197\n",
            "Epoch 548/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4604e-04 - accuracy: 0.9877 - val_loss: 0.0014 - val_accuracy: 0.9178\n",
            "Epoch 549/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4786e-04 - accuracy: 0.9875 - val_loss: 0.0014 - val_accuracy: 0.9160\n",
            "Epoch 550/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5177e-04 - accuracy: 0.9874 - val_loss: 0.0014 - val_accuracy: 0.9172\n",
            "Epoch 551/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5400e-04 - accuracy: 0.9871 - val_loss: 0.0014 - val_accuracy: 0.9170\n",
            "Epoch 552/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5889e-04 - accuracy: 0.9870 - val_loss: 0.0014 - val_accuracy: 0.9160\n",
            "Epoch 553/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5963e-04 - accuracy: 0.9870 - val_loss: 0.0014 - val_accuracy: 0.9128\n",
            "Epoch 554/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5433e-04 - accuracy: 0.9872 - val_loss: 0.0014 - val_accuracy: 0.9127\n",
            "Epoch 555/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 1.5354e-04 - accuracy: 0.9873 - val_loss: 0.0014 - val_accuracy: 0.9157\n",
            "Epoch 556/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5486e-04 - accuracy: 0.9873 - val_loss: 0.0014 - val_accuracy: 0.9157\n",
            "Epoch 557/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.6644e-04 - accuracy: 0.9867 - val_loss: 0.0015 - val_accuracy: 0.9120\n",
            "Epoch 558/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.8432e-04 - accuracy: 0.9857 - val_loss: 0.0015 - val_accuracy: 0.9108\n",
            "Epoch 559/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.7872e-04 - accuracy: 0.9860 - val_loss: 0.0017 - val_accuracy: 0.8938\n",
            "Epoch 560/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.0368e-04 - accuracy: 0.9849 - val_loss: 0.0016 - val_accuracy: 0.9002\n",
            "Epoch 561/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.9900e-04 - accuracy: 0.9849 - val_loss: 0.0017 - val_accuracy: 0.9013\n",
            "Epoch 562/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.3279e-04 - accuracy: 0.9832 - val_loss: 0.0016 - val_accuracy: 0.9007\n",
            "Epoch 563/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.2976e-04 - accuracy: 0.9834 - val_loss: 0.0016 - val_accuracy: 0.8998\n",
            "Epoch 564/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.1971e-04 - accuracy: 0.9840 - val_loss: 0.0015 - val_accuracy: 0.9068\n",
            "Epoch 565/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.0463e-04 - accuracy: 0.9850 - val_loss: 0.0016 - val_accuracy: 0.9043\n",
            "Epoch 566/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.9532e-04 - accuracy: 0.9855 - val_loss: 0.0015 - val_accuracy: 0.9062\n",
            "Epoch 567/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.0173e-04 - accuracy: 0.9853 - val_loss: 0.0017 - val_accuracy: 0.8972\n",
            "Epoch 568/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.1640e-04 - accuracy: 0.9845 - val_loss: 0.0016 - val_accuracy: 0.9068\n",
            "Epoch 569/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.1685e-04 - accuracy: 0.9845 - val_loss: 0.0017 - val_accuracy: 0.8990\n",
            "Epoch 570/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.3333e-04 - accuracy: 0.9835 - val_loss: 0.0015 - val_accuracy: 0.9092\n",
            "Epoch 571/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.2193e-04 - accuracy: 0.9840 - val_loss: 0.0017 - val_accuracy: 0.9012\n",
            "Epoch 572/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.4186e-04 - accuracy: 0.9826 - val_loss: 0.0015 - val_accuracy: 0.9103\n",
            "Epoch 573/1000\n",
            "24/24 [==============================] - 3s 129ms/step - loss: 2.1297e-04 - accuracy: 0.9844 - val_loss: 0.0014 - val_accuracy: 0.9140\n",
            "Epoch 574/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.9291e-04 - accuracy: 0.9858 - val_loss: 0.0015 - val_accuracy: 0.9095\n",
            "Epoch 575/1000\n",
            "24/24 [==============================] - 3s 129ms/step - loss: 1.7981e-04 - accuracy: 0.9866 - val_loss: 0.0014 - val_accuracy: 0.9142\n",
            "Epoch 576/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.8132e-04 - accuracy: 0.9860 - val_loss: 0.0014 - val_accuracy: 0.9123\n",
            "Epoch 577/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.7411e-04 - accuracy: 0.9866 - val_loss: 0.0015 - val_accuracy: 0.9133\n",
            "Epoch 578/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.6994e-04 - accuracy: 0.9867 - val_loss: 0.0014 - val_accuracy: 0.9178\n",
            "Epoch 579/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.6443e-04 - accuracy: 0.9870 - val_loss: 0.0013 - val_accuracy: 0.9207\n",
            "Epoch 580/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.6299e-04 - accuracy: 0.9871 - val_loss: 0.0014 - val_accuracy: 0.9125\n",
            "Epoch 581/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5527e-04 - accuracy: 0.9873 - val_loss: 0.0013 - val_accuracy: 0.9193\n",
            "Epoch 582/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 1.5266e-04 - accuracy: 0.9875 - val_loss: 0.0014 - val_accuracy: 0.9155\n",
            "Epoch 583/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 1.5141e-04 - accuracy: 0.9875 - val_loss: 0.0013 - val_accuracy: 0.9195\n",
            "Epoch 584/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5344e-04 - accuracy: 0.9873 - val_loss: 0.0014 - val_accuracy: 0.9152\n",
            "Epoch 585/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 1.7126e-04 - accuracy: 0.9865 - val_loss: 0.0015 - val_accuracy: 0.9112\n",
            "Epoch 586/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.6499e-04 - accuracy: 0.9868 - val_loss: 0.0013 - val_accuracy: 0.9187\n",
            "Epoch 587/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 1.5576e-04 - accuracy: 0.9874 - val_loss: 0.0014 - val_accuracy: 0.9175\n",
            "Epoch 588/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 1.5610e-04 - accuracy: 0.9873 - val_loss: 0.0013 - val_accuracy: 0.9187\n",
            "Epoch 589/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5452e-04 - accuracy: 0.9872 - val_loss: 0.0014 - val_accuracy: 0.9180\n",
            "Epoch 590/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5153e-04 - accuracy: 0.9874 - val_loss: 0.0014 - val_accuracy: 0.9148\n",
            "Epoch 591/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5330e-04 - accuracy: 0.9873 - val_loss: 0.0013 - val_accuracy: 0.9192\n",
            "Epoch 592/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5010e-04 - accuracy: 0.9876 - val_loss: 0.0013 - val_accuracy: 0.9230\n",
            "Epoch 593/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5128e-04 - accuracy: 0.9875 - val_loss: 0.0014 - val_accuracy: 0.9123\n",
            "Epoch 594/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5647e-04 - accuracy: 0.9872 - val_loss: 0.0013 - val_accuracy: 0.9187\n",
            "Epoch 595/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5605e-04 - accuracy: 0.9871 - val_loss: 0.0013 - val_accuracy: 0.9178\n",
            "Epoch 596/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5850e-04 - accuracy: 0.9871 - val_loss: 0.0013 - val_accuracy: 0.9190\n",
            "Epoch 597/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5263e-04 - accuracy: 0.9875 - val_loss: 0.0014 - val_accuracy: 0.9130\n",
            "Epoch 598/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5125e-04 - accuracy: 0.9876 - val_loss: 0.0013 - val_accuracy: 0.9227\n",
            "Epoch 599/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5087e-04 - accuracy: 0.9876 - val_loss: 0.0013 - val_accuracy: 0.9225\n",
            "Epoch 600/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4581e-04 - accuracy: 0.9877 - val_loss: 0.0013 - val_accuracy: 0.9208\n",
            "Epoch 601/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4475e-04 - accuracy: 0.9877 - val_loss: 0.0013 - val_accuracy: 0.9215\n",
            "Epoch 602/1000\n",
            "24/24 [==============================] - 3s 129ms/step - loss: 1.4613e-04 - accuracy: 0.9877 - val_loss: 0.0013 - val_accuracy: 0.9218\n",
            "Epoch 603/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4819e-04 - accuracy: 0.9875 - val_loss: 0.0013 - val_accuracy: 0.9222\n",
            "Epoch 604/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4990e-04 - accuracy: 0.9873 - val_loss: 0.0015 - val_accuracy: 0.9065\n",
            "Epoch 605/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5383e-04 - accuracy: 0.9873 - val_loss: 0.0014 - val_accuracy: 0.9187\n",
            "Epoch 606/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5395e-04 - accuracy: 0.9873 - val_loss: 0.0014 - val_accuracy: 0.9162\n",
            "Epoch 607/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5895e-04 - accuracy: 0.9870 - val_loss: 0.0015 - val_accuracy: 0.9075\n",
            "Epoch 608/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.6009e-04 - accuracy: 0.9870 - val_loss: 0.0016 - val_accuracy: 0.9068\n",
            "Epoch 609/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5927e-04 - accuracy: 0.9872 - val_loss: 0.0014 - val_accuracy: 0.9148\n",
            "Epoch 610/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5128e-04 - accuracy: 0.9875 - val_loss: 0.0013 - val_accuracy: 0.9223\n",
            "Epoch 611/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.6082e-04 - accuracy: 0.9870 - val_loss: 0.0014 - val_accuracy: 0.9143\n",
            "Epoch 612/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.6424e-04 - accuracy: 0.9869 - val_loss: 0.0015 - val_accuracy: 0.9087\n",
            "Epoch 613/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.2283e-04 - accuracy: 0.9831 - val_loss: 0.0017 - val_accuracy: 0.8970\n",
            "Epoch 614/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.9178e-04 - accuracy: 0.9852 - val_loss: 0.0016 - val_accuracy: 0.9037\n",
            "Epoch 615/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.8909e-04 - accuracy: 0.9858 - val_loss: 0.0014 - val_accuracy: 0.9147\n",
            "Epoch 616/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.9638e-04 - accuracy: 0.9854 - val_loss: 0.0015 - val_accuracy: 0.9103\n",
            "Epoch 617/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.7769e-04 - accuracy: 0.9865 - val_loss: 0.0015 - val_accuracy: 0.9113\n",
            "Epoch 618/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.8194e-04 - accuracy: 0.9861 - val_loss: 0.0013 - val_accuracy: 0.9202\n",
            "Epoch 619/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.8646e-04 - accuracy: 0.9860 - val_loss: 0.0014 - val_accuracy: 0.9142\n",
            "Epoch 620/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.9902e-04 - accuracy: 0.9848 - val_loss: 0.0014 - val_accuracy: 0.9115\n",
            "Epoch 621/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.9691e-04 - accuracy: 0.9854 - val_loss: 0.0014 - val_accuracy: 0.9123\n",
            "Epoch 622/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.7879e-04 - accuracy: 0.9863 - val_loss: 0.0014 - val_accuracy: 0.9138\n",
            "Epoch 623/1000\n",
            "24/24 [==============================] - 3s 129ms/step - loss: 1.7344e-04 - accuracy: 0.9867 - val_loss: 0.0014 - val_accuracy: 0.9115\n",
            "Epoch 624/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.7095e-04 - accuracy: 0.9868 - val_loss: 0.0013 - val_accuracy: 0.9197\n",
            "Epoch 625/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.6017e-04 - accuracy: 0.9872 - val_loss: 0.0013 - val_accuracy: 0.9213\n",
            "Epoch 626/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5369e-04 - accuracy: 0.9874 - val_loss: 0.0012 - val_accuracy: 0.9240\n",
            "Epoch 627/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5679e-04 - accuracy: 0.9873 - val_loss: 0.0013 - val_accuracy: 0.9203\n",
            "Epoch 628/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5232e-04 - accuracy: 0.9875 - val_loss: 0.0013 - val_accuracy: 0.9215\n",
            "Epoch 629/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5567e-04 - accuracy: 0.9872 - val_loss: 0.0013 - val_accuracy: 0.9210\n",
            "Epoch 630/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5907e-04 - accuracy: 0.9871 - val_loss: 0.0014 - val_accuracy: 0.9130\n",
            "Epoch 631/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5394e-04 - accuracy: 0.9873 - val_loss: 0.0013 - val_accuracy: 0.9198\n",
            "Epoch 632/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 1.6303e-04 - accuracy: 0.9867 - val_loss: 0.0013 - val_accuracy: 0.9178\n",
            "Epoch 633/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.6055e-04 - accuracy: 0.9869 - val_loss: 0.0014 - val_accuracy: 0.9138\n",
            "Epoch 634/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.7926e-04 - accuracy: 0.9858 - val_loss: 0.0014 - val_accuracy: 0.9173\n",
            "Epoch 635/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.6098e-04 - accuracy: 0.9870 - val_loss: 0.0014 - val_accuracy: 0.9153\n",
            "Epoch 636/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5374e-04 - accuracy: 0.9875 - val_loss: 0.0013 - val_accuracy: 0.9197\n",
            "Epoch 637/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5152e-04 - accuracy: 0.9875 - val_loss: 0.0014 - val_accuracy: 0.9170\n",
            "Epoch 638/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4949e-04 - accuracy: 0.9875 - val_loss: 0.0013 - val_accuracy: 0.9227\n",
            "Epoch 639/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4563e-04 - accuracy: 0.9877 - val_loss: 0.0013 - val_accuracy: 0.9202\n",
            "Epoch 640/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4531e-04 - accuracy: 0.9878 - val_loss: 0.0013 - val_accuracy: 0.9245\n",
            "Epoch 641/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4497e-04 - accuracy: 0.9877 - val_loss: 0.0012 - val_accuracy: 0.9297\n",
            "Epoch 642/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4235e-04 - accuracy: 0.9878 - val_loss: 0.0012 - val_accuracy: 0.9283\n",
            "Epoch 643/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4218e-04 - accuracy: 0.9878 - val_loss: 0.0012 - val_accuracy: 0.9267\n",
            "Epoch 644/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 1.4257e-04 - accuracy: 0.9878 - val_loss: 0.0012 - val_accuracy: 0.9250\n",
            "Epoch 645/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4197e-04 - accuracy: 0.9878 - val_loss: 0.0012 - val_accuracy: 0.9253\n",
            "Epoch 646/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4327e-04 - accuracy: 0.9878 - val_loss: 0.0012 - val_accuracy: 0.9262\n",
            "Epoch 647/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4304e-04 - accuracy: 0.9878 - val_loss: 0.0013 - val_accuracy: 0.9222\n",
            "Epoch 648/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4479e-04 - accuracy: 0.9878 - val_loss: 0.0013 - val_accuracy: 0.9227\n",
            "Epoch 649/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4262e-04 - accuracy: 0.9878 - val_loss: 0.0013 - val_accuracy: 0.9167\n",
            "Epoch 650/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4545e-04 - accuracy: 0.9877 - val_loss: 0.0013 - val_accuracy: 0.9218\n",
            "Epoch 651/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4300e-04 - accuracy: 0.9878 - val_loss: 0.0012 - val_accuracy: 0.9255\n",
            "Epoch 652/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4279e-04 - accuracy: 0.9878 - val_loss: 0.0012 - val_accuracy: 0.9278\n",
            "Epoch 653/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4187e-04 - accuracy: 0.9878 - val_loss: 0.0012 - val_accuracy: 0.9250\n",
            "Epoch 654/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4222e-04 - accuracy: 0.9878 - val_loss: 0.0012 - val_accuracy: 0.9255\n",
            "Epoch 655/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4230e-04 - accuracy: 0.9878 - val_loss: 0.0012 - val_accuracy: 0.9258\n",
            "Epoch 656/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4207e-04 - accuracy: 0.9878 - val_loss: 0.0012 - val_accuracy: 0.9275\n",
            "Epoch 657/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4238e-04 - accuracy: 0.9878 - val_loss: 0.0012 - val_accuracy: 0.9250\n",
            "Epoch 658/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 1.4872e-04 - accuracy: 0.9875 - val_loss: 0.0012 - val_accuracy: 0.9250\n",
            "Epoch 659/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4412e-04 - accuracy: 0.9878 - val_loss: 0.0012 - val_accuracy: 0.9238\n",
            "Epoch 660/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 1.4263e-04 - accuracy: 0.9878 - val_loss: 0.0012 - val_accuracy: 0.9282\n",
            "Epoch 661/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4222e-04 - accuracy: 0.9878 - val_loss: 0.0013 - val_accuracy: 0.9205\n",
            "Epoch 662/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 1.4433e-04 - accuracy: 0.9877 - val_loss: 0.0012 - val_accuracy: 0.9275\n",
            "Epoch 663/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 1.4601e-04 - accuracy: 0.9877 - val_loss: 0.0012 - val_accuracy: 0.9232\n",
            "Epoch 664/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5187e-04 - accuracy: 0.9873 - val_loss: 0.0013 - val_accuracy: 0.9198\n",
            "Epoch 665/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 1.4742e-04 - accuracy: 0.9876 - val_loss: 0.0013 - val_accuracy: 0.9228\n",
            "Epoch 666/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 1.5920e-04 - accuracy: 0.9869 - val_loss: 0.0013 - val_accuracy: 0.9185\n",
            "Epoch 667/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 1.6568e-04 - accuracy: 0.9868 - val_loss: 0.0014 - val_accuracy: 0.9183\n",
            "Epoch 668/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.7334e-04 - accuracy: 0.9865 - val_loss: 0.0014 - val_accuracy: 0.9143\n",
            "Epoch 669/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.9458e-04 - accuracy: 0.9851 - val_loss: 0.0015 - val_accuracy: 0.9122\n",
            "Epoch 670/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 3.2020e-04 - accuracy: 0.9778 - val_loss: 0.0016 - val_accuracy: 0.9078\n",
            "Epoch 671/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.6564e-04 - accuracy: 0.9819 - val_loss: 0.0014 - val_accuracy: 0.9142\n",
            "Epoch 672/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.1054e-04 - accuracy: 0.9844 - val_loss: 0.0015 - val_accuracy: 0.9087\n",
            "Epoch 673/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.0802e-04 - accuracy: 0.9846 - val_loss: 0.0016 - val_accuracy: 0.9035\n",
            "Epoch 674/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.3008e-04 - accuracy: 0.9835 - val_loss: 0.0015 - val_accuracy: 0.9115\n",
            "Epoch 675/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.0914e-04 - accuracy: 0.9849 - val_loss: 0.0014 - val_accuracy: 0.9177\n",
            "Epoch 676/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.1280e-04 - accuracy: 0.9847 - val_loss: 0.0015 - val_accuracy: 0.9102\n",
            "Epoch 677/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.8566e-04 - accuracy: 0.9859 - val_loss: 0.0015 - val_accuracy: 0.9110\n",
            "Epoch 678/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.8837e-04 - accuracy: 0.9860 - val_loss: 0.0016 - val_accuracy: 0.9075\n",
            "Epoch 679/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.8821e-04 - accuracy: 0.9859 - val_loss: 0.0014 - val_accuracy: 0.9182\n",
            "Epoch 680/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.8559e-04 - accuracy: 0.9863 - val_loss: 0.0014 - val_accuracy: 0.9170\n",
            "Epoch 681/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.6590e-04 - accuracy: 0.9872 - val_loss: 0.0013 - val_accuracy: 0.9195\n",
            "Epoch 682/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.6087e-04 - accuracy: 0.9874 - val_loss: 0.0012 - val_accuracy: 0.9252\n",
            "Epoch 683/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5376e-04 - accuracy: 0.9876 - val_loss: 0.0012 - val_accuracy: 0.9260\n",
            "Epoch 684/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5488e-04 - accuracy: 0.9875 - val_loss: 0.0012 - val_accuracy: 0.9265\n",
            "Epoch 685/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 1.5020e-04 - accuracy: 0.9877 - val_loss: 0.0012 - val_accuracy: 0.9248\n",
            "Epoch 686/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4598e-04 - accuracy: 0.9878 - val_loss: 0.0012 - val_accuracy: 0.9237\n",
            "Epoch 687/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4534e-04 - accuracy: 0.9878 - val_loss: 0.0012 - val_accuracy: 0.9260\n",
            "Epoch 688/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4698e-04 - accuracy: 0.9877 - val_loss: 0.0013 - val_accuracy: 0.9215\n",
            "Epoch 689/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5084e-04 - accuracy: 0.9875 - val_loss: 0.0012 - val_accuracy: 0.9267\n",
            "Epoch 690/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.6013e-04 - accuracy: 0.9869 - val_loss: 0.0014 - val_accuracy: 0.9165\n",
            "Epoch 691/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.6779e-04 - accuracy: 0.9866 - val_loss: 0.0014 - val_accuracy: 0.9163\n",
            "Epoch 692/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.6167e-04 - accuracy: 0.9870 - val_loss: 0.0012 - val_accuracy: 0.9240\n",
            "Epoch 693/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.6131e-04 - accuracy: 0.9869 - val_loss: 0.0013 - val_accuracy: 0.9223\n",
            "Epoch 694/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5841e-04 - accuracy: 0.9871 - val_loss: 0.0012 - val_accuracy: 0.9290\n",
            "Epoch 695/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5361e-04 - accuracy: 0.9875 - val_loss: 0.0012 - val_accuracy: 0.9268\n",
            "Epoch 696/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5013e-04 - accuracy: 0.9875 - val_loss: 0.0012 - val_accuracy: 0.9285\n",
            "Epoch 697/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5283e-04 - accuracy: 0.9874 - val_loss: 0.0013 - val_accuracy: 0.9200\n",
            "Epoch 698/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5332e-04 - accuracy: 0.9874 - val_loss: 0.0013 - val_accuracy: 0.9222\n",
            "Epoch 699/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5343e-04 - accuracy: 0.9873 - val_loss: 0.0013 - val_accuracy: 0.9197\n",
            "Epoch 700/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5092e-04 - accuracy: 0.9875 - val_loss: 0.0014 - val_accuracy: 0.9163\n",
            "Epoch 701/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5000e-04 - accuracy: 0.9876 - val_loss: 0.0012 - val_accuracy: 0.9270\n",
            "Epoch 702/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5068e-04 - accuracy: 0.9875 - val_loss: 0.0013 - val_accuracy: 0.9242\n",
            "Epoch 703/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5269e-04 - accuracy: 0.9873 - val_loss: 0.0013 - val_accuracy: 0.9197\n",
            "Epoch 704/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5187e-04 - accuracy: 0.9873 - val_loss: 0.0012 - val_accuracy: 0.9280\n",
            "Epoch 705/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5777e-04 - accuracy: 0.9871 - val_loss: 0.0012 - val_accuracy: 0.9263\n",
            "Epoch 706/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 1.6280e-04 - accuracy: 0.9869 - val_loss: 0.0013 - val_accuracy: 0.9192\n",
            "Epoch 707/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.7469e-04 - accuracy: 0.9861 - val_loss: 0.0013 - val_accuracy: 0.9223\n",
            "Epoch 708/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 2.0019e-04 - accuracy: 0.9850 - val_loss: 0.0014 - val_accuracy: 0.9143\n",
            "Epoch 709/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.1231e-04 - accuracy: 0.9841 - val_loss: 0.0014 - val_accuracy: 0.9128\n",
            "Epoch 710/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.9104e-04 - accuracy: 0.9858 - val_loss: 0.0013 - val_accuracy: 0.9225\n",
            "Epoch 711/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.6492e-04 - accuracy: 0.9869 - val_loss: 0.0013 - val_accuracy: 0.9250\n",
            "Epoch 712/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5894e-04 - accuracy: 0.9871 - val_loss: 0.0013 - val_accuracy: 0.9240\n",
            "Epoch 713/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5791e-04 - accuracy: 0.9874 - val_loss: 0.0013 - val_accuracy: 0.9223\n",
            "Epoch 714/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5449e-04 - accuracy: 0.9874 - val_loss: 0.0012 - val_accuracy: 0.9262\n",
            "Epoch 715/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 1.7131e-04 - accuracy: 0.9865 - val_loss: 0.0013 - val_accuracy: 0.9212\n",
            "Epoch 716/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.7459e-04 - accuracy: 0.9863 - val_loss: 0.0012 - val_accuracy: 0.9247\n",
            "Epoch 717/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.6187e-04 - accuracy: 0.9871 - val_loss: 0.0013 - val_accuracy: 0.9227\n",
            "Epoch 718/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 1.6119e-04 - accuracy: 0.9871 - val_loss: 0.0012 - val_accuracy: 0.9258\n",
            "Epoch 719/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.6007e-04 - accuracy: 0.9873 - val_loss: 0.0013 - val_accuracy: 0.9220\n",
            "Epoch 720/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5647e-04 - accuracy: 0.9875 - val_loss: 0.0013 - val_accuracy: 0.9240\n",
            "Epoch 721/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 1.4866e-04 - accuracy: 0.9877 - val_loss: 0.0012 - val_accuracy: 0.9262\n",
            "Epoch 722/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4709e-04 - accuracy: 0.9877 - val_loss: 0.0012 - val_accuracy: 0.9268\n",
            "Epoch 723/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5672e-04 - accuracy: 0.9872 - val_loss: 0.0012 - val_accuracy: 0.9270\n",
            "Epoch 724/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.6011e-04 - accuracy: 0.9870 - val_loss: 0.0013 - val_accuracy: 0.9198\n",
            "Epoch 725/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5754e-04 - accuracy: 0.9872 - val_loss: 0.0012 - val_accuracy: 0.9240\n",
            "Epoch 726/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5481e-04 - accuracy: 0.9875 - val_loss: 0.0013 - val_accuracy: 0.9235\n",
            "Epoch 727/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.6683e-04 - accuracy: 0.9869 - val_loss: 0.0012 - val_accuracy: 0.9260\n",
            "Epoch 728/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5242e-04 - accuracy: 0.9874 - val_loss: 0.0013 - val_accuracy: 0.9230\n",
            "Epoch 729/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4648e-04 - accuracy: 0.9878 - val_loss: 0.0012 - val_accuracy: 0.9298\n",
            "Epoch 730/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4360e-04 - accuracy: 0.9878 - val_loss: 0.0012 - val_accuracy: 0.9292\n",
            "Epoch 731/1000\n",
            "24/24 [==============================] - 3s 129ms/step - loss: 1.4555e-04 - accuracy: 0.9877 - val_loss: 0.0012 - val_accuracy: 0.9285\n",
            "Epoch 732/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.7824e-04 - accuracy: 0.9860 - val_loss: 0.0014 - val_accuracy: 0.9152\n",
            "Epoch 733/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.7374e-04 - accuracy: 0.9865 - val_loss: 0.0014 - val_accuracy: 0.9173\n",
            "Epoch 734/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.0510e-04 - accuracy: 0.9848 - val_loss: 0.0013 - val_accuracy: 0.9218\n",
            "Epoch 735/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.9979e-04 - accuracy: 0.9853 - val_loss: 0.0014 - val_accuracy: 0.9128\n",
            "Epoch 736/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.8523e-04 - accuracy: 0.9859 - val_loss: 0.0014 - val_accuracy: 0.9205\n",
            "Epoch 737/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.6624e-04 - accuracy: 0.9870 - val_loss: 0.0013 - val_accuracy: 0.9238\n",
            "Epoch 738/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.6199e-04 - accuracy: 0.9873 - val_loss: 0.0013 - val_accuracy: 0.9233\n",
            "Epoch 739/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5780e-04 - accuracy: 0.9875 - val_loss: 0.0012 - val_accuracy: 0.9313\n",
            "Epoch 740/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5588e-04 - accuracy: 0.9875 - val_loss: 0.0012 - val_accuracy: 0.9243\n",
            "Epoch 741/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 1.5335e-04 - accuracy: 0.9877 - val_loss: 0.0012 - val_accuracy: 0.9260\n",
            "Epoch 742/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 1.4928e-04 - accuracy: 0.9877 - val_loss: 0.0012 - val_accuracy: 0.9253\n",
            "Epoch 743/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4747e-04 - accuracy: 0.9878 - val_loss: 0.0011 - val_accuracy: 0.9302\n",
            "Epoch 744/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4299e-04 - accuracy: 0.9879 - val_loss: 0.0012 - val_accuracy: 0.9307\n",
            "Epoch 745/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4206e-04 - accuracy: 0.9879 - val_loss: 0.0012 - val_accuracy: 0.9300\n",
            "Epoch 746/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4231e-04 - accuracy: 0.9879 - val_loss: 0.0012 - val_accuracy: 0.9278\n",
            "Epoch 747/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4225e-04 - accuracy: 0.9879 - val_loss: 0.0011 - val_accuracy: 0.9323\n",
            "Epoch 748/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4126e-04 - accuracy: 0.9879 - val_loss: 0.0012 - val_accuracy: 0.9273\n",
            "Epoch 749/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4113e-04 - accuracy: 0.9879 - val_loss: 0.0011 - val_accuracy: 0.9317\n",
            "Epoch 750/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4572e-04 - accuracy: 0.9878 - val_loss: 0.0011 - val_accuracy: 0.9310\n",
            "Epoch 751/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4421e-04 - accuracy: 0.9878 - val_loss: 0.0012 - val_accuracy: 0.9273\n",
            "Epoch 752/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 1.4184e-04 - accuracy: 0.9879 - val_loss: 0.0011 - val_accuracy: 0.9322\n",
            "Epoch 753/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4092e-04 - accuracy: 0.9879 - val_loss: 0.0012 - val_accuracy: 0.9290\n",
            "Epoch 754/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4093e-04 - accuracy: 0.9879 - val_loss: 0.0012 - val_accuracy: 0.9297\n",
            "Epoch 755/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 1.4041e-04 - accuracy: 0.9879 - val_loss: 0.0011 - val_accuracy: 0.9317\n",
            "Epoch 756/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4125e-04 - accuracy: 0.9879 - val_loss: 0.0012 - val_accuracy: 0.9272\n",
            "Epoch 757/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4249e-04 - accuracy: 0.9878 - val_loss: 0.0012 - val_accuracy: 0.9252\n",
            "Epoch 758/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4167e-04 - accuracy: 0.9879 - val_loss: 0.0012 - val_accuracy: 0.9273\n",
            "Epoch 759/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4206e-04 - accuracy: 0.9879 - val_loss: 0.0012 - val_accuracy: 0.9270\n",
            "Epoch 760/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4282e-04 - accuracy: 0.9878 - val_loss: 0.0012 - val_accuracy: 0.9245\n",
            "Epoch 761/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4816e-04 - accuracy: 0.9876 - val_loss: 0.0014 - val_accuracy: 0.9160\n",
            "Epoch 762/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5096e-04 - accuracy: 0.9875 - val_loss: 0.0012 - val_accuracy: 0.9232\n",
            "Epoch 763/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4723e-04 - accuracy: 0.9876 - val_loss: 0.0012 - val_accuracy: 0.9243\n",
            "Epoch 764/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5337e-04 - accuracy: 0.9872 - val_loss: 0.0014 - val_accuracy: 0.9170\n",
            "Epoch 765/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.6047e-04 - accuracy: 0.9870 - val_loss: 0.0012 - val_accuracy: 0.9300\n",
            "Epoch 766/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5556e-04 - accuracy: 0.9876 - val_loss: 0.0013 - val_accuracy: 0.9188\n",
            "Epoch 767/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.6068e-04 - accuracy: 0.9872 - val_loss: 0.0013 - val_accuracy: 0.9228\n",
            "Epoch 768/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5053e-04 - accuracy: 0.9875 - val_loss: 0.0012 - val_accuracy: 0.9285\n",
            "Epoch 769/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 1.4658e-04 - accuracy: 0.9878 - val_loss: 0.0012 - val_accuracy: 0.9278\n",
            "Epoch 770/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5010e-04 - accuracy: 0.9875 - val_loss: 0.0013 - val_accuracy: 0.9212\n",
            "Epoch 771/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 1.4745e-04 - accuracy: 0.9877 - val_loss: 0.0012 - val_accuracy: 0.9282\n",
            "Epoch 772/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5125e-04 - accuracy: 0.9874 - val_loss: 0.0013 - val_accuracy: 0.9222\n",
            "Epoch 773/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5097e-04 - accuracy: 0.9875 - val_loss: 0.0012 - val_accuracy: 0.9283\n",
            "Epoch 774/1000\n",
            "24/24 [==============================] - 3s 129ms/step - loss: 1.5606e-04 - accuracy: 0.9873 - val_loss: 0.0013 - val_accuracy: 0.9245\n",
            "Epoch 775/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5517e-04 - accuracy: 0.9872 - val_loss: 0.0012 - val_accuracy: 0.9280\n",
            "Epoch 776/1000\n",
            "24/24 [==============================] - 3s 129ms/step - loss: 1.6424e-04 - accuracy: 0.9868 - val_loss: 0.0013 - val_accuracy: 0.9242\n",
            "Epoch 777/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.6645e-04 - accuracy: 0.9865 - val_loss: 0.0014 - val_accuracy: 0.9148\n",
            "Epoch 778/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.6285e-04 - accuracy: 0.9872 - val_loss: 0.0013 - val_accuracy: 0.9227\n",
            "Epoch 779/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.6109e-04 - accuracy: 0.9872 - val_loss: 0.0013 - val_accuracy: 0.9235\n",
            "Epoch 780/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5775e-04 - accuracy: 0.9874 - val_loss: 0.0012 - val_accuracy: 0.9252\n",
            "Epoch 781/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.6579e-04 - accuracy: 0.9868 - val_loss: 0.0016 - val_accuracy: 0.9040\n",
            "Epoch 782/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.6746e-04 - accuracy: 0.9868 - val_loss: 0.0014 - val_accuracy: 0.9163\n",
            "Epoch 783/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.7197e-04 - accuracy: 0.9864 - val_loss: 0.0015 - val_accuracy: 0.9102\n",
            "Epoch 784/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.1671e-04 - accuracy: 0.9841 - val_loss: 0.0014 - val_accuracy: 0.9163\n",
            "Epoch 785/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 1.9894e-04 - accuracy: 0.9850 - val_loss: 0.0014 - val_accuracy: 0.9173\n",
            "Epoch 786/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.1010e-04 - accuracy: 0.9847 - val_loss: 0.0012 - val_accuracy: 0.9247\n",
            "Epoch 787/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.0451e-04 - accuracy: 0.9850 - val_loss: 0.0014 - val_accuracy: 0.9185\n",
            "Epoch 788/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.9569e-04 - accuracy: 0.9855 - val_loss: 0.0015 - val_accuracy: 0.9080\n",
            "Epoch 789/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.9362e-04 - accuracy: 0.9856 - val_loss: 0.0013 - val_accuracy: 0.9208\n",
            "Epoch 790/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.0801e-04 - accuracy: 0.9850 - val_loss: 0.0013 - val_accuracy: 0.9202\n",
            "Epoch 791/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.8485e-04 - accuracy: 0.9865 - val_loss: 0.0013 - val_accuracy: 0.9233\n",
            "Epoch 792/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 1.7703e-04 - accuracy: 0.9867 - val_loss: 0.0013 - val_accuracy: 0.9213\n",
            "Epoch 793/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 1.6473e-04 - accuracy: 0.9872 - val_loss: 0.0013 - val_accuracy: 0.9192\n",
            "Epoch 794/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5796e-04 - accuracy: 0.9876 - val_loss: 0.0012 - val_accuracy: 0.9273\n",
            "Epoch 795/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5076e-04 - accuracy: 0.9878 - val_loss: 0.0012 - val_accuracy: 0.9278\n",
            "Epoch 796/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4879e-04 - accuracy: 0.9878 - val_loss: 0.0013 - val_accuracy: 0.9245\n",
            "Epoch 797/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5405e-04 - accuracy: 0.9875 - val_loss: 0.0012 - val_accuracy: 0.9288\n",
            "Epoch 798/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4877e-04 - accuracy: 0.9877 - val_loss: 0.0012 - val_accuracy: 0.9303\n",
            "Epoch 799/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4964e-04 - accuracy: 0.9875 - val_loss: 0.0012 - val_accuracy: 0.9263\n",
            "Epoch 800/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4920e-04 - accuracy: 0.9877 - val_loss: 0.0012 - val_accuracy: 0.9308\n",
            "Epoch 801/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4737e-04 - accuracy: 0.9877 - val_loss: 0.0012 - val_accuracy: 0.9305\n",
            "Epoch 802/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4565e-04 - accuracy: 0.9879 - val_loss: 0.0011 - val_accuracy: 0.9330\n",
            "Epoch 803/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4258e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9317\n",
            "Epoch 804/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4215e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9340\n",
            "Epoch 805/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4128e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9337\n",
            "Epoch 806/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4070e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9340\n",
            "Epoch 807/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4047e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9345\n",
            "Epoch 808/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.3975e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9325\n",
            "Epoch 809/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4015e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9337\n",
            "Epoch 810/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.3964e-04 - accuracy: 0.9879 - val_loss: 0.0011 - val_accuracy: 0.9318\n",
            "Epoch 811/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.3984e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9335\n",
            "Epoch 812/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 1.3968e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9347\n",
            "Epoch 813/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.3986e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9345\n",
            "Epoch 814/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.3973e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9347\n",
            "Epoch 815/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.3940e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9333\n",
            "Epoch 816/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.3968e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9325\n",
            "Epoch 817/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 1.3943e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9343\n",
            "Epoch 818/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 1.3907e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9327\n",
            "Epoch 819/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.3946e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9332\n",
            "Epoch 820/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 1.3937e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9312\n",
            "Epoch 821/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.3910e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9315\n",
            "Epoch 822/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.3910e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9317\n",
            "Epoch 823/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.3933e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9322\n",
            "Epoch 824/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 1.3879e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9322\n",
            "Epoch 825/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.3897e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9315\n",
            "Epoch 826/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.3882e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9310\n",
            "Epoch 827/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.3886e-04 - accuracy: 0.9880 - val_loss: 0.0012 - val_accuracy: 0.9292\n",
            "Epoch 828/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.3939e-04 - accuracy: 0.9880 - val_loss: 0.0012 - val_accuracy: 0.9300\n",
            "Epoch 829/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.3938e-04 - accuracy: 0.9880 - val_loss: 0.0012 - val_accuracy: 0.9280\n",
            "Epoch 830/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 1.3939e-04 - accuracy: 0.9879 - val_loss: 0.0012 - val_accuracy: 0.9302\n",
            "Epoch 831/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4261e-04 - accuracy: 0.9878 - val_loss: 0.0012 - val_accuracy: 0.9248\n",
            "Epoch 832/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4164e-04 - accuracy: 0.9879 - val_loss: 0.0011 - val_accuracy: 0.9307\n",
            "Epoch 833/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.3970e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9325\n",
            "Epoch 834/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4105e-04 - accuracy: 0.9879 - val_loss: 0.0012 - val_accuracy: 0.9288\n",
            "Epoch 835/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4347e-04 - accuracy: 0.9877 - val_loss: 0.0012 - val_accuracy: 0.9263\n",
            "Epoch 836/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4914e-04 - accuracy: 0.9875 - val_loss: 0.0013 - val_accuracy: 0.9205\n",
            "Epoch 837/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.7369e-04 - accuracy: 0.9860 - val_loss: 0.0018 - val_accuracy: 0.8923\n",
            "Epoch 838/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.1282e-04 - accuracy: 0.9842 - val_loss: 0.0014 - val_accuracy: 0.9168\n",
            "Epoch 839/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.4446e-04 - accuracy: 0.9826 - val_loss: 0.0014 - val_accuracy: 0.9143\n",
            "Epoch 840/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.4275e-04 - accuracy: 0.9825 - val_loss: 0.0015 - val_accuracy: 0.9112\n",
            "Epoch 841/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.5815e-04 - accuracy: 0.9815 - val_loss: 0.0016 - val_accuracy: 0.9060\n",
            "Epoch 842/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.4423e-04 - accuracy: 0.9829 - val_loss: 0.0018 - val_accuracy: 0.8915\n",
            "Epoch 843/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.9451e-04 - accuracy: 0.9803 - val_loss: 0.0013 - val_accuracy: 0.9225\n",
            "Epoch 844/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.1867e-04 - accuracy: 0.9844 - val_loss: 0.0016 - val_accuracy: 0.9027\n",
            "Epoch 845/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 2.1370e-04 - accuracy: 0.9846 - val_loss: 0.0014 - val_accuracy: 0.9152\n",
            "Epoch 846/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.7757e-04 - accuracy: 0.9866 - val_loss: 0.0013 - val_accuracy: 0.9207\n",
            "Epoch 847/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.6350e-04 - accuracy: 0.9872 - val_loss: 0.0012 - val_accuracy: 0.9263\n",
            "Epoch 848/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5943e-04 - accuracy: 0.9874 - val_loss: 0.0011 - val_accuracy: 0.9308\n",
            "Epoch 849/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.6283e-04 - accuracy: 0.9870 - val_loss: 0.0013 - val_accuracy: 0.9233\n",
            "Epoch 850/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.6080e-04 - accuracy: 0.9871 - val_loss: 0.0012 - val_accuracy: 0.9268\n",
            "Epoch 851/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.6258e-04 - accuracy: 0.9870 - val_loss: 0.0013 - val_accuracy: 0.9240\n",
            "Epoch 852/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5846e-04 - accuracy: 0.9872 - val_loss: 0.0012 - val_accuracy: 0.9273\n",
            "Epoch 853/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5454e-04 - accuracy: 0.9874 - val_loss: 0.0012 - val_accuracy: 0.9283\n",
            "Epoch 854/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5099e-04 - accuracy: 0.9876 - val_loss: 0.0014 - val_accuracy: 0.9152\n",
            "Epoch 855/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5980e-04 - accuracy: 0.9870 - val_loss: 0.0012 - val_accuracy: 0.9277\n",
            "Epoch 856/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5058e-04 - accuracy: 0.9876 - val_loss: 0.0012 - val_accuracy: 0.9282\n",
            "Epoch 857/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4812e-04 - accuracy: 0.9877 - val_loss: 0.0012 - val_accuracy: 0.9307\n",
            "Epoch 858/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4900e-04 - accuracy: 0.9877 - val_loss: 0.0012 - val_accuracy: 0.9302\n",
            "Epoch 859/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4382e-04 - accuracy: 0.9879 - val_loss: 0.0011 - val_accuracy: 0.9342\n",
            "Epoch 860/1000\n",
            "24/24 [==============================] - 3s 129ms/step - loss: 1.4232e-04 - accuracy: 0.9879 - val_loss: 0.0011 - val_accuracy: 0.9335\n",
            "Epoch 861/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4095e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9342\n",
            "Epoch 862/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4065e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9337\n",
            "Epoch 863/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4092e-04 - accuracy: 0.9879 - val_loss: 0.0011 - val_accuracy: 0.9332\n",
            "Epoch 864/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.3988e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9325\n",
            "Epoch 865/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.3993e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9347\n",
            "Epoch 866/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.3969e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9340\n",
            "Epoch 867/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.3992e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9353\n",
            "Epoch 868/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 1.3984e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9342\n",
            "Epoch 869/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.3991e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9337\n",
            "Epoch 870/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.3957e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9342\n",
            "Epoch 871/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.3956e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9335\n",
            "Epoch 872/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.3958e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9332\n",
            "Epoch 873/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.3958e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9320\n",
            "Epoch 874/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.3937e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9317\n",
            "Epoch 875/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.3958e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9323\n",
            "Epoch 876/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.3957e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9318\n",
            "Epoch 877/1000\n",
            "24/24 [==============================] - 3s 129ms/step - loss: 1.3951e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9320\n",
            "Epoch 878/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.3932e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9317\n",
            "Epoch 879/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.3928e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9310\n",
            "Epoch 880/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 1.3950e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9310\n",
            "Epoch 881/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.3909e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9310\n",
            "Epoch 882/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.3977e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9330\n",
            "Epoch 883/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4019e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9327\n",
            "Epoch 884/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 1.4009e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9310\n",
            "Epoch 885/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4260e-04 - accuracy: 0.9878 - val_loss: 0.0012 - val_accuracy: 0.9293\n",
            "Epoch 886/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4129e-04 - accuracy: 0.9880 - val_loss: 0.0012 - val_accuracy: 0.9315\n",
            "Epoch 887/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4139e-04 - accuracy: 0.9879 - val_loss: 0.0011 - val_accuracy: 0.9332\n",
            "Epoch 888/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4265e-04 - accuracy: 0.9878 - val_loss: 0.0012 - val_accuracy: 0.9293\n",
            "Epoch 889/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4978e-04 - accuracy: 0.9875 - val_loss: 0.0012 - val_accuracy: 0.9277\n",
            "Epoch 890/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5038e-04 - accuracy: 0.9874 - val_loss: 0.0012 - val_accuracy: 0.9258\n",
            "Epoch 891/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5323e-04 - accuracy: 0.9872 - val_loss: 0.0012 - val_accuracy: 0.9258\n",
            "Epoch 892/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.6422e-04 - accuracy: 0.9870 - val_loss: 0.0013 - val_accuracy: 0.9232\n",
            "Epoch 893/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.7073e-04 - accuracy: 0.9863 - val_loss: 0.0013 - val_accuracy: 0.9218\n",
            "Epoch 894/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.6051e-04 - accuracy: 0.9871 - val_loss: 0.0013 - val_accuracy: 0.9233\n",
            "Epoch 895/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5713e-04 - accuracy: 0.9875 - val_loss: 0.0013 - val_accuracy: 0.9258\n",
            "Epoch 896/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5098e-04 - accuracy: 0.9877 - val_loss: 0.0012 - val_accuracy: 0.9265\n",
            "Epoch 897/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 1.5670e-04 - accuracy: 0.9871 - val_loss: 0.0013 - val_accuracy: 0.9240\n",
            "Epoch 898/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5562e-04 - accuracy: 0.9872 - val_loss: 0.0013 - val_accuracy: 0.9262\n",
            "Epoch 899/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5714e-04 - accuracy: 0.9872 - val_loss: 0.0013 - val_accuracy: 0.9223\n",
            "Epoch 900/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.6362e-04 - accuracy: 0.9870 - val_loss: 0.0014 - val_accuracy: 0.9180\n",
            "Epoch 901/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.6579e-04 - accuracy: 0.9867 - val_loss: 0.0013 - val_accuracy: 0.9222\n",
            "Epoch 902/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.7086e-04 - accuracy: 0.9866 - val_loss: 0.0013 - val_accuracy: 0.9208\n",
            "Epoch 903/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.7511e-04 - accuracy: 0.9863 - val_loss: 0.0013 - val_accuracy: 0.9233\n",
            "Epoch 904/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.7042e-04 - accuracy: 0.9868 - val_loss: 0.0014 - val_accuracy: 0.9150\n",
            "Epoch 905/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.6893e-04 - accuracy: 0.9868 - val_loss: 0.0012 - val_accuracy: 0.9278\n",
            "Epoch 906/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.7418e-04 - accuracy: 0.9863 - val_loss: 0.0013 - val_accuracy: 0.9248\n",
            "Epoch 907/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.9207e-04 - accuracy: 0.9854 - val_loss: 0.0016 - val_accuracy: 0.9063\n",
            "Epoch 908/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.9022e-04 - accuracy: 0.9855 - val_loss: 0.0016 - val_accuracy: 0.9043\n",
            "Epoch 909/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.8177e-04 - accuracy: 0.9860 - val_loss: 0.0013 - val_accuracy: 0.9233\n",
            "Epoch 910/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 1.6081e-04 - accuracy: 0.9874 - val_loss: 0.0013 - val_accuracy: 0.9208\n",
            "Epoch 911/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.6478e-04 - accuracy: 0.9870 - val_loss: 0.0014 - val_accuracy: 0.9175\n",
            "Epoch 912/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 1.7818e-04 - accuracy: 0.9860 - val_loss: 0.0012 - val_accuracy: 0.9278\n",
            "Epoch 913/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.7524e-04 - accuracy: 0.9864 - val_loss: 0.0013 - val_accuracy: 0.9202\n",
            "Epoch 914/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.6470e-04 - accuracy: 0.9871 - val_loss: 0.0012 - val_accuracy: 0.9253\n",
            "Epoch 915/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.7370e-04 - accuracy: 0.9866 - val_loss: 0.0014 - val_accuracy: 0.9188\n",
            "Epoch 916/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.6320e-04 - accuracy: 0.9870 - val_loss: 0.0012 - val_accuracy: 0.9315\n",
            "Epoch 917/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.9450e-04 - accuracy: 0.9853 - val_loss: 0.0013 - val_accuracy: 0.9205\n",
            "Epoch 918/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.8197e-04 - accuracy: 0.9861 - val_loss: 0.0011 - val_accuracy: 0.9327\n",
            "Epoch 919/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.6704e-04 - accuracy: 0.9870 - val_loss: 0.0012 - val_accuracy: 0.9310\n",
            "Epoch 920/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.6301e-04 - accuracy: 0.9872 - val_loss: 0.0012 - val_accuracy: 0.9302\n",
            "Epoch 921/1000\n",
            "24/24 [==============================] - 3s 129ms/step - loss: 1.5674e-04 - accuracy: 0.9875 - val_loss: 0.0012 - val_accuracy: 0.9285\n",
            "Epoch 922/1000\n",
            "24/24 [==============================] - 3s 129ms/step - loss: 1.5068e-04 - accuracy: 0.9876 - val_loss: 0.0011 - val_accuracy: 0.9335\n",
            "Epoch 923/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4994e-04 - accuracy: 0.9877 - val_loss: 0.0012 - val_accuracy: 0.9290\n",
            "Epoch 924/1000\n",
            "24/24 [==============================] - 3s 129ms/step - loss: 1.4595e-04 - accuracy: 0.9878 - val_loss: 0.0012 - val_accuracy: 0.9285\n",
            "Epoch 925/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5097e-04 - accuracy: 0.9874 - val_loss: 0.0013 - val_accuracy: 0.9215\n",
            "Epoch 926/1000\n",
            "24/24 [==============================] - 3s 129ms/step - loss: 1.5457e-04 - accuracy: 0.9872 - val_loss: 0.0013 - val_accuracy: 0.9240\n",
            "Epoch 927/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.6015e-04 - accuracy: 0.9869 - val_loss: 0.0013 - val_accuracy: 0.9255\n",
            "Epoch 928/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5763e-04 - accuracy: 0.9872 - val_loss: 0.0012 - val_accuracy: 0.9268\n",
            "Epoch 929/1000\n",
            "24/24 [==============================] - 3s 129ms/step - loss: 1.5151e-04 - accuracy: 0.9875 - val_loss: 0.0013 - val_accuracy: 0.9235\n",
            "Epoch 930/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5443e-04 - accuracy: 0.9872 - val_loss: 0.0013 - val_accuracy: 0.9237\n",
            "Epoch 931/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.5476e-04 - accuracy: 0.9874 - val_loss: 0.0012 - val_accuracy: 0.9270\n",
            "Epoch 932/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4865e-04 - accuracy: 0.9878 - val_loss: 0.0011 - val_accuracy: 0.9312\n",
            "Epoch 933/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4348e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9348\n",
            "Epoch 934/1000\n",
            "24/24 [==============================] - 3s 129ms/step - loss: 1.4160e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9358\n",
            "Epoch 935/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4023e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9332\n",
            "Epoch 936/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.3969e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9347\n",
            "Epoch 937/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.3926e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9325\n",
            "Epoch 938/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.3901e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9337\n",
            "Epoch 939/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.3896e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9332\n",
            "Epoch 940/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.3876e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9323\n",
            "Epoch 941/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.3883e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9330\n",
            "Epoch 942/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.3845e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9318\n",
            "Epoch 943/1000\n",
            "24/24 [==============================] - 3s 129ms/step - loss: 1.3836e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9328\n",
            "Epoch 944/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.3859e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9325\n",
            "Epoch 945/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.3839e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9318\n",
            "Epoch 946/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.3840e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9320\n",
            "Epoch 947/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.3845e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9305\n",
            "Epoch 948/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.3855e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9313\n",
            "Epoch 949/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.3865e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9315\n",
            "Epoch 950/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.3826e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9318\n",
            "Epoch 951/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.3831e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9308\n",
            "Epoch 952/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.3808e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9320\n",
            "Epoch 953/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.3808e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9327\n",
            "Epoch 954/1000\n",
            "24/24 [==============================] - 3s 129ms/step - loss: 1.3815e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9328\n",
            "Epoch 955/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.3829e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9322\n",
            "Epoch 956/1000\n",
            "24/24 [==============================] - 3s 129ms/step - loss: 1.3839e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9323\n",
            "Epoch 957/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.3820e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9322\n",
            "Epoch 958/1000\n",
            "24/24 [==============================] - 3s 129ms/step - loss: 1.3824e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9317\n",
            "Epoch 959/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 1.3807e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9318\n",
            "Epoch 960/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.3817e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9308\n",
            "Epoch 961/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.3812e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9308\n",
            "Epoch 962/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.3812e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9312\n",
            "Epoch 963/1000\n",
            "24/24 [==============================] - 3s 129ms/step - loss: 1.3806e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9310\n",
            "Epoch 964/1000\n",
            "24/24 [==============================] - 3s 129ms/step - loss: 1.3808e-04 - accuracy: 0.9880 - val_loss: 0.0012 - val_accuracy: 0.9302\n",
            "Epoch 965/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.3813e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9320\n",
            "Epoch 966/1000\n",
            "24/24 [==============================] - 3s 129ms/step - loss: 1.3794e-04 - accuracy: 0.9880 - val_loss: 0.0012 - val_accuracy: 0.9310\n",
            "Epoch 967/1000\n",
            "24/24 [==============================] - 3s 129ms/step - loss: 1.3787e-04 - accuracy: 0.9880 - val_loss: 0.0012 - val_accuracy: 0.9303\n",
            "Epoch 968/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.3783e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9308\n",
            "Epoch 969/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.3799e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9298\n",
            "Epoch 970/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.3798e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9297\n",
            "Epoch 971/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.3788e-04 - accuracy: 0.9880 - val_loss: 0.0012 - val_accuracy: 0.9302\n",
            "Epoch 972/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.3790e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9315\n",
            "Epoch 973/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.3799e-04 - accuracy: 0.9880 - val_loss: 0.0012 - val_accuracy: 0.9308\n",
            "Epoch 974/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.3774e-04 - accuracy: 0.9880 - val_loss: 0.0012 - val_accuracy: 0.9288\n",
            "Epoch 975/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 1.3813e-04 - accuracy: 0.9880 - val_loss: 0.0012 - val_accuracy: 0.9292\n",
            "Epoch 976/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.3794e-04 - accuracy: 0.9880 - val_loss: 0.0012 - val_accuracy: 0.9280\n",
            "Epoch 977/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.3809e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9305\n",
            "Epoch 978/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.3763e-04 - accuracy: 0.9880 - val_loss: 0.0012 - val_accuracy: 0.9303\n",
            "Epoch 979/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.3873e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9307\n",
            "Epoch 980/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.3787e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9310\n",
            "Epoch 981/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.3782e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9312\n",
            "Epoch 982/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.3785e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9310\n",
            "Epoch 983/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.3779e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9313\n",
            "Epoch 984/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.3772e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9308\n",
            "Epoch 985/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.3778e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9307\n",
            "Epoch 986/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.3763e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9303\n",
            "Epoch 987/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.3764e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9310\n",
            "Epoch 988/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.3764e-04 - accuracy: 0.9880 - val_loss: 0.0012 - val_accuracy: 0.9310\n",
            "Epoch 989/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.3748e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9315\n",
            "Epoch 990/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.3757e-04 - accuracy: 0.9880 - val_loss: 0.0012 - val_accuracy: 0.9310\n",
            "Epoch 991/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.3797e-04 - accuracy: 0.9880 - val_loss: 0.0012 - val_accuracy: 0.9313\n",
            "Epoch 992/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.3767e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9317\n",
            "Epoch 993/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.3765e-04 - accuracy: 0.9880 - val_loss: 0.0012 - val_accuracy: 0.9283\n",
            "Epoch 994/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 1.3773e-04 - accuracy: 0.9880 - val_loss: 0.0012 - val_accuracy: 0.9297\n",
            "Epoch 995/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.3780e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9317\n",
            "Epoch 996/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.3756e-04 - accuracy: 0.9880 - val_loss: 0.0012 - val_accuracy: 0.9302\n",
            "Epoch 997/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.3745e-04 - accuracy: 0.9880 - val_loss: 0.0011 - val_accuracy: 0.9297\n",
            "Epoch 998/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.3801e-04 - accuracy: 0.9880 - val_loss: 0.0012 - val_accuracy: 0.9290\n",
            "Epoch 999/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.3843e-04 - accuracy: 0.9880 - val_loss: 0.0012 - val_accuracy: 0.9280\n",
            "Epoch 1000/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 1.4048e-04 - accuracy: 0.9879 - val_loss: 0.0012 - val_accuracy: 0.9297\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "id": "HVvegAfYQjT2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b796708-b68d-4eba-9ec9-0ecfdd32b114"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "188/188 [==============================] - 2s 10ms/step - loss: 0.0011 - accuracy: 0.9293\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.0011083399876952171, 0.9293333292007446]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('bandpass_1s.h5')"
      ],
      "metadata": {
        "id": "5g_vRO0ZFYHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 그래프 분석"
      ],
      "metadata": {
        "id": "uLvm3XtMKYKg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history=history.history\n",
        "epochs = range(1, len(history['loss']) + 1)\n",
        "acc = history['accuracy']\n",
        "loss = history['loss']\n",
        "val_acc = history['val_accuracy']\n",
        "val_loss = history['val_loss']\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.plot(epochs, acc, label='accuracy')\n",
        "plt.plot(epochs, val_acc, label='val_acc')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.plot(epochs, loss, label='loss', color='g')\n",
        "plt.plot(epochs, val_loss, label='val_loss', color='r')\n",
        "plt.legend()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 701
        },
        "id": "7YRFsECl7Qp0",
        "outputId": "826941b6-cee8-4548-eb9d-e0dfe0dd2163"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f11c4252250>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 예측해보기(predict)"
      ],
      "metadata": {
        "id": "B7id_mnNKhHg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKnoTLlLBPPN",
        "outputId": "d3231455-b8db-4afc-cc5c-55aa67130b3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "188/188 [==============================] - 3s 10ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_p=pd.DataFrame(y_test)\n",
        "\n",
        "for i in range(1,101):\n",
        "  y_test_p.loc[y_test_p[i] == 1.0 , 'y_test'] = i  \n",
        "y_test_p.y_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8EWDyMdHpWR",
        "outputId": "dd093915-90df-47c6-e717-2b8d0434c7a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       75.0\n",
              "1       90.0\n",
              "2       67.0\n",
              "3       91.0\n",
              "4       84.0\n",
              "        ... \n",
              "5995    44.0\n",
              "5996    37.0\n",
              "5997    99.0\n",
              "5998    88.0\n",
              "5999    31.0\n",
              "Name: y_test, Length: 6000, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ch1 = pd.DataFrame(np.rint(y_pred))\n",
        "# for i in range(1,101):\n",
        "#   ch1.loc[ch1[i] == 1.0 , 'pred'] = i \n",
        "\n",
        " \n",
        "#predict = pd.concat([data_test,ch1['pred'], y_test_p['y_test']],axis = 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "U6a3cpwDCkHl",
        "outputId": "35e79746-e41a-4713-b942-733f497e3b6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        0    1    2    3    4    5    6    7    8    9  ...   92   93   94  \\\n",
              "0     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
              "1     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
              "2     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
              "3     0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
              "4     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
              "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
              "5995  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
              "5996  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
              "5997  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
              "5998  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
              "5999  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
              "\n",
              "       95   96   97   98   99  100  pred  \n",
              "0     0.0  0.0  0.0  0.0  0.0  0.0  59.0  \n",
              "1     0.0  0.0  0.0  0.0  0.0  0.0  68.0  \n",
              "2     0.0  0.0  0.0  0.0  0.0  0.0  72.0  \n",
              "3     0.0  0.0  0.0  0.0  0.0  0.0   2.0  \n",
              "4     0.0  0.0  0.0  0.0  0.0  0.0  57.0  \n",
              "...   ...  ...  ...  ...  ...  ...   ...  \n",
              "5995  0.0  0.0  0.0  0.0  0.0  0.0   3.0  \n",
              "5996  0.0  0.0  0.0  0.0  0.0  0.0  87.0  \n",
              "5997  0.0  0.0  0.0  0.0  0.0  0.0  40.0  \n",
              "5998  0.0  0.0  0.0  0.0  0.0  0.0  68.0  \n",
              "5999  0.0  0.0  0.0  0.0  0.0  0.0  45.0  \n",
              "\n",
              "[6000 rows x 102 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d66cc86c-ae23-4a9f-8fb4-dbad40e0c5ea\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "      <th>100</th>\n",
              "      <th>pred</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>59.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>68.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>72.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>57.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5995</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5996</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>87.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5997</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5998</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>68.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5999</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>45.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6000 rows × 102 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d66cc86c-ae23-4a9f-8fb4-dbad40e0c5ea')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d66cc86c-ae23-4a9f-8fb4-dbad40e0c5ea button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d66cc86c-ae23-4a9f-8fb4-dbad40e0c5ea');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ex=np.argmax(y_pred, axis=1)\n",
        "ex=ex.astype('float64')\n",
        "ex=pd.DataFrame(ex)\n",
        "ex"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "lI3myxchcv-Q",
        "outputId": "f3d9e945-67dc-4d3a-c1e7-26f58fd8b53e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         0\n",
              "0     75.0\n",
              "1     90.0\n",
              "2     67.0\n",
              "3     91.0\n",
              "4     84.0\n",
              "...    ...\n",
              "5995  44.0\n",
              "5996  37.0\n",
              "5997  99.0\n",
              "5998  88.0\n",
              "5999  31.0\n",
              "\n",
              "[6000 rows x 1 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8bad303a-f373-476f-ad25-58f8a664d2ba\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>75.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>90.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>67.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>91.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>84.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5995</th>\n",
              "      <td>44.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5996</th>\n",
              "      <td>37.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5997</th>\n",
              "      <td>99.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5998</th>\n",
              "      <td>88.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5999</th>\n",
              "      <td>31.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6000 rows × 1 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8bad303a-f373-476f-ad25-58f8a664d2ba')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8bad303a-f373-476f-ad25-58f8a664d2ba button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8bad303a-f373-476f-ad25-58f8a664d2ba');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ch1 = pd.DataFrame(np.rint(y_pred))\n",
        "ch1['pred'] = ex\n",
        "data_test.reset_index(inplace=True, drop=True)\n",
        "predict = pd.concat([data_test,ch1['pred'], y_test_p['y_test']],axis = 1)"
      ],
      "metadata": {
        "id": "HSPb21Y4eMAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict['compare'] = np.where(predict['pred']==predict['y_test'],'yes', 'no')"
      ],
      "metadata": {
        "id": "yhe54j6XJzhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_1 = predict[predict[\"compare\"]=='no']\n",
        "predict_1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "mdgfuEehKIXX",
        "outputId": "35ded946-b41f-4202-d5d8-02489b84ca57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             0         1         2         3         4         5         6  \\\n",
              "8     0.034902  0.040244  0.046107  0.052067  0.057719  0.062743  0.066955   \n",
              "11    0.109717  0.110659  0.111964  0.114482  0.118950  0.125818  0.135116   \n",
              "26    0.014270  0.001147 -0.009664 -0.017843 -0.023431 -0.026793 -0.028535   \n",
              "27   -0.030546 -0.028344 -0.027399 -0.027423 -0.028062 -0.029064 -0.030394   \n",
              "35   -0.025196 -0.024448 -0.023728 -0.023056 -0.022438 -0.021864 -0.021319   \n",
              "...        ...       ...       ...       ...       ...       ...       ...   \n",
              "5904 -0.023422 -0.018496 -0.014607 -0.011922 -0.010307 -0.009338 -0.008411   \n",
              "5911  0.031127  0.030208  0.029620  0.029588  0.030140  0.031080  0.032028   \n",
              "5943  0.063536  0.071162  0.078900  0.086882  0.095225  0.103941  0.112881   \n",
              "5972 -0.043211 -0.041627 -0.039691 -0.037799 -0.036304 -0.035467 -0.035429   \n",
              "5986 -0.036023 -0.032761 -0.028856 -0.024080 -0.018298 -0.011573 -0.004257   \n",
              "\n",
              "             7         8         9  ...       249       250       251  \\\n",
              "8     0.070322  0.072960  0.075095  ...  0.085447  0.084625  0.083489   \n",
              "11    0.146405  0.158831  0.171282  ... -0.046100 -0.047511 -0.047318   \n",
              "26   -0.029376 -0.030009 -0.030962  ... -0.023292 -0.026665 -0.030282   \n",
              "27   -0.032242 -0.034932 -0.038768  ...  0.005832  0.002299 -0.006270   \n",
              "35   -0.020791 -0.020296 -0.019888  ...  0.045220  0.041943  0.036367   \n",
              "...        ...       ...       ...  ...       ...       ...       ...   \n",
              "5904 -0.006905 -0.004374 -0.000691  ... -0.045415 -0.049928 -0.055818   \n",
              "5911  0.032515  0.032121  0.030616  ... -0.026762 -0.027985 -0.028425   \n",
              "5943  0.121710  0.129939  0.136985  ... -0.053988 -0.052298 -0.050341   \n",
              "5972 -0.036219 -0.037772 -0.039970  ... -0.225394 -0.183337 -0.113905   \n",
              "5986  0.002999  0.009299  0.013657  ... -0.048809 -0.053204 -0.062194   \n",
              "\n",
              "           252       253       254       255  pred  y_test  compare  \n",
              "8     0.081097  0.076492  0.069015  0.058567  18.0     2.0       no  \n",
              "11   -0.046095 -0.044528 -0.043274 -0.042820   2.0    92.0       no  \n",
              "26   -0.033635 -0.035797 -0.035559 -0.031681  10.0     8.0       no  \n",
              "27   -0.020143 -0.038324 -0.058224 -0.075590  66.0    47.0       no  \n",
              "35    0.029059  0.020812  0.012487  0.004854  96.0     8.0       no  \n",
              "...        ...       ...       ...       ...   ...     ...      ...  \n",
              "5904 -0.062579 -0.069229 -0.074560 -0.077474  18.0    94.0       no  \n",
              "5911 -0.028331 -0.027923 -0.027419 -0.027036  95.0    49.0       no  \n",
              "5943 -0.048330 -0.046442 -0.044802 -0.043505  29.0    42.0       no  \n",
              "5972 -0.020055  0.090091  0.203977  0.306470  39.0    30.0       no  \n",
              "5986 -0.074844 -0.088793 -0.100251 -0.104371  22.0    12.0       no  \n",
              "\n",
              "[432 rows x 259 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b2b6d5b4-0b70-466c-8aa3-8305d2eb7bcc\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>249</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "      <th>pred</th>\n",
              "      <th>y_test</th>\n",
              "      <th>compare</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.034902</td>\n",
              "      <td>0.040244</td>\n",
              "      <td>0.046107</td>\n",
              "      <td>0.052067</td>\n",
              "      <td>0.057719</td>\n",
              "      <td>0.062743</td>\n",
              "      <td>0.066955</td>\n",
              "      <td>0.070322</td>\n",
              "      <td>0.072960</td>\n",
              "      <td>0.075095</td>\n",
              "      <td>...</td>\n",
              "      <td>0.085447</td>\n",
              "      <td>0.084625</td>\n",
              "      <td>0.083489</td>\n",
              "      <td>0.081097</td>\n",
              "      <td>0.076492</td>\n",
              "      <td>0.069015</td>\n",
              "      <td>0.058567</td>\n",
              "      <td>18.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.109717</td>\n",
              "      <td>0.110659</td>\n",
              "      <td>0.111964</td>\n",
              "      <td>0.114482</td>\n",
              "      <td>0.118950</td>\n",
              "      <td>0.125818</td>\n",
              "      <td>0.135116</td>\n",
              "      <td>0.146405</td>\n",
              "      <td>0.158831</td>\n",
              "      <td>0.171282</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.046100</td>\n",
              "      <td>-0.047511</td>\n",
              "      <td>-0.047318</td>\n",
              "      <td>-0.046095</td>\n",
              "      <td>-0.044528</td>\n",
              "      <td>-0.043274</td>\n",
              "      <td>-0.042820</td>\n",
              "      <td>2.0</td>\n",
              "      <td>92.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.014270</td>\n",
              "      <td>0.001147</td>\n",
              "      <td>-0.009664</td>\n",
              "      <td>-0.017843</td>\n",
              "      <td>-0.023431</td>\n",
              "      <td>-0.026793</td>\n",
              "      <td>-0.028535</td>\n",
              "      <td>-0.029376</td>\n",
              "      <td>-0.030009</td>\n",
              "      <td>-0.030962</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.023292</td>\n",
              "      <td>-0.026665</td>\n",
              "      <td>-0.030282</td>\n",
              "      <td>-0.033635</td>\n",
              "      <td>-0.035797</td>\n",
              "      <td>-0.035559</td>\n",
              "      <td>-0.031681</td>\n",
              "      <td>10.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>-0.030546</td>\n",
              "      <td>-0.028344</td>\n",
              "      <td>-0.027399</td>\n",
              "      <td>-0.027423</td>\n",
              "      <td>-0.028062</td>\n",
              "      <td>-0.029064</td>\n",
              "      <td>-0.030394</td>\n",
              "      <td>-0.032242</td>\n",
              "      <td>-0.034932</td>\n",
              "      <td>-0.038768</td>\n",
              "      <td>...</td>\n",
              "      <td>0.005832</td>\n",
              "      <td>0.002299</td>\n",
              "      <td>-0.006270</td>\n",
              "      <td>-0.020143</td>\n",
              "      <td>-0.038324</td>\n",
              "      <td>-0.058224</td>\n",
              "      <td>-0.075590</td>\n",
              "      <td>66.0</td>\n",
              "      <td>47.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>-0.025196</td>\n",
              "      <td>-0.024448</td>\n",
              "      <td>-0.023728</td>\n",
              "      <td>-0.023056</td>\n",
              "      <td>-0.022438</td>\n",
              "      <td>-0.021864</td>\n",
              "      <td>-0.021319</td>\n",
              "      <td>-0.020791</td>\n",
              "      <td>-0.020296</td>\n",
              "      <td>-0.019888</td>\n",
              "      <td>...</td>\n",
              "      <td>0.045220</td>\n",
              "      <td>0.041943</td>\n",
              "      <td>0.036367</td>\n",
              "      <td>0.029059</td>\n",
              "      <td>0.020812</td>\n",
              "      <td>0.012487</td>\n",
              "      <td>0.004854</td>\n",
              "      <td>96.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5904</th>\n",
              "      <td>-0.023422</td>\n",
              "      <td>-0.018496</td>\n",
              "      <td>-0.014607</td>\n",
              "      <td>-0.011922</td>\n",
              "      <td>-0.010307</td>\n",
              "      <td>-0.009338</td>\n",
              "      <td>-0.008411</td>\n",
              "      <td>-0.006905</td>\n",
              "      <td>-0.004374</td>\n",
              "      <td>-0.000691</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.045415</td>\n",
              "      <td>-0.049928</td>\n",
              "      <td>-0.055818</td>\n",
              "      <td>-0.062579</td>\n",
              "      <td>-0.069229</td>\n",
              "      <td>-0.074560</td>\n",
              "      <td>-0.077474</td>\n",
              "      <td>18.0</td>\n",
              "      <td>94.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5911</th>\n",
              "      <td>0.031127</td>\n",
              "      <td>0.030208</td>\n",
              "      <td>0.029620</td>\n",
              "      <td>0.029588</td>\n",
              "      <td>0.030140</td>\n",
              "      <td>0.031080</td>\n",
              "      <td>0.032028</td>\n",
              "      <td>0.032515</td>\n",
              "      <td>0.032121</td>\n",
              "      <td>0.030616</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.026762</td>\n",
              "      <td>-0.027985</td>\n",
              "      <td>-0.028425</td>\n",
              "      <td>-0.028331</td>\n",
              "      <td>-0.027923</td>\n",
              "      <td>-0.027419</td>\n",
              "      <td>-0.027036</td>\n",
              "      <td>95.0</td>\n",
              "      <td>49.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5943</th>\n",
              "      <td>0.063536</td>\n",
              "      <td>0.071162</td>\n",
              "      <td>0.078900</td>\n",
              "      <td>0.086882</td>\n",
              "      <td>0.095225</td>\n",
              "      <td>0.103941</td>\n",
              "      <td>0.112881</td>\n",
              "      <td>0.121710</td>\n",
              "      <td>0.129939</td>\n",
              "      <td>0.136985</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.053988</td>\n",
              "      <td>-0.052298</td>\n",
              "      <td>-0.050341</td>\n",
              "      <td>-0.048330</td>\n",
              "      <td>-0.046442</td>\n",
              "      <td>-0.044802</td>\n",
              "      <td>-0.043505</td>\n",
              "      <td>29.0</td>\n",
              "      <td>42.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5972</th>\n",
              "      <td>-0.043211</td>\n",
              "      <td>-0.041627</td>\n",
              "      <td>-0.039691</td>\n",
              "      <td>-0.037799</td>\n",
              "      <td>-0.036304</td>\n",
              "      <td>-0.035467</td>\n",
              "      <td>-0.035429</td>\n",
              "      <td>-0.036219</td>\n",
              "      <td>-0.037772</td>\n",
              "      <td>-0.039970</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.225394</td>\n",
              "      <td>-0.183337</td>\n",
              "      <td>-0.113905</td>\n",
              "      <td>-0.020055</td>\n",
              "      <td>0.090091</td>\n",
              "      <td>0.203977</td>\n",
              "      <td>0.306470</td>\n",
              "      <td>39.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5986</th>\n",
              "      <td>-0.036023</td>\n",
              "      <td>-0.032761</td>\n",
              "      <td>-0.028856</td>\n",
              "      <td>-0.024080</td>\n",
              "      <td>-0.018298</td>\n",
              "      <td>-0.011573</td>\n",
              "      <td>-0.004257</td>\n",
              "      <td>0.002999</td>\n",
              "      <td>0.009299</td>\n",
              "      <td>0.013657</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.048809</td>\n",
              "      <td>-0.053204</td>\n",
              "      <td>-0.062194</td>\n",
              "      <td>-0.074844</td>\n",
              "      <td>-0.088793</td>\n",
              "      <td>-0.100251</td>\n",
              "      <td>-0.104371</td>\n",
              "      <td>22.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>432 rows × 259 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b2b6d5b4-0b70-466c-8aa3-8305d2eb7bcc')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b2b6d5b4-0b70-466c-8aa3-8305d2eb7bcc button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b2b6d5b4-0b70-466c-8aa3-8305d2eb7bcc');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.highpass"
      ],
      "metadata": {
        "id": "3SxPZjD4u6eC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46bf1084-0e1c-4a75-faae-05630c1f105b",
        "id": "sT99AEGYu9a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip '/content/drive/MyDrive/model_line.zip' -d '/content/drive/MyDrive/model_line'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c6ba509-4e6d-42b3-8562-1567bcc35d70",
        "id": "JUiDqyLzu9a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/model_line.zip\n",
            "   creating: /content/drive/MyDrive/model_line/model_line/\n",
            "   creating: /content/drive/MyDrive/model_line/model_line/band/\n",
            "  inflating: /content/drive/MyDrive/model_line/model_line/band/test_band_line.csv  \n",
            "  inflating: /content/drive/MyDrive/model_line/model_line/band/train_band_line.csv  \n",
            "  inflating: /content/drive/MyDrive/model_line/model_line/band/valid_band_line.csv  \n",
            "   creating: /content/drive/MyDrive/model_line/model_line/high/\n",
            "  inflating: /content/drive/MyDrive/model_line/model_line/high/test_high_line.csv  \n",
            "  inflating: /content/drive/MyDrive/model_line/model_line/high/train_high_line.csv  \n",
            "  inflating: /content/drive/MyDrive/model_line/model_line/high/valid_high_line.csv  \n",
            "   creating: /content/drive/MyDrive/model_line/model_line/low/\n",
            "  inflating: /content/drive/MyDrive/model_line/model_line/low/test_low_line.csv  \n",
            "  inflating: /content/drive/MyDrive/model_line/model_line/low/train_low_line.csv  \n",
            "  inflating: /content/drive/MyDrive/model_line/model_line/low/valid_low_line.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqmjel0bu9a2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import itertools\n",
        "import time\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import (CosineAnnealingLR,\n",
        "                                      CosineAnnealingWarmRestarts,\n",
        "                                      StepLR,\n",
        "                                      ExponentialLR)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import accuracy_score, auc, f1_score, precision_score, recall_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm, re, sys\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "from keras.layers import Conv2D, Conv1D, MaxPooling2D, MaxPooling1D, Flatten, BatchNormalization, Dense\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.callbacks import CSVLogger, ModelCheckpoint"
      ],
      "metadata": {
        "id": "hMywic9Fu9a2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train = pd.read_csv('/content/drive/MyDrive/model_line/model_line/high/train_high_line.csv', header=None)\n",
        "data_test = pd.read_csv('/content/drive/MyDrive/model_line/model_line/high/test_high_line.csv', header=None)\n",
        "data_val= pd.read_csv('/content/drive/MyDrive/model_line/model_line/high/valid_high_line.csv', header=None)"
      ],
      "metadata": {
        "id": "UOBpnPrqu9a2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "outputId": "78c13791-5b09-4e5d-84b9-65a45a2ced17",
        "id": "JVpsDpVDu9a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           0         1         2         3         4         5         6    \\\n",
              "0     0.004293 -0.108721 -0.025203  0.050075  0.041975 -0.079525 -0.019323   \n",
              "1    -0.178025 -0.129949 -0.154714 -0.190879 -0.147544 -0.149849 -0.139385   \n",
              "2    -0.118486 -0.135978 -0.021999  0.014580 -0.094641 -0.135181 -0.068231   \n",
              "3     0.595354  0.768205  0.751377  0.729010  0.837712  0.799024  0.553267   \n",
              "4    -0.107180 -0.029394 -0.003439 -0.114145 -0.110911 -0.047808  0.014365   \n",
              "...        ...       ...       ...       ...       ...       ...       ...   \n",
              "5995 -0.006009  0.035463  0.031765  0.034116  0.041276  0.040276  0.043814   \n",
              "5996 -0.004209 -0.021269  0.014421  0.066451  0.034102  0.022313  0.039954   \n",
              "5997 -0.052598 -0.055633 -0.055577 -0.032002 -0.095826 -0.056251 -0.088205   \n",
              "5998 -0.039242 -0.044322 -0.024571 -0.017190 -0.044918 -0.038277 -0.066724   \n",
              "5999 -0.001325 -0.136461 -0.178526 -0.261551 -0.222405 -0.167719 -0.153012   \n",
              "\n",
              "           7         8         9    ...       246       247       248  \\\n",
              "0     0.040809  0.003013 -0.044322  ...  0.025460  0.090296 -0.007490   \n",
              "1    -0.184831 -0.160077 -0.125704  ... -0.158946 -0.192521 -0.104194   \n",
              "2     0.024199 -0.080811 -0.151051  ... -0.012902 -0.057865 -0.169278   \n",
              "3     0.214589  0.034092  0.025435  ... -0.060304 -0.051033  0.071048   \n",
              "4    -0.074943 -0.107671 -0.060639  ... -0.104624 -0.173071 -0.158586   \n",
              "...        ...       ...       ...  ...       ...       ...       ...   \n",
              "5995  0.049392  0.014919  0.082215  ...  0.032174  0.025207  0.017123   \n",
              "5996  0.015126  0.016828  0.095201  ... -0.002060 -0.007017 -0.001635   \n",
              "5997 -0.003379 -0.041393 -0.062797  ... -0.038018 -0.060236 -0.027453   \n",
              "5998 -0.033992 -0.049339 -0.018386  ... -0.089395 -0.019348 -0.078060   \n",
              "5999 -0.143434 -0.108636 -0.056848  ...  0.127477  0.115293  0.154681   \n",
              "\n",
              "           249       250       251       252       253       254       255  \n",
              "0    -0.026310  0.053336  0.110380  0.044460 -0.027323  0.009700  0.105980  \n",
              "1    -0.074016 -0.121687 -0.121997 -0.143646 -0.113273 -0.137490 -0.137856  \n",
              "2    -0.139103 -0.051629  0.003184 -0.126743 -0.173762 -0.090042 -0.026942  \n",
              "3     0.039479 -0.059981 -0.111832  0.002407  0.031495 -0.068237 -0.108500  \n",
              "4    -0.065081 -0.066074 -0.202336 -0.198327 -0.087227 -0.061546 -0.158424  \n",
              "...        ...       ...       ...       ...       ...       ...       ...  \n",
              "5995 -0.047797 -0.036055 -0.080359 -0.080849 -0.131536 -0.108450 -0.119801  \n",
              "5996  0.073427  0.119448  0.241599  0.262261  0.279691  0.237252  0.092732  \n",
              "5997 -0.048409 -0.078653 -0.040536 -0.018418 -0.062418 -0.006057  0.010865  \n",
              "5998 -0.082112 -0.081604 -0.070175 -0.080676 -0.072666  0.004694 -0.046835  \n",
              "5999  0.142378  0.226287  0.199556  0.225905  0.209185  0.220656  0.204857  \n",
              "\n",
              "[6000 rows x 256 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-89b8e1f0-83c5-4ce9-8b6e-9354eabea76e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>246</th>\n",
              "      <th>247</th>\n",
              "      <th>248</th>\n",
              "      <th>249</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.004293</td>\n",
              "      <td>-0.108721</td>\n",
              "      <td>-0.025203</td>\n",
              "      <td>0.050075</td>\n",
              "      <td>0.041975</td>\n",
              "      <td>-0.079525</td>\n",
              "      <td>-0.019323</td>\n",
              "      <td>0.040809</td>\n",
              "      <td>0.003013</td>\n",
              "      <td>-0.044322</td>\n",
              "      <td>...</td>\n",
              "      <td>0.025460</td>\n",
              "      <td>0.090296</td>\n",
              "      <td>-0.007490</td>\n",
              "      <td>-0.026310</td>\n",
              "      <td>0.053336</td>\n",
              "      <td>0.110380</td>\n",
              "      <td>0.044460</td>\n",
              "      <td>-0.027323</td>\n",
              "      <td>0.009700</td>\n",
              "      <td>0.105980</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.178025</td>\n",
              "      <td>-0.129949</td>\n",
              "      <td>-0.154714</td>\n",
              "      <td>-0.190879</td>\n",
              "      <td>-0.147544</td>\n",
              "      <td>-0.149849</td>\n",
              "      <td>-0.139385</td>\n",
              "      <td>-0.184831</td>\n",
              "      <td>-0.160077</td>\n",
              "      <td>-0.125704</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.158946</td>\n",
              "      <td>-0.192521</td>\n",
              "      <td>-0.104194</td>\n",
              "      <td>-0.074016</td>\n",
              "      <td>-0.121687</td>\n",
              "      <td>-0.121997</td>\n",
              "      <td>-0.143646</td>\n",
              "      <td>-0.113273</td>\n",
              "      <td>-0.137490</td>\n",
              "      <td>-0.137856</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.118486</td>\n",
              "      <td>-0.135978</td>\n",
              "      <td>-0.021999</td>\n",
              "      <td>0.014580</td>\n",
              "      <td>-0.094641</td>\n",
              "      <td>-0.135181</td>\n",
              "      <td>-0.068231</td>\n",
              "      <td>0.024199</td>\n",
              "      <td>-0.080811</td>\n",
              "      <td>-0.151051</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.012902</td>\n",
              "      <td>-0.057865</td>\n",
              "      <td>-0.169278</td>\n",
              "      <td>-0.139103</td>\n",
              "      <td>-0.051629</td>\n",
              "      <td>0.003184</td>\n",
              "      <td>-0.126743</td>\n",
              "      <td>-0.173762</td>\n",
              "      <td>-0.090042</td>\n",
              "      <td>-0.026942</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.595354</td>\n",
              "      <td>0.768205</td>\n",
              "      <td>0.751377</td>\n",
              "      <td>0.729010</td>\n",
              "      <td>0.837712</td>\n",
              "      <td>0.799024</td>\n",
              "      <td>0.553267</td>\n",
              "      <td>0.214589</td>\n",
              "      <td>0.034092</td>\n",
              "      <td>0.025435</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.060304</td>\n",
              "      <td>-0.051033</td>\n",
              "      <td>0.071048</td>\n",
              "      <td>0.039479</td>\n",
              "      <td>-0.059981</td>\n",
              "      <td>-0.111832</td>\n",
              "      <td>0.002407</td>\n",
              "      <td>0.031495</td>\n",
              "      <td>-0.068237</td>\n",
              "      <td>-0.108500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.107180</td>\n",
              "      <td>-0.029394</td>\n",
              "      <td>-0.003439</td>\n",
              "      <td>-0.114145</td>\n",
              "      <td>-0.110911</td>\n",
              "      <td>-0.047808</td>\n",
              "      <td>0.014365</td>\n",
              "      <td>-0.074943</td>\n",
              "      <td>-0.107671</td>\n",
              "      <td>-0.060639</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.104624</td>\n",
              "      <td>-0.173071</td>\n",
              "      <td>-0.158586</td>\n",
              "      <td>-0.065081</td>\n",
              "      <td>-0.066074</td>\n",
              "      <td>-0.202336</td>\n",
              "      <td>-0.198327</td>\n",
              "      <td>-0.087227</td>\n",
              "      <td>-0.061546</td>\n",
              "      <td>-0.158424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5995</th>\n",
              "      <td>-0.006009</td>\n",
              "      <td>0.035463</td>\n",
              "      <td>0.031765</td>\n",
              "      <td>0.034116</td>\n",
              "      <td>0.041276</td>\n",
              "      <td>0.040276</td>\n",
              "      <td>0.043814</td>\n",
              "      <td>0.049392</td>\n",
              "      <td>0.014919</td>\n",
              "      <td>0.082215</td>\n",
              "      <td>...</td>\n",
              "      <td>0.032174</td>\n",
              "      <td>0.025207</td>\n",
              "      <td>0.017123</td>\n",
              "      <td>-0.047797</td>\n",
              "      <td>-0.036055</td>\n",
              "      <td>-0.080359</td>\n",
              "      <td>-0.080849</td>\n",
              "      <td>-0.131536</td>\n",
              "      <td>-0.108450</td>\n",
              "      <td>-0.119801</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5996</th>\n",
              "      <td>-0.004209</td>\n",
              "      <td>-0.021269</td>\n",
              "      <td>0.014421</td>\n",
              "      <td>0.066451</td>\n",
              "      <td>0.034102</td>\n",
              "      <td>0.022313</td>\n",
              "      <td>0.039954</td>\n",
              "      <td>0.015126</td>\n",
              "      <td>0.016828</td>\n",
              "      <td>0.095201</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.002060</td>\n",
              "      <td>-0.007017</td>\n",
              "      <td>-0.001635</td>\n",
              "      <td>0.073427</td>\n",
              "      <td>0.119448</td>\n",
              "      <td>0.241599</td>\n",
              "      <td>0.262261</td>\n",
              "      <td>0.279691</td>\n",
              "      <td>0.237252</td>\n",
              "      <td>0.092732</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5997</th>\n",
              "      <td>-0.052598</td>\n",
              "      <td>-0.055633</td>\n",
              "      <td>-0.055577</td>\n",
              "      <td>-0.032002</td>\n",
              "      <td>-0.095826</td>\n",
              "      <td>-0.056251</td>\n",
              "      <td>-0.088205</td>\n",
              "      <td>-0.003379</td>\n",
              "      <td>-0.041393</td>\n",
              "      <td>-0.062797</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.038018</td>\n",
              "      <td>-0.060236</td>\n",
              "      <td>-0.027453</td>\n",
              "      <td>-0.048409</td>\n",
              "      <td>-0.078653</td>\n",
              "      <td>-0.040536</td>\n",
              "      <td>-0.018418</td>\n",
              "      <td>-0.062418</td>\n",
              "      <td>-0.006057</td>\n",
              "      <td>0.010865</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5998</th>\n",
              "      <td>-0.039242</td>\n",
              "      <td>-0.044322</td>\n",
              "      <td>-0.024571</td>\n",
              "      <td>-0.017190</td>\n",
              "      <td>-0.044918</td>\n",
              "      <td>-0.038277</td>\n",
              "      <td>-0.066724</td>\n",
              "      <td>-0.033992</td>\n",
              "      <td>-0.049339</td>\n",
              "      <td>-0.018386</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.089395</td>\n",
              "      <td>-0.019348</td>\n",
              "      <td>-0.078060</td>\n",
              "      <td>-0.082112</td>\n",
              "      <td>-0.081604</td>\n",
              "      <td>-0.070175</td>\n",
              "      <td>-0.080676</td>\n",
              "      <td>-0.072666</td>\n",
              "      <td>0.004694</td>\n",
              "      <td>-0.046835</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5999</th>\n",
              "      <td>-0.001325</td>\n",
              "      <td>-0.136461</td>\n",
              "      <td>-0.178526</td>\n",
              "      <td>-0.261551</td>\n",
              "      <td>-0.222405</td>\n",
              "      <td>-0.167719</td>\n",
              "      <td>-0.153012</td>\n",
              "      <td>-0.143434</td>\n",
              "      <td>-0.108636</td>\n",
              "      <td>-0.056848</td>\n",
              "      <td>...</td>\n",
              "      <td>0.127477</td>\n",
              "      <td>0.115293</td>\n",
              "      <td>0.154681</td>\n",
              "      <td>0.142378</td>\n",
              "      <td>0.226287</td>\n",
              "      <td>0.199556</td>\n",
              "      <td>0.225905</td>\n",
              "      <td>0.209185</td>\n",
              "      <td>0.220656</td>\n",
              "      <td>0.204857</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6000 rows × 256 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-89b8e1f0-83c5-4ce9-8b6e-9354eabea76e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-89b8e1f0-83c5-4ce9-8b6e-9354eabea76e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-89b8e1f0-83c5-4ce9-8b6e-9354eabea76e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3498e76-d0e3-4861-b405-14f8b925db75",
        "id": "70QtguG8u9a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(24000, 256)"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 라벨생성"
      ],
      "metadata": {
        "id": "qWUEooCuu9a3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "arr = np.array([])\n",
        "\n",
        "for i in range(1,101):\n",
        "  for j in range(240):\n",
        "    arr = np.append(arr, np.array([i]))"
      ],
      "metadata": {
        "id": "U8dg-Mkru9a3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train[256]=arr"
      ],
      "metadata": {
        "id": "0VSRZdHsu9a3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "outputId": "350f2101-e4b1-4bc5-cf5c-8781d4417497",
        "id": "JwrdroqGu9a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            0         1         2         3         4         5         6    \\\n",
              "0     -0.011566  0.077466  0.083038  0.015121 -0.024216  0.026618  0.112943   \n",
              "1     -0.105411  0.006199  0.035219 -0.038238 -0.087385 -0.047431  0.057264   \n",
              "2     -0.124635 -0.101820 -0.004986 -0.009573 -0.097080 -0.078287 -0.033934   \n",
              "3     -0.070665 -0.110059 -0.041584  0.011091 -0.073945 -0.116371 -0.065348   \n",
              "4      0.164484  0.137004  0.165505  0.176745  0.186086  0.149056  0.190527   \n",
              "...         ...       ...       ...       ...       ...       ...       ...   \n",
              "23995  0.047641  0.025272  0.001593  0.008873 -0.022388 -0.055809 -0.067971   \n",
              "23996 -0.017653  0.005880 -0.008398 -0.053956 -0.045114 -0.050892 -0.014791   \n",
              "23997  0.020267  0.062224 -0.014968  0.012301  0.073691  0.025622  0.056645   \n",
              "23998 -0.091136 -0.072359 -0.082491 -0.096773 -0.108624 -0.106044 -0.096813   \n",
              "23999 -0.081848 -0.092246 -0.069385 -0.066354 -0.034663 -0.035583 -0.052443   \n",
              "\n",
              "            7         8         9    ...       247       248       249  \\\n",
              "0      0.067018 -0.022367  0.035899  ... -0.080380 -0.088511  0.005901   \n",
              "1      0.011541 -0.063622 -0.054354  ... -0.102607 -0.121740 -0.038993   \n",
              "2      0.011788 -0.100740 -0.105788  ... -0.087241 -0.110820 -0.037730   \n",
              "3      0.005895 -0.075523 -0.138241  ... -0.074093 -0.113909 -0.113364   \n",
              "4      0.256168  0.278809  0.242770  ...  0.168813  0.158065  0.156737   \n",
              "...         ...       ...       ...  ...       ...       ...       ...   \n",
              "23995 -0.044163 -0.045987 -0.032190  ... -0.062754  0.002815 -0.032905   \n",
              "23996 -0.025570 -0.044989 -0.027848  ... -0.017953  0.000676  0.008635   \n",
              "23997  0.040058  0.073452  0.061997  ... -0.057440 -0.065800 -0.044000   \n",
              "23998 -0.094621 -0.049099 -0.103425  ... -0.095215 -0.045811 -0.093347   \n",
              "23999 -0.033073 -0.060994 -0.040115  ... -0.064874 -0.019088 -0.028823   \n",
              "\n",
              "            250       251       252       253       254       255    256  \n",
              "0     -0.000657 -0.066214 -0.061900 -0.011674  0.064162 -0.020820    1.0  \n",
              "1     -0.017367 -0.083201 -0.108995 -0.055099  0.004076 -0.050829    1.0  \n",
              "2      0.000719 -0.066072 -0.119163 -0.068725  0.003512 -0.040881    1.0  \n",
              "3     -0.053677 -0.015389 -0.088530 -0.122749 -0.019986 -0.033522    1.0  \n",
              "4      0.236019  0.242662  0.237836  0.242839  0.272923  0.371568    1.0  \n",
              "...         ...       ...       ...       ...       ...       ...    ...  \n",
              "23995 -0.016336 -0.045078  0.004621 -0.020431  0.005698 -0.048605  100.0  \n",
              "23996 -0.010605  0.004606  0.032398  0.011841 -0.004365  0.032981  100.0  \n",
              "23997 -0.082399 -0.083447 -0.065335 -0.088911 -0.080017 -0.076062  100.0  \n",
              "23998 -0.052513 -0.086620 -0.069087 -0.056044 -0.095822 -0.080069  100.0  \n",
              "23999 -0.049239 -0.026504 -0.037821 -0.061718 -0.066845 -0.059923  100.0  \n",
              "\n",
              "[24000 rows x 257 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2bd29241-a2fb-4ec5-8c53-6b7695b2ea82\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>247</th>\n",
              "      <th>248</th>\n",
              "      <th>249</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "      <th>256</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.011566</td>\n",
              "      <td>0.077466</td>\n",
              "      <td>0.083038</td>\n",
              "      <td>0.015121</td>\n",
              "      <td>-0.024216</td>\n",
              "      <td>0.026618</td>\n",
              "      <td>0.112943</td>\n",
              "      <td>0.067018</td>\n",
              "      <td>-0.022367</td>\n",
              "      <td>0.035899</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.080380</td>\n",
              "      <td>-0.088511</td>\n",
              "      <td>0.005901</td>\n",
              "      <td>-0.000657</td>\n",
              "      <td>-0.066214</td>\n",
              "      <td>-0.061900</td>\n",
              "      <td>-0.011674</td>\n",
              "      <td>0.064162</td>\n",
              "      <td>-0.020820</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.105411</td>\n",
              "      <td>0.006199</td>\n",
              "      <td>0.035219</td>\n",
              "      <td>-0.038238</td>\n",
              "      <td>-0.087385</td>\n",
              "      <td>-0.047431</td>\n",
              "      <td>0.057264</td>\n",
              "      <td>0.011541</td>\n",
              "      <td>-0.063622</td>\n",
              "      <td>-0.054354</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.102607</td>\n",
              "      <td>-0.121740</td>\n",
              "      <td>-0.038993</td>\n",
              "      <td>-0.017367</td>\n",
              "      <td>-0.083201</td>\n",
              "      <td>-0.108995</td>\n",
              "      <td>-0.055099</td>\n",
              "      <td>0.004076</td>\n",
              "      <td>-0.050829</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.124635</td>\n",
              "      <td>-0.101820</td>\n",
              "      <td>-0.004986</td>\n",
              "      <td>-0.009573</td>\n",
              "      <td>-0.097080</td>\n",
              "      <td>-0.078287</td>\n",
              "      <td>-0.033934</td>\n",
              "      <td>0.011788</td>\n",
              "      <td>-0.100740</td>\n",
              "      <td>-0.105788</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.087241</td>\n",
              "      <td>-0.110820</td>\n",
              "      <td>-0.037730</td>\n",
              "      <td>0.000719</td>\n",
              "      <td>-0.066072</td>\n",
              "      <td>-0.119163</td>\n",
              "      <td>-0.068725</td>\n",
              "      <td>0.003512</td>\n",
              "      <td>-0.040881</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.070665</td>\n",
              "      <td>-0.110059</td>\n",
              "      <td>-0.041584</td>\n",
              "      <td>0.011091</td>\n",
              "      <td>-0.073945</td>\n",
              "      <td>-0.116371</td>\n",
              "      <td>-0.065348</td>\n",
              "      <td>0.005895</td>\n",
              "      <td>-0.075523</td>\n",
              "      <td>-0.138241</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.074093</td>\n",
              "      <td>-0.113909</td>\n",
              "      <td>-0.113364</td>\n",
              "      <td>-0.053677</td>\n",
              "      <td>-0.015389</td>\n",
              "      <td>-0.088530</td>\n",
              "      <td>-0.122749</td>\n",
              "      <td>-0.019986</td>\n",
              "      <td>-0.033522</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.164484</td>\n",
              "      <td>0.137004</td>\n",
              "      <td>0.165505</td>\n",
              "      <td>0.176745</td>\n",
              "      <td>0.186086</td>\n",
              "      <td>0.149056</td>\n",
              "      <td>0.190527</td>\n",
              "      <td>0.256168</td>\n",
              "      <td>0.278809</td>\n",
              "      <td>0.242770</td>\n",
              "      <td>...</td>\n",
              "      <td>0.168813</td>\n",
              "      <td>0.158065</td>\n",
              "      <td>0.156737</td>\n",
              "      <td>0.236019</td>\n",
              "      <td>0.242662</td>\n",
              "      <td>0.237836</td>\n",
              "      <td>0.242839</td>\n",
              "      <td>0.272923</td>\n",
              "      <td>0.371568</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23995</th>\n",
              "      <td>0.047641</td>\n",
              "      <td>0.025272</td>\n",
              "      <td>0.001593</td>\n",
              "      <td>0.008873</td>\n",
              "      <td>-0.022388</td>\n",
              "      <td>-0.055809</td>\n",
              "      <td>-0.067971</td>\n",
              "      <td>-0.044163</td>\n",
              "      <td>-0.045987</td>\n",
              "      <td>-0.032190</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.062754</td>\n",
              "      <td>0.002815</td>\n",
              "      <td>-0.032905</td>\n",
              "      <td>-0.016336</td>\n",
              "      <td>-0.045078</td>\n",
              "      <td>0.004621</td>\n",
              "      <td>-0.020431</td>\n",
              "      <td>0.005698</td>\n",
              "      <td>-0.048605</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23996</th>\n",
              "      <td>-0.017653</td>\n",
              "      <td>0.005880</td>\n",
              "      <td>-0.008398</td>\n",
              "      <td>-0.053956</td>\n",
              "      <td>-0.045114</td>\n",
              "      <td>-0.050892</td>\n",
              "      <td>-0.014791</td>\n",
              "      <td>-0.025570</td>\n",
              "      <td>-0.044989</td>\n",
              "      <td>-0.027848</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.017953</td>\n",
              "      <td>0.000676</td>\n",
              "      <td>0.008635</td>\n",
              "      <td>-0.010605</td>\n",
              "      <td>0.004606</td>\n",
              "      <td>0.032398</td>\n",
              "      <td>0.011841</td>\n",
              "      <td>-0.004365</td>\n",
              "      <td>0.032981</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23997</th>\n",
              "      <td>0.020267</td>\n",
              "      <td>0.062224</td>\n",
              "      <td>-0.014968</td>\n",
              "      <td>0.012301</td>\n",
              "      <td>0.073691</td>\n",
              "      <td>0.025622</td>\n",
              "      <td>0.056645</td>\n",
              "      <td>0.040058</td>\n",
              "      <td>0.073452</td>\n",
              "      <td>0.061997</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.057440</td>\n",
              "      <td>-0.065800</td>\n",
              "      <td>-0.044000</td>\n",
              "      <td>-0.082399</td>\n",
              "      <td>-0.083447</td>\n",
              "      <td>-0.065335</td>\n",
              "      <td>-0.088911</td>\n",
              "      <td>-0.080017</td>\n",
              "      <td>-0.076062</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23998</th>\n",
              "      <td>-0.091136</td>\n",
              "      <td>-0.072359</td>\n",
              "      <td>-0.082491</td>\n",
              "      <td>-0.096773</td>\n",
              "      <td>-0.108624</td>\n",
              "      <td>-0.106044</td>\n",
              "      <td>-0.096813</td>\n",
              "      <td>-0.094621</td>\n",
              "      <td>-0.049099</td>\n",
              "      <td>-0.103425</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.095215</td>\n",
              "      <td>-0.045811</td>\n",
              "      <td>-0.093347</td>\n",
              "      <td>-0.052513</td>\n",
              "      <td>-0.086620</td>\n",
              "      <td>-0.069087</td>\n",
              "      <td>-0.056044</td>\n",
              "      <td>-0.095822</td>\n",
              "      <td>-0.080069</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23999</th>\n",
              "      <td>-0.081848</td>\n",
              "      <td>-0.092246</td>\n",
              "      <td>-0.069385</td>\n",
              "      <td>-0.066354</td>\n",
              "      <td>-0.034663</td>\n",
              "      <td>-0.035583</td>\n",
              "      <td>-0.052443</td>\n",
              "      <td>-0.033073</td>\n",
              "      <td>-0.060994</td>\n",
              "      <td>-0.040115</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.064874</td>\n",
              "      <td>-0.019088</td>\n",
              "      <td>-0.028823</td>\n",
              "      <td>-0.049239</td>\n",
              "      <td>-0.026504</td>\n",
              "      <td>-0.037821</td>\n",
              "      <td>-0.061718</td>\n",
              "      <td>-0.066845</td>\n",
              "      <td>-0.059923</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>24000 rows × 257 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2bd29241-a2fb-4ec5-8c53-6b7695b2ea82')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2bd29241-a2fb-4ec5-8c53-6b7695b2ea82 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2bd29241-a2fb-4ec5-8c53-6b7695b2ea82');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "arr_t = np.array([])\n",
        "\n",
        "for i in range(1,101):\n",
        "  for j in range(60):\n",
        "    arr_t = np.append(arr_t, np.array([i]))"
      ],
      "metadata": {
        "id": "WFjZ9noiu9a4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arr_v = np.array([])\n",
        "\n",
        "for i in range(1,101):\n",
        "  for j in range(60):\n",
        "    arr_v = np.append(arr_v, np.array([i]))"
      ],
      "metadata": {
        "id": "mkJ_shJTu9a4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_test[256]=arr_t\n",
        "data_val[256]=arr_v\n",
        "data_val"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "outputId": "4dbf70a0-18c5-4fba-c13a-db363ebc3be1",
        "id": "AVrjkGSnu9a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           0         1         2         3         4         5         6    \\\n",
              "0    -0.088677 -0.125790 -0.030982  0.007508 -0.029261 -0.106478 -0.080494   \n",
              "1    -0.166172 -0.207399 -0.221436 -0.207283 -0.178781 -0.161000 -0.193089   \n",
              "2    -0.145358 -0.148342 -0.020535 -0.017398 -0.130030 -0.147643 -0.060204   \n",
              "3     0.071963  0.015509 -0.112935 -0.062499  0.079877 -0.016757 -0.127871   \n",
              "4    -0.041662  0.030685 -0.020187 -0.129210 -0.085054  0.044992  0.021448   \n",
              "...        ...       ...       ...       ...       ...       ...       ...   \n",
              "5995  0.032231  0.030567 -0.020890  0.050221  0.017189 -0.008196 -0.015203   \n",
              "5996 -0.196898 -0.177660 -0.151701 -0.170922 -0.196274 -0.164125 -0.167807   \n",
              "5997  0.014198 -0.011697 -0.009543 -0.048490 -0.005837  0.002504 -0.030444   \n",
              "5998 -0.048497 -0.030079 -0.042822 -0.063725 -0.028908 -0.011291 -0.015405   \n",
              "5999 -0.040551 -0.032769 -0.024399 -0.043038 -0.040068 -0.059028 -0.083389   \n",
              "\n",
              "           7         8         9    ...       247       248       249  \\\n",
              "0    -0.011038 -0.021531 -0.121852  ...  0.017174 -0.070759 -0.096122   \n",
              "1    -0.206398 -0.157608 -0.133839  ... -0.207781 -0.165102 -0.191124   \n",
              "2     0.013275 -0.095876 -0.130336  ... -0.085840 -0.182494 -0.152887   \n",
              "3    -0.039045  0.030101  0.042338  ... -0.033357  0.015213 -0.075836   \n",
              "4    -0.110557 -0.113222 -0.015198  ... -0.124194 -0.064253 -0.014054   \n",
              "...        ...       ...       ...  ...       ...       ...       ...   \n",
              "5995 -0.032563 -0.038065 -0.040280  ... -0.038393 -0.042812 -0.032542   \n",
              "5996 -0.145068 -0.153429 -0.147951  ... -0.014796  0.048950  0.007776   \n",
              "5997  0.011946  0.002635  0.027564  ... -0.081514 -0.086519 -0.102934   \n",
              "5998 -0.037688 -0.033962 -0.027736  ... -0.023651 -0.016484 -0.058906   \n",
              "5999 -0.066660 -0.039522 -0.092334  ... -0.051936 -0.021097  0.032533   \n",
              "\n",
              "           250       251       252       253       254       255    256  \n",
              "0     0.007007 -0.004853 -0.026522 -0.062090  0.001993  0.019837    1.0  \n",
              "1    -0.196126 -0.205478 -0.184471 -0.151804 -0.194417 -0.193371    1.0  \n",
              "2    -0.014291 -0.043764 -0.122897 -0.128260 -0.017722 -0.028884    1.0  \n",
              "3    -0.154705 -0.100604 -0.008633  0.015829  0.039830  0.248372    1.0  \n",
              "4    -0.089064 -0.151946 -0.088977 -0.031070 -0.035743 -0.117336    1.0  \n",
              "...        ...       ...       ...       ...       ...       ...    ...  \n",
              "5995  0.037526 -0.009246  0.006291  0.019648 -0.020407 -0.015023  100.0  \n",
              "5996  0.021993  0.008940  0.032578  0.035396  0.003384  0.046122  100.0  \n",
              "5997 -0.041559 -0.065864 -0.047559 -0.085704 -0.048828 -0.085453  100.0  \n",
              "5998  0.005502 -0.050809 -0.055031 -0.001622 -0.067492 -0.039172  100.0  \n",
              "5999  0.108753  0.185233  0.263694  0.291106  0.295209  0.143481  100.0  \n",
              "\n",
              "[6000 rows x 257 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b3c7adab-7b8a-4e98-9489-37a900c0fb56\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>247</th>\n",
              "      <th>248</th>\n",
              "      <th>249</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "      <th>256</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.088677</td>\n",
              "      <td>-0.125790</td>\n",
              "      <td>-0.030982</td>\n",
              "      <td>0.007508</td>\n",
              "      <td>-0.029261</td>\n",
              "      <td>-0.106478</td>\n",
              "      <td>-0.080494</td>\n",
              "      <td>-0.011038</td>\n",
              "      <td>-0.021531</td>\n",
              "      <td>-0.121852</td>\n",
              "      <td>...</td>\n",
              "      <td>0.017174</td>\n",
              "      <td>-0.070759</td>\n",
              "      <td>-0.096122</td>\n",
              "      <td>0.007007</td>\n",
              "      <td>-0.004853</td>\n",
              "      <td>-0.026522</td>\n",
              "      <td>-0.062090</td>\n",
              "      <td>0.001993</td>\n",
              "      <td>0.019837</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.166172</td>\n",
              "      <td>-0.207399</td>\n",
              "      <td>-0.221436</td>\n",
              "      <td>-0.207283</td>\n",
              "      <td>-0.178781</td>\n",
              "      <td>-0.161000</td>\n",
              "      <td>-0.193089</td>\n",
              "      <td>-0.206398</td>\n",
              "      <td>-0.157608</td>\n",
              "      <td>-0.133839</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.207781</td>\n",
              "      <td>-0.165102</td>\n",
              "      <td>-0.191124</td>\n",
              "      <td>-0.196126</td>\n",
              "      <td>-0.205478</td>\n",
              "      <td>-0.184471</td>\n",
              "      <td>-0.151804</td>\n",
              "      <td>-0.194417</td>\n",
              "      <td>-0.193371</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.145358</td>\n",
              "      <td>-0.148342</td>\n",
              "      <td>-0.020535</td>\n",
              "      <td>-0.017398</td>\n",
              "      <td>-0.130030</td>\n",
              "      <td>-0.147643</td>\n",
              "      <td>-0.060204</td>\n",
              "      <td>0.013275</td>\n",
              "      <td>-0.095876</td>\n",
              "      <td>-0.130336</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.085840</td>\n",
              "      <td>-0.182494</td>\n",
              "      <td>-0.152887</td>\n",
              "      <td>-0.014291</td>\n",
              "      <td>-0.043764</td>\n",
              "      <td>-0.122897</td>\n",
              "      <td>-0.128260</td>\n",
              "      <td>-0.017722</td>\n",
              "      <td>-0.028884</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.071963</td>\n",
              "      <td>0.015509</td>\n",
              "      <td>-0.112935</td>\n",
              "      <td>-0.062499</td>\n",
              "      <td>0.079877</td>\n",
              "      <td>-0.016757</td>\n",
              "      <td>-0.127871</td>\n",
              "      <td>-0.039045</td>\n",
              "      <td>0.030101</td>\n",
              "      <td>0.042338</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.033357</td>\n",
              "      <td>0.015213</td>\n",
              "      <td>-0.075836</td>\n",
              "      <td>-0.154705</td>\n",
              "      <td>-0.100604</td>\n",
              "      <td>-0.008633</td>\n",
              "      <td>0.015829</td>\n",
              "      <td>0.039830</td>\n",
              "      <td>0.248372</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.041662</td>\n",
              "      <td>0.030685</td>\n",
              "      <td>-0.020187</td>\n",
              "      <td>-0.129210</td>\n",
              "      <td>-0.085054</td>\n",
              "      <td>0.044992</td>\n",
              "      <td>0.021448</td>\n",
              "      <td>-0.110557</td>\n",
              "      <td>-0.113222</td>\n",
              "      <td>-0.015198</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.124194</td>\n",
              "      <td>-0.064253</td>\n",
              "      <td>-0.014054</td>\n",
              "      <td>-0.089064</td>\n",
              "      <td>-0.151946</td>\n",
              "      <td>-0.088977</td>\n",
              "      <td>-0.031070</td>\n",
              "      <td>-0.035743</td>\n",
              "      <td>-0.117336</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5995</th>\n",
              "      <td>0.032231</td>\n",
              "      <td>0.030567</td>\n",
              "      <td>-0.020890</td>\n",
              "      <td>0.050221</td>\n",
              "      <td>0.017189</td>\n",
              "      <td>-0.008196</td>\n",
              "      <td>-0.015203</td>\n",
              "      <td>-0.032563</td>\n",
              "      <td>-0.038065</td>\n",
              "      <td>-0.040280</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.038393</td>\n",
              "      <td>-0.042812</td>\n",
              "      <td>-0.032542</td>\n",
              "      <td>0.037526</td>\n",
              "      <td>-0.009246</td>\n",
              "      <td>0.006291</td>\n",
              "      <td>0.019648</td>\n",
              "      <td>-0.020407</td>\n",
              "      <td>-0.015023</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5996</th>\n",
              "      <td>-0.196898</td>\n",
              "      <td>-0.177660</td>\n",
              "      <td>-0.151701</td>\n",
              "      <td>-0.170922</td>\n",
              "      <td>-0.196274</td>\n",
              "      <td>-0.164125</td>\n",
              "      <td>-0.167807</td>\n",
              "      <td>-0.145068</td>\n",
              "      <td>-0.153429</td>\n",
              "      <td>-0.147951</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.014796</td>\n",
              "      <td>0.048950</td>\n",
              "      <td>0.007776</td>\n",
              "      <td>0.021993</td>\n",
              "      <td>0.008940</td>\n",
              "      <td>0.032578</td>\n",
              "      <td>0.035396</td>\n",
              "      <td>0.003384</td>\n",
              "      <td>0.046122</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5997</th>\n",
              "      <td>0.014198</td>\n",
              "      <td>-0.011697</td>\n",
              "      <td>-0.009543</td>\n",
              "      <td>-0.048490</td>\n",
              "      <td>-0.005837</td>\n",
              "      <td>0.002504</td>\n",
              "      <td>-0.030444</td>\n",
              "      <td>0.011946</td>\n",
              "      <td>0.002635</td>\n",
              "      <td>0.027564</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.081514</td>\n",
              "      <td>-0.086519</td>\n",
              "      <td>-0.102934</td>\n",
              "      <td>-0.041559</td>\n",
              "      <td>-0.065864</td>\n",
              "      <td>-0.047559</td>\n",
              "      <td>-0.085704</td>\n",
              "      <td>-0.048828</td>\n",
              "      <td>-0.085453</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5998</th>\n",
              "      <td>-0.048497</td>\n",
              "      <td>-0.030079</td>\n",
              "      <td>-0.042822</td>\n",
              "      <td>-0.063725</td>\n",
              "      <td>-0.028908</td>\n",
              "      <td>-0.011291</td>\n",
              "      <td>-0.015405</td>\n",
              "      <td>-0.037688</td>\n",
              "      <td>-0.033962</td>\n",
              "      <td>-0.027736</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.023651</td>\n",
              "      <td>-0.016484</td>\n",
              "      <td>-0.058906</td>\n",
              "      <td>0.005502</td>\n",
              "      <td>-0.050809</td>\n",
              "      <td>-0.055031</td>\n",
              "      <td>-0.001622</td>\n",
              "      <td>-0.067492</td>\n",
              "      <td>-0.039172</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5999</th>\n",
              "      <td>-0.040551</td>\n",
              "      <td>-0.032769</td>\n",
              "      <td>-0.024399</td>\n",
              "      <td>-0.043038</td>\n",
              "      <td>-0.040068</td>\n",
              "      <td>-0.059028</td>\n",
              "      <td>-0.083389</td>\n",
              "      <td>-0.066660</td>\n",
              "      <td>-0.039522</td>\n",
              "      <td>-0.092334</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.051936</td>\n",
              "      <td>-0.021097</td>\n",
              "      <td>0.032533</td>\n",
              "      <td>0.108753</td>\n",
              "      <td>0.185233</td>\n",
              "      <td>0.263694</td>\n",
              "      <td>0.291106</td>\n",
              "      <td>0.295209</td>\n",
              "      <td>0.143481</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6000 rows × 257 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b3c7adab-7b8a-4e98-9489-37a900c0fb56')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b3c7adab-7b8a-4e98-9489-37a900c0fb56 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b3c7adab-7b8a-4e98-9489-37a900c0fb56');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# y_train = arr\n",
        "# y_test = arr_t\n",
        "# y_val = arr_v"
      ],
      "metadata": {
        "id": "m5ppKkGLu9a4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터 전체 섞기"
      ],
      "metadata": {
        "id": "sadv6pVlu9a4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_total=pd.concat([data_train, data_test, data_val])\n",
        "data_total"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "outputId": "12347800-84c7-44e3-f315-9264498fa629",
        "id": "K_cN21DRu9a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           0         1         2         3         4         5         6    \\\n",
              "0    -0.011566  0.077466  0.083038  0.015121 -0.024216  0.026618  0.112943   \n",
              "1    -0.105411  0.006199  0.035219 -0.038238 -0.087385 -0.047431  0.057264   \n",
              "2    -0.124635 -0.101820 -0.004986 -0.009573 -0.097080 -0.078287 -0.033934   \n",
              "3    -0.070665 -0.110059 -0.041584  0.011091 -0.073945 -0.116371 -0.065348   \n",
              "4     0.164484  0.137004  0.165505  0.176745  0.186086  0.149056  0.190527   \n",
              "...        ...       ...       ...       ...       ...       ...       ...   \n",
              "5995  0.032231  0.030567 -0.020890  0.050221  0.017189 -0.008196 -0.015203   \n",
              "5996 -0.196898 -0.177660 -0.151701 -0.170922 -0.196274 -0.164125 -0.167807   \n",
              "5997  0.014198 -0.011697 -0.009543 -0.048490 -0.005837  0.002504 -0.030444   \n",
              "5998 -0.048497 -0.030079 -0.042822 -0.063725 -0.028908 -0.011291 -0.015405   \n",
              "5999 -0.040551 -0.032769 -0.024399 -0.043038 -0.040068 -0.059028 -0.083389   \n",
              "\n",
              "           7         8         9    ...       247       248       249  \\\n",
              "0     0.067018 -0.022367  0.035899  ... -0.080380 -0.088511  0.005901   \n",
              "1     0.011541 -0.063622 -0.054354  ... -0.102607 -0.121740 -0.038993   \n",
              "2     0.011788 -0.100740 -0.105788  ... -0.087241 -0.110820 -0.037730   \n",
              "3     0.005895 -0.075523 -0.138241  ... -0.074093 -0.113909 -0.113364   \n",
              "4     0.256168  0.278809  0.242770  ...  0.168813  0.158065  0.156737   \n",
              "...        ...       ...       ...  ...       ...       ...       ...   \n",
              "5995 -0.032563 -0.038065 -0.040280  ... -0.038393 -0.042812 -0.032542   \n",
              "5996 -0.145068 -0.153429 -0.147951  ... -0.014796  0.048950  0.007776   \n",
              "5997  0.011946  0.002635  0.027564  ... -0.081514 -0.086519 -0.102934   \n",
              "5998 -0.037688 -0.033962 -0.027736  ... -0.023651 -0.016484 -0.058906   \n",
              "5999 -0.066660 -0.039522 -0.092334  ... -0.051936 -0.021097  0.032533   \n",
              "\n",
              "           250       251       252       253       254       255    256  \n",
              "0    -0.000657 -0.066214 -0.061900 -0.011674  0.064162 -0.020820    1.0  \n",
              "1    -0.017367 -0.083201 -0.108995 -0.055099  0.004076 -0.050829    1.0  \n",
              "2     0.000719 -0.066072 -0.119163 -0.068725  0.003512 -0.040881    1.0  \n",
              "3    -0.053677 -0.015389 -0.088530 -0.122749 -0.019986 -0.033522    1.0  \n",
              "4     0.236019  0.242662  0.237836  0.242839  0.272923  0.371568    1.0  \n",
              "...        ...       ...       ...       ...       ...       ...    ...  \n",
              "5995  0.037526 -0.009246  0.006291  0.019648 -0.020407 -0.015023  100.0  \n",
              "5996  0.021993  0.008940  0.032578  0.035396  0.003384  0.046122  100.0  \n",
              "5997 -0.041559 -0.065864 -0.047559 -0.085704 -0.048828 -0.085453  100.0  \n",
              "5998  0.005502 -0.050809 -0.055031 -0.001622 -0.067492 -0.039172  100.0  \n",
              "5999  0.108753  0.185233  0.263694  0.291106  0.295209  0.143481  100.0  \n",
              "\n",
              "[36000 rows x 257 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2c2b49d9-a02b-40f8-b0d3-c98583a6dbfc\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>247</th>\n",
              "      <th>248</th>\n",
              "      <th>249</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "      <th>256</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.011566</td>\n",
              "      <td>0.077466</td>\n",
              "      <td>0.083038</td>\n",
              "      <td>0.015121</td>\n",
              "      <td>-0.024216</td>\n",
              "      <td>0.026618</td>\n",
              "      <td>0.112943</td>\n",
              "      <td>0.067018</td>\n",
              "      <td>-0.022367</td>\n",
              "      <td>0.035899</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.080380</td>\n",
              "      <td>-0.088511</td>\n",
              "      <td>0.005901</td>\n",
              "      <td>-0.000657</td>\n",
              "      <td>-0.066214</td>\n",
              "      <td>-0.061900</td>\n",
              "      <td>-0.011674</td>\n",
              "      <td>0.064162</td>\n",
              "      <td>-0.020820</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.105411</td>\n",
              "      <td>0.006199</td>\n",
              "      <td>0.035219</td>\n",
              "      <td>-0.038238</td>\n",
              "      <td>-0.087385</td>\n",
              "      <td>-0.047431</td>\n",
              "      <td>0.057264</td>\n",
              "      <td>0.011541</td>\n",
              "      <td>-0.063622</td>\n",
              "      <td>-0.054354</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.102607</td>\n",
              "      <td>-0.121740</td>\n",
              "      <td>-0.038993</td>\n",
              "      <td>-0.017367</td>\n",
              "      <td>-0.083201</td>\n",
              "      <td>-0.108995</td>\n",
              "      <td>-0.055099</td>\n",
              "      <td>0.004076</td>\n",
              "      <td>-0.050829</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.124635</td>\n",
              "      <td>-0.101820</td>\n",
              "      <td>-0.004986</td>\n",
              "      <td>-0.009573</td>\n",
              "      <td>-0.097080</td>\n",
              "      <td>-0.078287</td>\n",
              "      <td>-0.033934</td>\n",
              "      <td>0.011788</td>\n",
              "      <td>-0.100740</td>\n",
              "      <td>-0.105788</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.087241</td>\n",
              "      <td>-0.110820</td>\n",
              "      <td>-0.037730</td>\n",
              "      <td>0.000719</td>\n",
              "      <td>-0.066072</td>\n",
              "      <td>-0.119163</td>\n",
              "      <td>-0.068725</td>\n",
              "      <td>0.003512</td>\n",
              "      <td>-0.040881</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.070665</td>\n",
              "      <td>-0.110059</td>\n",
              "      <td>-0.041584</td>\n",
              "      <td>0.011091</td>\n",
              "      <td>-0.073945</td>\n",
              "      <td>-0.116371</td>\n",
              "      <td>-0.065348</td>\n",
              "      <td>0.005895</td>\n",
              "      <td>-0.075523</td>\n",
              "      <td>-0.138241</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.074093</td>\n",
              "      <td>-0.113909</td>\n",
              "      <td>-0.113364</td>\n",
              "      <td>-0.053677</td>\n",
              "      <td>-0.015389</td>\n",
              "      <td>-0.088530</td>\n",
              "      <td>-0.122749</td>\n",
              "      <td>-0.019986</td>\n",
              "      <td>-0.033522</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.164484</td>\n",
              "      <td>0.137004</td>\n",
              "      <td>0.165505</td>\n",
              "      <td>0.176745</td>\n",
              "      <td>0.186086</td>\n",
              "      <td>0.149056</td>\n",
              "      <td>0.190527</td>\n",
              "      <td>0.256168</td>\n",
              "      <td>0.278809</td>\n",
              "      <td>0.242770</td>\n",
              "      <td>...</td>\n",
              "      <td>0.168813</td>\n",
              "      <td>0.158065</td>\n",
              "      <td>0.156737</td>\n",
              "      <td>0.236019</td>\n",
              "      <td>0.242662</td>\n",
              "      <td>0.237836</td>\n",
              "      <td>0.242839</td>\n",
              "      <td>0.272923</td>\n",
              "      <td>0.371568</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5995</th>\n",
              "      <td>0.032231</td>\n",
              "      <td>0.030567</td>\n",
              "      <td>-0.020890</td>\n",
              "      <td>0.050221</td>\n",
              "      <td>0.017189</td>\n",
              "      <td>-0.008196</td>\n",
              "      <td>-0.015203</td>\n",
              "      <td>-0.032563</td>\n",
              "      <td>-0.038065</td>\n",
              "      <td>-0.040280</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.038393</td>\n",
              "      <td>-0.042812</td>\n",
              "      <td>-0.032542</td>\n",
              "      <td>0.037526</td>\n",
              "      <td>-0.009246</td>\n",
              "      <td>0.006291</td>\n",
              "      <td>0.019648</td>\n",
              "      <td>-0.020407</td>\n",
              "      <td>-0.015023</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5996</th>\n",
              "      <td>-0.196898</td>\n",
              "      <td>-0.177660</td>\n",
              "      <td>-0.151701</td>\n",
              "      <td>-0.170922</td>\n",
              "      <td>-0.196274</td>\n",
              "      <td>-0.164125</td>\n",
              "      <td>-0.167807</td>\n",
              "      <td>-0.145068</td>\n",
              "      <td>-0.153429</td>\n",
              "      <td>-0.147951</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.014796</td>\n",
              "      <td>0.048950</td>\n",
              "      <td>0.007776</td>\n",
              "      <td>0.021993</td>\n",
              "      <td>0.008940</td>\n",
              "      <td>0.032578</td>\n",
              "      <td>0.035396</td>\n",
              "      <td>0.003384</td>\n",
              "      <td>0.046122</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5997</th>\n",
              "      <td>0.014198</td>\n",
              "      <td>-0.011697</td>\n",
              "      <td>-0.009543</td>\n",
              "      <td>-0.048490</td>\n",
              "      <td>-0.005837</td>\n",
              "      <td>0.002504</td>\n",
              "      <td>-0.030444</td>\n",
              "      <td>0.011946</td>\n",
              "      <td>0.002635</td>\n",
              "      <td>0.027564</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.081514</td>\n",
              "      <td>-0.086519</td>\n",
              "      <td>-0.102934</td>\n",
              "      <td>-0.041559</td>\n",
              "      <td>-0.065864</td>\n",
              "      <td>-0.047559</td>\n",
              "      <td>-0.085704</td>\n",
              "      <td>-0.048828</td>\n",
              "      <td>-0.085453</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5998</th>\n",
              "      <td>-0.048497</td>\n",
              "      <td>-0.030079</td>\n",
              "      <td>-0.042822</td>\n",
              "      <td>-0.063725</td>\n",
              "      <td>-0.028908</td>\n",
              "      <td>-0.011291</td>\n",
              "      <td>-0.015405</td>\n",
              "      <td>-0.037688</td>\n",
              "      <td>-0.033962</td>\n",
              "      <td>-0.027736</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.023651</td>\n",
              "      <td>-0.016484</td>\n",
              "      <td>-0.058906</td>\n",
              "      <td>0.005502</td>\n",
              "      <td>-0.050809</td>\n",
              "      <td>-0.055031</td>\n",
              "      <td>-0.001622</td>\n",
              "      <td>-0.067492</td>\n",
              "      <td>-0.039172</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5999</th>\n",
              "      <td>-0.040551</td>\n",
              "      <td>-0.032769</td>\n",
              "      <td>-0.024399</td>\n",
              "      <td>-0.043038</td>\n",
              "      <td>-0.040068</td>\n",
              "      <td>-0.059028</td>\n",
              "      <td>-0.083389</td>\n",
              "      <td>-0.066660</td>\n",
              "      <td>-0.039522</td>\n",
              "      <td>-0.092334</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.051936</td>\n",
              "      <td>-0.021097</td>\n",
              "      <td>0.032533</td>\n",
              "      <td>0.108753</td>\n",
              "      <td>0.185233</td>\n",
              "      <td>0.263694</td>\n",
              "      <td>0.291106</td>\n",
              "      <td>0.295209</td>\n",
              "      <td>0.143481</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>36000 rows × 257 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2c2b49d9-a02b-40f8-b0d3-c98583a6dbfc')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2c2b49d9-a02b-40f8-b0d3-c98583a6dbfc button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2c2b49d9-a02b-40f8-b0d3-c98583a6dbfc');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_total=data_total.sample(frac=1)"
      ],
      "metadata": {
        "id": "LvcMuX_Su9a4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train=data_total.iloc[0:24000, :]\n",
        "data_val=data_total.iloc[24000:30000, :]\n",
        "data_test=data_total.iloc[30000:36000,:]"
      ],
      "metadata": {
        "id": "AvtkEzJWu9a6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "outputId": "a693ebf0-1ba9-4a26-8994-e85bec29fcf0",
        "id": "gVebQZwdu9a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            0         1         2         3         4         5         6    \\\n",
              "2160  -0.067399 -0.055313 -0.074646 -0.114919 -0.088871 -0.111103 -0.079715   \n",
              "20919 -0.039001 -0.049287 -0.064703 -0.075447 -0.068599 -0.085801 -0.038972   \n",
              "973    0.013016 -0.011119 -0.039795  0.015698 -0.013839 -0.056547 -0.028326   \n",
              "1238  -0.012839 -0.007891 -0.000335 -0.017209 -0.042385  0.004278 -0.018220   \n",
              "4149   0.162093  0.259302  0.278531  0.214980  0.245281  0.184952  0.160723   \n",
              "...         ...       ...       ...       ...       ...       ...       ...   \n",
              "8202  -0.006040 -0.023679 -0.003979  0.004199  0.014016  0.003021 -0.013575   \n",
              "5298  -0.234748 -0.194218 -0.143088 -0.116647 -0.096146 -0.075835 -0.060663   \n",
              "4371  -0.042573 -0.026527 -0.038931 -0.040005 -0.065119 -0.035112  0.002794   \n",
              "3576   0.039162 -0.001734  0.004371 -0.030724  0.003882  0.129788  0.027695   \n",
              "2246  -0.037558 -0.028723 -0.035768  0.002306 -0.049590 -0.022046  0.006337   \n",
              "\n",
              "            7         8         9    ...       247       248       249  \\\n",
              "2160  -0.080806 -0.070167 -0.029497  ...  0.017222  0.009899 -0.004473   \n",
              "20919 -0.034171 -0.073489 -0.067426  ... -0.050403 -0.042220 -0.078167   \n",
              "973    0.008265 -0.002105 -0.010835  ... -0.108325 -0.112022 -0.108531   \n",
              "1238   0.001170 -0.015320 -0.020522  ... -0.041861 -0.045602 -0.031203   \n",
              "4149   0.035515 -0.023882 -0.040479  ... -0.051584 -0.040018 -0.069031   \n",
              "...         ...       ...       ...  ...       ...       ...       ...   \n",
              "8202  -0.007312 -0.046171 -0.016271  ... -0.066009 -0.075146 -0.032192   \n",
              "5298  -0.047532 -0.003239 -0.032327  ... -0.004386 -0.045173 -0.044530   \n",
              "4371  -0.054849 -0.045523 -0.051146  ...  0.046142  0.003203  0.054244   \n",
              "3576   0.035203  0.010411 -0.036581  ... -0.120116 -0.078387 -0.139557   \n",
              "2246  -0.037121 -0.009859 -0.001557  ...  0.073934  0.034510  0.027490   \n",
              "\n",
              "            250       251       252       253       254       255   256  \n",
              "2160   0.059794  0.014011 -0.047852 -0.048926 -0.075550 -0.053034  37.0  \n",
              "20919 -0.022235 -0.015043 -0.028401 -0.049329 -0.014087 -0.047666  88.0  \n",
              "973   -0.119081 -0.121462 -0.090844 -0.160078 -0.088232 -0.114928  17.0  \n",
              "1238   0.011176 -0.056984 -0.035815 -0.028645 -0.029925 -0.026754   6.0  \n",
              "4149  -0.051542 -0.050382 -0.028941 -0.031798 -0.047034 -0.036189  70.0  \n",
              "...         ...       ...       ...       ...       ...       ...   ...  \n",
              "8202   0.024472  0.087386  0.153660  0.207485  0.299180  0.282465  35.0  \n",
              "5298  -0.004445 -0.020140 -0.004285 -0.043359 -0.019732 -0.003674  89.0  \n",
              "4371   0.003735  0.000176 -0.011942  0.004859  0.008150 -0.002779  73.0  \n",
              "3576  -0.167027 -0.096697 -0.065567 -0.172236 -0.129005 -0.096974  15.0  \n",
              "2246  -0.025038  0.002396  0.033644 -0.058196 -0.029304 -0.041908  38.0  \n",
              "\n",
              "[24000 rows x 257 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-30e4542f-fe0b-4d28-b890-48c380d9e7bf\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>247</th>\n",
              "      <th>248</th>\n",
              "      <th>249</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "      <th>256</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2160</th>\n",
              "      <td>-0.067399</td>\n",
              "      <td>-0.055313</td>\n",
              "      <td>-0.074646</td>\n",
              "      <td>-0.114919</td>\n",
              "      <td>-0.088871</td>\n",
              "      <td>-0.111103</td>\n",
              "      <td>-0.079715</td>\n",
              "      <td>-0.080806</td>\n",
              "      <td>-0.070167</td>\n",
              "      <td>-0.029497</td>\n",
              "      <td>...</td>\n",
              "      <td>0.017222</td>\n",
              "      <td>0.009899</td>\n",
              "      <td>-0.004473</td>\n",
              "      <td>0.059794</td>\n",
              "      <td>0.014011</td>\n",
              "      <td>-0.047852</td>\n",
              "      <td>-0.048926</td>\n",
              "      <td>-0.075550</td>\n",
              "      <td>-0.053034</td>\n",
              "      <td>37.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20919</th>\n",
              "      <td>-0.039001</td>\n",
              "      <td>-0.049287</td>\n",
              "      <td>-0.064703</td>\n",
              "      <td>-0.075447</td>\n",
              "      <td>-0.068599</td>\n",
              "      <td>-0.085801</td>\n",
              "      <td>-0.038972</td>\n",
              "      <td>-0.034171</td>\n",
              "      <td>-0.073489</td>\n",
              "      <td>-0.067426</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.050403</td>\n",
              "      <td>-0.042220</td>\n",
              "      <td>-0.078167</td>\n",
              "      <td>-0.022235</td>\n",
              "      <td>-0.015043</td>\n",
              "      <td>-0.028401</td>\n",
              "      <td>-0.049329</td>\n",
              "      <td>-0.014087</td>\n",
              "      <td>-0.047666</td>\n",
              "      <td>88.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>973</th>\n",
              "      <td>0.013016</td>\n",
              "      <td>-0.011119</td>\n",
              "      <td>-0.039795</td>\n",
              "      <td>0.015698</td>\n",
              "      <td>-0.013839</td>\n",
              "      <td>-0.056547</td>\n",
              "      <td>-0.028326</td>\n",
              "      <td>0.008265</td>\n",
              "      <td>-0.002105</td>\n",
              "      <td>-0.010835</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.108325</td>\n",
              "      <td>-0.112022</td>\n",
              "      <td>-0.108531</td>\n",
              "      <td>-0.119081</td>\n",
              "      <td>-0.121462</td>\n",
              "      <td>-0.090844</td>\n",
              "      <td>-0.160078</td>\n",
              "      <td>-0.088232</td>\n",
              "      <td>-0.114928</td>\n",
              "      <td>17.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1238</th>\n",
              "      <td>-0.012839</td>\n",
              "      <td>-0.007891</td>\n",
              "      <td>-0.000335</td>\n",
              "      <td>-0.017209</td>\n",
              "      <td>-0.042385</td>\n",
              "      <td>0.004278</td>\n",
              "      <td>-0.018220</td>\n",
              "      <td>0.001170</td>\n",
              "      <td>-0.015320</td>\n",
              "      <td>-0.020522</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.041861</td>\n",
              "      <td>-0.045602</td>\n",
              "      <td>-0.031203</td>\n",
              "      <td>0.011176</td>\n",
              "      <td>-0.056984</td>\n",
              "      <td>-0.035815</td>\n",
              "      <td>-0.028645</td>\n",
              "      <td>-0.029925</td>\n",
              "      <td>-0.026754</td>\n",
              "      <td>6.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4149</th>\n",
              "      <td>0.162093</td>\n",
              "      <td>0.259302</td>\n",
              "      <td>0.278531</td>\n",
              "      <td>0.214980</td>\n",
              "      <td>0.245281</td>\n",
              "      <td>0.184952</td>\n",
              "      <td>0.160723</td>\n",
              "      <td>0.035515</td>\n",
              "      <td>-0.023882</td>\n",
              "      <td>-0.040479</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.051584</td>\n",
              "      <td>-0.040018</td>\n",
              "      <td>-0.069031</td>\n",
              "      <td>-0.051542</td>\n",
              "      <td>-0.050382</td>\n",
              "      <td>-0.028941</td>\n",
              "      <td>-0.031798</td>\n",
              "      <td>-0.047034</td>\n",
              "      <td>-0.036189</td>\n",
              "      <td>70.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8202</th>\n",
              "      <td>-0.006040</td>\n",
              "      <td>-0.023679</td>\n",
              "      <td>-0.003979</td>\n",
              "      <td>0.004199</td>\n",
              "      <td>0.014016</td>\n",
              "      <td>0.003021</td>\n",
              "      <td>-0.013575</td>\n",
              "      <td>-0.007312</td>\n",
              "      <td>-0.046171</td>\n",
              "      <td>-0.016271</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.066009</td>\n",
              "      <td>-0.075146</td>\n",
              "      <td>-0.032192</td>\n",
              "      <td>0.024472</td>\n",
              "      <td>0.087386</td>\n",
              "      <td>0.153660</td>\n",
              "      <td>0.207485</td>\n",
              "      <td>0.299180</td>\n",
              "      <td>0.282465</td>\n",
              "      <td>35.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5298</th>\n",
              "      <td>-0.234748</td>\n",
              "      <td>-0.194218</td>\n",
              "      <td>-0.143088</td>\n",
              "      <td>-0.116647</td>\n",
              "      <td>-0.096146</td>\n",
              "      <td>-0.075835</td>\n",
              "      <td>-0.060663</td>\n",
              "      <td>-0.047532</td>\n",
              "      <td>-0.003239</td>\n",
              "      <td>-0.032327</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.004386</td>\n",
              "      <td>-0.045173</td>\n",
              "      <td>-0.044530</td>\n",
              "      <td>-0.004445</td>\n",
              "      <td>-0.020140</td>\n",
              "      <td>-0.004285</td>\n",
              "      <td>-0.043359</td>\n",
              "      <td>-0.019732</td>\n",
              "      <td>-0.003674</td>\n",
              "      <td>89.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4371</th>\n",
              "      <td>-0.042573</td>\n",
              "      <td>-0.026527</td>\n",
              "      <td>-0.038931</td>\n",
              "      <td>-0.040005</td>\n",
              "      <td>-0.065119</td>\n",
              "      <td>-0.035112</td>\n",
              "      <td>0.002794</td>\n",
              "      <td>-0.054849</td>\n",
              "      <td>-0.045523</td>\n",
              "      <td>-0.051146</td>\n",
              "      <td>...</td>\n",
              "      <td>0.046142</td>\n",
              "      <td>0.003203</td>\n",
              "      <td>0.054244</td>\n",
              "      <td>0.003735</td>\n",
              "      <td>0.000176</td>\n",
              "      <td>-0.011942</td>\n",
              "      <td>0.004859</td>\n",
              "      <td>0.008150</td>\n",
              "      <td>-0.002779</td>\n",
              "      <td>73.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3576</th>\n",
              "      <td>0.039162</td>\n",
              "      <td>-0.001734</td>\n",
              "      <td>0.004371</td>\n",
              "      <td>-0.030724</td>\n",
              "      <td>0.003882</td>\n",
              "      <td>0.129788</td>\n",
              "      <td>0.027695</td>\n",
              "      <td>0.035203</td>\n",
              "      <td>0.010411</td>\n",
              "      <td>-0.036581</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.120116</td>\n",
              "      <td>-0.078387</td>\n",
              "      <td>-0.139557</td>\n",
              "      <td>-0.167027</td>\n",
              "      <td>-0.096697</td>\n",
              "      <td>-0.065567</td>\n",
              "      <td>-0.172236</td>\n",
              "      <td>-0.129005</td>\n",
              "      <td>-0.096974</td>\n",
              "      <td>15.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2246</th>\n",
              "      <td>-0.037558</td>\n",
              "      <td>-0.028723</td>\n",
              "      <td>-0.035768</td>\n",
              "      <td>0.002306</td>\n",
              "      <td>-0.049590</td>\n",
              "      <td>-0.022046</td>\n",
              "      <td>0.006337</td>\n",
              "      <td>-0.037121</td>\n",
              "      <td>-0.009859</td>\n",
              "      <td>-0.001557</td>\n",
              "      <td>...</td>\n",
              "      <td>0.073934</td>\n",
              "      <td>0.034510</td>\n",
              "      <td>0.027490</td>\n",
              "      <td>-0.025038</td>\n",
              "      <td>0.002396</td>\n",
              "      <td>0.033644</td>\n",
              "      <td>-0.058196</td>\n",
              "      <td>-0.029304</td>\n",
              "      <td>-0.041908</td>\n",
              "      <td>38.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>24000 rows × 257 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-30e4542f-fe0b-4d28-b890-48c380d9e7bf')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-30e4542f-fe0b-4d28-b890-48c380d9e7bf button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-30e4542f-fe0b-4d28-b890-48c380d9e7bf');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터 따로 섞기"
      ],
      "metadata": {
        "id": "tShn-lNVu9a6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_train = data_train.sample(frac=1).reset_index(drop=True)\n",
        "data_test = data_test.sample(frac=1).reset_index(drop=True)\n",
        "data_val = data_val.sample(frac=1).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "9Ctc_6aAu9a6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "outputId": "e071d1b2-f4ad-4b00-cf90-3a013213ee23",
        "id": "UVfpUDhNu9a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            0         1         2         3         4         5         6    \\\n",
              "0      0.013167  0.017490  0.018646  0.016446  0.011114  0.003245 -0.006287   \n",
              "1     -0.115512 -0.115289 -0.113396 -0.110355 -0.106877 -0.103780 -0.101890   \n",
              "2     -0.009042 -0.008103 -0.008920 -0.010897 -0.013278 -0.015248 -0.016029   \n",
              "3      0.402142  0.302911  0.185292  0.066715 -0.037182 -0.115259 -0.162345   \n",
              "4     -0.007963  0.005535  0.017452  0.026380  0.031221  0.031279  0.026293   \n",
              "...         ...       ...       ...       ...       ...       ...       ...   \n",
              "23995 -0.109838 -0.112002 -0.113641 -0.114621 -0.114863 -0.114353 -0.113177   \n",
              "23996 -0.040675 -0.054378 -0.068501 -0.081380 -0.091338 -0.096999 -0.097585   \n",
              "23997 -0.019681 -0.041208 -0.061138 -0.078153 -0.091414 -0.100656 -0.106163   \n",
              "23998  0.003074 -0.012424 -0.024224 -0.032849 -0.039069 -0.043762 -0.047765   \n",
              "23999 -0.016414 -0.019264 -0.023513 -0.029241 -0.036146 -0.043594 -0.050755   \n",
              "\n",
              "            7         8         9    ...       247       248       249  \\\n",
              "0     -0.016464 -0.026283 -0.034921  ... -0.004174 -0.002091  0.000968   \n",
              "1     -0.101913 -0.104281 -0.108999  ...  0.008242  0.022894  0.039154   \n",
              "2     -0.014967 -0.011609 -0.005777  ... -0.041403 -0.040630 -0.041345   \n",
              "3     -0.179306 -0.171868 -0.148656  ... -0.079294 -0.081055 -0.082744   \n",
              "4      0.016416  0.002152 -0.015730  ...  0.008343 -0.091526 -0.188443   \n",
              "...         ...       ...       ...  ...       ...       ...       ...   \n",
              "23995 -0.111554 -0.109847 -0.108525  ... -0.108024 -0.108699 -0.108706   \n",
              "23996 -0.093149 -0.084687 -0.074089  ...  0.001157  0.069442  0.143595   \n",
              "23997 -0.108632 -0.108992 -0.108220  ... -0.045588 -0.048637 -0.051353   \n",
              "23998 -0.051742 -0.056075 -0.060821  ... -0.119279 -0.118505 -0.116912   \n",
              "23999 -0.056785 -0.061007 -0.063049  ... -0.090826 -0.090601 -0.087764   \n",
              "\n",
              "            250       251       252       253       254       255   256  \n",
              "0      0.004741  0.008918  0.013208  0.017364  0.021204  0.024627  72.0  \n",
              "1      0.056616  0.074825  0.093340  0.111823  0.130108  0.148236  54.0  \n",
              "2     -0.043381 -0.046298 -0.049451 -0.052130 -0.053720 -0.053853  61.0  \n",
              "3     -0.083994 -0.084521 -0.084185 -0.083029 -0.081281 -0.079306  40.0  \n",
              "4     -0.269607 -0.325108 -0.349555 -0.342722 -0.309150 -0.256869   9.0  \n",
              "...         ...       ...       ...       ...       ...       ...   ...  \n",
              "23995 -0.108044 -0.106867 -0.105405 -0.103864 -0.102335 -0.100754  62.0  \n",
              "23996  0.214329  0.271583  0.306394  0.312725  0.288831  0.237771  41.0  \n",
              "23997 -0.053427 -0.054666 -0.054988 -0.054436 -0.053175 -0.051491  35.0  \n",
              "23998 -0.114928 -0.113008 -0.111522 -0.110699 -0.110611 -0.111212  98.0  \n",
              "23999 -0.082253 -0.074366 -0.064672 -0.053875 -0.042684 -0.031708  81.0  \n",
              "\n",
              "[24000 rows x 257 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-20518848-deb8-470c-bef6-2095db7f43fb\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>247</th>\n",
              "      <th>248</th>\n",
              "      <th>249</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "      <th>256</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.013167</td>\n",
              "      <td>0.017490</td>\n",
              "      <td>0.018646</td>\n",
              "      <td>0.016446</td>\n",
              "      <td>0.011114</td>\n",
              "      <td>0.003245</td>\n",
              "      <td>-0.006287</td>\n",
              "      <td>-0.016464</td>\n",
              "      <td>-0.026283</td>\n",
              "      <td>-0.034921</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.004174</td>\n",
              "      <td>-0.002091</td>\n",
              "      <td>0.000968</td>\n",
              "      <td>0.004741</td>\n",
              "      <td>0.008918</td>\n",
              "      <td>0.013208</td>\n",
              "      <td>0.017364</td>\n",
              "      <td>0.021204</td>\n",
              "      <td>0.024627</td>\n",
              "      <td>72.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.115512</td>\n",
              "      <td>-0.115289</td>\n",
              "      <td>-0.113396</td>\n",
              "      <td>-0.110355</td>\n",
              "      <td>-0.106877</td>\n",
              "      <td>-0.103780</td>\n",
              "      <td>-0.101890</td>\n",
              "      <td>-0.101913</td>\n",
              "      <td>-0.104281</td>\n",
              "      <td>-0.108999</td>\n",
              "      <td>...</td>\n",
              "      <td>0.008242</td>\n",
              "      <td>0.022894</td>\n",
              "      <td>0.039154</td>\n",
              "      <td>0.056616</td>\n",
              "      <td>0.074825</td>\n",
              "      <td>0.093340</td>\n",
              "      <td>0.111823</td>\n",
              "      <td>0.130108</td>\n",
              "      <td>0.148236</td>\n",
              "      <td>54.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.009042</td>\n",
              "      <td>-0.008103</td>\n",
              "      <td>-0.008920</td>\n",
              "      <td>-0.010897</td>\n",
              "      <td>-0.013278</td>\n",
              "      <td>-0.015248</td>\n",
              "      <td>-0.016029</td>\n",
              "      <td>-0.014967</td>\n",
              "      <td>-0.011609</td>\n",
              "      <td>-0.005777</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.041403</td>\n",
              "      <td>-0.040630</td>\n",
              "      <td>-0.041345</td>\n",
              "      <td>-0.043381</td>\n",
              "      <td>-0.046298</td>\n",
              "      <td>-0.049451</td>\n",
              "      <td>-0.052130</td>\n",
              "      <td>-0.053720</td>\n",
              "      <td>-0.053853</td>\n",
              "      <td>61.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.402142</td>\n",
              "      <td>0.302911</td>\n",
              "      <td>0.185292</td>\n",
              "      <td>0.066715</td>\n",
              "      <td>-0.037182</td>\n",
              "      <td>-0.115259</td>\n",
              "      <td>-0.162345</td>\n",
              "      <td>-0.179306</td>\n",
              "      <td>-0.171868</td>\n",
              "      <td>-0.148656</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.079294</td>\n",
              "      <td>-0.081055</td>\n",
              "      <td>-0.082744</td>\n",
              "      <td>-0.083994</td>\n",
              "      <td>-0.084521</td>\n",
              "      <td>-0.084185</td>\n",
              "      <td>-0.083029</td>\n",
              "      <td>-0.081281</td>\n",
              "      <td>-0.079306</td>\n",
              "      <td>40.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.007963</td>\n",
              "      <td>0.005535</td>\n",
              "      <td>0.017452</td>\n",
              "      <td>0.026380</td>\n",
              "      <td>0.031221</td>\n",
              "      <td>0.031279</td>\n",
              "      <td>0.026293</td>\n",
              "      <td>0.016416</td>\n",
              "      <td>0.002152</td>\n",
              "      <td>-0.015730</td>\n",
              "      <td>...</td>\n",
              "      <td>0.008343</td>\n",
              "      <td>-0.091526</td>\n",
              "      <td>-0.188443</td>\n",
              "      <td>-0.269607</td>\n",
              "      <td>-0.325108</td>\n",
              "      <td>-0.349555</td>\n",
              "      <td>-0.342722</td>\n",
              "      <td>-0.309150</td>\n",
              "      <td>-0.256869</td>\n",
              "      <td>9.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23995</th>\n",
              "      <td>-0.109838</td>\n",
              "      <td>-0.112002</td>\n",
              "      <td>-0.113641</td>\n",
              "      <td>-0.114621</td>\n",
              "      <td>-0.114863</td>\n",
              "      <td>-0.114353</td>\n",
              "      <td>-0.113177</td>\n",
              "      <td>-0.111554</td>\n",
              "      <td>-0.109847</td>\n",
              "      <td>-0.108525</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.108024</td>\n",
              "      <td>-0.108699</td>\n",
              "      <td>-0.108706</td>\n",
              "      <td>-0.108044</td>\n",
              "      <td>-0.106867</td>\n",
              "      <td>-0.105405</td>\n",
              "      <td>-0.103864</td>\n",
              "      <td>-0.102335</td>\n",
              "      <td>-0.100754</td>\n",
              "      <td>62.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23996</th>\n",
              "      <td>-0.040675</td>\n",
              "      <td>-0.054378</td>\n",
              "      <td>-0.068501</td>\n",
              "      <td>-0.081380</td>\n",
              "      <td>-0.091338</td>\n",
              "      <td>-0.096999</td>\n",
              "      <td>-0.097585</td>\n",
              "      <td>-0.093149</td>\n",
              "      <td>-0.084687</td>\n",
              "      <td>-0.074089</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001157</td>\n",
              "      <td>0.069442</td>\n",
              "      <td>0.143595</td>\n",
              "      <td>0.214329</td>\n",
              "      <td>0.271583</td>\n",
              "      <td>0.306394</td>\n",
              "      <td>0.312725</td>\n",
              "      <td>0.288831</td>\n",
              "      <td>0.237771</td>\n",
              "      <td>41.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23997</th>\n",
              "      <td>-0.019681</td>\n",
              "      <td>-0.041208</td>\n",
              "      <td>-0.061138</td>\n",
              "      <td>-0.078153</td>\n",
              "      <td>-0.091414</td>\n",
              "      <td>-0.100656</td>\n",
              "      <td>-0.106163</td>\n",
              "      <td>-0.108632</td>\n",
              "      <td>-0.108992</td>\n",
              "      <td>-0.108220</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.045588</td>\n",
              "      <td>-0.048637</td>\n",
              "      <td>-0.051353</td>\n",
              "      <td>-0.053427</td>\n",
              "      <td>-0.054666</td>\n",
              "      <td>-0.054988</td>\n",
              "      <td>-0.054436</td>\n",
              "      <td>-0.053175</td>\n",
              "      <td>-0.051491</td>\n",
              "      <td>35.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23998</th>\n",
              "      <td>0.003074</td>\n",
              "      <td>-0.012424</td>\n",
              "      <td>-0.024224</td>\n",
              "      <td>-0.032849</td>\n",
              "      <td>-0.039069</td>\n",
              "      <td>-0.043762</td>\n",
              "      <td>-0.047765</td>\n",
              "      <td>-0.051742</td>\n",
              "      <td>-0.056075</td>\n",
              "      <td>-0.060821</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.119279</td>\n",
              "      <td>-0.118505</td>\n",
              "      <td>-0.116912</td>\n",
              "      <td>-0.114928</td>\n",
              "      <td>-0.113008</td>\n",
              "      <td>-0.111522</td>\n",
              "      <td>-0.110699</td>\n",
              "      <td>-0.110611</td>\n",
              "      <td>-0.111212</td>\n",
              "      <td>98.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23999</th>\n",
              "      <td>-0.016414</td>\n",
              "      <td>-0.019264</td>\n",
              "      <td>-0.023513</td>\n",
              "      <td>-0.029241</td>\n",
              "      <td>-0.036146</td>\n",
              "      <td>-0.043594</td>\n",
              "      <td>-0.050755</td>\n",
              "      <td>-0.056785</td>\n",
              "      <td>-0.061007</td>\n",
              "      <td>-0.063049</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.090826</td>\n",
              "      <td>-0.090601</td>\n",
              "      <td>-0.087764</td>\n",
              "      <td>-0.082253</td>\n",
              "      <td>-0.074366</td>\n",
              "      <td>-0.064672</td>\n",
              "      <td>-0.053875</td>\n",
              "      <td>-0.042684</td>\n",
              "      <td>-0.031708</td>\n",
              "      <td>81.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>24000 rows × 257 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-20518848-deb8-470c-bef6-2095db7f43fb')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-20518848-deb8-470c-bef6-2095db7f43fb button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-20518848-deb8-470c-bef6-2095db7f43fb');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 전처리"
      ],
      "metadata": {
        "id": "YwbRDs03u9a7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = to_categorical(data_train[256])\n",
        "y_test = to_categorical(data_test[256])\n",
        "y_val = to_categorical(data_val[256])\n",
        "# y_train = to_categorical(arr)\n",
        "# y_test = to_categorical(arr_t)\n",
        "# y_val = to_categorical(arr_v)\n",
        "y_train[23999]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0dc6d115-4dba-4585-bbc4-b9cbb86368b8",
        "id": "vmLBSA6mu9a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_train.drop([256], axis=1, inplace=True)\n",
        "data_test.drop([256], axis=1, inplace=True)\n",
        "data_val.drop([256], axis=1, inplace=True)\n",
        "data_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "outputId": "24efc099-547d-468b-9675-73fb218863cb",
        "id": "63uCXv0Mu9a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py:4906: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  return super().drop(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            0         1         2         3         4         5         6    \\\n",
              "2160  -0.067399 -0.055313 -0.074646 -0.114919 -0.088871 -0.111103 -0.079715   \n",
              "20919 -0.039001 -0.049287 -0.064703 -0.075447 -0.068599 -0.085801 -0.038972   \n",
              "973    0.013016 -0.011119 -0.039795  0.015698 -0.013839 -0.056547 -0.028326   \n",
              "1238  -0.012839 -0.007891 -0.000335 -0.017209 -0.042385  0.004278 -0.018220   \n",
              "4149   0.162093  0.259302  0.278531  0.214980  0.245281  0.184952  0.160723   \n",
              "...         ...       ...       ...       ...       ...       ...       ...   \n",
              "8202  -0.006040 -0.023679 -0.003979  0.004199  0.014016  0.003021 -0.013575   \n",
              "5298  -0.234748 -0.194218 -0.143088 -0.116647 -0.096146 -0.075835 -0.060663   \n",
              "4371  -0.042573 -0.026527 -0.038931 -0.040005 -0.065119 -0.035112  0.002794   \n",
              "3576   0.039162 -0.001734  0.004371 -0.030724  0.003882  0.129788  0.027695   \n",
              "2246  -0.037558 -0.028723 -0.035768  0.002306 -0.049590 -0.022046  0.006337   \n",
              "\n",
              "            7         8         9    ...       246       247       248  \\\n",
              "2160  -0.080806 -0.070167 -0.029497  ...  0.018093  0.017222  0.009899   \n",
              "20919 -0.034171 -0.073489 -0.067426  ... -0.120486 -0.050403 -0.042220   \n",
              "973    0.008265 -0.002105 -0.010835  ... -0.084478 -0.108325 -0.112022   \n",
              "1238   0.001170 -0.015320 -0.020522  ... -0.037809 -0.041861 -0.045602   \n",
              "4149   0.035515 -0.023882 -0.040479  ...  0.028871 -0.051584 -0.040018   \n",
              "...         ...       ...       ...  ...       ...       ...       ...   \n",
              "8202  -0.007312 -0.046171 -0.016271  ... -0.098162 -0.066009 -0.075146   \n",
              "5298  -0.047532 -0.003239 -0.032327  ... -0.024578 -0.004386 -0.045173   \n",
              "4371  -0.054849 -0.045523 -0.051146  ...  0.037911  0.046142  0.003203   \n",
              "3576   0.035203  0.010411 -0.036581  ... -0.109445 -0.120116 -0.078387   \n",
              "2246  -0.037121 -0.009859 -0.001557  ...  0.059359  0.073934  0.034510   \n",
              "\n",
              "            249       250       251       252       253       254       255  \n",
              "2160  -0.004473  0.059794  0.014011 -0.047852 -0.048926 -0.075550 -0.053034  \n",
              "20919 -0.078167 -0.022235 -0.015043 -0.028401 -0.049329 -0.014087 -0.047666  \n",
              "973   -0.108531 -0.119081 -0.121462 -0.090844 -0.160078 -0.088232 -0.114928  \n",
              "1238  -0.031203  0.011176 -0.056984 -0.035815 -0.028645 -0.029925 -0.026754  \n",
              "4149  -0.069031 -0.051542 -0.050382 -0.028941 -0.031798 -0.047034 -0.036189  \n",
              "...         ...       ...       ...       ...       ...       ...       ...  \n",
              "8202  -0.032192  0.024472  0.087386  0.153660  0.207485  0.299180  0.282465  \n",
              "5298  -0.044530 -0.004445 -0.020140 -0.004285 -0.043359 -0.019732 -0.003674  \n",
              "4371   0.054244  0.003735  0.000176 -0.011942  0.004859  0.008150 -0.002779  \n",
              "3576  -0.139557 -0.167027 -0.096697 -0.065567 -0.172236 -0.129005 -0.096974  \n",
              "2246   0.027490 -0.025038  0.002396  0.033644 -0.058196 -0.029304 -0.041908  \n",
              "\n",
              "[24000 rows x 256 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c3ef7109-733c-4108-8534-3b18bf5b43ea\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>246</th>\n",
              "      <th>247</th>\n",
              "      <th>248</th>\n",
              "      <th>249</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2160</th>\n",
              "      <td>-0.067399</td>\n",
              "      <td>-0.055313</td>\n",
              "      <td>-0.074646</td>\n",
              "      <td>-0.114919</td>\n",
              "      <td>-0.088871</td>\n",
              "      <td>-0.111103</td>\n",
              "      <td>-0.079715</td>\n",
              "      <td>-0.080806</td>\n",
              "      <td>-0.070167</td>\n",
              "      <td>-0.029497</td>\n",
              "      <td>...</td>\n",
              "      <td>0.018093</td>\n",
              "      <td>0.017222</td>\n",
              "      <td>0.009899</td>\n",
              "      <td>-0.004473</td>\n",
              "      <td>0.059794</td>\n",
              "      <td>0.014011</td>\n",
              "      <td>-0.047852</td>\n",
              "      <td>-0.048926</td>\n",
              "      <td>-0.075550</td>\n",
              "      <td>-0.053034</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20919</th>\n",
              "      <td>-0.039001</td>\n",
              "      <td>-0.049287</td>\n",
              "      <td>-0.064703</td>\n",
              "      <td>-0.075447</td>\n",
              "      <td>-0.068599</td>\n",
              "      <td>-0.085801</td>\n",
              "      <td>-0.038972</td>\n",
              "      <td>-0.034171</td>\n",
              "      <td>-0.073489</td>\n",
              "      <td>-0.067426</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.120486</td>\n",
              "      <td>-0.050403</td>\n",
              "      <td>-0.042220</td>\n",
              "      <td>-0.078167</td>\n",
              "      <td>-0.022235</td>\n",
              "      <td>-0.015043</td>\n",
              "      <td>-0.028401</td>\n",
              "      <td>-0.049329</td>\n",
              "      <td>-0.014087</td>\n",
              "      <td>-0.047666</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>973</th>\n",
              "      <td>0.013016</td>\n",
              "      <td>-0.011119</td>\n",
              "      <td>-0.039795</td>\n",
              "      <td>0.015698</td>\n",
              "      <td>-0.013839</td>\n",
              "      <td>-0.056547</td>\n",
              "      <td>-0.028326</td>\n",
              "      <td>0.008265</td>\n",
              "      <td>-0.002105</td>\n",
              "      <td>-0.010835</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.084478</td>\n",
              "      <td>-0.108325</td>\n",
              "      <td>-0.112022</td>\n",
              "      <td>-0.108531</td>\n",
              "      <td>-0.119081</td>\n",
              "      <td>-0.121462</td>\n",
              "      <td>-0.090844</td>\n",
              "      <td>-0.160078</td>\n",
              "      <td>-0.088232</td>\n",
              "      <td>-0.114928</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1238</th>\n",
              "      <td>-0.012839</td>\n",
              "      <td>-0.007891</td>\n",
              "      <td>-0.000335</td>\n",
              "      <td>-0.017209</td>\n",
              "      <td>-0.042385</td>\n",
              "      <td>0.004278</td>\n",
              "      <td>-0.018220</td>\n",
              "      <td>0.001170</td>\n",
              "      <td>-0.015320</td>\n",
              "      <td>-0.020522</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.037809</td>\n",
              "      <td>-0.041861</td>\n",
              "      <td>-0.045602</td>\n",
              "      <td>-0.031203</td>\n",
              "      <td>0.011176</td>\n",
              "      <td>-0.056984</td>\n",
              "      <td>-0.035815</td>\n",
              "      <td>-0.028645</td>\n",
              "      <td>-0.029925</td>\n",
              "      <td>-0.026754</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4149</th>\n",
              "      <td>0.162093</td>\n",
              "      <td>0.259302</td>\n",
              "      <td>0.278531</td>\n",
              "      <td>0.214980</td>\n",
              "      <td>0.245281</td>\n",
              "      <td>0.184952</td>\n",
              "      <td>0.160723</td>\n",
              "      <td>0.035515</td>\n",
              "      <td>-0.023882</td>\n",
              "      <td>-0.040479</td>\n",
              "      <td>...</td>\n",
              "      <td>0.028871</td>\n",
              "      <td>-0.051584</td>\n",
              "      <td>-0.040018</td>\n",
              "      <td>-0.069031</td>\n",
              "      <td>-0.051542</td>\n",
              "      <td>-0.050382</td>\n",
              "      <td>-0.028941</td>\n",
              "      <td>-0.031798</td>\n",
              "      <td>-0.047034</td>\n",
              "      <td>-0.036189</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8202</th>\n",
              "      <td>-0.006040</td>\n",
              "      <td>-0.023679</td>\n",
              "      <td>-0.003979</td>\n",
              "      <td>0.004199</td>\n",
              "      <td>0.014016</td>\n",
              "      <td>0.003021</td>\n",
              "      <td>-0.013575</td>\n",
              "      <td>-0.007312</td>\n",
              "      <td>-0.046171</td>\n",
              "      <td>-0.016271</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.098162</td>\n",
              "      <td>-0.066009</td>\n",
              "      <td>-0.075146</td>\n",
              "      <td>-0.032192</td>\n",
              "      <td>0.024472</td>\n",
              "      <td>0.087386</td>\n",
              "      <td>0.153660</td>\n",
              "      <td>0.207485</td>\n",
              "      <td>0.299180</td>\n",
              "      <td>0.282465</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5298</th>\n",
              "      <td>-0.234748</td>\n",
              "      <td>-0.194218</td>\n",
              "      <td>-0.143088</td>\n",
              "      <td>-0.116647</td>\n",
              "      <td>-0.096146</td>\n",
              "      <td>-0.075835</td>\n",
              "      <td>-0.060663</td>\n",
              "      <td>-0.047532</td>\n",
              "      <td>-0.003239</td>\n",
              "      <td>-0.032327</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.024578</td>\n",
              "      <td>-0.004386</td>\n",
              "      <td>-0.045173</td>\n",
              "      <td>-0.044530</td>\n",
              "      <td>-0.004445</td>\n",
              "      <td>-0.020140</td>\n",
              "      <td>-0.004285</td>\n",
              "      <td>-0.043359</td>\n",
              "      <td>-0.019732</td>\n",
              "      <td>-0.003674</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4371</th>\n",
              "      <td>-0.042573</td>\n",
              "      <td>-0.026527</td>\n",
              "      <td>-0.038931</td>\n",
              "      <td>-0.040005</td>\n",
              "      <td>-0.065119</td>\n",
              "      <td>-0.035112</td>\n",
              "      <td>0.002794</td>\n",
              "      <td>-0.054849</td>\n",
              "      <td>-0.045523</td>\n",
              "      <td>-0.051146</td>\n",
              "      <td>...</td>\n",
              "      <td>0.037911</td>\n",
              "      <td>0.046142</td>\n",
              "      <td>0.003203</td>\n",
              "      <td>0.054244</td>\n",
              "      <td>0.003735</td>\n",
              "      <td>0.000176</td>\n",
              "      <td>-0.011942</td>\n",
              "      <td>0.004859</td>\n",
              "      <td>0.008150</td>\n",
              "      <td>-0.002779</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3576</th>\n",
              "      <td>0.039162</td>\n",
              "      <td>-0.001734</td>\n",
              "      <td>0.004371</td>\n",
              "      <td>-0.030724</td>\n",
              "      <td>0.003882</td>\n",
              "      <td>0.129788</td>\n",
              "      <td>0.027695</td>\n",
              "      <td>0.035203</td>\n",
              "      <td>0.010411</td>\n",
              "      <td>-0.036581</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.109445</td>\n",
              "      <td>-0.120116</td>\n",
              "      <td>-0.078387</td>\n",
              "      <td>-0.139557</td>\n",
              "      <td>-0.167027</td>\n",
              "      <td>-0.096697</td>\n",
              "      <td>-0.065567</td>\n",
              "      <td>-0.172236</td>\n",
              "      <td>-0.129005</td>\n",
              "      <td>-0.096974</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2246</th>\n",
              "      <td>-0.037558</td>\n",
              "      <td>-0.028723</td>\n",
              "      <td>-0.035768</td>\n",
              "      <td>0.002306</td>\n",
              "      <td>-0.049590</td>\n",
              "      <td>-0.022046</td>\n",
              "      <td>0.006337</td>\n",
              "      <td>-0.037121</td>\n",
              "      <td>-0.009859</td>\n",
              "      <td>-0.001557</td>\n",
              "      <td>...</td>\n",
              "      <td>0.059359</td>\n",
              "      <td>0.073934</td>\n",
              "      <td>0.034510</td>\n",
              "      <td>0.027490</td>\n",
              "      <td>-0.025038</td>\n",
              "      <td>0.002396</td>\n",
              "      <td>0.033644</td>\n",
              "      <td>-0.058196</td>\n",
              "      <td>-0.029304</td>\n",
              "      <td>-0.041908</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>24000 rows × 256 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c3ef7109-733c-4108-8534-3b18bf5b43ea')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c3ef7109-733c-4108-8534-3b18bf5b43ea button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c3ef7109-733c-4108-8534-3b18bf5b43ea');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train=data_train.values\n",
        "X_test=data_test.values\n",
        "X_val=data_val.values"
      ],
      "metadata": {
        "id": "PytXfxUzu9a7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(X_val.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)\n",
        "print(y_val.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17cc8578-61ba-4dd4-81e1-dd084aa0e3ab",
        "id": "rkKYvKTOu9a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(24000, 256)\n",
            "(6000, 256)\n",
            "(6000, 256)\n",
            "(24000, 101)\n",
            "(6000, 101)\n",
            "(6000, 101)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# X_train = X_train.reshape(1, 256, 24000)\n",
        "# X_test = X_test.reshape(1, 256, 6000)\n",
        "# X_val = X_val.reshape(1, 256, 6000)\n",
        "X_train = X_train.reshape(24000, 256, 1)\n",
        "X_test = X_test.reshape(6000, 256, 1)\n",
        "X_val = X_val.reshape(6000, 256, 1)\n",
        "X_train.shape, X_test.shape, X_val.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf10b8f0-1b07-4548-b56a-f43571e6ad5f",
        "id": "zWwjM0s4u9a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((24000, 256, 1), (6000, 256, 1), (6000, 256, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## cnn-lstm 모델"
      ],
      "metadata": {
        "id": "_Pp5u4elu9a7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model():\n",
        "    model = Sequential()\n",
        "    model.add(Conv1D(filters=64, kernel_size=6, activation='relu', \n",
        "                    padding='same', input_shape=(256, 1)))\n",
        "    model.add(BatchNormalization())\n",
        "    \n",
        "    # adding a pooling layer\n",
        "    model.add(MaxPooling1D(pool_size=(3), strides=2, padding='same'))\n",
        "    \n",
        "    model.add(Conv1D(filters=64, kernel_size=6, activation='relu', \n",
        "                    padding='same', input_shape=(256, 1)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling1D(pool_size=(3), strides=2, padding='same'))\n",
        "    \n",
        "    model.add(Conv1D(filters=64, kernel_size=6, activation='relu', \n",
        "                    padding='same', input_shape=(256, 1)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling1D(pool_size=(3), strides=2, padding='same'))\n",
        "    \n",
        "    model.add(Flatten())\n",
        "    #model.add(Dense(64, activation='relu'))\n",
        "    #model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(101, activation='softmax'))\n",
        "    \n",
        "    model.compile(optimizer='adam', loss='mse',metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "ZM1vFO65u9a8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model()\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2397946-5a7a-4f1f-e09b-f35229db7ab7",
        "id": "cIjsuzyhu9a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_18\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_17 (Conv1D)          (None, 256, 64)           448       \n",
            "                                                                 \n",
            " batch_normalization_12 (Bat  (None, 256, 64)          256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling1d_17 (MaxPoolin  (None, 128, 64)          0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_18 (Conv1D)          (None, 128, 64)           24640     \n",
            "                                                                 \n",
            " batch_normalization_13 (Bat  (None, 128, 64)          256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling1d_18 (MaxPoolin  (None, 64, 64)           0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_19 (Conv1D)          (None, 64, 64)            24640     \n",
            "                                                                 \n",
            " batch_normalization_14 (Bat  (None, 64, 64)           256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling1d_19 (MaxPoolin  (None, 32, 64)           0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " flatten_13 (Flatten)        (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_24 (Dense)            (None, 100)               204900    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 255,396\n",
            "Trainable params: 255,012\n",
            "Non-trainable params: 384\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## cnn-gru 모델"
      ],
      "metadata": {
        "id": "uolWtZKZu9a8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Input, Embedding, GRU, LSTM, MaxPooling1D, GlobalMaxPool1D, CuDNNGRU\n",
        "from keras.layers import Dropout, Dense, Activation, Flatten, Conv1D, SpatialDropout1D\n",
        "from keras.models import Sequential\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv1D(filters=64, kernel_size=6, activation='relu', padding='same', input_shape=(256, 1)))\n",
        "model.add(SpatialDropout1D(0.2))\n",
        "model.add(MaxPooling1D(4))\n",
        "model.add(CuDNNGRU(64))\n",
        "\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Dense(101, activation='softmax'))\n",
        "model.compile(optimizer='adam', loss='mse',metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33f77877-e599-4ded-8400-373f5cec14bc",
        "id": "XM37Hpdlu9a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_13 (Conv1D)          (None, 256, 64)           448       \n",
            "                                                                 \n",
            " spatial_dropout1d_4 (Spatia  (None, 256, 64)          0         \n",
            " lDropout1D)                                                     \n",
            "                                                                 \n",
            " max_pooling1d_13 (MaxPoolin  (None, 64, 64)           0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " cu_dnngru_4 (CuDNNGRU)      (None, 64)                24960     \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 101)               6565      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 31,973\n",
            "Trainable params: 31,973\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## gru모델"
      ],
      "metadata": {
        "id": "q1pJydE3u9a8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(GRU(units=100, return_sequences=True, input_shape=(256,1))) #shape[1]=열\n",
        "model.add(GRU(units=50, return_sequences=True))\n",
        "model.add(GRU(units=50, return_sequences=True))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Dense(101, activation='softmax'))\n",
        "model.compile(optimizer='adam', loss='mse',metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a91bcb1-a338-461c-c221-8087ce00e0cd",
        "id": "90hodE8Tu9a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_12\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " gru_34 (GRU)                (None, 256, 100)          30900     \n",
            "                                                                 \n",
            " gru_35 (GRU)                (None, 256, 50)           22800     \n",
            "                                                                 \n",
            " gru_36 (GRU)                (None, 256, 50)           15300     \n",
            "                                                                 \n",
            " flatten_7 (Flatten)         (None, 12800)             0         \n",
            "                                                                 \n",
            " dropout_15 (Dropout)        (None, 12800)             0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 101)               1292901   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,361,901\n",
            "Trainable params: 1,361,901\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSTM"
      ],
      "metadata": {
        "id": "vDWxnMP_u9a8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(units=100, return_sequences=True, input_shape=(256,1))) #shape[1]=열\n",
        "model.add(LSTM(units=50, return_sequences=True))\n",
        "model.add(LSTM(units=50, return_sequences=True))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Dense(101, activation='softmax'))\n",
        "model.compile(optimizer='adam', loss='mse',metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8055881e-f18e-4941-e7d3-1d853b39d2e3",
        "id": "J1TLq8qju9a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_3 (LSTM)               (None, 256, 100)          40800     \n",
            "                                                                 \n",
            " lstm_4 (LSTM)               (None, 256, 50)           30200     \n",
            "                                                                 \n",
            " lstm_5 (LSTM)               (None, 256, 50)           20200     \n",
            "                                                                 \n",
            " flatten_6 (Flatten)         (None, 12800)             0         \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 12800)             0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 101)               1292901   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,384,101\n",
            "Trainable params: 1,384,101\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RNN모델"
      ],
      "metadata": {
        "id": "NCK6kHrRu9a9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Input, Embedding, GRU, LSTM, SimpleRNN, MaxPooling1D, GlobalMaxPool1D, CuDNNGRU\n",
        "from keras.layers import Dropout, Dense, Activation, Flatten, Conv1D, SpatialDropout1D\n",
        "from keras.models import Sequential\n",
        "\n",
        "model = Sequential()\n",
        "model.add(SimpleRNN(units=100, return_sequences=True, input_shape=(256,1))) #shape[1]=열\n",
        "model.add(SimpleRNN(units=50, return_sequences=True))\n",
        "model.add(SimpleRNN(units=50, return_sequences=True))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(101, activation='softmax'))\n",
        "model.compile(optimizer='adam', loss='mse',metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f56aa04-4c34-4860-fc44-299596d4f96f",
        "id": "3_RikgPFu9a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " simple_rnn_12 (SimpleRNN)   (None, 256, 100)          10200     \n",
            "                                                                 \n",
            " simple_rnn_13 (SimpleRNN)   (None, 256, 50)           7550      \n",
            "                                                                 \n",
            " simple_rnn_14 (SimpleRNN)   (None, 256, 50)           5050      \n",
            "                                                                 \n",
            " flatten_4 (Flatten)         (None, 12800)             0         \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 12800)             0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 101)               1292901   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,315,701\n",
            "Trainable params: 1,315,701\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNN"
      ],
      "metadata": {
        "id": "jBPhTMHcu9a9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model():\n",
        "  model = Sequential()\n",
        "  model.add(Conv1D(256, 8, padding='same', activation='relu', input_shape= (256,1)))\n",
        "  model.add(MaxPooling1D())\n",
        "  model.add(Conv1D(256, 8, padding='same', activation='relu'))\n",
        "  model.add(MaxPooling1D())\n",
        "  model.add(Conv1D(64, 8, padding='same',activation='relu'))\n",
        "  model.add(MaxPooling1D())\n",
        "  model.add(Conv1D(64, 8, padding='same', activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(101, activation='softmax')) # activation='softmax'\n",
        "\n",
        "  model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
        "  model.summary()\n",
        "  return model"
      ],
      "metadata": {
        "id": "d1bLovz2u9a9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fc01b9a-a5a8-4976-c711-e65c2587ea5d",
        "id": "JPR5zowtu9a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_4 (Conv1D)           (None, 256, 256)          2304      \n",
            "                                                                 \n",
            " max_pooling1d_3 (MaxPooling  (None, 128, 256)         0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " conv1d_5 (Conv1D)           (None, 128, 256)          524544    \n",
            "                                                                 \n",
            " max_pooling1d_4 (MaxPooling  (None, 64, 256)          0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " conv1d_6 (Conv1D)           (None, 64, 64)            131136    \n",
            "                                                                 \n",
            " max_pooling1d_5 (MaxPooling  (None, 32, 64)           0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " conv1d_7 (Conv1D)           (None, 32, 64)            32832     \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 32, 64)           256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 101)               206949    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 898,021\n",
            "Trainable params: 897,893\n",
            "Non-trainable params: 128\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델학습/평가"
      ],
      "metadata": {
        "id": "ESqvgOV9u9a9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train, y_train, batch_size = 512, epochs = 1000, verbose = 1, validation_data=(X_val, y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ba770aa-03a5-42d9-8c36-94c14223cf28",
        "id": "EBLzTT1du9a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "47/47 [==============================] - 8s 92ms/step - loss: 0.0098 - accuracy: 0.0096 - val_loss: 0.0098 - val_accuracy: 0.0120\n",
            "Epoch 2/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0098 - accuracy: 0.0097 - val_loss: 0.0098 - val_accuracy: 0.0120\n",
            "Epoch 3/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0098 - accuracy: 0.0122 - val_loss: 0.0098 - val_accuracy: 0.0107\n",
            "Epoch 4/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0098 - accuracy: 0.0127 - val_loss: 0.0098 - val_accuracy: 0.0118\n",
            "Epoch 5/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 0.0098 - accuracy: 0.0175 - val_loss: 0.0098 - val_accuracy: 0.0160\n",
            "Epoch 6/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 0.0098 - accuracy: 0.0265 - val_loss: 0.0098 - val_accuracy: 0.0245\n",
            "Epoch 7/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 0.0097 - accuracy: 0.0429 - val_loss: 0.0098 - val_accuracy: 0.0290\n",
            "Epoch 8/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 0.0097 - accuracy: 0.0541 - val_loss: 0.0098 - val_accuracy: 0.0432\n",
            "Epoch 9/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 0.0097 - accuracy: 0.0633 - val_loss: 0.0097 - val_accuracy: 0.0455\n",
            "Epoch 10/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 0.0096 - accuracy: 0.0799 - val_loss: 0.0098 - val_accuracy: 0.0548\n",
            "Epoch 11/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 0.0096 - accuracy: 0.0955 - val_loss: 0.0097 - val_accuracy: 0.0662\n",
            "Epoch 12/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0095 - accuracy: 0.1093 - val_loss: 0.0097 - val_accuracy: 0.0710\n",
            "Epoch 13/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0094 - accuracy: 0.1283 - val_loss: 0.0096 - val_accuracy: 0.1058\n",
            "Epoch 14/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0093 - accuracy: 0.1513 - val_loss: 0.0096 - val_accuracy: 0.1112\n",
            "Epoch 15/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0092 - accuracy: 0.1754 - val_loss: 0.0095 - val_accuracy: 0.1372\n",
            "Epoch 16/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0090 - accuracy: 0.1988 - val_loss: 0.0095 - val_accuracy: 0.1482\n",
            "Epoch 17/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0089 - accuracy: 0.2200 - val_loss: 0.0093 - val_accuracy: 0.1613\n",
            "Epoch 18/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0087 - accuracy: 0.2433 - val_loss: 0.0093 - val_accuracy: 0.1823\n",
            "Epoch 19/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0086 - accuracy: 0.2642 - val_loss: 0.0091 - val_accuracy: 0.1993\n",
            "Epoch 20/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0083 - accuracy: 0.2905 - val_loss: 0.0090 - val_accuracy: 0.2217\n",
            "Epoch 21/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0081 - accuracy: 0.3211 - val_loss: 0.0089 - val_accuracy: 0.2472\n",
            "Epoch 22/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0079 - accuracy: 0.3493 - val_loss: 0.0086 - val_accuracy: 0.2775\n",
            "Epoch 23/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0076 - accuracy: 0.3808 - val_loss: 0.0084 - val_accuracy: 0.3012\n",
            "Epoch 24/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0073 - accuracy: 0.4144 - val_loss: 0.0083 - val_accuracy: 0.3247\n",
            "Epoch 25/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0070 - accuracy: 0.4455 - val_loss: 0.0080 - val_accuracy: 0.3633\n",
            "Epoch 26/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0066 - accuracy: 0.4857 - val_loss: 0.0077 - val_accuracy: 0.3933\n",
            "Epoch 27/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 0.0062 - accuracy: 0.5212 - val_loss: 0.0074 - val_accuracy: 0.4297\n",
            "Epoch 28/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 0.0059 - accuracy: 0.5512 - val_loss: 0.0072 - val_accuracy: 0.4493\n",
            "Epoch 29/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 0.0054 - accuracy: 0.5902 - val_loss: 0.0066 - val_accuracy: 0.5018\n",
            "Epoch 30/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 0.0049 - accuracy: 0.6308 - val_loss: 0.0060 - val_accuracy: 0.5542\n",
            "Epoch 31/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 0.0044 - accuracy: 0.6702 - val_loss: 0.0056 - val_accuracy: 0.5882\n",
            "Epoch 32/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0040 - accuracy: 0.7034 - val_loss: 0.0053 - val_accuracy: 0.6163\n",
            "Epoch 33/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 0.0037 - accuracy: 0.7305 - val_loss: 0.0048 - val_accuracy: 0.6525\n",
            "Epoch 34/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 0.0032 - accuracy: 0.7677 - val_loss: 0.0044 - val_accuracy: 0.6858\n",
            "Epoch 35/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0028 - accuracy: 0.7983 - val_loss: 0.0041 - val_accuracy: 0.7048\n",
            "Epoch 36/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 0.0026 - accuracy: 0.8177 - val_loss: 0.0039 - val_accuracy: 0.7230\n",
            "Epoch 37/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0023 - accuracy: 0.8354 - val_loss: 0.0036 - val_accuracy: 0.7415\n",
            "Epoch 38/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 0.0020 - accuracy: 0.8554 - val_loss: 0.0034 - val_accuracy: 0.7593\n",
            "Epoch 39/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0018 - accuracy: 0.8733 - val_loss: 0.0032 - val_accuracy: 0.7727\n",
            "Epoch 40/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0017 - accuracy: 0.8823 - val_loss: 0.0030 - val_accuracy: 0.7897\n",
            "Epoch 41/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0015 - accuracy: 0.8930 - val_loss: 0.0030 - val_accuracy: 0.7908\n",
            "Epoch 42/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0015 - accuracy: 0.8978 - val_loss: 0.0028 - val_accuracy: 0.8022\n",
            "Epoch 43/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0013 - accuracy: 0.9126 - val_loss: 0.0026 - val_accuracy: 0.8217\n",
            "Epoch 44/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0011 - accuracy: 0.9213 - val_loss: 0.0025 - val_accuracy: 0.8247\n",
            "Epoch 45/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0010 - accuracy: 0.9287 - val_loss: 0.0026 - val_accuracy: 0.8163\n",
            "Epoch 46/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.7937e-04 - accuracy: 0.9317 - val_loss: 0.0023 - val_accuracy: 0.8408\n",
            "Epoch 47/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.9661e-04 - accuracy: 0.9376 - val_loss: 0.0025 - val_accuracy: 0.8307\n",
            "Epoch 48/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.1827e-04 - accuracy: 0.9431 - val_loss: 0.0022 - val_accuracy: 0.8477\n",
            "Epoch 49/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.5891e-04 - accuracy: 0.9475 - val_loss: 0.0022 - val_accuracy: 0.8510\n",
            "Epoch 50/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.0182e-04 - accuracy: 0.9513 - val_loss: 0.0021 - val_accuracy: 0.8538\n",
            "Epoch 51/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.1998e-04 - accuracy: 0.9565 - val_loss: 0.0020 - val_accuracy: 0.8652\n",
            "Epoch 52/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.6651e-04 - accuracy: 0.9598 - val_loss: 0.0020 - val_accuracy: 0.8678\n",
            "Epoch 53/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.5514e-04 - accuracy: 0.9614 - val_loss: 0.0019 - val_accuracy: 0.8658\n",
            "Epoch 54/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.3023e-04 - accuracy: 0.9634 - val_loss: 0.0020 - val_accuracy: 0.8700\n",
            "Epoch 55/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.0445e-04 - accuracy: 0.9654 - val_loss: 0.0019 - val_accuracy: 0.8695\n",
            "Epoch 56/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.6726e-04 - accuracy: 0.9676 - val_loss: 0.0018 - val_accuracy: 0.8770\n",
            "Epoch 57/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.2098e-04 - accuracy: 0.9707 - val_loss: 0.0019 - val_accuracy: 0.8742\n",
            "Epoch 58/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.0217e-04 - accuracy: 0.9721 - val_loss: 0.0019 - val_accuracy: 0.8737\n",
            "Epoch 59/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 4.0362e-04 - accuracy: 0.9718 - val_loss: 0.0018 - val_accuracy: 0.8818\n",
            "Epoch 60/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.9923e-04 - accuracy: 0.9726 - val_loss: 0.0018 - val_accuracy: 0.8740\n",
            "Epoch 61/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.8791e-04 - accuracy: 0.9732 - val_loss: 0.0017 - val_accuracy: 0.8843\n",
            "Epoch 62/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.4990e-04 - accuracy: 0.9753 - val_loss: 0.0017 - val_accuracy: 0.8848\n",
            "Epoch 63/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.4841e-04 - accuracy: 0.9758 - val_loss: 0.0017 - val_accuracy: 0.8812\n",
            "Epoch 64/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.3722e-04 - accuracy: 0.9761 - val_loss: 0.0018 - val_accuracy: 0.8815\n",
            "Epoch 65/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.4255e-04 - accuracy: 0.9758 - val_loss: 0.0018 - val_accuracy: 0.8797\n",
            "Epoch 66/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.4006e-04 - accuracy: 0.9764 - val_loss: 0.0018 - val_accuracy: 0.8788\n",
            "Epoch 67/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.7079e-04 - accuracy: 0.9748 - val_loss: 0.0019 - val_accuracy: 0.8740\n",
            "Epoch 68/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.6680e-04 - accuracy: 0.9753 - val_loss: 0.0019 - val_accuracy: 0.8738\n",
            "Epoch 69/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.5435e-04 - accuracy: 0.9768 - val_loss: 0.0019 - val_accuracy: 0.8755\n",
            "Epoch 70/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.4221e-04 - accuracy: 0.9770 - val_loss: 0.0018 - val_accuracy: 0.8828\n",
            "Epoch 71/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.2525e-04 - accuracy: 0.9781 - val_loss: 0.0019 - val_accuracy: 0.8765\n",
            "Epoch 72/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.2558e-04 - accuracy: 0.9785 - val_loss: 0.0018 - val_accuracy: 0.8837\n",
            "Epoch 73/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.2418e-04 - accuracy: 0.9778 - val_loss: 0.0018 - val_accuracy: 0.8827\n",
            "Epoch 74/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.2007e-04 - accuracy: 0.9783 - val_loss: 0.0018 - val_accuracy: 0.8790\n",
            "Epoch 75/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.0660e-04 - accuracy: 0.9790 - val_loss: 0.0018 - val_accuracy: 0.8828\n",
            "Epoch 76/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.9242e-04 - accuracy: 0.9804 - val_loss: 0.0017 - val_accuracy: 0.8892\n",
            "Epoch 77/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.1408e-04 - accuracy: 0.9790 - val_loss: 0.0017 - val_accuracy: 0.8890\n",
            "Epoch 78/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.1773e-04 - accuracy: 0.9791 - val_loss: 0.0018 - val_accuracy: 0.8788\n",
            "Epoch 79/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.1876e-04 - accuracy: 0.9787 - val_loss: 0.0017 - val_accuracy: 0.8882\n",
            "Epoch 80/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.9859e-04 - accuracy: 0.9801 - val_loss: 0.0017 - val_accuracy: 0.8888\n",
            "Epoch 81/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.8276e-04 - accuracy: 0.9815 - val_loss: 0.0017 - val_accuracy: 0.8895\n",
            "Epoch 82/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.6011e-04 - accuracy: 0.9825 - val_loss: 0.0018 - val_accuracy: 0.8845\n",
            "Epoch 83/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.7311e-04 - accuracy: 0.9820 - val_loss: 0.0017 - val_accuracy: 0.8855\n",
            "Epoch 84/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.7079e-04 - accuracy: 0.9820 - val_loss: 0.0016 - val_accuracy: 0.8937\n",
            "Epoch 85/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.6553e-04 - accuracy: 0.9825 - val_loss: 0.0016 - val_accuracy: 0.8948\n",
            "Epoch 86/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.7730e-04 - accuracy: 0.9821 - val_loss: 0.0018 - val_accuracy: 0.8828\n",
            "Epoch 87/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.7884e-04 - accuracy: 0.9824 - val_loss: 0.0017 - val_accuracy: 0.8903\n",
            "Epoch 88/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.5807e-04 - accuracy: 0.9828 - val_loss: 0.0016 - val_accuracy: 0.8972\n",
            "Epoch 89/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.5160e-04 - accuracy: 0.9834 - val_loss: 0.0016 - val_accuracy: 0.8917\n",
            "Epoch 90/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.4367e-04 - accuracy: 0.9841 - val_loss: 0.0016 - val_accuracy: 0.8943\n",
            "Epoch 91/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.5359e-04 - accuracy: 0.9835 - val_loss: 0.0016 - val_accuracy: 0.8947\n",
            "Epoch 92/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.2460e-04 - accuracy: 0.9854 - val_loss: 0.0016 - val_accuracy: 0.8992\n",
            "Epoch 93/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.0515e-04 - accuracy: 0.9865 - val_loss: 0.0016 - val_accuracy: 0.8992\n",
            "Epoch 94/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.2423e-04 - accuracy: 0.9857 - val_loss: 0.0017 - val_accuracy: 0.8895\n",
            "Epoch 95/1000\n",
            "47/47 [==============================] - 3s 75ms/step - loss: 2.2769e-04 - accuracy: 0.9854 - val_loss: 0.0017 - val_accuracy: 0.8895\n",
            "Epoch 96/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3920e-04 - accuracy: 0.9849 - val_loss: 0.0016 - val_accuracy: 0.8952\n",
            "Epoch 97/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 2.4052e-04 - accuracy: 0.9846 - val_loss: 0.0017 - val_accuracy: 0.8887\n",
            "Epoch 98/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.7287e-04 - accuracy: 0.9825 - val_loss: 0.0017 - val_accuracy: 0.8928\n",
            "Epoch 99/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3957e-04 - accuracy: 0.9852 - val_loss: 0.0015 - val_accuracy: 0.9013\n",
            "Epoch 100/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3645e-04 - accuracy: 0.9847 - val_loss: 0.0017 - val_accuracy: 0.8883\n",
            "Epoch 101/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.6348e-04 - accuracy: 0.9835 - val_loss: 0.0017 - val_accuracy: 0.8890\n",
            "Epoch 102/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.4708e-04 - accuracy: 0.9845 - val_loss: 0.0017 - val_accuracy: 0.8902\n",
            "Epoch 103/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.5403e-04 - accuracy: 0.9837 - val_loss: 0.0016 - val_accuracy: 0.8945\n",
            "Epoch 104/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 2.8466e-04 - accuracy: 0.9820 - val_loss: 0.0020 - val_accuracy: 0.8733\n",
            "Epoch 105/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.6373e-04 - accuracy: 0.9835 - val_loss: 0.0017 - val_accuracy: 0.8938\n",
            "Epoch 106/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.4498e-04 - accuracy: 0.9847 - val_loss: 0.0017 - val_accuracy: 0.8900\n",
            "Epoch 107/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3919e-04 - accuracy: 0.9848 - val_loss: 0.0017 - val_accuracy: 0.8963\n",
            "Epoch 108/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.9926e-04 - accuracy: 0.9877 - val_loss: 0.0016 - val_accuracy: 0.8970\n",
            "Epoch 109/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.8599e-04 - accuracy: 0.9883 - val_loss: 0.0015 - val_accuracy: 0.9045\n",
            "Epoch 110/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.7188e-04 - accuracy: 0.9892 - val_loss: 0.0016 - val_accuracy: 0.9007\n",
            "Epoch 111/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.9446e-04 - accuracy: 0.9880 - val_loss: 0.0016 - val_accuracy: 0.8970\n",
            "Epoch 112/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.8132e-04 - accuracy: 0.9888 - val_loss: 0.0016 - val_accuracy: 0.9000\n",
            "Epoch 113/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.7433e-04 - accuracy: 0.9890 - val_loss: 0.0016 - val_accuracy: 0.9012\n",
            "Epoch 114/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.6613e-04 - accuracy: 0.9898 - val_loss: 0.0015 - val_accuracy: 0.9097\n",
            "Epoch 115/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.5932e-04 - accuracy: 0.9902 - val_loss: 0.0014 - val_accuracy: 0.9137\n",
            "Epoch 116/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.5464e-04 - accuracy: 0.9905 - val_loss: 0.0014 - val_accuracy: 0.9090\n",
            "Epoch 117/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.6016e-04 - accuracy: 0.9899 - val_loss: 0.0015 - val_accuracy: 0.9043\n",
            "Epoch 118/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.6950e-04 - accuracy: 0.9894 - val_loss: 0.0015 - val_accuracy: 0.9035\n",
            "Epoch 119/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.7468e-04 - accuracy: 0.9892 - val_loss: 0.0016 - val_accuracy: 0.8998\n",
            "Epoch 120/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.7714e-04 - accuracy: 0.9895 - val_loss: 0.0015 - val_accuracy: 0.9043\n",
            "Epoch 121/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.8942e-04 - accuracy: 0.9885 - val_loss: 0.0015 - val_accuracy: 0.9040\n",
            "Epoch 122/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.9476e-04 - accuracy: 0.9880 - val_loss: 0.0015 - val_accuracy: 0.9043\n",
            "Epoch 123/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 2.4419e-04 - accuracy: 0.9847 - val_loss: 0.0016 - val_accuracy: 0.8990\n",
            "Epoch 124/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.0250e-04 - accuracy: 0.9876 - val_loss: 0.0015 - val_accuracy: 0.9020\n",
            "Epoch 125/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3819e-04 - accuracy: 0.9853 - val_loss: 0.0015 - val_accuracy: 0.9035\n",
            "Epoch 126/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.5645e-04 - accuracy: 0.9840 - val_loss: 0.0018 - val_accuracy: 0.8875\n",
            "Epoch 127/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3832e-04 - accuracy: 0.9853 - val_loss: 0.0017 - val_accuracy: 0.8957\n",
            "Epoch 128/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.1069e-04 - accuracy: 0.9870 - val_loss: 0.0016 - val_accuracy: 0.9032\n",
            "Epoch 129/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.8682e-04 - accuracy: 0.9887 - val_loss: 0.0015 - val_accuracy: 0.9063\n",
            "Epoch 130/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.6462e-04 - accuracy: 0.9898 - val_loss: 0.0015 - val_accuracy: 0.9028\n",
            "Epoch 131/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.6234e-04 - accuracy: 0.9898 - val_loss: 0.0015 - val_accuracy: 0.9042\n",
            "Epoch 132/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.5673e-04 - accuracy: 0.9905 - val_loss: 0.0015 - val_accuracy: 0.9072\n",
            "Epoch 133/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.5212e-04 - accuracy: 0.9910 - val_loss: 0.0014 - val_accuracy: 0.9137\n",
            "Epoch 134/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.5818e-04 - accuracy: 0.9904 - val_loss: 0.0015 - val_accuracy: 0.9043\n",
            "Epoch 135/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.5609e-04 - accuracy: 0.9905 - val_loss: 0.0014 - val_accuracy: 0.9128\n",
            "Epoch 136/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.6300e-04 - accuracy: 0.9902 - val_loss: 0.0015 - val_accuracy: 0.9108\n",
            "Epoch 137/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.5167e-04 - accuracy: 0.9910 - val_loss: 0.0014 - val_accuracy: 0.9135\n",
            "Epoch 138/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.5092e-04 - accuracy: 0.9910 - val_loss: 0.0015 - val_accuracy: 0.9107\n",
            "Epoch 139/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.5551e-04 - accuracy: 0.9907 - val_loss: 0.0016 - val_accuracy: 0.9028\n",
            "Epoch 140/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.5738e-04 - accuracy: 0.9907 - val_loss: 0.0016 - val_accuracy: 0.8992\n",
            "Epoch 141/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.5710e-04 - accuracy: 0.9907 - val_loss: 0.0015 - val_accuracy: 0.9097\n",
            "Epoch 142/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.7885e-04 - accuracy: 0.9892 - val_loss: 0.0018 - val_accuracy: 0.8907\n",
            "Epoch 143/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.9754e-04 - accuracy: 0.9878 - val_loss: 0.0017 - val_accuracy: 0.8943\n",
            "Epoch 144/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3592e-04 - accuracy: 0.9850 - val_loss: 0.0018 - val_accuracy: 0.8872\n",
            "Epoch 145/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.9483e-04 - accuracy: 0.9881 - val_loss: 0.0015 - val_accuracy: 0.9093\n",
            "Epoch 146/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.8972e-04 - accuracy: 0.9883 - val_loss: 0.0015 - val_accuracy: 0.9070\n",
            "Epoch 147/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.7858e-04 - accuracy: 0.9889 - val_loss: 0.0015 - val_accuracy: 0.9050\n",
            "Epoch 148/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.8502e-04 - accuracy: 0.9889 - val_loss: 0.0017 - val_accuracy: 0.8980\n",
            "Epoch 149/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.2659e-04 - accuracy: 0.9860 - val_loss: 0.0015 - val_accuracy: 0.9065\n",
            "Epoch 150/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.7110e-04 - accuracy: 0.9897 - val_loss: 0.0017 - val_accuracy: 0.8962\n",
            "Epoch 151/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.8039e-04 - accuracy: 0.9890 - val_loss: 0.0014 - val_accuracy: 0.9145\n",
            "Epoch 152/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.7287e-04 - accuracy: 0.9893 - val_loss: 0.0016 - val_accuracy: 0.9043\n",
            "Epoch 153/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.5849e-04 - accuracy: 0.9902 - val_loss: 0.0014 - val_accuracy: 0.9128\n",
            "Epoch 154/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.7110e-04 - accuracy: 0.9890 - val_loss: 0.0017 - val_accuracy: 0.8943\n",
            "Epoch 155/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.8194e-04 - accuracy: 0.9885 - val_loss: 0.0014 - val_accuracy: 0.9103\n",
            "Epoch 156/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.6649e-04 - accuracy: 0.9898 - val_loss: 0.0016 - val_accuracy: 0.9043\n",
            "Epoch 157/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.2677e-04 - accuracy: 0.9924 - val_loss: 0.0014 - val_accuracy: 0.9158\n",
            "Epoch 158/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.2306e-04 - accuracy: 0.9923 - val_loss: 0.0015 - val_accuracy: 0.9107\n",
            "Epoch 159/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.0830e-04 - accuracy: 0.9936 - val_loss: 0.0015 - val_accuracy: 0.9100\n",
            "Epoch 160/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.1232e-04 - accuracy: 0.9932 - val_loss: 0.0014 - val_accuracy: 0.9115\n",
            "Epoch 161/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.5768e-04 - accuracy: 0.9900 - val_loss: 0.0017 - val_accuracy: 0.8998\n",
            "Epoch 162/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.6236e-05 - accuracy: 0.9942 - val_loss: 0.0013 - val_accuracy: 0.9218\n",
            "Epoch 163/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.7193e-05 - accuracy: 0.9957 - val_loss: 0.0014 - val_accuracy: 0.9173\n",
            "Epoch 164/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.7312e-05 - accuracy: 0.9950 - val_loss: 0.0013 - val_accuracy: 0.9173\n",
            "Epoch 165/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.7419e-05 - accuracy: 0.9957 - val_loss: 0.0013 - val_accuracy: 0.9215\n",
            "Epoch 166/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.2291e-05 - accuracy: 0.9959 - val_loss: 0.0012 - val_accuracy: 0.9238\n",
            "Epoch 167/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.3505e-05 - accuracy: 0.9952 - val_loss: 0.0013 - val_accuracy: 0.9167\n",
            "Epoch 168/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.6014e-05 - accuracy: 0.9949 - val_loss: 0.0013 - val_accuracy: 0.9217\n",
            "Epoch 169/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 8.9799e-05 - accuracy: 0.9950 - val_loss: 0.0013 - val_accuracy: 0.9227\n",
            "Epoch 170/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.9187e-05 - accuracy: 0.9947 - val_loss: 0.0014 - val_accuracy: 0.9172\n",
            "Epoch 171/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.0261e-04 - accuracy: 0.9939 - val_loss: 0.0014 - val_accuracy: 0.9132\n",
            "Epoch 172/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.8487e-05 - accuracy: 0.9950 - val_loss: 0.0014 - val_accuracy: 0.9172\n",
            "Epoch 173/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.4828e-05 - accuracy: 0.9958 - val_loss: 0.0014 - val_accuracy: 0.9118\n",
            "Epoch 174/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.2066e-05 - accuracy: 0.9960 - val_loss: 0.0013 - val_accuracy: 0.9172\n",
            "Epoch 175/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 5.9962e-05 - accuracy: 0.9967 - val_loss: 0.0014 - val_accuracy: 0.9180\n",
            "Epoch 176/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 9.0771e-05 - accuracy: 0.9945 - val_loss: 0.0015 - val_accuracy: 0.9077\n",
            "Epoch 177/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.0860e-04 - accuracy: 0.9935 - val_loss: 0.0015 - val_accuracy: 0.9095\n",
            "Epoch 178/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.0557e-04 - accuracy: 0.9938 - val_loss: 0.0015 - val_accuracy: 0.9117\n",
            "Epoch 179/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.0298e-04 - accuracy: 0.9938 - val_loss: 0.0014 - val_accuracy: 0.9183\n",
            "Epoch 180/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.9130e-05 - accuracy: 0.9955 - val_loss: 0.0013 - val_accuracy: 0.9212\n",
            "Epoch 181/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.7982e-05 - accuracy: 0.9964 - val_loss: 0.0014 - val_accuracy: 0.9160\n",
            "Epoch 182/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.5097e-05 - accuracy: 0.9950 - val_loss: 0.0013 - val_accuracy: 0.9197\n",
            "Epoch 183/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.5883e-05 - accuracy: 0.9958 - val_loss: 0.0013 - val_accuracy: 0.9193\n",
            "Epoch 184/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.0042e-04 - accuracy: 0.9944 - val_loss: 0.0014 - val_accuracy: 0.9198\n",
            "Epoch 185/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.0502e-04 - accuracy: 0.9935 - val_loss: 0.0014 - val_accuracy: 0.9125\n",
            "Epoch 186/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 9.6350e-05 - accuracy: 0.9945 - val_loss: 0.0014 - val_accuracy: 0.9165\n",
            "Epoch 187/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.7580e-05 - accuracy: 0.9948 - val_loss: 0.0014 - val_accuracy: 0.9165\n",
            "Epoch 188/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.4027e-04 - accuracy: 0.9915 - val_loss: 0.0015 - val_accuracy: 0.9100\n",
            "Epoch 189/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.3068e-04 - accuracy: 0.9918 - val_loss: 0.0016 - val_accuracy: 0.9068\n",
            "Epoch 190/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.1131e-04 - accuracy: 0.9934 - val_loss: 0.0013 - val_accuracy: 0.9198\n",
            "Epoch 191/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.1602e-05 - accuracy: 0.9947 - val_loss: 0.0015 - val_accuracy: 0.9100\n",
            "Epoch 192/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.5734e-05 - accuracy: 0.9951 - val_loss: 0.0013 - val_accuracy: 0.9202\n",
            "Epoch 193/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.9514e-05 - accuracy: 0.9954 - val_loss: 0.0012 - val_accuracy: 0.9243\n",
            "Epoch 194/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.2386e-05 - accuracy: 0.9959 - val_loss: 0.0013 - val_accuracy: 0.9207\n",
            "Epoch 195/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.0145e-05 - accuracy: 0.9952 - val_loss: 0.0015 - val_accuracy: 0.9108\n",
            "Epoch 196/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.6036e-05 - accuracy: 0.9951 - val_loss: 0.0013 - val_accuracy: 0.9192\n",
            "Epoch 197/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 8.7886e-05 - accuracy: 0.9950 - val_loss: 0.0014 - val_accuracy: 0.9142\n",
            "Epoch 198/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.9866e-05 - accuracy: 0.9948 - val_loss: 0.0013 - val_accuracy: 0.9208\n",
            "Epoch 199/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.1685e-05 - accuracy: 0.9953 - val_loss: 0.0014 - val_accuracy: 0.9152\n",
            "Epoch 200/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.1058e-04 - accuracy: 0.9935 - val_loss: 0.0014 - val_accuracy: 0.9152\n",
            "Epoch 201/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.1347e-04 - accuracy: 0.9934 - val_loss: 0.0015 - val_accuracy: 0.9108\n",
            "Epoch 202/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.0967e-04 - accuracy: 0.9934 - val_loss: 0.0013 - val_accuracy: 0.9223\n",
            "Epoch 203/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.5304e-05 - accuracy: 0.9956 - val_loss: 0.0014 - val_accuracy: 0.9178\n",
            "Epoch 204/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.1930e-05 - accuracy: 0.9955 - val_loss: 0.0014 - val_accuracy: 0.9178\n",
            "Epoch 205/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.7463e-05 - accuracy: 0.9964 - val_loss: 0.0013 - val_accuracy: 0.9220\n",
            "Epoch 206/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.5396e-05 - accuracy: 0.9958 - val_loss: 0.0013 - val_accuracy: 0.9233\n",
            "Epoch 207/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.5261e-05 - accuracy: 0.9962 - val_loss: 0.0013 - val_accuracy: 0.9223\n",
            "Epoch 208/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.5854e-05 - accuracy: 0.9957 - val_loss: 0.0012 - val_accuracy: 0.9277\n",
            "Epoch 209/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.3510e-05 - accuracy: 0.9953 - val_loss: 0.0015 - val_accuracy: 0.9123\n",
            "Epoch 210/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.2272e-05 - accuracy: 0.9951 - val_loss: 0.0014 - val_accuracy: 0.9180\n",
            "Epoch 211/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.7804e-05 - accuracy: 0.9957 - val_loss: 0.0013 - val_accuracy: 0.9220\n",
            "Epoch 212/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 6.9592e-05 - accuracy: 0.9961 - val_loss: 0.0012 - val_accuracy: 0.9248\n",
            "Epoch 213/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.7888e-05 - accuracy: 0.9950 - val_loss: 0.0014 - val_accuracy: 0.9165\n",
            "Epoch 214/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.8601e-05 - accuracy: 0.9952 - val_loss: 0.0013 - val_accuracy: 0.9245\n",
            "Epoch 215/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.5986e-05 - accuracy: 0.9941 - val_loss: 0.0013 - val_accuracy: 0.9245\n",
            "Epoch 216/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.8710e-05 - accuracy: 0.9948 - val_loss: 0.0013 - val_accuracy: 0.9258\n",
            "Epoch 217/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.0117e-04 - accuracy: 0.9943 - val_loss: 0.0014 - val_accuracy: 0.9188\n",
            "Epoch 218/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.8240e-05 - accuracy: 0.9955 - val_loss: 0.0014 - val_accuracy: 0.9185\n",
            "Epoch 219/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.0493e-04 - accuracy: 0.9936 - val_loss: 0.0016 - val_accuracy: 0.9068\n",
            "Epoch 220/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.1001e-04 - accuracy: 0.9934 - val_loss: 0.0014 - val_accuracy: 0.9162\n",
            "Epoch 221/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.5808e-05 - accuracy: 0.9949 - val_loss: 0.0012 - val_accuracy: 0.9277\n",
            "Epoch 222/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.4323e-05 - accuracy: 0.9951 - val_loss: 0.0013 - val_accuracy: 0.9240\n",
            "Epoch 223/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.4929e-05 - accuracy: 0.9958 - val_loss: 0.0013 - val_accuracy: 0.9228\n",
            "Epoch 224/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 6.2759e-05 - accuracy: 0.9965 - val_loss: 0.0012 - val_accuracy: 0.9295\n",
            "Epoch 225/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 7.0968e-05 - accuracy: 0.9961 - val_loss: 0.0015 - val_accuracy: 0.9113\n",
            "Epoch 226/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.5439e-05 - accuracy: 0.9955 - val_loss: 0.0013 - val_accuracy: 0.9193\n",
            "Epoch 227/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.2875e-05 - accuracy: 0.9959 - val_loss: 0.0013 - val_accuracy: 0.9260\n",
            "Epoch 228/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 6.3101e-05 - accuracy: 0.9965 - val_loss: 0.0013 - val_accuracy: 0.9245\n",
            "Epoch 229/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.5849e-05 - accuracy: 0.9949 - val_loss: 0.0018 - val_accuracy: 0.8917\n",
            "Epoch 230/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.1514e-04 - accuracy: 0.9933 - val_loss: 0.0013 - val_accuracy: 0.9270\n",
            "Epoch 231/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.1550e-04 - accuracy: 0.9932 - val_loss: 0.0015 - val_accuracy: 0.9112\n",
            "Epoch 232/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.1417e-04 - accuracy: 0.9933 - val_loss: 0.0015 - val_accuracy: 0.9103\n",
            "Epoch 233/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.0053e-04 - accuracy: 0.9937 - val_loss: 0.0012 - val_accuracy: 0.9262\n",
            "Epoch 234/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 8.1890e-05 - accuracy: 0.9951 - val_loss: 0.0013 - val_accuracy: 0.9242\n",
            "Epoch 235/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.0171e-04 - accuracy: 0.9939 - val_loss: 0.0014 - val_accuracy: 0.9205\n",
            "Epoch 236/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.1321e-04 - accuracy: 0.9931 - val_loss: 0.0013 - val_accuracy: 0.9218\n",
            "Epoch 237/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.9257e-05 - accuracy: 0.9943 - val_loss: 0.0017 - val_accuracy: 0.9000\n",
            "Epoch 238/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.0733e-04 - accuracy: 0.9935 - val_loss: 0.0016 - val_accuracy: 0.9108\n",
            "Epoch 239/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.0937e-04 - accuracy: 0.9937 - val_loss: 0.0013 - val_accuracy: 0.9220\n",
            "Epoch 240/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.0575e-04 - accuracy: 0.9939 - val_loss: 0.0013 - val_accuracy: 0.9232\n",
            "Epoch 241/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 7.0027e-05 - accuracy: 0.9959 - val_loss: 0.0013 - val_accuracy: 0.9253\n",
            "Epoch 242/1000\n",
            "47/47 [==============================] - 3s 75ms/step - loss: 5.9777e-05 - accuracy: 0.9968 - val_loss: 0.0012 - val_accuracy: 0.9295\n",
            "Epoch 243/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.6807e-05 - accuracy: 0.9969 - val_loss: 0.0013 - val_accuracy: 0.9265\n",
            "Epoch 244/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.0781e-05 - accuracy: 0.9967 - val_loss: 0.0012 - val_accuracy: 0.9328\n",
            "Epoch 245/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.3428e-05 - accuracy: 0.9972 - val_loss: 0.0012 - val_accuracy: 0.9275\n",
            "Epoch 246/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.1944e-05 - accuracy: 0.9972 - val_loss: 0.0011 - val_accuracy: 0.9342\n",
            "Epoch 247/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 5.1751e-05 - accuracy: 0.9972 - val_loss: 0.0013 - val_accuracy: 0.9262\n",
            "Epoch 248/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 7.0054e-05 - accuracy: 0.9959 - val_loss: 0.0012 - val_accuracy: 0.9305\n",
            "Epoch 249/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 6.8167e-05 - accuracy: 0.9961 - val_loss: 0.0012 - val_accuracy: 0.9285\n",
            "Epoch 250/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.7461e-05 - accuracy: 0.9960 - val_loss: 0.0014 - val_accuracy: 0.9207\n",
            "Epoch 251/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.9469e-05 - accuracy: 0.9948 - val_loss: 0.0019 - val_accuracy: 0.8908\n",
            "Epoch 252/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.0549e-04 - accuracy: 0.9939 - val_loss: 0.0014 - val_accuracy: 0.9203\n",
            "Epoch 253/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.6837e-05 - accuracy: 0.9942 - val_loss: 0.0013 - val_accuracy: 0.9260\n",
            "Epoch 254/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.9271e-05 - accuracy: 0.9942 - val_loss: 0.0016 - val_accuracy: 0.9093\n",
            "Epoch 255/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.9487e-05 - accuracy: 0.9943 - val_loss: 0.0016 - val_accuracy: 0.9093\n",
            "Epoch 256/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.0017e-04 - accuracy: 0.9940 - val_loss: 0.0014 - val_accuracy: 0.9192\n",
            "Epoch 257/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.2281e-05 - accuracy: 0.9946 - val_loss: 0.0015 - val_accuracy: 0.9097\n",
            "Epoch 258/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.6876e-05 - accuracy: 0.9949 - val_loss: 0.0013 - val_accuracy: 0.9248\n",
            "Epoch 259/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.0362e-05 - accuracy: 0.9959 - val_loss: 0.0012 - val_accuracy: 0.9305\n",
            "Epoch 260/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.3020e-05 - accuracy: 0.9959 - val_loss: 0.0013 - val_accuracy: 0.9262\n",
            "Epoch 261/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.2245e-05 - accuracy: 0.9957 - val_loss: 0.0012 - val_accuracy: 0.9310\n",
            "Epoch 262/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.4975e-05 - accuracy: 0.9959 - val_loss: 0.0012 - val_accuracy: 0.9283\n",
            "Epoch 263/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.4915e-05 - accuracy: 0.9958 - val_loss: 0.0012 - val_accuracy: 0.9298\n",
            "Epoch 264/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.5975e-05 - accuracy: 0.9956 - val_loss: 0.0013 - val_accuracy: 0.9258\n",
            "Epoch 265/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.3942e-05 - accuracy: 0.9950 - val_loss: 0.0013 - val_accuracy: 0.9232\n",
            "Epoch 266/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.3981e-05 - accuracy: 0.9958 - val_loss: 0.0013 - val_accuracy: 0.9225\n",
            "Epoch 267/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.4900e-05 - accuracy: 0.9965 - val_loss: 0.0012 - val_accuracy: 0.9282\n",
            "Epoch 268/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.9964e-05 - accuracy: 0.9967 - val_loss: 0.0014 - val_accuracy: 0.9188\n",
            "Epoch 269/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.5415e-05 - accuracy: 0.9964 - val_loss: 0.0013 - val_accuracy: 0.9250\n",
            "Epoch 270/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.9484e-05 - accuracy: 0.9974 - val_loss: 0.0012 - val_accuracy: 0.9322\n",
            "Epoch 271/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.0418e-05 - accuracy: 0.9973 - val_loss: 0.0013 - val_accuracy: 0.9235\n",
            "Epoch 272/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.8972e-05 - accuracy: 0.9955 - val_loss: 0.0014 - val_accuracy: 0.9188\n",
            "Epoch 273/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.7867e-05 - accuracy: 0.9955 - val_loss: 0.0013 - val_accuracy: 0.9245\n",
            "Epoch 274/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.9854e-05 - accuracy: 0.9947 - val_loss: 0.0013 - val_accuracy: 0.9210\n",
            "Epoch 275/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.0441e-04 - accuracy: 0.9937 - val_loss: 0.0014 - val_accuracy: 0.9195\n",
            "Epoch 276/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.5154e-05 - accuracy: 0.9942 - val_loss: 0.0011 - val_accuracy: 0.9340\n",
            "Epoch 277/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.8155e-05 - accuracy: 0.9941 - val_loss: 0.0012 - val_accuracy: 0.9262\n",
            "Epoch 278/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.7252e-05 - accuracy: 0.9964 - val_loss: 0.0012 - val_accuracy: 0.9300\n",
            "Epoch 279/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.6573e-05 - accuracy: 0.9955 - val_loss: 0.0013 - val_accuracy: 0.9272\n",
            "Epoch 280/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.2065e-05 - accuracy: 0.9953 - val_loss: 0.0014 - val_accuracy: 0.9182\n",
            "Epoch 281/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.9483e-05 - accuracy: 0.9962 - val_loss: 0.0013 - val_accuracy: 0.9228\n",
            "Epoch 282/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.6905e-05 - accuracy: 0.9964 - val_loss: 0.0013 - val_accuracy: 0.9260\n",
            "Epoch 283/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.4675e-05 - accuracy: 0.9963 - val_loss: 0.0014 - val_accuracy: 0.9175\n",
            "Epoch 284/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.3081e-05 - accuracy: 0.9952 - val_loss: 0.0014 - val_accuracy: 0.9183\n",
            "Epoch 285/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.4984e-05 - accuracy: 0.9952 - val_loss: 0.0013 - val_accuracy: 0.9272\n",
            "Epoch 286/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.6516e-05 - accuracy: 0.9950 - val_loss: 0.0015 - val_accuracy: 0.9160\n",
            "Epoch 287/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.5142e-05 - accuracy: 0.9957 - val_loss: 0.0012 - val_accuracy: 0.9320\n",
            "Epoch 288/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.6961e-05 - accuracy: 0.9967 - val_loss: 0.0012 - val_accuracy: 0.9290\n",
            "Epoch 289/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.0759e-05 - accuracy: 0.9946 - val_loss: 0.0014 - val_accuracy: 0.9157\n",
            "Epoch 290/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.7333e-05 - accuracy: 0.9949 - val_loss: 0.0015 - val_accuracy: 0.9148\n",
            "Epoch 291/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.2163e-05 - accuracy: 0.9960 - val_loss: 0.0012 - val_accuracy: 0.9303\n",
            "Epoch 292/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.2496e-05 - accuracy: 0.9971 - val_loss: 0.0013 - val_accuracy: 0.9267\n",
            "Epoch 293/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.7289e-05 - accuracy: 0.9968 - val_loss: 0.0012 - val_accuracy: 0.9332\n",
            "Epoch 294/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.2963e-05 - accuracy: 0.9970 - val_loss: 0.0011 - val_accuracy: 0.9383\n",
            "Epoch 295/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.8243e-05 - accuracy: 0.9974 - val_loss: 0.0011 - val_accuracy: 0.9342\n",
            "Epoch 296/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.1827e-05 - accuracy: 0.9965 - val_loss: 0.0011 - val_accuracy: 0.9348\n",
            "Epoch 297/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.0498e-05 - accuracy: 0.9946 - val_loss: 0.0012 - val_accuracy: 0.9287\n",
            "Epoch 298/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 9.6776e-05 - accuracy: 0.9942 - val_loss: 0.0013 - val_accuracy: 0.9272\n",
            "Epoch 299/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.4538e-05 - accuracy: 0.9951 - val_loss: 0.0013 - val_accuracy: 0.9248\n",
            "Epoch 300/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.6220e-05 - accuracy: 0.9956 - val_loss: 0.0012 - val_accuracy: 0.9297\n",
            "Epoch 301/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.0916e-05 - accuracy: 0.9959 - val_loss: 0.0012 - val_accuracy: 0.9292\n",
            "Epoch 302/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.4986e-05 - accuracy: 0.9965 - val_loss: 0.0013 - val_accuracy: 0.9270\n",
            "Epoch 303/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.3515e-05 - accuracy: 0.9958 - val_loss: 0.0014 - val_accuracy: 0.9182\n",
            "Epoch 304/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.0642e-04 - accuracy: 0.9937 - val_loss: 0.0011 - val_accuracy: 0.9360\n",
            "Epoch 305/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.0563e-05 - accuracy: 0.9960 - val_loss: 0.0012 - val_accuracy: 0.9332\n",
            "Epoch 306/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.5840e-05 - accuracy: 0.9964 - val_loss: 0.0011 - val_accuracy: 0.9377\n",
            "Epoch 307/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.8875e-05 - accuracy: 0.9966 - val_loss: 0.0011 - val_accuracy: 0.9382\n",
            "Epoch 308/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.1125e-05 - accuracy: 0.9973 - val_loss: 0.0011 - val_accuracy: 0.9367\n",
            "Epoch 309/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.2063e-05 - accuracy: 0.9973 - val_loss: 0.0012 - val_accuracy: 0.9300\n",
            "Epoch 310/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.6391e-05 - accuracy: 0.9970 - val_loss: 0.0011 - val_accuracy: 0.9350\n",
            "Epoch 311/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.5840e-05 - accuracy: 0.9962 - val_loss: 0.0011 - val_accuracy: 0.9362\n",
            "Epoch 312/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.3647e-05 - accuracy: 0.9959 - val_loss: 0.0013 - val_accuracy: 0.9250\n",
            "Epoch 313/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.3620e-05 - accuracy: 0.9960 - val_loss: 0.0012 - val_accuracy: 0.9315\n",
            "Epoch 314/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.3984e-05 - accuracy: 0.9964 - val_loss: 0.0012 - val_accuracy: 0.9313\n",
            "Epoch 315/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.7688e-05 - accuracy: 0.9961 - val_loss: 0.0014 - val_accuracy: 0.9222\n",
            "Epoch 316/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.1456e-05 - accuracy: 0.9954 - val_loss: 0.0013 - val_accuracy: 0.9263\n",
            "Epoch 317/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.5923e-05 - accuracy: 0.9945 - val_loss: 0.0013 - val_accuracy: 0.9225\n",
            "Epoch 318/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.6736e-05 - accuracy: 0.9950 - val_loss: 0.0012 - val_accuracy: 0.9302\n",
            "Epoch 319/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.1188e-04 - accuracy: 0.9934 - val_loss: 0.0015 - val_accuracy: 0.9115\n",
            "Epoch 320/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 8.4087e-05 - accuracy: 0.9954 - val_loss: 0.0014 - val_accuracy: 0.9228\n",
            "Epoch 321/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.3219e-05 - accuracy: 0.9964 - val_loss: 0.0013 - val_accuracy: 0.9242\n",
            "Epoch 322/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.7708e-05 - accuracy: 0.9969 - val_loss: 0.0011 - val_accuracy: 0.9350\n",
            "Epoch 323/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.5671e-05 - accuracy: 0.9970 - val_loss: 0.0012 - val_accuracy: 0.9297\n",
            "Epoch 324/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.5114e-05 - accuracy: 0.9964 - val_loss: 0.0013 - val_accuracy: 0.9280\n",
            "Epoch 325/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.5134e-05 - accuracy: 0.9970 - val_loss: 0.0012 - val_accuracy: 0.9340\n",
            "Epoch 326/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.0977e-05 - accuracy: 0.9973 - val_loss: 0.0010 - val_accuracy: 0.9395\n",
            "Epoch 327/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.6425e-05 - accuracy: 0.9977 - val_loss: 0.0010 - val_accuracy: 0.9407\n",
            "Epoch 328/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.7517e-05 - accuracy: 0.9975 - val_loss: 0.0010 - val_accuracy: 0.9402\n",
            "Epoch 329/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.5738e-05 - accuracy: 0.9976 - val_loss: 0.0012 - val_accuracy: 0.9345\n",
            "Epoch 330/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.7562e-05 - accuracy: 0.9970 - val_loss: 0.0011 - val_accuracy: 0.9362\n",
            "Epoch 331/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.6618e-05 - accuracy: 0.9949 - val_loss: 0.0013 - val_accuracy: 0.9273\n",
            "Epoch 332/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.0289e-04 - accuracy: 0.9938 - val_loss: 0.0015 - val_accuracy: 0.9150\n",
            "Epoch 333/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.0252e-04 - accuracy: 0.9941 - val_loss: 0.0013 - val_accuracy: 0.9252\n",
            "Epoch 334/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.1176e-04 - accuracy: 0.9936 - val_loss: 0.0013 - val_accuracy: 0.9235\n",
            "Epoch 335/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.3506e-05 - accuracy: 0.9959 - val_loss: 0.0012 - val_accuracy: 0.9303\n",
            "Epoch 336/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.7905e-05 - accuracy: 0.9955 - val_loss: 0.0012 - val_accuracy: 0.9293\n",
            "Epoch 337/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.5992e-05 - accuracy: 0.9962 - val_loss: 0.0012 - val_accuracy: 0.9282\n",
            "Epoch 338/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.8178e-05 - accuracy: 0.9962 - val_loss: 0.0012 - val_accuracy: 0.9317\n",
            "Epoch 339/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.4204e-05 - accuracy: 0.9972 - val_loss: 0.0011 - val_accuracy: 0.9387\n",
            "Epoch 340/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.9567e-05 - accuracy: 0.9957 - val_loss: 0.0018 - val_accuracy: 0.8998\n",
            "Epoch 341/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.8205e-04 - accuracy: 0.9893 - val_loss: 0.0015 - val_accuracy: 0.9153\n",
            "Epoch 342/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.5696e-04 - accuracy: 0.9906 - val_loss: 0.0015 - val_accuracy: 0.9137\n",
            "Epoch 343/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.3598e-04 - accuracy: 0.9921 - val_loss: 0.0014 - val_accuracy: 0.9215\n",
            "Epoch 344/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.3894e-05 - accuracy: 0.9946 - val_loss: 0.0013 - val_accuracy: 0.9237\n",
            "Epoch 345/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.1192e-05 - accuracy: 0.9949 - val_loss: 0.0013 - val_accuracy: 0.9300\n",
            "Epoch 346/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.5992e-05 - accuracy: 0.9963 - val_loss: 0.0012 - val_accuracy: 0.9325\n",
            "Epoch 347/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.3186e-05 - accuracy: 0.9971 - val_loss: 0.0012 - val_accuracy: 0.9310\n",
            "Epoch 348/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.2279e-05 - accuracy: 0.9972 - val_loss: 0.0012 - val_accuracy: 0.9325\n",
            "Epoch 349/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.1404e-05 - accuracy: 0.9966 - val_loss: 0.0012 - val_accuracy: 0.9347\n",
            "Epoch 350/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.9412e-05 - accuracy: 0.9967 - val_loss: 0.0012 - val_accuracy: 0.9310\n",
            "Epoch 351/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.0473e-05 - accuracy: 0.9972 - val_loss: 0.0011 - val_accuracy: 0.9360\n",
            "Epoch 352/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.1186e-05 - accuracy: 0.9972 - val_loss: 0.0012 - val_accuracy: 0.9342\n",
            "Epoch 353/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.8295e-05 - accuracy: 0.9968 - val_loss: 0.0013 - val_accuracy: 0.9285\n",
            "Epoch 354/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.7532e-05 - accuracy: 0.9962 - val_loss: 0.0011 - val_accuracy: 0.9368\n",
            "Epoch 355/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.4029e-05 - accuracy: 0.9965 - val_loss: 0.0011 - val_accuracy: 0.9347\n",
            "Epoch 356/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.9870e-05 - accuracy: 0.9967 - val_loss: 0.0013 - val_accuracy: 0.9283\n",
            "Epoch 357/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.4125e-05 - accuracy: 0.9964 - val_loss: 0.0011 - val_accuracy: 0.9385\n",
            "Epoch 358/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.9271e-05 - accuracy: 0.9973 - val_loss: 0.0011 - val_accuracy: 0.9390\n",
            "Epoch 359/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.3161e-05 - accuracy: 0.9971 - val_loss: 0.0012 - val_accuracy: 0.9303\n",
            "Epoch 360/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.2245e-05 - accuracy: 0.9959 - val_loss: 0.0013 - val_accuracy: 0.9257\n",
            "Epoch 361/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.1433e-05 - accuracy: 0.9960 - val_loss: 0.0012 - val_accuracy: 0.9315\n",
            "Epoch 362/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.1715e-05 - accuracy: 0.9967 - val_loss: 0.0013 - val_accuracy: 0.9258\n",
            "Epoch 363/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.0445e-04 - accuracy: 0.9940 - val_loss: 0.0014 - val_accuracy: 0.9228\n",
            "Epoch 364/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.6371e-05 - accuracy: 0.9944 - val_loss: 0.0013 - val_accuracy: 0.9273\n",
            "Epoch 365/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.2839e-05 - accuracy: 0.9952 - val_loss: 0.0015 - val_accuracy: 0.9135\n",
            "Epoch 366/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.6806e-05 - accuracy: 0.9959 - val_loss: 0.0013 - val_accuracy: 0.9222\n",
            "Epoch 367/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.5521e-05 - accuracy: 0.9955 - val_loss: 0.0013 - val_accuracy: 0.9280\n",
            "Epoch 368/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.0881e-05 - accuracy: 0.9960 - val_loss: 0.0012 - val_accuracy: 0.9328\n",
            "Epoch 369/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.2101e-05 - accuracy: 0.9959 - val_loss: 0.0014 - val_accuracy: 0.9167\n",
            "Epoch 370/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.5486e-05 - accuracy: 0.9958 - val_loss: 0.0012 - val_accuracy: 0.9315\n",
            "Epoch 371/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.1008e-05 - accuracy: 0.9960 - val_loss: 0.0013 - val_accuracy: 0.9262\n",
            "Epoch 372/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.8653e-05 - accuracy: 0.9955 - val_loss: 0.0013 - val_accuracy: 0.9298\n",
            "Epoch 373/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.9501e-05 - accuracy: 0.9967 - val_loss: 0.0012 - val_accuracy: 0.9310\n",
            "Epoch 374/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.3836e-05 - accuracy: 0.9970 - val_loss: 0.0013 - val_accuracy: 0.9237\n",
            "Epoch 375/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.1737e-05 - accuracy: 0.9967 - val_loss: 0.0013 - val_accuracy: 0.9270\n",
            "Epoch 376/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 5.8931e-05 - accuracy: 0.9967 - val_loss: 0.0012 - val_accuracy: 0.9288\n",
            "Epoch 377/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.0438e-05 - accuracy: 0.9960 - val_loss: 0.0012 - val_accuracy: 0.9290\n",
            "Epoch 378/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.0725e-05 - accuracy: 0.9968 - val_loss: 0.0011 - val_accuracy: 0.9360\n",
            "Epoch 379/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.8938e-05 - accuracy: 0.9960 - val_loss: 0.0012 - val_accuracy: 0.9320\n",
            "Epoch 380/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.4148e-05 - accuracy: 0.9970 - val_loss: 0.0012 - val_accuracy: 0.9323\n",
            "Epoch 381/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.0719e-05 - accuracy: 0.9973 - val_loss: 0.0011 - val_accuracy: 0.9388\n",
            "Epoch 382/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.7782e-05 - accuracy: 0.9975 - val_loss: 0.0011 - val_accuracy: 0.9368\n",
            "Epoch 383/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.4133e-05 - accuracy: 0.9977 - val_loss: 0.0010 - val_accuracy: 0.9418\n",
            "Epoch 384/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.2487e-05 - accuracy: 0.9978 - val_loss: 9.9868e-04 - val_accuracy: 0.9425\n",
            "Epoch 385/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.6074e-05 - accuracy: 0.9975 - val_loss: 0.0010 - val_accuracy: 0.9413\n",
            "Epoch 386/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.0815e-05 - accuracy: 0.9973 - val_loss: 0.0012 - val_accuracy: 0.9347\n",
            "Epoch 387/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.2559e-05 - accuracy: 0.9966 - val_loss: 0.0011 - val_accuracy: 0.9370\n",
            "Epoch 388/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.0533e-05 - accuracy: 0.9965 - val_loss: 0.0015 - val_accuracy: 0.9162\n",
            "Epoch 389/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.2168e-05 - accuracy: 0.9949 - val_loss: 0.0013 - val_accuracy: 0.9270\n",
            "Epoch 390/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.8243e-05 - accuracy: 0.9955 - val_loss: 0.0014 - val_accuracy: 0.9225\n",
            "Epoch 391/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.6074e-05 - accuracy: 0.9955 - val_loss: 0.0012 - val_accuracy: 0.9322\n",
            "Epoch 392/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.1366e-05 - accuracy: 0.9967 - val_loss: 0.0013 - val_accuracy: 0.9300\n",
            "Epoch 393/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.6115e-05 - accuracy: 0.9945 - val_loss: 0.0013 - val_accuracy: 0.9268\n",
            "Epoch 394/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.1236e-04 - accuracy: 0.9935 - val_loss: 0.0013 - val_accuracy: 0.9252\n",
            "Epoch 395/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.3576e-04 - accuracy: 0.9921 - val_loss: 0.0017 - val_accuracy: 0.9053\n",
            "Epoch 396/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.2207e-04 - accuracy: 0.9930 - val_loss: 0.0012 - val_accuracy: 0.9348\n",
            "Epoch 397/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.6955e-05 - accuracy: 0.9956 - val_loss: 0.0014 - val_accuracy: 0.9213\n",
            "Epoch 398/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.2900e-05 - accuracy: 0.9945 - val_loss: 0.0011 - val_accuracy: 0.9368\n",
            "Epoch 399/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.2968e-05 - accuracy: 0.9965 - val_loss: 0.0012 - val_accuracy: 0.9318\n",
            "Epoch 400/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.1731e-05 - accuracy: 0.9965 - val_loss: 0.0010 - val_accuracy: 0.9422\n",
            "Epoch 401/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.5302e-05 - accuracy: 0.9969 - val_loss: 0.0012 - val_accuracy: 0.9343\n",
            "Epoch 402/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.9708e-05 - accuracy: 0.9973 - val_loss: 0.0010 - val_accuracy: 0.9440\n",
            "Epoch 403/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.3606e-05 - accuracy: 0.9970 - val_loss: 0.0012 - val_accuracy: 0.9343\n",
            "Epoch 404/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.6168e-05 - accuracy: 0.9975 - val_loss: 0.0011 - val_accuracy: 0.9412\n",
            "Epoch 405/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.5814e-05 - accuracy: 0.9975 - val_loss: 9.5647e-04 - val_accuracy: 0.9455\n",
            "Epoch 406/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.3103e-05 - accuracy: 0.9977 - val_loss: 0.0011 - val_accuracy: 0.9390\n",
            "Epoch 407/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.4642e-05 - accuracy: 0.9977 - val_loss: 0.0011 - val_accuracy: 0.9407\n",
            "Epoch 408/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.8103e-05 - accuracy: 0.9975 - val_loss: 0.0011 - val_accuracy: 0.9402\n",
            "Epoch 409/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.5219e-05 - accuracy: 0.9970 - val_loss: 0.0011 - val_accuracy: 0.9398\n",
            "Epoch 410/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.5385e-05 - accuracy: 0.9945 - val_loss: 0.0016 - val_accuracy: 0.9082\n",
            "Epoch 411/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.0417e-04 - accuracy: 0.9938 - val_loss: 0.0011 - val_accuracy: 0.9383\n",
            "Epoch 412/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.3316e-05 - accuracy: 0.9948 - val_loss: 0.0012 - val_accuracy: 0.9293\n",
            "Epoch 413/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.6122e-05 - accuracy: 0.9957 - val_loss: 0.0011 - val_accuracy: 0.9373\n",
            "Epoch 414/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.9469e-05 - accuracy: 0.9967 - val_loss: 0.0011 - val_accuracy: 0.9368\n",
            "Epoch 415/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.0024e-05 - accuracy: 0.9967 - val_loss: 0.0011 - val_accuracy: 0.9382\n",
            "Epoch 416/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.4636e-05 - accuracy: 0.9969 - val_loss: 0.0012 - val_accuracy: 0.9343\n",
            "Epoch 417/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 6.9657e-05 - accuracy: 0.9962 - val_loss: 0.0013 - val_accuracy: 0.9260\n",
            "Epoch 418/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.9853e-05 - accuracy: 0.9974 - val_loss: 0.0010 - val_accuracy: 0.9428\n",
            "Epoch 419/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.8756e-05 - accuracy: 0.9975 - val_loss: 0.0012 - val_accuracy: 0.9337\n",
            "Epoch 420/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.0537e-05 - accuracy: 0.9965 - val_loss: 0.0012 - val_accuracy: 0.9303\n",
            "Epoch 421/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.9450e-05 - accuracy: 0.9966 - val_loss: 0.0011 - val_accuracy: 0.9372\n",
            "Epoch 422/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.3962e-05 - accuracy: 0.9970 - val_loss: 0.0012 - val_accuracy: 0.9327\n",
            "Epoch 423/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.7620e-05 - accuracy: 0.9968 - val_loss: 0.0014 - val_accuracy: 0.9192\n",
            "Epoch 424/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.9889e-05 - accuracy: 0.9960 - val_loss: 0.0013 - val_accuracy: 0.9285\n",
            "Epoch 425/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.6676e-05 - accuracy: 0.9963 - val_loss: 0.0012 - val_accuracy: 0.9327\n",
            "Epoch 426/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.5442e-05 - accuracy: 0.9955 - val_loss: 0.0012 - val_accuracy: 0.9343\n",
            "Epoch 427/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.0302e-04 - accuracy: 0.9943 - val_loss: 0.0013 - val_accuracy: 0.9285\n",
            "Epoch 428/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.9030e-05 - accuracy: 0.9960 - val_loss: 0.0012 - val_accuracy: 0.9315\n",
            "Epoch 429/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.1355e-05 - accuracy: 0.9952 - val_loss: 0.0011 - val_accuracy: 0.9387\n",
            "Epoch 430/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.0957e-04 - accuracy: 0.9938 - val_loss: 0.0013 - val_accuracy: 0.9252\n",
            "Epoch 431/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.0656e-05 - accuracy: 0.9948 - val_loss: 0.0013 - val_accuracy: 0.9283\n",
            "Epoch 432/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.4307e-05 - accuracy: 0.9945 - val_loss: 0.0013 - val_accuracy: 0.9292\n",
            "Epoch 433/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.6535e-05 - accuracy: 0.9958 - val_loss: 0.0011 - val_accuracy: 0.9342\n",
            "Epoch 434/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.4209e-05 - accuracy: 0.9962 - val_loss: 0.0012 - val_accuracy: 0.9322\n",
            "Epoch 435/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.6810e-05 - accuracy: 0.9962 - val_loss: 0.0011 - val_accuracy: 0.9400\n",
            "Epoch 436/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.8854e-05 - accuracy: 0.9968 - val_loss: 0.0011 - val_accuracy: 0.9380\n",
            "Epoch 437/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.3855e-05 - accuracy: 0.9970 - val_loss: 0.0011 - val_accuracy: 0.9347\n",
            "Epoch 438/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.5629e-05 - accuracy: 0.9976 - val_loss: 0.0012 - val_accuracy: 0.9337\n",
            "Epoch 439/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.8556e-05 - accuracy: 0.9974 - val_loss: 0.0010 - val_accuracy: 0.9442\n",
            "Epoch 440/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.5945e-05 - accuracy: 0.9975 - val_loss: 0.0012 - val_accuracy: 0.9318\n",
            "Epoch 441/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.6680e-05 - accuracy: 0.9968 - val_loss: 0.0010 - val_accuracy: 0.9412\n",
            "Epoch 442/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.5208e-05 - accuracy: 0.9977 - val_loss: 0.0010 - val_accuracy: 0.9420\n",
            "Epoch 443/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.2390e-05 - accuracy: 0.9978 - val_loss: 0.0010 - val_accuracy: 0.9422\n",
            "Epoch 444/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.2258e-05 - accuracy: 0.9978 - val_loss: 0.0010 - val_accuracy: 0.9430\n",
            "Epoch 445/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.0615e-05 - accuracy: 0.9968 - val_loss: 0.0011 - val_accuracy: 0.9387\n",
            "Epoch 446/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.3686e-05 - accuracy: 0.9945 - val_loss: 0.0013 - val_accuracy: 0.9262\n",
            "Epoch 447/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.3624e-05 - accuracy: 0.9965 - val_loss: 0.0012 - val_accuracy: 0.9338\n",
            "Epoch 448/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 5.9914e-05 - accuracy: 0.9969 - val_loss: 0.0012 - val_accuracy: 0.9347\n",
            "Epoch 449/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.9088e-05 - accuracy: 0.9967 - val_loss: 0.0011 - val_accuracy: 0.9355\n",
            "Epoch 450/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.6035e-05 - accuracy: 0.9970 - val_loss: 0.0012 - val_accuracy: 0.9345\n",
            "Epoch 451/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.4126e-05 - accuracy: 0.9948 - val_loss: 0.0013 - val_accuracy: 0.9248\n",
            "Epoch 452/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.5229e-05 - accuracy: 0.9964 - val_loss: 0.0011 - val_accuracy: 0.9368\n",
            "Epoch 453/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.6134e-05 - accuracy: 0.9971 - val_loss: 0.0011 - val_accuracy: 0.9408\n",
            "Epoch 454/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.4239e-05 - accuracy: 0.9971 - val_loss: 0.0011 - val_accuracy: 0.9375\n",
            "Epoch 455/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.0623e-05 - accuracy: 0.9961 - val_loss: 0.0011 - val_accuracy: 0.9382\n",
            "Epoch 456/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.0125e-05 - accuracy: 0.9956 - val_loss: 0.0010 - val_accuracy: 0.9425\n",
            "Epoch 457/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.4114e-05 - accuracy: 0.9960 - val_loss: 0.0012 - val_accuracy: 0.9335\n",
            "Epoch 458/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.1450e-05 - accuracy: 0.9966 - val_loss: 0.0011 - val_accuracy: 0.9383\n",
            "Epoch 459/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.1437e-05 - accuracy: 0.9973 - val_loss: 0.0010 - val_accuracy: 0.9420\n",
            "Epoch 460/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.1558e-05 - accuracy: 0.9961 - val_loss: 0.0013 - val_accuracy: 0.9273\n",
            "Epoch 461/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.5170e-05 - accuracy: 0.9970 - val_loss: 0.0012 - val_accuracy: 0.9343\n",
            "Epoch 462/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.2041e-05 - accuracy: 0.9967 - val_loss: 0.0012 - val_accuracy: 0.9325\n",
            "Epoch 463/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.2785e-05 - accuracy: 0.9965 - val_loss: 0.0011 - val_accuracy: 0.9345\n",
            "Epoch 464/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.8634e-05 - accuracy: 0.9960 - val_loss: 0.0015 - val_accuracy: 0.9177\n",
            "Epoch 465/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.7563e-05 - accuracy: 0.9963 - val_loss: 0.0012 - val_accuracy: 0.9320\n",
            "Epoch 466/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.0063e-05 - accuracy: 0.9969 - val_loss: 0.0011 - val_accuracy: 0.9367\n",
            "Epoch 467/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.1658e-05 - accuracy: 0.9966 - val_loss: 0.0012 - val_accuracy: 0.9350\n",
            "Epoch 468/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 5.2032e-05 - accuracy: 0.9971 - val_loss: 0.0011 - val_accuracy: 0.9358\n",
            "Epoch 469/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.8962e-05 - accuracy: 0.9968 - val_loss: 0.0012 - val_accuracy: 0.9343\n",
            "Epoch 470/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.3099e-05 - accuracy: 0.9958 - val_loss: 0.0012 - val_accuracy: 0.9310\n",
            "Epoch 471/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.7325e-05 - accuracy: 0.9969 - val_loss: 0.0012 - val_accuracy: 0.9338\n",
            "Epoch 472/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.8288e-05 - accuracy: 0.9975 - val_loss: 0.0011 - val_accuracy: 0.9383\n",
            "Epoch 473/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.8948e-05 - accuracy: 0.9973 - val_loss: 0.0011 - val_accuracy: 0.9398\n",
            "Epoch 474/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.0906e-05 - accuracy: 0.9965 - val_loss: 0.0014 - val_accuracy: 0.9245\n",
            "Epoch 475/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.9127e-05 - accuracy: 0.9960 - val_loss: 0.0011 - val_accuracy: 0.9407\n",
            "Epoch 476/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.6049e-05 - accuracy: 0.9958 - val_loss: 0.0013 - val_accuracy: 0.9255\n",
            "Epoch 477/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 6.5847e-05 - accuracy: 0.9965 - val_loss: 0.0011 - val_accuracy: 0.9367\n",
            "Epoch 478/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.3465e-05 - accuracy: 0.9946 - val_loss: 0.0013 - val_accuracy: 0.9258\n",
            "Epoch 479/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.8403e-05 - accuracy: 0.9958 - val_loss: 0.0013 - val_accuracy: 0.9278\n",
            "Epoch 480/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.7585e-05 - accuracy: 0.9969 - val_loss: 0.0012 - val_accuracy: 0.9352\n",
            "Epoch 481/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.0780e-05 - accuracy: 0.9967 - val_loss: 0.0012 - val_accuracy: 0.9352\n",
            "Epoch 482/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.2264e-05 - accuracy: 0.9973 - val_loss: 0.0013 - val_accuracy: 0.9287\n",
            "Epoch 483/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.7344e-05 - accuracy: 0.9964 - val_loss: 0.0012 - val_accuracy: 0.9322\n",
            "Epoch 484/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.8389e-05 - accuracy: 0.9968 - val_loss: 0.0011 - val_accuracy: 0.9348\n",
            "Epoch 485/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.9590e-05 - accuracy: 0.9967 - val_loss: 0.0012 - val_accuracy: 0.9328\n",
            "Epoch 486/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.0631e-05 - accuracy: 0.9967 - val_loss: 0.0010 - val_accuracy: 0.9413\n",
            "Epoch 487/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.4005e-05 - accuracy: 0.9972 - val_loss: 9.9902e-04 - val_accuracy: 0.9438\n",
            "Epoch 488/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.5515e-05 - accuracy: 0.9970 - val_loss: 0.0012 - val_accuracy: 0.9350\n",
            "Epoch 489/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.6041e-05 - accuracy: 0.9963 - val_loss: 0.0012 - val_accuracy: 0.9343\n",
            "Epoch 490/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.9774e-05 - accuracy: 0.9955 - val_loss: 0.0014 - val_accuracy: 0.9217\n",
            "Epoch 491/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.0587e-05 - accuracy: 0.9968 - val_loss: 0.0012 - val_accuracy: 0.9330\n",
            "Epoch 492/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.9346e-05 - accuracy: 0.9973 - val_loss: 0.0013 - val_accuracy: 0.9237\n",
            "Epoch 493/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.0932e-05 - accuracy: 0.9960 - val_loss: 0.0014 - val_accuracy: 0.9233\n",
            "Epoch 494/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.8656e-05 - accuracy: 0.9950 - val_loss: 0.0016 - val_accuracy: 0.9132\n",
            "Epoch 495/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.2857e-04 - accuracy: 0.9927 - val_loss: 0.0013 - val_accuracy: 0.9265\n",
            "Epoch 496/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.2641e-04 - accuracy: 0.9927 - val_loss: 0.0014 - val_accuracy: 0.9237\n",
            "Epoch 497/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.5383e-05 - accuracy: 0.9951 - val_loss: 0.0013 - val_accuracy: 0.9290\n",
            "Epoch 498/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.3953e-05 - accuracy: 0.9966 - val_loss: 0.0012 - val_accuracy: 0.9353\n",
            "Epoch 499/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.9769e-05 - accuracy: 0.9967 - val_loss: 0.0014 - val_accuracy: 0.9217\n",
            "Epoch 500/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.2839e-05 - accuracy: 0.9973 - val_loss: 0.0010 - val_accuracy: 0.9437\n",
            "Epoch 501/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.6748e-05 - accuracy: 0.9976 - val_loss: 9.8774e-04 - val_accuracy: 0.9443\n",
            "Epoch 502/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.6625e-05 - accuracy: 0.9975 - val_loss: 9.9468e-04 - val_accuracy: 0.9443\n",
            "Epoch 503/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.0712e-05 - accuracy: 0.9973 - val_loss: 0.0012 - val_accuracy: 0.9335\n",
            "Epoch 504/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.6126e-05 - accuracy: 0.9962 - val_loss: 0.0012 - val_accuracy: 0.9323\n",
            "Epoch 505/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.8346e-05 - accuracy: 0.9960 - val_loss: 0.0012 - val_accuracy: 0.9315\n",
            "Epoch 506/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.1210e-05 - accuracy: 0.9967 - val_loss: 0.0012 - val_accuracy: 0.9353\n",
            "Epoch 507/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.0902e-05 - accuracy: 0.9966 - val_loss: 9.5959e-04 - val_accuracy: 0.9453\n",
            "Epoch 508/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.8854e-05 - accuracy: 0.9974 - val_loss: 0.0014 - val_accuracy: 0.9228\n",
            "Epoch 509/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.8640e-05 - accuracy: 0.9968 - val_loss: 0.0011 - val_accuracy: 0.9415\n",
            "Epoch 510/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.1063e-05 - accuracy: 0.9972 - val_loss: 0.0011 - val_accuracy: 0.9378\n",
            "Epoch 511/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.3392e-05 - accuracy: 0.9971 - val_loss: 0.0011 - val_accuracy: 0.9385\n",
            "Epoch 512/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.2859e-05 - accuracy: 0.9973 - val_loss: 0.0011 - val_accuracy: 0.9407\n",
            "Epoch 513/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.7927e-05 - accuracy: 0.9975 - val_loss: 0.0010 - val_accuracy: 0.9440\n",
            "Epoch 514/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.7772e-05 - accuracy: 0.9975 - val_loss: 0.0010 - val_accuracy: 0.9443\n",
            "Epoch 515/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.7962e-05 - accuracy: 0.9975 - val_loss: 0.0010 - val_accuracy: 0.9438\n",
            "Epoch 516/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.9709e-05 - accuracy: 0.9974 - val_loss: 0.0010 - val_accuracy: 0.9417\n",
            "Epoch 517/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.4065e-05 - accuracy: 0.9972 - val_loss: 9.3048e-04 - val_accuracy: 0.9487\n",
            "Epoch 518/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.0708e-05 - accuracy: 0.9973 - val_loss: 0.0010 - val_accuracy: 0.9447\n",
            "Epoch 519/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.7886e-05 - accuracy: 0.9975 - val_loss: 0.0012 - val_accuracy: 0.9325\n",
            "Epoch 520/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 4.8926e-05 - accuracy: 0.9975 - val_loss: 0.0010 - val_accuracy: 0.9443\n",
            "Epoch 521/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.1264e-05 - accuracy: 0.9973 - val_loss: 0.0012 - val_accuracy: 0.9325\n",
            "Epoch 522/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.9177e-05 - accuracy: 0.9948 - val_loss: 0.0014 - val_accuracy: 0.9217\n",
            "Epoch 523/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.5170e-05 - accuracy: 0.9951 - val_loss: 0.0014 - val_accuracy: 0.9253\n",
            "Epoch 524/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.8683e-05 - accuracy: 0.9943 - val_loss: 0.0012 - val_accuracy: 0.9323\n",
            "Epoch 525/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.7480e-05 - accuracy: 0.9957 - val_loss: 0.0015 - val_accuracy: 0.9130\n",
            "Epoch 526/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.7318e-05 - accuracy: 0.9958 - val_loss: 0.0012 - val_accuracy: 0.9348\n",
            "Epoch 527/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.5101e-05 - accuracy: 0.9952 - val_loss: 0.0014 - val_accuracy: 0.9218\n",
            "Epoch 528/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.1218e-05 - accuracy: 0.9954 - val_loss: 0.0012 - val_accuracy: 0.9327\n",
            "Epoch 529/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.3121e-05 - accuracy: 0.9953 - val_loss: 0.0013 - val_accuracy: 0.9295\n",
            "Epoch 530/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.3271e-05 - accuracy: 0.9953 - val_loss: 0.0011 - val_accuracy: 0.9352\n",
            "Epoch 531/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.2154e-05 - accuracy: 0.9959 - val_loss: 0.0011 - val_accuracy: 0.9393\n",
            "Epoch 532/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.6924e-05 - accuracy: 0.9957 - val_loss: 0.0013 - val_accuracy: 0.9310\n",
            "Epoch 533/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.7909e-05 - accuracy: 0.9963 - val_loss: 0.0011 - val_accuracy: 0.9368\n",
            "Epoch 534/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.2800e-05 - accuracy: 0.9972 - val_loss: 0.0010 - val_accuracy: 0.9433\n",
            "Epoch 535/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.4645e-05 - accuracy: 0.9965 - val_loss: 0.0012 - val_accuracy: 0.9333\n",
            "Epoch 536/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.6936e-05 - accuracy: 0.9970 - val_loss: 9.8179e-04 - val_accuracy: 0.9457\n",
            "Epoch 537/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.5353e-05 - accuracy: 0.9977 - val_loss: 0.0010 - val_accuracy: 0.9428\n",
            "Epoch 538/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.6818e-05 - accuracy: 0.9975 - val_loss: 0.0011 - val_accuracy: 0.9405\n",
            "Epoch 539/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.5683e-05 - accuracy: 0.9976 - val_loss: 9.6265e-04 - val_accuracy: 0.9463\n",
            "Epoch 540/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.6474e-05 - accuracy: 0.9976 - val_loss: 8.7686e-04 - val_accuracy: 0.9523\n",
            "Epoch 541/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.3649e-05 - accuracy: 0.9977 - val_loss: 9.5896e-04 - val_accuracy: 0.9462\n",
            "Epoch 542/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.7533e-05 - accuracy: 0.9975 - val_loss: 9.4723e-04 - val_accuracy: 0.9470\n",
            "Epoch 543/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.5550e-05 - accuracy: 0.9976 - val_loss: 0.0010 - val_accuracy: 0.9433\n",
            "Epoch 544/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.3793e-05 - accuracy: 0.9978 - val_loss: 8.8064e-04 - val_accuracy: 0.9508\n",
            "Epoch 545/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.2257e-05 - accuracy: 0.9978 - val_loss: 8.8358e-04 - val_accuracy: 0.9495\n",
            "Epoch 546/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.2184e-05 - accuracy: 0.9979 - val_loss: 0.0010 - val_accuracy: 0.9433\n",
            "Epoch 547/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.5939e-05 - accuracy: 0.9976 - val_loss: 9.5982e-04 - val_accuracy: 0.9467\n",
            "Epoch 548/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.4702e-05 - accuracy: 0.9970 - val_loss: 9.9939e-04 - val_accuracy: 0.9447\n",
            "Epoch 549/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.3951e-05 - accuracy: 0.9965 - val_loss: 0.0015 - val_accuracy: 0.9145\n",
            "Epoch 550/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.3954e-05 - accuracy: 0.9964 - val_loss: 0.0010 - val_accuracy: 0.9427\n",
            "Epoch 551/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.7967e-05 - accuracy: 0.9964 - val_loss: 0.0011 - val_accuracy: 0.9408\n",
            "Epoch 552/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.6073e-05 - accuracy: 0.9968 - val_loss: 0.0012 - val_accuracy: 0.9353\n",
            "Epoch 553/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.3993e-05 - accuracy: 0.9971 - val_loss: 0.0010 - val_accuracy: 0.9420\n",
            "Epoch 554/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.2551e-05 - accuracy: 0.9960 - val_loss: 0.0010 - val_accuracy: 0.9425\n",
            "Epoch 555/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.6630e-05 - accuracy: 0.9969 - val_loss: 0.0012 - val_accuracy: 0.9360\n",
            "Epoch 556/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.0070e-05 - accuracy: 0.9967 - val_loss: 0.0013 - val_accuracy: 0.9277\n",
            "Epoch 557/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.9480e-05 - accuracy: 0.9967 - val_loss: 0.0012 - val_accuracy: 0.9330\n",
            "Epoch 558/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.7511e-05 - accuracy: 0.9975 - val_loss: 0.0011 - val_accuracy: 0.9407\n",
            "Epoch 559/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.5374e-05 - accuracy: 0.9970 - val_loss: 0.0010 - val_accuracy: 0.9433\n",
            "Epoch 560/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.5386e-05 - accuracy: 0.9971 - val_loss: 9.7450e-04 - val_accuracy: 0.9453\n",
            "Epoch 561/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.6091e-05 - accuracy: 0.9951 - val_loss: 0.0013 - val_accuracy: 0.9267\n",
            "Epoch 562/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.2821e-05 - accuracy: 0.9954 - val_loss: 0.0011 - val_accuracy: 0.9355\n",
            "Epoch 563/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.7226e-05 - accuracy: 0.9945 - val_loss: 0.0017 - val_accuracy: 0.9057\n",
            "Epoch 564/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.1865e-04 - accuracy: 0.9931 - val_loss: 0.0013 - val_accuracy: 0.9263\n",
            "Epoch 565/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.1244e-04 - accuracy: 0.9937 - val_loss: 0.0012 - val_accuracy: 0.9317\n",
            "Epoch 566/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.7959e-05 - accuracy: 0.9956 - val_loss: 0.0014 - val_accuracy: 0.9213\n",
            "Epoch 567/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.7625e-05 - accuracy: 0.9952 - val_loss: 0.0015 - val_accuracy: 0.9182\n",
            "Epoch 568/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.6702e-05 - accuracy: 0.9958 - val_loss: 0.0011 - val_accuracy: 0.9395\n",
            "Epoch 569/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.1484e-05 - accuracy: 0.9966 - val_loss: 9.6888e-04 - val_accuracy: 0.9467\n",
            "Epoch 570/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.0135e-05 - accuracy: 0.9974 - val_loss: 9.9785e-04 - val_accuracy: 0.9448\n",
            "Epoch 571/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.0483e-05 - accuracy: 0.9973 - val_loss: 9.7521e-04 - val_accuracy: 0.9465\n",
            "Epoch 572/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.0462e-05 - accuracy: 0.9973 - val_loss: 0.0010 - val_accuracy: 0.9413\n",
            "Epoch 573/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.7741e-05 - accuracy: 0.9962 - val_loss: 0.0012 - val_accuracy: 0.9358\n",
            "Epoch 574/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.0343e-05 - accuracy: 0.9968 - val_loss: 0.0011 - val_accuracy: 0.9377\n",
            "Epoch 575/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.9526e-05 - accuracy: 0.9960 - val_loss: 0.0011 - val_accuracy: 0.9395\n",
            "Epoch 576/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.9803e-05 - accuracy: 0.9974 - val_loss: 0.0011 - val_accuracy: 0.9398\n",
            "Epoch 577/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.9650e-05 - accuracy: 0.9973 - val_loss: 9.4523e-04 - val_accuracy: 0.9473\n",
            "Epoch 578/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.3249e-05 - accuracy: 0.9965 - val_loss: 0.0011 - val_accuracy: 0.9417\n",
            "Epoch 579/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.2651e-05 - accuracy: 0.9973 - val_loss: 0.0012 - val_accuracy: 0.9357\n",
            "Epoch 580/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.3963e-05 - accuracy: 0.9971 - val_loss: 0.0011 - val_accuracy: 0.9408\n",
            "Epoch 581/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.2842e-05 - accuracy: 0.9971 - val_loss: 9.9600e-04 - val_accuracy: 0.9453\n",
            "Epoch 582/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.0246e-05 - accuracy: 0.9973 - val_loss: 9.6788e-04 - val_accuracy: 0.9460\n",
            "Epoch 583/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.5384e-05 - accuracy: 0.9977 - val_loss: 9.0046e-04 - val_accuracy: 0.9502\n",
            "Epoch 584/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.7834e-05 - accuracy: 0.9975 - val_loss: 0.0010 - val_accuracy: 0.9417\n",
            "Epoch 585/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.2782e-05 - accuracy: 0.9960 - val_loss: 0.0013 - val_accuracy: 0.9247\n",
            "Epoch 586/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.0020e-05 - accuracy: 0.9962 - val_loss: 0.0012 - val_accuracy: 0.9322\n",
            "Epoch 587/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.9993e-05 - accuracy: 0.9962 - val_loss: 0.0011 - val_accuracy: 0.9383\n",
            "Epoch 588/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.9900e-05 - accuracy: 0.9967 - val_loss: 0.0011 - val_accuracy: 0.9373\n",
            "Epoch 589/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.2575e-05 - accuracy: 0.9965 - val_loss: 0.0011 - val_accuracy: 0.9402\n",
            "Epoch 590/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.0577e-05 - accuracy: 0.9974 - val_loss: 0.0011 - val_accuracy: 0.9385\n",
            "Epoch 591/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.2543e-05 - accuracy: 0.9973 - val_loss: 0.0010 - val_accuracy: 0.9413\n",
            "Epoch 592/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.6462e-05 - accuracy: 0.9970 - val_loss: 9.9391e-04 - val_accuracy: 0.9452\n",
            "Epoch 593/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.6602e-05 - accuracy: 0.9976 - val_loss: 9.7687e-04 - val_accuracy: 0.9453\n",
            "Epoch 594/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.5893e-05 - accuracy: 0.9976 - val_loss: 9.2593e-04 - val_accuracy: 0.9488\n",
            "Epoch 595/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.9720e-05 - accuracy: 0.9974 - val_loss: 0.0011 - val_accuracy: 0.9405\n",
            "Epoch 596/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.9569e-05 - accuracy: 0.9968 - val_loss: 0.0010 - val_accuracy: 0.9430\n",
            "Epoch 597/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.5012e-05 - accuracy: 0.9970 - val_loss: 0.0010 - val_accuracy: 0.9448\n",
            "Epoch 598/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.9904e-05 - accuracy: 0.9957 - val_loss: 0.0010 - val_accuracy: 0.9447\n",
            "Epoch 599/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.9327e-05 - accuracy: 0.9975 - val_loss: 9.3098e-04 - val_accuracy: 0.9483\n",
            "Epoch 600/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.1365e-05 - accuracy: 0.9972 - val_loss: 0.0010 - val_accuracy: 0.9435\n",
            "Epoch 601/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.2819e-05 - accuracy: 0.9972 - val_loss: 0.0011 - val_accuracy: 0.9418\n",
            "Epoch 602/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.0348e-05 - accuracy: 0.9974 - val_loss: 0.0011 - val_accuracy: 0.9420\n",
            "Epoch 603/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.9816e-05 - accuracy: 0.9974 - val_loss: 0.0011 - val_accuracy: 0.9382\n",
            "Epoch 604/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 6.1671e-05 - accuracy: 0.9966 - val_loss: 0.0012 - val_accuracy: 0.9350\n",
            "Epoch 605/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.3663e-05 - accuracy: 0.9966 - val_loss: 0.0010 - val_accuracy: 0.9427\n",
            "Epoch 606/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.4316e-05 - accuracy: 0.9958 - val_loss: 0.0011 - val_accuracy: 0.9387\n",
            "Epoch 607/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.8349e-05 - accuracy: 0.9950 - val_loss: 0.0011 - val_accuracy: 0.9373\n",
            "Epoch 608/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.3225e-05 - accuracy: 0.9949 - val_loss: 0.0011 - val_accuracy: 0.9403\n",
            "Epoch 609/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.0790e-05 - accuracy: 0.9961 - val_loss: 0.0011 - val_accuracy: 0.9415\n",
            "Epoch 610/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.0477e-05 - accuracy: 0.9967 - val_loss: 0.0011 - val_accuracy: 0.9397\n",
            "Epoch 611/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.1012e-05 - accuracy: 0.9967 - val_loss: 0.0014 - val_accuracy: 0.9253\n",
            "Epoch 612/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.5116e-05 - accuracy: 0.9964 - val_loss: 0.0013 - val_accuracy: 0.9297\n",
            "Epoch 613/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.0744e-05 - accuracy: 0.9974 - val_loss: 0.0010 - val_accuracy: 0.9438\n",
            "Epoch 614/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.9263e-05 - accuracy: 0.9974 - val_loss: 0.0011 - val_accuracy: 0.9390\n",
            "Epoch 615/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.0916e-05 - accuracy: 0.9973 - val_loss: 0.0010 - val_accuracy: 0.9412\n",
            "Epoch 616/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.9778e-05 - accuracy: 0.9973 - val_loss: 0.0010 - val_accuracy: 0.9430\n",
            "Epoch 617/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.4125e-05 - accuracy: 0.9970 - val_loss: 0.0012 - val_accuracy: 0.9317\n",
            "Epoch 618/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.4056e-05 - accuracy: 0.9955 - val_loss: 0.0012 - val_accuracy: 0.9347\n",
            "Epoch 619/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.3100e-05 - accuracy: 0.9965 - val_loss: 0.0011 - val_accuracy: 0.9382\n",
            "Epoch 620/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.5689e-05 - accuracy: 0.9958 - val_loss: 0.0011 - val_accuracy: 0.9380\n",
            "Epoch 621/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.6241e-05 - accuracy: 0.9962 - val_loss: 0.0011 - val_accuracy: 0.9372\n",
            "Epoch 622/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.4225e-05 - accuracy: 0.9971 - val_loss: 0.0011 - val_accuracy: 0.9368\n",
            "Epoch 623/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.9614e-05 - accuracy: 0.9962 - val_loss: 0.0010 - val_accuracy: 0.9423\n",
            "Epoch 624/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.2438e-05 - accuracy: 0.9972 - val_loss: 0.0011 - val_accuracy: 0.9392\n",
            "Epoch 625/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.9430e-05 - accuracy: 0.9968 - val_loss: 0.0010 - val_accuracy: 0.9437\n",
            "Epoch 626/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.2056e-05 - accuracy: 0.9967 - val_loss: 0.0010 - val_accuracy: 0.9423\n",
            "Epoch 627/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.3460e-05 - accuracy: 0.9972 - val_loss: 0.0011 - val_accuracy: 0.9418\n",
            "Epoch 628/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.1979e-05 - accuracy: 0.9967 - val_loss: 0.0012 - val_accuracy: 0.9348\n",
            "Epoch 629/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.7693e-05 - accuracy: 0.9963 - val_loss: 0.0012 - val_accuracy: 0.9333\n",
            "Epoch 630/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.3292e-05 - accuracy: 0.9965 - val_loss: 0.0012 - val_accuracy: 0.9323\n",
            "Epoch 631/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.1980e-05 - accuracy: 0.9972 - val_loss: 9.9793e-04 - val_accuracy: 0.9450\n",
            "Epoch 632/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.8636e-05 - accuracy: 0.9974 - val_loss: 9.8925e-04 - val_accuracy: 0.9448\n",
            "Epoch 633/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.6711e-05 - accuracy: 0.9969 - val_loss: 0.0010 - val_accuracy: 0.9432\n",
            "Epoch 634/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.0057e-05 - accuracy: 0.9966 - val_loss: 0.0011 - val_accuracy: 0.9398\n",
            "Epoch 635/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.1211e-05 - accuracy: 0.9972 - val_loss: 9.6078e-04 - val_accuracy: 0.9467\n",
            "Epoch 636/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.0912e-05 - accuracy: 0.9967 - val_loss: 0.0014 - val_accuracy: 0.9247\n",
            "Epoch 637/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.8757e-05 - accuracy: 0.9945 - val_loss: 0.0012 - val_accuracy: 0.9338\n",
            "Epoch 638/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.9884e-05 - accuracy: 0.9956 - val_loss: 0.0012 - val_accuracy: 0.9322\n",
            "Epoch 639/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.9981e-05 - accuracy: 0.9968 - val_loss: 0.0011 - val_accuracy: 0.9398\n",
            "Epoch 640/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.5372e-05 - accuracy: 0.9964 - val_loss: 0.0013 - val_accuracy: 0.9295\n",
            "Epoch 641/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.8211e-05 - accuracy: 0.9957 - val_loss: 0.0013 - val_accuracy: 0.9298\n",
            "Epoch 642/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.2710e-05 - accuracy: 0.9973 - val_loss: 9.2688e-04 - val_accuracy: 0.9485\n",
            "Epoch 643/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.2129e-05 - accuracy: 0.9972 - val_loss: 9.5943e-04 - val_accuracy: 0.9470\n",
            "Epoch 644/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.1273e-05 - accuracy: 0.9973 - val_loss: 0.0011 - val_accuracy: 0.9390\n",
            "Epoch 645/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.5243e-05 - accuracy: 0.9976 - val_loss: 0.0011 - val_accuracy: 0.9408\n",
            "Epoch 646/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.3746e-05 - accuracy: 0.9977 - val_loss: 8.9389e-04 - val_accuracy: 0.9505\n",
            "Epoch 647/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.6688e-05 - accuracy: 0.9975 - val_loss: 0.0012 - val_accuracy: 0.9345\n",
            "Epoch 648/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.0059e-05 - accuracy: 0.9971 - val_loss: 9.7025e-04 - val_accuracy: 0.9452\n",
            "Epoch 649/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.7714e-05 - accuracy: 0.9969 - val_loss: 9.7182e-04 - val_accuracy: 0.9462\n",
            "Epoch 650/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.4299e-05 - accuracy: 0.9977 - val_loss: 0.0011 - val_accuracy: 0.9405\n",
            "Epoch 651/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.4654e-05 - accuracy: 0.9977 - val_loss: 9.8600e-04 - val_accuracy: 0.9460\n",
            "Epoch 652/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.3932e-05 - accuracy: 0.9977 - val_loss: 0.0011 - val_accuracy: 0.9423\n",
            "Epoch 653/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.7587e-05 - accuracy: 0.9975 - val_loss: 9.2894e-04 - val_accuracy: 0.9487\n",
            "Epoch 654/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.1141e-05 - accuracy: 0.9979 - val_loss: 9.0579e-04 - val_accuracy: 0.9495\n",
            "Epoch 655/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.3563e-05 - accuracy: 0.9978 - val_loss: 9.6469e-04 - val_accuracy: 0.9468\n",
            "Epoch 656/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 3.9772e-05 - accuracy: 0.9980 - val_loss: 9.3842e-04 - val_accuracy: 0.9478\n",
            "Epoch 657/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.9406e-05 - accuracy: 0.9980 - val_loss: 9.6600e-04 - val_accuracy: 0.9470\n",
            "Epoch 658/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.9699e-05 - accuracy: 0.9980 - val_loss: 0.0010 - val_accuracy: 0.9450\n",
            "Epoch 659/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.9615e-05 - accuracy: 0.9980 - val_loss: 9.3685e-04 - val_accuracy: 0.9492\n",
            "Epoch 660/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.1535e-05 - accuracy: 0.9979 - val_loss: 9.4962e-04 - val_accuracy: 0.9477\n",
            "Epoch 661/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.3977e-05 - accuracy: 0.9977 - val_loss: 9.9401e-04 - val_accuracy: 0.9443\n",
            "Epoch 662/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.8108e-05 - accuracy: 0.9964 - val_loss: 0.0015 - val_accuracy: 0.9192\n",
            "Epoch 663/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.6142e-05 - accuracy: 0.9948 - val_loss: 0.0011 - val_accuracy: 0.9403\n",
            "Epoch 664/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.2269e-05 - accuracy: 0.9962 - val_loss: 0.0012 - val_accuracy: 0.9338\n",
            "Epoch 665/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.8011e-05 - accuracy: 0.9964 - val_loss: 9.9344e-04 - val_accuracy: 0.9450\n",
            "Epoch 666/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.6469e-05 - accuracy: 0.9963 - val_loss: 0.0013 - val_accuracy: 0.9263\n",
            "Epoch 667/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.2867e-05 - accuracy: 0.9958 - val_loss: 0.0012 - val_accuracy: 0.9347\n",
            "Epoch 668/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.9456e-05 - accuracy: 0.9962 - val_loss: 0.0011 - val_accuracy: 0.9390\n",
            "Epoch 669/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.2843e-05 - accuracy: 0.9971 - val_loss: 0.0011 - val_accuracy: 0.9370\n",
            "Epoch 670/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.4863e-05 - accuracy: 0.9977 - val_loss: 0.0011 - val_accuracy: 0.9372\n",
            "Epoch 671/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.4969e-05 - accuracy: 0.9971 - val_loss: 0.0013 - val_accuracy: 0.9308\n",
            "Epoch 672/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 7.8208e-05 - accuracy: 0.9958 - val_loss: 0.0010 - val_accuracy: 0.9455\n",
            "Epoch 673/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.0188e-05 - accuracy: 0.9962 - val_loss: 0.0013 - val_accuracy: 0.9262\n",
            "Epoch 674/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.0679e-05 - accuracy: 0.9961 - val_loss: 0.0012 - val_accuracy: 0.9335\n",
            "Epoch 675/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.0140e-04 - accuracy: 0.9942 - val_loss: 0.0014 - val_accuracy: 0.9212\n",
            "Epoch 676/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.0289e-04 - accuracy: 0.9942 - val_loss: 0.0011 - val_accuracy: 0.9413\n",
            "Epoch 677/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 7.2143e-05 - accuracy: 0.9960 - val_loss: 0.0011 - val_accuracy: 0.9413\n",
            "Epoch 678/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.8176e-05 - accuracy: 0.9963 - val_loss: 0.0011 - val_accuracy: 0.9375\n",
            "Epoch 679/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.3120e-05 - accuracy: 0.9971 - val_loss: 9.8461e-04 - val_accuracy: 0.9455\n",
            "Epoch 680/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.0957e-05 - accuracy: 0.9972 - val_loss: 0.0010 - val_accuracy: 0.9430\n",
            "Epoch 681/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.9686e-05 - accuracy: 0.9974 - val_loss: 9.9116e-04 - val_accuracy: 0.9460\n",
            "Epoch 682/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.2580e-05 - accuracy: 0.9973 - val_loss: 0.0011 - val_accuracy: 0.9383\n",
            "Epoch 683/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.1261e-05 - accuracy: 0.9973 - val_loss: 0.0010 - val_accuracy: 0.9433\n",
            "Epoch 684/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.9915e-05 - accuracy: 0.9974 - val_loss: 0.0010 - val_accuracy: 0.9447\n",
            "Epoch 685/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.8725e-05 - accuracy: 0.9973 - val_loss: 0.0011 - val_accuracy: 0.9373\n",
            "Epoch 686/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.2772e-05 - accuracy: 0.9972 - val_loss: 9.7800e-04 - val_accuracy: 0.9458\n",
            "Epoch 687/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.0360e-05 - accuracy: 0.9980 - val_loss: 9.2098e-04 - val_accuracy: 0.9488\n",
            "Epoch 688/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.0805e-05 - accuracy: 0.9979 - val_loss: 9.8645e-04 - val_accuracy: 0.9463\n",
            "Epoch 689/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.2326e-05 - accuracy: 0.9977 - val_loss: 8.8741e-04 - val_accuracy: 0.9513\n",
            "Epoch 690/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.5501e-05 - accuracy: 0.9976 - val_loss: 8.8724e-04 - val_accuracy: 0.9512\n",
            "Epoch 691/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.3457e-05 - accuracy: 0.9978 - val_loss: 9.1775e-04 - val_accuracy: 0.9492\n",
            "Epoch 692/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.2935e-05 - accuracy: 0.9978 - val_loss: 8.3229e-04 - val_accuracy: 0.9545\n",
            "Epoch 693/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.2277e-05 - accuracy: 0.9978 - val_loss: 9.1057e-04 - val_accuracy: 0.9500\n",
            "Epoch 694/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.3050e-05 - accuracy: 0.9978 - val_loss: 9.5167e-04 - val_accuracy: 0.9477\n",
            "Epoch 695/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.1182e-05 - accuracy: 0.9979 - val_loss: 9.6500e-04 - val_accuracy: 0.9468\n",
            "Epoch 696/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.0628e-05 - accuracy: 0.9980 - val_loss: 8.8505e-04 - val_accuracy: 0.9510\n",
            "Epoch 697/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.0396e-05 - accuracy: 0.9980 - val_loss: 8.3402e-04 - val_accuracy: 0.9537\n",
            "Epoch 698/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.9480e-05 - accuracy: 0.9980 - val_loss: 8.3937e-04 - val_accuracy: 0.9525\n",
            "Epoch 699/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.1155e-05 - accuracy: 0.9979 - val_loss: 9.0755e-04 - val_accuracy: 0.9495\n",
            "Epoch 700/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.6433e-05 - accuracy: 0.9975 - val_loss: 8.5921e-04 - val_accuracy: 0.9522\n",
            "Epoch 701/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.4625e-05 - accuracy: 0.9977 - val_loss: 0.0010 - val_accuracy: 0.9455\n",
            "Epoch 702/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.3686e-05 - accuracy: 0.9977 - val_loss: 9.1775e-04 - val_accuracy: 0.9497\n",
            "Epoch 703/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.3488e-05 - accuracy: 0.9972 - val_loss: 0.0011 - val_accuracy: 0.9403\n",
            "Epoch 704/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.3471e-05 - accuracy: 0.9961 - val_loss: 0.0012 - val_accuracy: 0.9333\n",
            "Epoch 705/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.2550e-05 - accuracy: 0.9967 - val_loss: 0.0012 - val_accuracy: 0.9335\n",
            "Epoch 706/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.8090e-05 - accuracy: 0.9967 - val_loss: 0.0011 - val_accuracy: 0.9383\n",
            "Epoch 707/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.3383e-05 - accuracy: 0.9953 - val_loss: 0.0013 - val_accuracy: 0.9307\n",
            "Epoch 708/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.2187e-05 - accuracy: 0.9954 - val_loss: 0.0013 - val_accuracy: 0.9253\n",
            "Epoch 709/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 8.5390e-05 - accuracy: 0.9953 - val_loss: 0.0012 - val_accuracy: 0.9328\n",
            "Epoch 710/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.7722e-05 - accuracy: 0.9968 - val_loss: 0.0011 - val_accuracy: 0.9370\n",
            "Epoch 711/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.6998e-05 - accuracy: 0.9976 - val_loss: 0.0011 - val_accuracy: 0.9390\n",
            "Epoch 712/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.5240e-05 - accuracy: 0.9970 - val_loss: 0.0011 - val_accuracy: 0.9383\n",
            "Epoch 713/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.1556e-05 - accuracy: 0.9973 - val_loss: 0.0013 - val_accuracy: 0.9295\n",
            "Epoch 714/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.6869e-05 - accuracy: 0.9970 - val_loss: 0.0010 - val_accuracy: 0.9453\n",
            "Epoch 715/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.7179e-05 - accuracy: 0.9969 - val_loss: 9.6748e-04 - val_accuracy: 0.9478\n",
            "Epoch 716/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.4593e-05 - accuracy: 0.9971 - val_loss: 0.0011 - val_accuracy: 0.9387\n",
            "Epoch 717/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.3189e-05 - accuracy: 0.9972 - val_loss: 9.1388e-04 - val_accuracy: 0.9503\n",
            "Epoch 718/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.8094e-05 - accuracy: 0.9974 - val_loss: 0.0010 - val_accuracy: 0.9435\n",
            "Epoch 719/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 6.2228e-05 - accuracy: 0.9966 - val_loss: 0.0011 - val_accuracy: 0.9408\n",
            "Epoch 720/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.8060e-05 - accuracy: 0.9975 - val_loss: 9.7128e-04 - val_accuracy: 0.9465\n",
            "Epoch 721/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.3394e-05 - accuracy: 0.9977 - val_loss: 9.3782e-04 - val_accuracy: 0.9487\n",
            "Epoch 722/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.3089e-05 - accuracy: 0.9978 - val_loss: 9.6698e-04 - val_accuracy: 0.9467\n",
            "Epoch 723/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.3044e-05 - accuracy: 0.9977 - val_loss: 0.0011 - val_accuracy: 0.9417\n",
            "Epoch 724/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 7.8474e-05 - accuracy: 0.9955 - val_loss: 0.0011 - val_accuracy: 0.9368\n",
            "Epoch 725/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.0068e-05 - accuracy: 0.9967 - val_loss: 0.0013 - val_accuracy: 0.9315\n",
            "Epoch 726/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.5272e-05 - accuracy: 0.9958 - val_loss: 0.0012 - val_accuracy: 0.9365\n",
            "Epoch 727/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.3287e-05 - accuracy: 0.9960 - val_loss: 0.0013 - val_accuracy: 0.9292\n",
            "Epoch 728/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 7.0171e-05 - accuracy: 0.9960 - val_loss: 0.0011 - val_accuracy: 0.9385\n",
            "Epoch 729/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.1812e-05 - accuracy: 0.9966 - val_loss: 0.0011 - val_accuracy: 0.9420\n",
            "Epoch 730/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.7390e-05 - accuracy: 0.9957 - val_loss: 0.0012 - val_accuracy: 0.9363\n",
            "Epoch 731/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.5950e-05 - accuracy: 0.9965 - val_loss: 0.0011 - val_accuracy: 0.9418\n",
            "Epoch 732/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.5554e-05 - accuracy: 0.9970 - val_loss: 0.0011 - val_accuracy: 0.9397\n",
            "Epoch 733/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.8373e-05 - accuracy: 0.9970 - val_loss: 0.0011 - val_accuracy: 0.9395\n",
            "Epoch 734/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 6.5315e-05 - accuracy: 0.9965 - val_loss: 9.0883e-04 - val_accuracy: 0.9503\n",
            "Epoch 735/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.5287e-05 - accuracy: 0.9971 - val_loss: 0.0010 - val_accuracy: 0.9430\n",
            "Epoch 736/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.5598e-05 - accuracy: 0.9959 - val_loss: 0.0012 - val_accuracy: 0.9340\n",
            "Epoch 737/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.5559e-05 - accuracy: 0.9971 - val_loss: 9.3415e-04 - val_accuracy: 0.9492\n",
            "Epoch 738/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.9832e-05 - accuracy: 0.9973 - val_loss: 0.0010 - val_accuracy: 0.9427\n",
            "Epoch 739/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.1606e-05 - accuracy: 0.9972 - val_loss: 9.2003e-04 - val_accuracy: 0.9498\n",
            "Epoch 740/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.9814e-05 - accuracy: 0.9973 - val_loss: 0.0012 - val_accuracy: 0.9340\n",
            "Epoch 741/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.6545e-05 - accuracy: 0.9970 - val_loss: 9.4281e-04 - val_accuracy: 0.9478\n",
            "Epoch 742/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.8387e-05 - accuracy: 0.9973 - val_loss: 9.2006e-04 - val_accuracy: 0.9497\n",
            "Epoch 743/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.8519e-05 - accuracy: 0.9974 - val_loss: 9.4186e-04 - val_accuracy: 0.9480\n",
            "Epoch 744/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.4549e-05 - accuracy: 0.9977 - val_loss: 0.0010 - val_accuracy: 0.9450\n",
            "Epoch 745/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.4274e-05 - accuracy: 0.9977 - val_loss: 8.4707e-04 - val_accuracy: 0.9517\n",
            "Epoch 746/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.4818e-05 - accuracy: 0.9976 - val_loss: 0.0010 - val_accuracy: 0.9425\n",
            "Epoch 747/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.2958e-05 - accuracy: 0.9978 - val_loss: 0.0010 - val_accuracy: 0.9430\n",
            "Epoch 748/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.5102e-05 - accuracy: 0.9977 - val_loss: 0.0011 - val_accuracy: 0.9380\n",
            "Epoch 749/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.5708e-05 - accuracy: 0.9970 - val_loss: 9.4179e-04 - val_accuracy: 0.9478\n",
            "Epoch 750/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.5191e-05 - accuracy: 0.9970 - val_loss: 0.0011 - val_accuracy: 0.9403\n",
            "Epoch 751/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 6.0836e-05 - accuracy: 0.9967 - val_loss: 0.0010 - val_accuracy: 0.9443\n",
            "Epoch 752/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 8.9464e-05 - accuracy: 0.9952 - val_loss: 0.0011 - val_accuracy: 0.9377\n",
            "Epoch 753/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.0607e-05 - accuracy: 0.9968 - val_loss: 0.0012 - val_accuracy: 0.9360\n",
            "Epoch 754/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.4896e-05 - accuracy: 0.9970 - val_loss: 0.0012 - val_accuracy: 0.9338\n",
            "Epoch 755/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 9.0680e-05 - accuracy: 0.9950 - val_loss: 0.0014 - val_accuracy: 0.9237\n",
            "Epoch 756/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.7790e-05 - accuracy: 0.9964 - val_loss: 0.0014 - val_accuracy: 0.9212\n",
            "Epoch 757/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.0246e-04 - accuracy: 0.9942 - val_loss: 0.0012 - val_accuracy: 0.9365\n",
            "Epoch 758/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.5044e-05 - accuracy: 0.9965 - val_loss: 0.0010 - val_accuracy: 0.9423\n",
            "Epoch 759/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 6.1251e-05 - accuracy: 0.9967 - val_loss: 0.0012 - val_accuracy: 0.9342\n",
            "Epoch 760/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.3970e-05 - accuracy: 0.9966 - val_loss: 0.0012 - val_accuracy: 0.9370\n",
            "Epoch 761/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.6604e-05 - accuracy: 0.9970 - val_loss: 0.0011 - val_accuracy: 0.9397\n",
            "Epoch 762/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.6377e-05 - accuracy: 0.9975 - val_loss: 9.7380e-04 - val_accuracy: 0.9458\n",
            "Epoch 763/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.6990e-05 - accuracy: 0.9975 - val_loss: 0.0011 - val_accuracy: 0.9417\n",
            "Epoch 764/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.5554e-05 - accuracy: 0.9977 - val_loss: 9.8763e-04 - val_accuracy: 0.9458\n",
            "Epoch 765/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.2933e-05 - accuracy: 0.9978 - val_loss: 8.9810e-04 - val_accuracy: 0.9497\n",
            "Epoch 766/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.6258e-05 - accuracy: 0.9976 - val_loss: 0.0010 - val_accuracy: 0.9427\n",
            "Epoch 767/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.1481e-05 - accuracy: 0.9966 - val_loss: 0.0011 - val_accuracy: 0.9375\n",
            "Epoch 768/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.5804e-05 - accuracy: 0.9971 - val_loss: 0.0010 - val_accuracy: 0.9457\n",
            "Epoch 769/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 6.3595e-05 - accuracy: 0.9965 - val_loss: 0.0013 - val_accuracy: 0.9302\n",
            "Epoch 770/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.4190e-05 - accuracy: 0.9971 - val_loss: 0.0011 - val_accuracy: 0.9407\n",
            "Epoch 771/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.4887e-05 - accuracy: 0.9977 - val_loss: 0.0011 - val_accuracy: 0.9407\n",
            "Epoch 772/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.9643e-05 - accuracy: 0.9973 - val_loss: 9.7865e-04 - val_accuracy: 0.9460\n",
            "Epoch 773/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 6.6591e-05 - accuracy: 0.9964 - val_loss: 0.0011 - val_accuracy: 0.9375\n",
            "Epoch 774/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.8913e-05 - accuracy: 0.9974 - val_loss: 0.0010 - val_accuracy: 0.9430\n",
            "Epoch 775/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 7.3916e-05 - accuracy: 0.9960 - val_loss: 0.0013 - val_accuracy: 0.9292\n",
            "Epoch 776/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 6.9554e-05 - accuracy: 0.9961 - val_loss: 0.0012 - val_accuracy: 0.9350\n",
            "Epoch 777/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.5963e-05 - accuracy: 0.9970 - val_loss: 9.5951e-04 - val_accuracy: 0.9467\n",
            "Epoch 778/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.4888e-05 - accuracy: 0.9977 - val_loss: 9.3081e-04 - val_accuracy: 0.9490\n",
            "Epoch 779/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.8699e-05 - accuracy: 0.9974 - val_loss: 9.5171e-04 - val_accuracy: 0.9483\n",
            "Epoch 780/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.6675e-05 - accuracy: 0.9975 - val_loss: 0.0011 - val_accuracy: 0.9390\n",
            "Epoch 781/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 6.0755e-05 - accuracy: 0.9968 - val_loss: 9.1967e-04 - val_accuracy: 0.9498\n",
            "Epoch 782/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.3212e-05 - accuracy: 0.9971 - val_loss: 0.0012 - val_accuracy: 0.9358\n",
            "Epoch 783/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.3668e-05 - accuracy: 0.9972 - val_loss: 0.0011 - val_accuracy: 0.9372\n",
            "Epoch 784/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.0372e-05 - accuracy: 0.9973 - val_loss: 0.0014 - val_accuracy: 0.9215\n",
            "Epoch 785/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.7151e-05 - accuracy: 0.9969 - val_loss: 9.7768e-04 - val_accuracy: 0.9472\n",
            "Epoch 786/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 6.9330e-05 - accuracy: 0.9962 - val_loss: 9.4774e-04 - val_accuracy: 0.9472\n",
            "Epoch 787/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.8787e-05 - accuracy: 0.9973 - val_loss: 9.1333e-04 - val_accuracy: 0.9493\n",
            "Epoch 788/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.0005e-05 - accuracy: 0.9974 - val_loss: 9.7331e-04 - val_accuracy: 0.9457\n",
            "Epoch 789/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.5010e-05 - accuracy: 0.9976 - val_loss: 9.0879e-04 - val_accuracy: 0.9508\n",
            "Epoch 790/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.2624e-05 - accuracy: 0.9977 - val_loss: 0.0010 - val_accuracy: 0.9430\n",
            "Epoch 791/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.5433e-05 - accuracy: 0.9970 - val_loss: 0.0011 - val_accuracy: 0.9400\n",
            "Epoch 792/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 6.2392e-05 - accuracy: 0.9967 - val_loss: 9.8150e-04 - val_accuracy: 0.9462\n",
            "Epoch 793/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.6794e-05 - accuracy: 0.9970 - val_loss: 9.5975e-04 - val_accuracy: 0.9467\n",
            "Epoch 794/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.5417e-05 - accuracy: 0.9977 - val_loss: 8.8354e-04 - val_accuracy: 0.9513\n",
            "Epoch 795/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.2886e-05 - accuracy: 0.9978 - val_loss: 9.8757e-04 - val_accuracy: 0.9455\n",
            "Epoch 796/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.8616e-05 - accuracy: 0.9974 - val_loss: 9.4167e-04 - val_accuracy: 0.9478\n",
            "Epoch 797/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.7781e-05 - accuracy: 0.9975 - val_loss: 0.0011 - val_accuracy: 0.9378\n",
            "Epoch 798/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.9749e-05 - accuracy: 0.9968 - val_loss: 0.0011 - val_accuracy: 0.9400\n",
            "Epoch 799/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.8434e-05 - accuracy: 0.9962 - val_loss: 0.0012 - val_accuracy: 0.9323\n",
            "Epoch 800/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.1208e-05 - accuracy: 0.9961 - val_loss: 0.0011 - val_accuracy: 0.9420\n",
            "Epoch 801/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.4251e-05 - accuracy: 0.9971 - val_loss: 0.0012 - val_accuracy: 0.9345\n",
            "Epoch 802/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.3207e-05 - accuracy: 0.9971 - val_loss: 0.0012 - val_accuracy: 0.9325\n",
            "Epoch 803/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.8319e-05 - accuracy: 0.9969 - val_loss: 9.5635e-04 - val_accuracy: 0.9478\n",
            "Epoch 804/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.7939e-05 - accuracy: 0.9974 - val_loss: 9.5161e-04 - val_accuracy: 0.9478\n",
            "Epoch 805/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.8636e-05 - accuracy: 0.9974 - val_loss: 9.3523e-04 - val_accuracy: 0.9493\n",
            "Epoch 806/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.8651e-05 - accuracy: 0.9975 - val_loss: 8.2805e-04 - val_accuracy: 0.9548\n",
            "Epoch 807/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.2071e-05 - accuracy: 0.9978 - val_loss: 9.1707e-04 - val_accuracy: 0.9502\n",
            "Epoch 808/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.9237e-05 - accuracy: 0.9974 - val_loss: 9.2787e-04 - val_accuracy: 0.9485\n",
            "Epoch 809/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.3823e-05 - accuracy: 0.9977 - val_loss: 9.6197e-04 - val_accuracy: 0.9470\n",
            "Epoch 810/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.4401e-05 - accuracy: 0.9977 - val_loss: 8.6751e-04 - val_accuracy: 0.9520\n",
            "Epoch 811/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.4221e-05 - accuracy: 0.9977 - val_loss: 8.3895e-04 - val_accuracy: 0.9540\n",
            "Epoch 812/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 3.9948e-05 - accuracy: 0.9980 - val_loss: 8.3057e-04 - val_accuracy: 0.9535\n",
            "Epoch 813/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 3.8686e-05 - accuracy: 0.9980 - val_loss: 7.5917e-04 - val_accuracy: 0.9585\n",
            "Epoch 814/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 3.9046e-05 - accuracy: 0.9980 - val_loss: 9.1509e-04 - val_accuracy: 0.9498\n",
            "Epoch 815/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 3.9188e-05 - accuracy: 0.9980 - val_loss: 8.2309e-04 - val_accuracy: 0.9542\n",
            "Epoch 816/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.4989e-05 - accuracy: 0.9976 - val_loss: 8.7403e-04 - val_accuracy: 0.9515\n",
            "Epoch 817/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.8752e-05 - accuracy: 0.9975 - val_loss: 8.9972e-04 - val_accuracy: 0.9513\n",
            "Epoch 818/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.6186e-05 - accuracy: 0.9976 - val_loss: 0.0011 - val_accuracy: 0.9383\n",
            "Epoch 819/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.4397e-05 - accuracy: 0.9977 - val_loss: 9.7962e-04 - val_accuracy: 0.9458\n",
            "Epoch 820/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.2371e-05 - accuracy: 0.9979 - val_loss: 9.8388e-04 - val_accuracy: 0.9458\n",
            "Epoch 821/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.9830e-05 - accuracy: 0.9980 - val_loss: 8.7434e-04 - val_accuracy: 0.9523\n",
            "Epoch 822/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.2263e-05 - accuracy: 0.9977 - val_loss: 9.1336e-04 - val_accuracy: 0.9512\n",
            "Epoch 823/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 6.6186e-05 - accuracy: 0.9964 - val_loss: 0.0011 - val_accuracy: 0.9407\n",
            "Epoch 824/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 9.9777e-05 - accuracy: 0.9946 - val_loss: 0.0013 - val_accuracy: 0.9303\n",
            "Epoch 825/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 1.0991e-04 - accuracy: 0.9939 - val_loss: 0.0013 - val_accuracy: 0.9292\n",
            "Epoch 826/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 6.8840e-05 - accuracy: 0.9962 - val_loss: 0.0011 - val_accuracy: 0.9413\n",
            "Epoch 827/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 7.1306e-05 - accuracy: 0.9962 - val_loss: 0.0010 - val_accuracy: 0.9435\n",
            "Epoch 828/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.8352e-05 - accuracy: 0.9975 - val_loss: 0.0010 - val_accuracy: 0.9450\n",
            "Epoch 829/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.0373e-05 - accuracy: 0.9973 - val_loss: 0.0010 - val_accuracy: 0.9438\n",
            "Epoch 830/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.3360e-05 - accuracy: 0.9977 - val_loss: 0.0011 - val_accuracy: 0.9407\n",
            "Epoch 831/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.1484e-05 - accuracy: 0.9973 - val_loss: 8.6773e-04 - val_accuracy: 0.9527\n",
            "Epoch 832/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.8694e-05 - accuracy: 0.9974 - val_loss: 9.1620e-04 - val_accuracy: 0.9497\n",
            "Epoch 833/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.8177e-05 - accuracy: 0.9973 - val_loss: 0.0010 - val_accuracy: 0.9443\n",
            "Epoch 834/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 6.0197e-05 - accuracy: 0.9969 - val_loss: 9.8167e-04 - val_accuracy: 0.9468\n",
            "Epoch 835/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.1826e-05 - accuracy: 0.9978 - val_loss: 9.6303e-04 - val_accuracy: 0.9470\n",
            "Epoch 836/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.1863e-05 - accuracy: 0.9978 - val_loss: 9.9386e-04 - val_accuracy: 0.9457\n",
            "Epoch 837/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.9393e-05 - accuracy: 0.9969 - val_loss: 0.0010 - val_accuracy: 0.9425\n",
            "Epoch 838/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.9759e-05 - accuracy: 0.9973 - val_loss: 9.7183e-04 - val_accuracy: 0.9460\n",
            "Epoch 839/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.7187e-05 - accuracy: 0.9975 - val_loss: 9.1164e-04 - val_accuracy: 0.9500\n",
            "Epoch 840/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.6946e-05 - accuracy: 0.9975 - val_loss: 8.7859e-04 - val_accuracy: 0.9522\n",
            "Epoch 841/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.8529e-05 - accuracy: 0.9974 - val_loss: 9.4086e-04 - val_accuracy: 0.9470\n",
            "Epoch 842/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.4415e-05 - accuracy: 0.9977 - val_loss: 8.7362e-04 - val_accuracy: 0.9525\n",
            "Epoch 843/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.0012e-05 - accuracy: 0.9980 - val_loss: 9.4230e-04 - val_accuracy: 0.9497\n",
            "Epoch 844/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.6135e-05 - accuracy: 0.9975 - val_loss: 9.2255e-04 - val_accuracy: 0.9490\n",
            "Epoch 845/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.1993e-05 - accuracy: 0.9971 - val_loss: 0.0011 - val_accuracy: 0.9415\n",
            "Epoch 846/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.1686e-05 - accuracy: 0.9967 - val_loss: 0.0010 - val_accuracy: 0.9445\n",
            "Epoch 847/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.3517e-05 - accuracy: 0.9972 - val_loss: 0.0011 - val_accuracy: 0.9387\n",
            "Epoch 848/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.4653e-05 - accuracy: 0.9976 - val_loss: 0.0011 - val_accuracy: 0.9378\n",
            "Epoch 849/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.6916e-05 - accuracy: 0.9970 - val_loss: 0.0011 - val_accuracy: 0.9385\n",
            "Epoch 850/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 1.0506e-04 - accuracy: 0.9942 - val_loss: 0.0011 - val_accuracy: 0.9417\n",
            "Epoch 851/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 7.6657e-05 - accuracy: 0.9959 - val_loss: 0.0013 - val_accuracy: 0.9307\n",
            "Epoch 852/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 6.7484e-05 - accuracy: 0.9964 - val_loss: 0.0010 - val_accuracy: 0.9440\n",
            "Epoch 853/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.6985e-05 - accuracy: 0.9968 - val_loss: 0.0010 - val_accuracy: 0.9430\n",
            "Epoch 854/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.0255e-05 - accuracy: 0.9974 - val_loss: 9.6459e-04 - val_accuracy: 0.9478\n",
            "Epoch 855/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.3052e-05 - accuracy: 0.9978 - val_loss: 8.3207e-04 - val_accuracy: 0.9545\n",
            "Epoch 856/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.1649e-05 - accuracy: 0.9979 - val_loss: 8.2996e-04 - val_accuracy: 0.9553\n",
            "Epoch 857/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.9938e-05 - accuracy: 0.9973 - val_loss: 0.0012 - val_accuracy: 0.9367\n",
            "Epoch 858/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.5573e-05 - accuracy: 0.9970 - val_loss: 9.9340e-04 - val_accuracy: 0.9450\n",
            "Epoch 859/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 7.1625e-05 - accuracy: 0.9962 - val_loss: 0.0012 - val_accuracy: 0.9365\n",
            "Epoch 860/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 6.7964e-05 - accuracy: 0.9963 - val_loss: 0.0011 - val_accuracy: 0.9368\n",
            "Epoch 861/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.4836e-05 - accuracy: 0.9971 - val_loss: 0.0010 - val_accuracy: 0.9437\n",
            "Epoch 862/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.5156e-05 - accuracy: 0.9976 - val_loss: 8.8903e-04 - val_accuracy: 0.9510\n",
            "Epoch 863/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.6761e-05 - accuracy: 0.9975 - val_loss: 9.5539e-04 - val_accuracy: 0.9472\n",
            "Epoch 864/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.1865e-05 - accuracy: 0.9978 - val_loss: 9.6016e-04 - val_accuracy: 0.9482\n",
            "Epoch 865/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.1968e-05 - accuracy: 0.9972 - val_loss: 0.0010 - val_accuracy: 0.9417\n",
            "Epoch 866/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.5621e-05 - accuracy: 0.9976 - val_loss: 9.8557e-04 - val_accuracy: 0.9457\n",
            "Epoch 867/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.6941e-05 - accuracy: 0.9975 - val_loss: 0.0012 - val_accuracy: 0.9347\n",
            "Epoch 868/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.2069e-05 - accuracy: 0.9974 - val_loss: 9.4368e-04 - val_accuracy: 0.9493\n",
            "Epoch 869/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.7035e-05 - accuracy: 0.9975 - val_loss: 0.0011 - val_accuracy: 0.9420\n",
            "Epoch 870/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.9020e-05 - accuracy: 0.9974 - val_loss: 0.0011 - val_accuracy: 0.9378\n",
            "Epoch 871/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.2058e-05 - accuracy: 0.9973 - val_loss: 9.5898e-04 - val_accuracy: 0.9473\n",
            "Epoch 872/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.5862e-05 - accuracy: 0.9970 - val_loss: 0.0013 - val_accuracy: 0.9288\n",
            "Epoch 873/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 6.1216e-05 - accuracy: 0.9967 - val_loss: 0.0010 - val_accuracy: 0.9455\n",
            "Epoch 874/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.7993e-05 - accuracy: 0.9975 - val_loss: 0.0011 - val_accuracy: 0.9393\n",
            "Epoch 875/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.5430e-05 - accuracy: 0.9965 - val_loss: 0.0013 - val_accuracy: 0.9267\n",
            "Epoch 876/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 6.1762e-05 - accuracy: 0.9967 - val_loss: 9.6197e-04 - val_accuracy: 0.9462\n",
            "Epoch 877/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.0919e-05 - accuracy: 0.9979 - val_loss: 9.4800e-04 - val_accuracy: 0.9473\n",
            "Epoch 878/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.2135e-05 - accuracy: 0.9978 - val_loss: 9.1227e-04 - val_accuracy: 0.9495\n",
            "Epoch 879/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 3.9424e-05 - accuracy: 0.9980 - val_loss: 9.1252e-04 - val_accuracy: 0.9495\n",
            "Epoch 880/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.1859e-05 - accuracy: 0.9978 - val_loss: 0.0010 - val_accuracy: 0.9410\n",
            "Epoch 881/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.7898e-05 - accuracy: 0.9975 - val_loss: 0.0010 - val_accuracy: 0.9438\n",
            "Epoch 882/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.5704e-05 - accuracy: 0.9977 - val_loss: 9.4205e-04 - val_accuracy: 0.9485\n",
            "Epoch 883/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.0346e-05 - accuracy: 0.9979 - val_loss: 9.0219e-04 - val_accuracy: 0.9503\n",
            "Epoch 884/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.4096e-05 - accuracy: 0.9977 - val_loss: 9.2383e-04 - val_accuracy: 0.9510\n",
            "Epoch 885/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.6041e-05 - accuracy: 0.9976 - val_loss: 9.6302e-04 - val_accuracy: 0.9473\n",
            "Epoch 886/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.5145e-05 - accuracy: 0.9971 - val_loss: 0.0011 - val_accuracy: 0.9383\n",
            "Epoch 887/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 6.0943e-05 - accuracy: 0.9967 - val_loss: 0.0012 - val_accuracy: 0.9367\n",
            "Epoch 888/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 7.1440e-05 - accuracy: 0.9961 - val_loss: 0.0013 - val_accuracy: 0.9300\n",
            "Epoch 889/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 6.5648e-05 - accuracy: 0.9965 - val_loss: 0.0011 - val_accuracy: 0.9403\n",
            "Epoch 890/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.1324e-05 - accuracy: 0.9973 - val_loss: 0.0012 - val_accuracy: 0.9360\n",
            "Epoch 891/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.8293e-05 - accuracy: 0.9975 - val_loss: 9.1863e-04 - val_accuracy: 0.9498\n",
            "Epoch 892/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.3175e-05 - accuracy: 0.9971 - val_loss: 0.0012 - val_accuracy: 0.9325\n",
            "Epoch 893/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.1636e-05 - accuracy: 0.9967 - val_loss: 0.0011 - val_accuracy: 0.9388\n",
            "Epoch 894/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.5016e-05 - accuracy: 0.9977 - val_loss: 9.2592e-04 - val_accuracy: 0.9493\n",
            "Epoch 895/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.5747e-05 - accuracy: 0.9976 - val_loss: 0.0013 - val_accuracy: 0.9285\n",
            "Epoch 896/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.9502e-05 - accuracy: 0.9950 - val_loss: 0.0011 - val_accuracy: 0.9375\n",
            "Epoch 897/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 1.3523e-04 - accuracy: 0.9926 - val_loss: 0.0012 - val_accuracy: 0.9357\n",
            "Epoch 898/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 8.0587e-05 - accuracy: 0.9956 - val_loss: 0.0012 - val_accuracy: 0.9332\n",
            "Epoch 899/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 8.1233e-05 - accuracy: 0.9955 - val_loss: 9.0528e-04 - val_accuracy: 0.9490\n",
            "Epoch 900/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 6.5477e-05 - accuracy: 0.9964 - val_loss: 0.0012 - val_accuracy: 0.9362\n",
            "Epoch 901/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 6.8234e-05 - accuracy: 0.9963 - val_loss: 0.0010 - val_accuracy: 0.9462\n",
            "Epoch 902/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.5950e-05 - accuracy: 0.9970 - val_loss: 9.5861e-04 - val_accuracy: 0.9485\n",
            "Epoch 903/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.3750e-05 - accuracy: 0.9977 - val_loss: 0.0010 - val_accuracy: 0.9435\n",
            "Epoch 904/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.3291e-05 - accuracy: 0.9977 - val_loss: 9.5858e-04 - val_accuracy: 0.9488\n",
            "Epoch 905/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.0388e-05 - accuracy: 0.9980 - val_loss: 9.7243e-04 - val_accuracy: 0.9470\n",
            "Epoch 906/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.5973e-05 - accuracy: 0.9975 - val_loss: 0.0011 - val_accuracy: 0.9377\n",
            "Epoch 907/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.6376e-05 - accuracy: 0.9970 - val_loss: 0.0011 - val_accuracy: 0.9410\n",
            "Epoch 908/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.7172e-05 - accuracy: 0.9975 - val_loss: 9.9552e-04 - val_accuracy: 0.9463\n",
            "Epoch 909/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.9911e-05 - accuracy: 0.9974 - val_loss: 9.5442e-04 - val_accuracy: 0.9465\n",
            "Epoch 910/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.8196e-05 - accuracy: 0.9974 - val_loss: 9.6607e-04 - val_accuracy: 0.9477\n",
            "Epoch 911/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.2208e-05 - accuracy: 0.9978 - val_loss: 9.5206e-04 - val_accuracy: 0.9472\n",
            "Epoch 912/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.8163e-05 - accuracy: 0.9969 - val_loss: 0.0011 - val_accuracy: 0.9423\n",
            "Epoch 913/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 5.2301e-05 - accuracy: 0.9972 - val_loss: 0.0011 - val_accuracy: 0.9385\n",
            "Epoch 914/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.6281e-05 - accuracy: 0.9970 - val_loss: 9.9736e-04 - val_accuracy: 0.9457\n",
            "Epoch 915/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.4426e-05 - accuracy: 0.9965 - val_loss: 0.0010 - val_accuracy: 0.9428\n",
            "Epoch 916/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.2409e-05 - accuracy: 0.9973 - val_loss: 0.0010 - val_accuracy: 0.9438\n",
            "Epoch 917/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.2271e-05 - accuracy: 0.9973 - val_loss: 9.0502e-04 - val_accuracy: 0.9500\n",
            "Epoch 918/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.2799e-05 - accuracy: 0.9977 - val_loss: 9.7486e-04 - val_accuracy: 0.9467\n",
            "Epoch 919/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.3411e-05 - accuracy: 0.9971 - val_loss: 0.0010 - val_accuracy: 0.9430\n",
            "Epoch 920/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.0804e-05 - accuracy: 0.9979 - val_loss: 9.0959e-04 - val_accuracy: 0.9515\n",
            "Epoch 921/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.0579e-05 - accuracy: 0.9979 - val_loss: 9.4265e-04 - val_accuracy: 0.9495\n",
            "Epoch 922/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 3.7894e-05 - accuracy: 0.9981 - val_loss: 9.2417e-04 - val_accuracy: 0.9493\n",
            "Epoch 923/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 3.6212e-05 - accuracy: 0.9982 - val_loss: 9.0698e-04 - val_accuracy: 0.9503\n",
            "Epoch 924/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 3.6253e-05 - accuracy: 0.9982 - val_loss: 9.3649e-04 - val_accuracy: 0.9493\n",
            "Epoch 925/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 3.6441e-05 - accuracy: 0.9982 - val_loss: 0.0010 - val_accuracy: 0.9438\n",
            "Epoch 926/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 3.9160e-05 - accuracy: 0.9980 - val_loss: 8.8750e-04 - val_accuracy: 0.9515\n",
            "Epoch 927/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.9652e-05 - accuracy: 0.9979 - val_loss: 0.0010 - val_accuracy: 0.9445\n",
            "Epoch 928/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.0453e-05 - accuracy: 0.9979 - val_loss: 9.8799e-04 - val_accuracy: 0.9468\n",
            "Epoch 929/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.1806e-05 - accuracy: 0.9973 - val_loss: 0.0011 - val_accuracy: 0.9413\n",
            "Epoch 930/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.8168e-05 - accuracy: 0.9976 - val_loss: 8.4204e-04 - val_accuracy: 0.9540\n",
            "Epoch 931/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 3.8787e-05 - accuracy: 0.9980 - val_loss: 8.5780e-04 - val_accuracy: 0.9533\n",
            "Epoch 932/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 3.7684e-05 - accuracy: 0.9981 - val_loss: 8.5755e-04 - val_accuracy: 0.9522\n",
            "Epoch 933/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 3.6599e-05 - accuracy: 0.9981 - val_loss: 9.0800e-04 - val_accuracy: 0.9503\n",
            "Epoch 934/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.5013e-05 - accuracy: 0.9977 - val_loss: 9.5502e-04 - val_accuracy: 0.9473\n",
            "Epoch 935/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 3.8342e-05 - accuracy: 0.9980 - val_loss: 9.2618e-04 - val_accuracy: 0.9495\n",
            "Epoch 936/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.6188e-05 - accuracy: 0.9975 - val_loss: 8.6475e-04 - val_accuracy: 0.9515\n",
            "Epoch 937/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 6.0325e-05 - accuracy: 0.9967 - val_loss: 0.0012 - val_accuracy: 0.9380\n",
            "Epoch 938/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 6.8165e-05 - accuracy: 0.9963 - val_loss: 0.0013 - val_accuracy: 0.9318\n",
            "Epoch 939/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 6.7767e-05 - accuracy: 0.9962 - val_loss: 0.0010 - val_accuracy: 0.9432\n",
            "Epoch 940/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.5106e-05 - accuracy: 0.9970 - val_loss: 0.0012 - val_accuracy: 0.9363\n",
            "Epoch 941/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 7.6083e-05 - accuracy: 0.9957 - val_loss: 0.0010 - val_accuracy: 0.9435\n",
            "Epoch 942/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 8.2676e-05 - accuracy: 0.9956 - val_loss: 0.0011 - val_accuracy: 0.9377\n",
            "Epoch 943/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 6.9013e-05 - accuracy: 0.9962 - val_loss: 0.0011 - val_accuracy: 0.9405\n",
            "Epoch 944/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.8642e-05 - accuracy: 0.9968 - val_loss: 9.5860e-04 - val_accuracy: 0.9480\n",
            "Epoch 945/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 6.7366e-05 - accuracy: 0.9964 - val_loss: 0.0011 - val_accuracy: 0.9395\n",
            "Epoch 946/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.3770e-05 - accuracy: 0.9972 - val_loss: 0.0011 - val_accuracy: 0.9423\n",
            "Epoch 947/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.5895e-05 - accuracy: 0.9976 - val_loss: 9.8844e-04 - val_accuracy: 0.9462\n",
            "Epoch 948/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 6.2375e-05 - accuracy: 0.9967 - val_loss: 9.3854e-04 - val_accuracy: 0.9493\n",
            "Epoch 949/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.2144e-05 - accuracy: 0.9978 - val_loss: 0.0010 - val_accuracy: 0.9442\n",
            "Epoch 950/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.3427e-05 - accuracy: 0.9977 - val_loss: 8.6589e-04 - val_accuracy: 0.9525\n",
            "Epoch 951/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.1531e-05 - accuracy: 0.9979 - val_loss: 0.0010 - val_accuracy: 0.9457\n",
            "Epoch 952/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.0423e-05 - accuracy: 0.9979 - val_loss: 8.4937e-04 - val_accuracy: 0.9535\n",
            "Epoch 953/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 3.7038e-05 - accuracy: 0.9981 - val_loss: 8.8561e-04 - val_accuracy: 0.9528\n",
            "Epoch 954/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 3.9340e-05 - accuracy: 0.9980 - val_loss: 8.5819e-04 - val_accuracy: 0.9518\n",
            "Epoch 955/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 3.8706e-05 - accuracy: 0.9980 - val_loss: 9.2262e-04 - val_accuracy: 0.9488\n",
            "Epoch 956/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 3.6381e-05 - accuracy: 0.9982 - val_loss: 9.1570e-04 - val_accuracy: 0.9493\n",
            "Epoch 957/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 3.6848e-05 - accuracy: 0.9981 - val_loss: 9.3574e-04 - val_accuracy: 0.9493\n",
            "Epoch 958/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 3.9445e-05 - accuracy: 0.9980 - val_loss: 9.5334e-04 - val_accuracy: 0.9488\n",
            "Epoch 959/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.2769e-05 - accuracy: 0.9977 - val_loss: 9.2262e-04 - val_accuracy: 0.9490\n",
            "Epoch 960/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.0765e-05 - accuracy: 0.9979 - val_loss: 0.0011 - val_accuracy: 0.9380\n",
            "Epoch 961/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 1.0384e-04 - accuracy: 0.9943 - val_loss: 0.0012 - val_accuracy: 0.9342\n",
            "Epoch 962/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 8.0642e-05 - accuracy: 0.9957 - val_loss: 0.0012 - val_accuracy: 0.9323\n",
            "Epoch 963/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 7.0841e-05 - accuracy: 0.9963 - val_loss: 0.0012 - val_accuracy: 0.9380\n",
            "Epoch 964/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.2464e-05 - accuracy: 0.9978 - val_loss: 0.0010 - val_accuracy: 0.9455\n",
            "Epoch 965/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.3940e-05 - accuracy: 0.9977 - val_loss: 0.0010 - val_accuracy: 0.9453\n",
            "Epoch 966/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.5460e-05 - accuracy: 0.9971 - val_loss: 0.0010 - val_accuracy: 0.9447\n",
            "Epoch 967/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.3242e-05 - accuracy: 0.9966 - val_loss: 9.7767e-04 - val_accuracy: 0.9467\n",
            "Epoch 968/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.6647e-05 - accuracy: 0.9975 - val_loss: 0.0011 - val_accuracy: 0.9412\n",
            "Epoch 969/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.2756e-05 - accuracy: 0.9977 - val_loss: 0.0010 - val_accuracy: 0.9443\n",
            "Epoch 970/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.3916e-05 - accuracy: 0.9977 - val_loss: 9.1271e-04 - val_accuracy: 0.9502\n",
            "Epoch 971/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.6540e-05 - accuracy: 0.9975 - val_loss: 9.9451e-04 - val_accuracy: 0.9458\n",
            "Epoch 972/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.7271e-05 - accuracy: 0.9975 - val_loss: 8.8109e-04 - val_accuracy: 0.9530\n",
            "Epoch 973/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 3.8810e-05 - accuracy: 0.9980 - val_loss: 8.2463e-04 - val_accuracy: 0.9543\n",
            "Epoch 974/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 3.8100e-05 - accuracy: 0.9980 - val_loss: 8.9490e-04 - val_accuracy: 0.9508\n",
            "Epoch 975/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 3.8340e-05 - accuracy: 0.9980 - val_loss: 9.5481e-04 - val_accuracy: 0.9482\n",
            "Epoch 976/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.4466e-05 - accuracy: 0.9976 - val_loss: 0.0010 - val_accuracy: 0.9445\n",
            "Epoch 977/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.9783e-05 - accuracy: 0.9974 - val_loss: 9.2156e-04 - val_accuracy: 0.9502\n",
            "Epoch 978/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.5450e-05 - accuracy: 0.9971 - val_loss: 9.6557e-04 - val_accuracy: 0.9475\n",
            "Epoch 979/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.3129e-05 - accuracy: 0.9977 - val_loss: 0.0010 - val_accuracy: 0.9453\n",
            "Epoch 980/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.4860e-05 - accuracy: 0.9977 - val_loss: 8.9842e-04 - val_accuracy: 0.9505\n",
            "Epoch 981/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.3890e-05 - accuracy: 0.9977 - val_loss: 9.7635e-04 - val_accuracy: 0.9472\n",
            "Epoch 982/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 6.4062e-05 - accuracy: 0.9965 - val_loss: 0.0013 - val_accuracy: 0.9272\n",
            "Epoch 983/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 6.9655e-05 - accuracy: 0.9962 - val_loss: 0.0010 - val_accuracy: 0.9435\n",
            "Epoch 984/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 7.1552e-05 - accuracy: 0.9960 - val_loss: 0.0012 - val_accuracy: 0.9357\n",
            "Epoch 985/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 7.8210e-05 - accuracy: 0.9956 - val_loss: 0.0011 - val_accuracy: 0.9383\n",
            "Epoch 986/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.3876e-05 - accuracy: 0.9977 - val_loss: 0.0011 - val_accuracy: 0.9428\n",
            "Epoch 987/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.9319e-05 - accuracy: 0.9973 - val_loss: 9.4259e-04 - val_accuracy: 0.9485\n",
            "Epoch 988/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.9653e-05 - accuracy: 0.9980 - val_loss: 0.0011 - val_accuracy: 0.9425\n",
            "Epoch 989/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.5070e-05 - accuracy: 0.9977 - val_loss: 9.4364e-04 - val_accuracy: 0.9487\n",
            "Epoch 990/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.9383e-05 - accuracy: 0.9973 - val_loss: 9.1829e-04 - val_accuracy: 0.9503\n",
            "Epoch 991/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.2108e-05 - accuracy: 0.9971 - val_loss: 9.5645e-04 - val_accuracy: 0.9477\n",
            "Epoch 992/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.0859e-05 - accuracy: 0.9974 - val_loss: 0.0011 - val_accuracy: 0.9405\n",
            "Epoch 993/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.6724e-05 - accuracy: 0.9975 - val_loss: 0.0010 - val_accuracy: 0.9452\n",
            "Epoch 994/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.2501e-05 - accuracy: 0.9978 - val_loss: 9.9506e-04 - val_accuracy: 0.9452\n",
            "Epoch 995/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.4546e-05 - accuracy: 0.9977 - val_loss: 9.6412e-04 - val_accuracy: 0.9477\n",
            "Epoch 996/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.3995e-05 - accuracy: 0.9977 - val_loss: 9.1801e-04 - val_accuracy: 0.9495\n",
            "Epoch 997/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.1158e-05 - accuracy: 0.9979 - val_loss: 9.4431e-04 - val_accuracy: 0.9487\n",
            "Epoch 998/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.7962e-05 - accuracy: 0.9975 - val_loss: 0.0010 - val_accuracy: 0.9443\n",
            "Epoch 999/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 6.7224e-05 - accuracy: 0.9963 - val_loss: 0.0012 - val_accuracy: 0.9342\n",
            "Epoch 1000/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.8537e-05 - accuracy: 0.9964 - val_loss: 0.0014 - val_accuracy: 0.9258\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49a41793-25a2-4cef-e40f-ed052ac4dc8a",
        "id": "BidnGe07u9a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "188/188 [==============================] - 2s 9ms/step - loss: 0.0014 - accuracy: 0.9223\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.0014241042081266642, 0.9223333597183228]"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('highpass_1s.h5')"
      ],
      "metadata": {
        "id": "wQoXmZ0EFg_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 그래프 분석"
      ],
      "metadata": {
        "id": "m79hkJfuu9a9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history=history.history\n",
        "epochs = range(1, len(history['loss']) + 1)\n",
        "acc = history['accuracy']\n",
        "loss = history['loss']\n",
        "val_acc = history['val_accuracy']\n",
        "val_loss = history['val_loss']\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.plot(epochs, acc, label='accuracy')\n",
        "plt.plot(epochs, val_acc, label='val_acc')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.plot(epochs, loss, label='loss', color='g')\n",
        "plt.plot(epochs, val_loss, label='val_loss', color='r')\n",
        "plt.legend()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 701
        },
        "outputId": "0db4cc8b-fc09-4807-ad1c-74720c0ffb54",
        "id": "ztdlucKyu9a-"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f10d6c9f5b0>"
            ]
          },
          "metadata": {},
          "execution_count": 83
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmEAAAFNCAYAAABIc7ibAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdd5xcZdXA8d+Zvr1kN9lsekJCEkIPhCIdpIMiKiBYQFCKr4jia0HFXl9UFHsXBREUERGkg/TQA6Gk92Sz2TK7O32e948zk5ntk7CT2YXz/Xzms3Pv3LnzzJ3Zueee59zninMOY4wxxhiza3lK3QBjjDHGmLciC8KMMcYYY0rAgjBjjDHGmBKwIMwYY4wxpgQsCDPGGGOMKQELwowxxhhjSsCCMGPeRETk3yLygZFetpREZJWIHFuE9T4gIh/O3H+fiPynkGV34nWmikiXiHh3tq3GmDcnC8KMKbHMDjp7S4tIJG/6fTuyLufcic6534/0sqORiHxGRB4aYH6DiMRFZEGh63LO/ck59/YRalevoNE5t8Y5V+mcS43E+gd4PRGRFSLycjHWb4wpHgvCjCmxzA660jlXCawBTs2b96fsciLiK10rR6XrgUNEZEaf+WcBLzrnlpSgTaVwODAemCkiB+zKF7bvpDFvjAVhxoxSInKkiKwTkf8VkU3Ab0WkTkRuF5EWEWnL3J+c95z8LrYPish/ReR7mWVXisiJO7nsDBF5SETCInKPiFwnItcP0u5C2vhVEXkks77/iEhD3uPnichqEWkVkc8Ptn2cc+uA+4Dz+jz0fuAPw7WjT5s/KCL/zZs+TkReEZEOEfkxIHmPzRKR+zLt2yoifxKR2sxjfwSmAv/MZDI/LSLTRcRlAxYRaRaR20Rkm4gsE5EL89Z9tYjcJCJ/yGybl0Rk4WDbIOMDwD+AOzL389/XHiJyd+a1NovI5zLzvSLyORFZnnmdp0VkSt+2Zpbt+z15RES+LyKtwNVDbY/Mc6aIyN8yn0OriPxYRAKZNu2Zt9x4EekRkcZh3q8xbxoWhBkzujUB9cA04CL0f/a3mempQAT48RDPXwS8CjQA3wF+LSKyE8v+GXgSGAdcTf/AJ18hbTwH+BCawQkAnwIQkfnATzPrb8683oCBU8bv89siIrsD+2Tau6PbKruOBuBvwFXotlgOHJq/CPDNTPvmAVPQbYJz7jx6ZzO/M8BL3Aisyzz/TOAbInJ03uOnZZapBW4bqs0iUp5Zx58yt7NEJJB5rAq4B7gz81q7AfdmnnoFcDZwElANnA/0DLlhchYBK4AJwNeH2h6idXC3A6uB6cAk4EbnXDzzHs/NW+/ZwL3OuZYC22HM2Oecs5vd7DZKbsAq4NjM/SOBOBAaYvl9gLa86QeAD2fufxBYlvdYOeCAph1ZFg1gkkB53uPXA9cX+J4GauNVedOXAHdm7n8R3UlnH6vIbINjB1l3OdAJHJKZ/jrwj53cVv/N3H8/8HjecoIGTR8eZL3vAJ4d6DPMTE/PbEsfGqCkgKq8x78J/C5z/2rgnrzH5gORIbbtuUBLZt0hoAN4Z+axs/Pb1ed5rwKnDzB/e1uH2E5rhvm8t28P4OBs+wZYbhEasEpmejHwnlL+/9nNbrv6ZpkwY0a3FudcNDshIuUi8vNMd10n8BBQK4Ofebcpe8c5l810VO7gss3Atrx5AGsHa3CBbdyUd78nr03N+et2znUDrYO9VqZNfwXen8navQ/4ww60YyB92+Dyp0VkgojcKCLrM+u9Hs2YFSK7LcN581ajGaKsvtsmJIPXXn0AuMk5l8x8T24h1yU5Bc3iDWSox4bT67MfZntMAVY755J9V+KcewJ9f0eKyFw0U3fbTrbJmDHJgjBjRjfXZ/qTwO7AIudcNVqUDXk1S0WwEajPdH1lTRli+TfSxo3568685rhhnvN74D3AcUAV8M832I6+bRB6v99voJ/Lnpn1nttnnX0/s3wb0G1ZlTdvKrB+mDb1k6lvOxo4V0Q2idYNngmclOlSXQvMHOTpa4FZA8zvzvzN/6yb+izT9/0NtT3WAlOHCCJ/n1n+PODm/AMOY94KLAgzZmypQmub2kWkHvhSsV/QObca7Sq6OlNQfTBwapHaeDNwioi8LVPb9BWG/516GGgHfkGu3uiNtONfwB4ickYmePgfegciVUAX0CEik4Ar+zx/M4MEP865tcCjwDdFJCQiewEXoNmjHXUe8BoaaO6Tuc1Bu07PRmuxJorI5SISFJEqEVmUee6vgK+KyGxRe4nIOKf1WOvRwM4rIuczcLCWb6jt8SQa1H5LRCoy7zm/vu564J1oIPaHndgGxoxpFoQZM7b8ACgDtgKPo0XXu8L70PqeVuBrwF+A2CDL7nQbnXMvAZeihfUbgTY0qBjqOQ7dgU+j9458p9rhnNsKvBv4Fvp+ZwOP5C3yZWA/tP7qX2gRf75vAleJSLuIfGqAlzgbrb3aAPwd+JJz7p5C2tbHB4CfOOc25d+AnwEfyHR5HocGzJuA14GjMs+9BrgJ+A9aU/drdFsBXIgGUq3AHmjQOJRBt4fTsdFORbsa16Cf5XvzHl8LPINm0h7e8U1gzNiWLYg0xpiCichfgFecc0XPxJk3NxH5DbDBOXdVqdtizK5mQZgxZliig4BuA1YCbwduBQ52zj1b0oaZMU1EpgPPAfs651aWtjXG7HrWHWmMKUQTOlRBF3AtcLEFYOaNEJGvAkuA71oAZt6qLBNmjDHGGFMClgkzxhhjjCkBC8KMMcYYY0pgsAH0Rq2GhgY3ffr0UjfDGGOMMWZYTz/99Fbn3IAXph9zQdj06dNZvHhxqZthjDHGGDMsEVk92GPWHWmMMcYYUwIWhBljjDHGlIAFYcYYY4wxJWBBmDHGGGNMCVgQZowxxhhTAhaEGWOMMcaUQNGCMBH5jYhsEZElgzwuInKtiCwTkRdEZL9itcUYY4wxZrQpZibsd8AJQzx+IjA7c7sI+GkR22KMMcYYM6oULQhzzj0EbBtikdOBPzj1OFArIhOL1R5jjDHGmNGklCPmTwLW5k2vy8zbWJrmmF3FOYeIlLoZJFJpOiMJeuIpgn4P46tCwz4nnXZ0RBK09cSZ0VAx5PtIpx0OaO2K0RlNMKW+nKDP22uZVNqxrq2H8oCPhspAv/W1dccJ+b0s29JFIp1man05DZXB7Y9HEymSaUdlUP+V48k0GzsiTKgOkXYOv9eDV4StXTECPg+15YEB29oZTRDyeQn4PEQTKVa1drM1HKcs4GVmQwV1FQGccwDb2+ico6UrRmtXnK5YEo9AQ2WQaeMqcM7RHU+RSjlWtXaze1MVaeco83v7vUfnHOvaImzujDKuMsjabT0EfB4OnF5PTyKFV4S0c3TFkkyoDtERSQDg8wgVQR+ReIrueJKaMj/LW7qoLw8wvjq0ffu09cSpKfOTynx2zTVlbA5HaagM4vd6SKcd69sjbOyI0lQdYkp9GZs7Y0QSKRKpNJG4bmOfR/B5hdryAJNqy/ptw0g8RTyZpq0nzqbOKJFM2xOpNCG/l/kTq6kt99PWk6A84KUnnmJrV4zm2rLtn1/W5s4o8WSaiTUhlrV0URn0EUumSaUdAuw2vhLnoD2SoCUco70nTm15gPFVQda3R7SdZQEmVAcREVJpRyKVZuXWbsr8XhqrglQEfXTHkkQSKZzT7+KE6iBrt0UI+vX4fHxVEOegK55kQ3uEzkgSn1cI+jxs6YyRdo600+0sArVlAQ6aWY/Pq8/f0hmltTvO3Kaq7Z97W3ecbT1xYok0aedo7Y5v/0zzeQRqyvwEvB72n1aHRwSPR1jX1kM4mqQ84MUjQnc8SSyRZmtXjHkTq2nO+2zauuN0x5NUl/nZ1hUnmtTPZHNnjEQqjc8r7Da+kvFVISKJ1PbPIZL5bLaEY4SjCWY2VFIe9LJqazcTa8uoLw/QEo7R1hNnYk2Irpi+RkckwebOKBUBH3XlAZa3dFFb7mduUzXd8STbuuNE4inC0eT2/zWX+R9wAA4cjrryAPtMqSXtYOnGTjqjCapDfv3NiiYZXxWkK5bEOfB6hCl1ZWzoiDK3qQq/18MrmzrZ0B5lwaRqGiuDvLC+g3gyjd8r+DwevB6hriJAMpUm4PPw/Np2vB4PPq+QSjmm1JczbVw5nZEEW8KxzLaHuU3V27fPc2vbAagM+kim9f8ku21Wb+umI5IgHE1m/sf186gu0+27tSuG3+thcl058yZW4RXB6xGSaceKlm62dcfpiMRJpByptD7Z5xXGVQTZf1odT69uo7k2xLbuOOFoEo8I23ri1JcHqAr52NYdZ1ZjJVPHlfPyhk42tEeoKfdTHvCypTOGxyNUBn3sOamGgK+0pfFj4rJFInIR2mXJ1KlTS9waM5zXN4eJJdP8/KEVdEQSdMeSzJ9YTXWZjxfXd/LM6jZO3nMi+0+r47ePriKVTnP47EYqgj4Om93AurYI9yzdzIb2CNPHVeD3epg1voJt3QkeeHUL7T0Jasr8TKkvZ11bD7MnVNFYGWRTZ4Q9mmt4bXOYR5e30lAZpK07zmn7NHP6Ps3ct3QLIb+XppoQGzsi/OjeZYRjye3tntFQwQHT69h/Wh1bu+K0hGM8vboNhwYzQZ+H59d2EEmkANh7Si0nLWjiiZXbWLOth0g8RSSR4oDpdaxu7WFLOEZVyMfq1h4AmmtCHD6nkefWtrO+PUI4miTo8xBLpgGoDvkI+r3sPbkGjwgvbehkfXuk17YVgY8cPoutXTFeWNfOqtYeKoM+bvrIQdy7dAu/fHgFW7vi25f3e/WHzTkNWoI+D3Oaqgj6PHRGkmTjoZc2dOLz6A9htj35ptaXk3aOyXVlfO6keVx920s8t7adzO9jL6fv08zyli6WrO/s91htuZ+aMj8TqkNMH1fOhvYoz69t7/U5DGXauHLWbOvZ/qPeUBno9X6zJtaE6IwkiGYCl3wegbSD6ePK+eAh0/nlwyt7bedsgDSU/afVIUBF0EfI72FDe5TlLV3DPi/g8xDvs329HuGEPZrYc3INi1e1saKlixVbu4dcT1XQR08i1e+99TWhOsj8idU8sqyVeKr369aV+wlHkyTz1uH3ColU73V6PTLs6/TVUBkg4PWwqTNK2ul3e7fxlSRSjiUbOrZ/foUK+jyIwILmGhavbhty2bpyDbhTaT0QKIaBttNQAl5Pv+0/nNrM+8gGMoUq83u3/0YBhPweookde23Qzz3tXK/PakZDBQKs2dbT63vzRgV9Hhqrgqxriwy/cIH0oEAPegbj8wifPWkeF7xtxoi97o4St6P/DTuycpHpwO3OuQUDPPZz4AHn3A2Z6VeBI51zQ2bCFi5c6OzakTuvM5rA7/Hw6uawHk3UhJg2roJJdWXUlPm3L7dsS5hn17Tz6qZw5ojJkUyn8XqEcw6cyvjqEOFoIpN5ibKpI0pdhZ9P3vQ8qzJBB8DMxgrqygO8vjlMZzRJU3WISXVlPJ35Ia0t9xP0edjcGevVzok1IZpqQrSEY2xoj2zf2c9srGD2+EoiiTTr23qoDPlZvqWLrrydeG25n0N3a6Azc3T98OtbB9wW+0+r47S9mynze3l9S5jVrT08+FrL9iCkzO9lfnM1NWV+4sk0W8JRDpheT2NVkO5YknuWbmFlZmdZHfKxcHo9ZQEvL67rwOcRtoRjTK4r4/R9JlFT5ufae19nW0+cg2aOoyOSYMn6Ds45cCoLJlXTE0+xbEsX3bEkjy5vZUs4xviqIOe/bQY98RS7T6iiLODhj4+t5v5XWwANIvadWsffn12//T3tM6WWM/efTHtPHJ/Xw7buOH6vUB7wsaUzyn9e3szGjihzJlTSVFOG3yOEo0n2nVqLzyts6tDAcf9pdYyvCtIdT/L82g7+vWQjPfFUrx/J+ROreffCydSVBxhXGcA5uOHJNfx7ySZmNFRw5v6TiSXTNFWH2NQRweMRVrf2cPfLm7d/XjVlfk7ZayLzm6tpriljSzjKxJoylm7s5Ff/XclZB0zhwddaGF8VYo/mal5c38HuTVU4pz/cG9ojTKoroyLgY8XWLhZMqqG9J8HTq9t4Zk0bh89uZH5zNbGEHvFXBL08sWIbk+vK+PeSTazZ1kNzTYhLjtqNqfXlvLCunY0dUabWl1MZ8lFT5ifk82qGIO1IpBx/e2YdD7++lfqKADVlfpLpNPUVAWY0VDCxpmz7dzzk95J2joDXQziaZOnGTlq6YoT8XiLx5PZs1L9f3MSza9rojqcI+DwcOmsch+7WQCrt2NgRZXJdGWnnGF8VwucVVrZ0s64tQkNVgIbKII1VQWrLAmztivHQay0snF5PTZmfDe0RHlvRyovrOzhm7njqKwLMnlBJKg33v7KFWDLN+OogE6pCVIV8OGBTR4QZDZXEkylWbu3mb8+s5+S9JpJIOd42exzjKoIk02m2dSeY0VCBzyN4RAj6PTgHz69r59p7X2e/qXWIwLT6csqDPp5f284Dr7YQ9Hv4wMHTmdlYQdDnxSMQ9HuZVBsCemdIU2lHe0+cbd1xnli5jQ3tEda2RThgeh17TqrZnkHyiBDweagO+bn+8dWICBOqg7RHEsydUEVjVZDlLV1MHVexPUCbUB3anol6fXMX9yzdjN/r4b5XthD0ebjo8Jmada4KEvJ5eWlDB8m0Y/q4Cp5f186mjiizJ1SyW2MlmzujVIZ8bOqIEfR5mDexmpYuzU7ObapmW3eMZ9a0E/J52L2pmpDfQ3nARyKVpiyg2wAEEd0CIsJjy1v54b2v0VxbxvmHzmBGQwXhaJKumGZ8e+JJ6isCCEI0kWL1th6SqTTX3P0aJyxo4qjdxzOlXn9nN3ZEWTitnrpyP4m0I5nSA5NsBjKZSrNwej0VAR/xVBqPwNq2CC9v6KQi4GXW+Eq2dsW44cm1LN3YyazGCo6dN4GDZo4j6PfQEo5RHfITTaRojyTYGo4xo1G/G5PryvF7c5mmzZ1RQn4v9RV+ook0a7b18MK6Dpa3dOERmNVYyYTqEJVBH7tnDha9HkFE6I4luezPz1AW8HH2gVPweoTqkJ+JNSFSaUdteYC2njhbu2KMrwrxyLKtvL4lTF15gFP2aiaaTBGJp5hQHQI0+/7COv3fOGS3hgH3ESNFRJ52zi0c8LESBmEnA5cBJwGLgGudcwcOt04Lwgb39Oo2Vrd2s+/UOibVltEZTWzvuoomUlx+43Pc9fKmAY9CA14Pfq/wuZPn0RKO8cN7X++3XDZrMtRXJuD1cMZ+k7hp8Vq+cMp8PnRo7gijtStGVchPwOfZHpQtmFRN0OdldWs3a7dF+NmDyzlu/gTOO2gaHk+u2+uZNe2knWP/qXXb52clU2nimR+WJes72XdqLSF/rttv8aptbAnH2L2pitauOD3xJHXlAfZort7ebZLV0ZMgHEswriJIWaB312FfzjlWbO2mvjxAXcXA3Xz5UmlHPKk/vKm0Y2tXLPOD0N+qrd1Uhny9uh6zr7lmW8/2TI6IcO6vnuC/y7by6RN255IjdxuyDem0ozOaGLRbcjjLtoT52zPrOXHBRBZMqh6wa3F9e4SJNWV4+3xOWYlUGo8IXdEk1WW+Qbt0i91tvbkzym3PbeCEBU1MqS/foeeOdNvae+I8u6adRTPrKQ+Mng6KkXyfPXHtNsr/3xyNkql0v9+FUmjviVMR9PUKYoYTT6aL1r3W0ZPgvlc3c+pezSXbPum06/f7PxaUJAgTkRuAI4EGYDPwJcAP4Jz7meh/9o/RMyh7gA8554aNriwI02zW7c9vxOcROqMJVrV28/DrW7d3e+XzeoSJNSE2d0ZJpBxn7DuJuooAe0+pZdGMeta19bCuLcKfHl/Dk6ty51Gcsd8kLjxsJrPHV5JMZ2qLPMKyLWGOveYhAN6xTzO3PreBY+aO57R9mtncGeXYeROY2VhJJJ4aNogxI6MlW7fSWFnqphhjjOmjZJmwYngrBWHptOPZtW08v7aDrV0x/vHcBhZMquaZNe20hHPdd5VBH/MnVrNgUg2n7D2Rl9Z38LMHV9BUE6Ii6OOJFa3Mm1jNxUfO4vg9mgZ8rXA0wX2vbGFNaw/NtWWcsd+kQY+A127rIZpIMXtCFd2xJBXB0XPkbowxxowmFoSNEUs3dvKdO19h5dZuUs6RSDo2dUYB7Qos93spC3iZUl/OlcfvzsSaMiqCXhoqgv1StPndCJ3RBJUB35hM4xpjjDFj2VBBmKUwRoHrH1/Nr/+7ktWt3VSF/Bwyaxwrt3bj/HDpUbM4Ys54KoJexvWpDxpKfharOuQfYkljjDHGlIIFYSX2yLKtfOEfS5gzvopLjtyNcxZN7TXOjTHGGGPenCwIK6HHV7Ry4R8Ws1tjJbdccki/ARuNMcYY8+ZV+vNw36KeWNHK+3/9JBNrQvzpw4ssADPGmLeiV/4FnRtK3QpTIhaElYBzjm/d+QqNVUFuufiQ7ZdYMcaYt6zurfDizToQYdtq6Bnq0sNj2Jal8Oyf9P7zf4Ebz4Fr9xv6/To39ACNfW1aAv/8OIQ3vbG2jrTurbDyIUgXcCWDdApalxf2vtc+CRuff+PtKwFLv5TAo8tbeXZNO199x4KdHjTTGDOKPfojWPM4HPU5WPsE7HseeEfgBJktS8EbgHGzes9Pp8GzC46p1y0GXwiaMuNvd20Bjw/K6yEWhkBlblTnQqVTcP/X4eH/0+l4N9z5WUh0Q1kdnHMTTBlkHO8tS8GlYfx8nR7otV+5A6a/DRI9EOsCl9Igb9ndcNJ3h2lbGra+BuPn9n8sGQccbHkZJu6jrx3eDC/8Bfb/AKx+FNpWwUEX937e706Bnq3QthIeyrx+MgLfmQHn3wUPfQ8qJ8BpP8p9pte/C0I18O7f6vSGZ+GXx+h7Oe9WmHVU79f4y/v0tZv3g33PhYevgX3OhprJGgiV1ff+vrSvhXQC/OXw5C9h7kkwaX9IJaFlKdRO0++dP6TzvL7Bv3POwaqHIdIGG57TzyYe1iCzYx08/VvdXvu9X7fB27/Wf5s/+0f45//o9Pg94KTvQP1MqGzq/ZrpFDz4bb15A3D+neCvgPWLYY93QqBCl0sl4ZV/wqyjdTtuWwFVE8Ff+vprG6KiBN7788dY1drNg1ceNepHjzamINtWQt30Hd8Bj0bOQU8rVORdymT90/DHd8IHboeJew3+3EibBiM/2LP3/Al7wll/grppveen0/palY25ea/fozvZPd4JDbvBU7/WnfLkhfB/u4M3CF/Yklv+lTvgxrPhgrvh5X9A53p49+92+u0TyVyb8eXbdEcpArdeCuGNsPxefeysG6B9Ddz9BUjFoaoZwhvggA/Dyf8HqUQu6Iz3aGCVfY/ta7T7bfx8SEbhe7OHb9Nli6Ehbznn4J6r4ZEf6PRux0KkHc77m+5ks7a8Aj9ZNPh63/EzWHG//u0bULQu14Do+T/D+/+hgUP3Vt1xh2rg12+HLS9l1vNTePUOWPpPnd7rLHjhxtz9M36u2+TP781tQ9DgZuLesPQ2nfb4NRgCuORxGD9P39e3M9+bqYfAfufBfV+HznW59XzkIV0PaCD4f3P0/tuu0MDj96fo9PHfgLs+B9MOhRlHaLC46r9wywUgXv3Od22GcbvBpU/C3V+Ex36sz512KBx8qWbuaqdB+2qYfAAc+2V48a8Q74IjPqP/K3+/aPBt3tdn10OwEpbfB0/+CqonwlO/yj0uHg20AQ78CJz4bdj0IrQu0+c8+8eB17vXe+GMX8D939AgDWD22/V2x5X6ffroI+ArfiLExgkbRZ5atY13/+wxvnjKfM4v4UVDjRlSOg1LbobqSTD90IGX2bYCHrlWfxDXL4bTr9Oj7nxbXoHNS2DPM4d/zZvPh6Y9obxBj5gP+2TuBzKd1p39kltg6kH6w1y/g/8/L9wEy+7VHWJf//w4vHQrHH2VZkzu/iKc/x8NJG65QN9r1qKL4cRv9V9HNhgayuk/0XZ3b4W5p8Bv3g7rnoLPb9KdeyICX88MqDx+D7jofvjaeJ2esCdsflHvZ5df+XBuB5vvuK9q9ie8UXe20Q79nF76uwZ289+hO6HurfD4TzSIPuhi3e43fyi3nkufgsY5cHVN/9fIClRppiPrXb/Wbfbu32kg+avjYN2T8IWt+l5/e2L/dYhHA9xERLf9Pufoc7+fyXCd9mMNPrLWPA6/Ob7/eo75Ehx2hQZ57Wth9X/h3q8M3vYsb0CDAV8AVj0C1c1w7T69l6mfmfseLDwfFv8m99iMw7WbbTCfeFn/R256v36up/5A3+ehl2sAGe2AP79Hl8k6/ScaID3/54HXWTtNA/Mlt+j0xY9CtBN+e0Lv5WqmQsea4bdB1u4nw6v/go/+F/52kWb6svZ8twZchdrtWGiYo9+xrHmn6Wd768UahO/xTn3/y+/r//zZx8Mhl2nG9ZYL9H/giE/DTef1Xq5pT5h6MDz5i97zj/uKbufBnPFL2Os9hb+fnWTjhI0iNzyxhuqQj7MPnFrqpphdpXOjpvRnHd17fioJr98FTXtB7ZShn//Hd2o24UP/gtq8786GZzXzMuPwgZ/rnHZLPP1b2O8D/buxBhJph18eldvhXHg/TNqv/3LX7tt7+h+Xwn+/D3ufrT+sW16Gv35Ij+ynv02zOdlM2Ut/166Sumlw28egerLuTLI7FNBtsu+5+h6ufyeseCD3WLAaPrs29x5FNFB74Jvw0Hd0B3X5C7nlox3wtwv1/tFX9d7e6TQ8ez2kk1ok3bpM5//m7fo30OdyUE/8tHcQFu2Ee7/c++jdXw6XPqHdag2zdbsA/OOS3DJH/K8GJaBBw7hZGgBkbXlJd8xZm1/M7eyfvwGe/p3WwYRqIdquy+x9jmZ27v5C7zb7QrrDy9r4vO5sH7suF9gtvQ3mntz7eWsehTs/w6DmnqIZvm0rNHP0pzN1Zwlw1+f1u7vuSZ2+76vwyA8HXs8XWnOZqNnH5uZ/4iX4/gLt3gpWwfzT4bk/57Iz+aYeDM/8Hg75GFwzHxgkwVBWp4Hi9Wfk5qXisPoRaN4XfnfSwM/LD8QX/6Z3hiZbj3T+Xfo9ePGvcPBlUNUE/7kqF0wCnPJ97V49/87cvIpxmhEFzSTedZV+htl5AA8gmUYAACAASURBVMd8UTNhD35bg8R3ZAKbVf/V7NVPD8kt27yv/jaABmDTD9P3nc249XXid+Dfn9Ys6/Ff0yDswW/3DsAg935BM0qv/0fvv/MX+p6X3a3TB18Gax7TLFnDbP0f7d6i/9/j58OCM6BxLvz0YP0t6OvIz8Hk/TX7lu0y3PSifl9fuV2nm/bUeUd+Vj9z8cC42dqVevcX9bXyA7AjPwcPfEO7Yj+xRL9XKx/cJUHYUCwTtgv1xJMs/No9nLZ3M9961xBdGmbXcU5/rKomahp8R3Vu1MzJUMHNL4/RI9zDPqk/pNA/gzFxbzjgwt5H+5E2eOLnmh3IdrsAXL4kF0RkMxRXd+Qe3/SidqPEOrWGJ9ap8xd9VFP5+e+9b/fhPz8OS2/XmpV8H38h15W26hE9Es3fQYDWkKx/evDt8LYr4IALYPVj8LcP64/o4VdqdiDfoZmslMenQcfW1zWj0ddJ3wNfEO75sn52zfvCM3/IPV49GfZ4B1Q0an3O63fp/IMuheO+nOsuu+vzuZ16eUP/9/7Bf+nnc8enc1mJz66HTS/ozvv1u3sHBQ1zdKeWX6fz2HXaDTSY8XvAxY/oev5zlWagfnKQ1vxkA6+yejjv7/CLI3o/99xbNMDtaYWZR0IyBn85L/d+C3HE/+a6bJr20m6wF/7Se5n8YG/GEbrzm3pQ7jsU64JvThr8NWqnalckwLFXa3ci5AK5wdz4vtyOd9HFGgRn+cs1oKqfocXof/tw/+eP200D62zGrmI8XPl6rpuvPBMATVigBxD/+bw+r36m1qMtuUWD+74a5mhG8aHv5OZ98lUN2h/8tnbduTRcM6/38z75GlRN6L++xb+F2y+Hz67Tg651T+n7u+JleO0/GiyI9P+/7WrR52W3EcCnV2qNWdZxX9Wu4m8M8ht37i1aS9UwW7fHjxfmDkYGc3UH/Ciz3BdatD7v96dq123zPgM/Z9MS3W6+ACSi8PU+22HKIs0Ozh0gEH75tlwGbP7pcOZvNQCdcXj/37FHf6T/R6CB5VWb9f4zf9DvW8U4+NN79AD1sieHfp8jwDJho8QdL26iJ57i9H2G+KEyI6Njve6YvEN8xbu26I9NNBPAXPJE7wLcRER/GKMdMO1g3fGIwP3f1KLht39N602iHXDZ09rNA7oT/PN79egs2p7rYnj4/7ROId6tmYx8G5+H2y7TbpaTvqNHkeuezv34zzg8V/D62p06nT3Szdr8EvzhdOhuGfj9PvEzrfs4/utan/K1Rq1BOfxKLbjd4wzNroAeVf7Ps7q+tlV6xPvMH+DkawbOFJx7ix6lD/YjD/Dfa/SWtenF/gEYaBdE2yqtb9q2PDf/vFu1HiWRuVD9HZ/KPdazVdeXr3Nd/4xJ+Th4/Dq9XdUCHWtzy8w8sne2DeAzayFUrfezny/Ar4/rnyXI2ve8/oXSodrc/X3ep59hTyvMPEozV1tegr9/RAuZfWW6Mzzx25q92PNMePyn2vb8uqgF74IzftW/lskXhNN/rMHLYVdoduD6M7SGZ+H5+j1Z+4RmK8bP0y6jygmw/H7NWtVN19qhvkHYmb/R9vzyKC1ob9y99+PZIujBtK/RQP3M3+hrHHq5Fr3XTB76eXNOyAUYT/xU29qV2al+fmNuufqZ8Le85x36cX2NZAyumQuHfUK7Jhdl6pXKajWY9pfrZ3DDWbkADHQn3zAbDv+0Zkg3vaBZrNWPatanYQ4c/XmtD8s+r6xeA4y3fzW3nvf8off3vHL8wO9z4Yf0Bhr0r3tKt1NZHez93txyfQOOykZ47/XalfyDzAkT5fUayK98UN//fu+HQDkc/QV4+VY4/pu9DwKrmmFCXrbu1B/C7/pkRfN98F/698N3A6IHNBP3gs+sHvw5kDuhA/Q3J+uYL+lvY80Q+8bdT8odCCw4EzxemHnEwMtW5G3jjz6c22b7fyA3f/IBeqASadfvQolYELaLpNKO79z5CvMnVrNoRn2pmzO29WyDf12hO4rKCRowbHhGC2df/bcWxC79JxzyP5r2XvuUprZjYd3RZtfRtyD4J4vgU8ug9XWYdoh2neQfAWd/wLI7+2O/nAvgfry/Fkb7y+CBb+mP+or7c8895ftw+yfguj5neX1mrR6xppM6/dz1egP9kcja8926c79mnmZFnvyF7sDy3XO1BmCN87RItmNt/233+HXQ8kquuDyd0BQ95FL3kw/QbVs3XQOfa/fJBTx963A+vVJ/8KH/qeTv/j389QMMa9FHtXZj/TPaDTJlESz+tT522Kfg4e/p/VlHaUbu6d/B/XlnVM06Gk69VutXJu7dO1OSJV740L+1aD1b87T8vlw2BuBtn9Ad/upHcgXW2QAMNGiJhbVrsW8A1rQnTDlIM5kT+hTlQ+5Hvqxeu5F+uI8GYQdflvueZIOe2mm60zjwwtzz+55hB/p9HOyMyMrxmR1kRvb71bC77uxe/oduc0/eiUEfvF27+qYerO3sa9ws/U7kZ13zDXRSxmVP6/9Glr9c15Fdvm8gN5C9z9ad862XaI3b/h/UTFOgqvdyvqDWJf7jUp0+Lq8W7LPrNUhcdHHvM+KCma7m3Y6BSQu1+3XcbM1K+jKXifN44MjP6AHA9MP1Pbz4V5h3qj5+0MW5IGygIu95p8GJ39WALtZZ2Mkrs47Wbs3mAcoABiKi2fH9P5Q7CGuco7d8h39Kb5H23vP79gJMzXRt7n22BjxP/BSW3aPbdM93a3coaID4Rnz4Pj1R4bArhl/W69O6t1f+pQHZULLbeP47Bv+OTc4kptY/rZ9/iVgQtoss3djJlnCMz500761xIe1oh2aaNjyn/7yTF8Kc43vXM4GeORUo7//8m8/XLsLjv54ZIyetO46GOfCzQ3PPze9yOfWH8J8v5LrfHr1Wb/kOvkyPkG/7n4Hbfd2BENmmWbG+haL3fbX39Ff67Kh+fdzA6wStx4q0a+1QVvUk3cl/Zg3c97XexaugR8KVEzT4WnCm/rA075er66jPdIG2rdQi5OX36/s7/us6f+vrGqh1bdEupl9latKW39v7DK187/p17yL6qqY+C+QFWodengvAoPfO5ZybtHsQ4IRvaZfQnzLrfecv9D28cnvv15t2sN5As3JLbtEC7WwQBnrUv/BD+sNZP0O3WUWj7oDO/zeseaJ3EDb3FK1Pm3GEPnfzS7nHbsjLLoB+HjOP1J3qQIXovqAesWfru7Im7Q8fvnfonWt2ZyWZoOldv4Ilfxv4x7/fNh/EjpyY0LyvBqB7nqlnwE09qP8yvmAuE5PviqX6v5YNngqx77l6okLDbnqg8a1M93l0kABuKF6fBiWLPqJB84wj4MCLctuy7+vudqx2jebLBlsD/dZkNe+jQVj5uFwAljX3ZK1b8/o041Y7VYNV6B3IDkQkl30r1NyT4aIHoWaIWtGBnPqD4ZcBPSg471YtV3j6t70ztaCB5+c26gkLXh9MXaS1W/ueN7JnQE/eX2+FqplU2Lacc7zWwR3zxcGXmbQ/ILoNLAh783t8hdbPHDRzXIlbsovc9jENmrJeuFG72E6+Rs+WGz9f65we+aHuoOtnauYkvBF+c4IWcYIO3ti1qXdGJKtvzcsD39LU+1Ae+7F2Bfa0anBw0MUa5CV64HtzNAADHb9n7RMaPJ2SyXzceE6upiVftqh1MJc+qT/Uh12hP/TegKb6O9fr44EK/bGYc4LWZLWtzD33pO9q/UNW4+6aLQL40B3w/I1wz5c0U+X1a3uzGmb37r76xMvarXj75b3b94F/ap3XxL3712LkZw2yR9nZrqEJCxjUxL01G/O5DZo5aFuVe2zv9+rrbH0t80M4gHmn6Nl0Xr+efp9fHF/RAOfcqOMZgXapZk05UM+ka1oAdTO0qyh/pzEur0sxa8IC/U7mdxPtdlzvTGSWiB4I5Gch33bF8Dum7E4uu9zkhbkj8Q/fC7/K2wl0bmRIF94//I6/r3f8BLq/0nvYjeF88A4NCLNZjx1x+nWQ/dqGqjWD+cO9Bq8VKsQhH9cgORvcD6aqCaqGXmRAe5yhQf1gwW22tMHj0Ux5sb2RbVWIWUfp7YgrB348P2ANVmmX5lgRqtHM7pDLVMOH79ETBErIgrBd5PEVrcxoqKCpZpSMjp9K6Nk+Q3UHOActr+oynRsG7q93TlPzc0/RHcPN52sB+tYBijrzz37b97zc+C6ty/Q2UD1RV2bE5/wA7Kw/a0CU7/BP5wpkj/mSBlt9C8fHzdauxp5W3Rlnu3hENBCqnZrrZlpys/5tmK3vq2lPuPxFuPtLGrD5yzRwqZuu3RKbXhx8vJr87N+so3OjRR/ysdx8f5nWN1y2WIuxs8MSzDut97qyP4wHXqQ7m+wR/mt3aq1R3+6HfDWTNNNx/zdyQS5oUDHY2ZX5Tv2BBteg2aWhhp3IBh3ZOqHaqfpesl1swarBA7CsbOF8dvyjvrJBYH79lUjvkxv68gW1MHrbityp/B95SLOF+eNLnXvz4Ou44G548Dvatbv/BzVgHM72bs0BgrW+WYi9zxp6XQOdqTrs69f0fn+FGGxokqGc9L3eAXdW3TQNHsfP6/9YoTye4QOwN2LKAfpd2NHsE2i3fP6Zg2ZsyB4IlZAFYbuAc44nVm7j5D134uy7YrnjSk1DZ8cBGsgzv9fMTNYp39dThhvm6E6rcrx2C2VP/W+cq/VG2UyJr0zrUtpWaYo/f3ygbMAy7zTdkeWfLj6U/AJ40CxCttYrG4Q179s/AAPdqQQqYONzA2dx+u4MQbuo8h335f7LgNYknfJ9HW8pvFF38qse1uL+vqMye7yZU/IHyGZ4ffT6t+ybYdnnXK17O/gynQ7m1SzNGWDcpIHsfoIW2X/8ee2eG6776+y/5IIpX+a9TFo4dPbH3+dgw+OF9w4SpO6saQfrEAbDFXb3VTUhFxTNOFzbtiNnxpbV5oKB4QLJrPJMBuqoAc6QzK+ruaplZEbWL5X8Ora+diZ43NUGC/iHU251vmbnWBC2C7SEY4SjSeY3Vw+/8K6QSmoABnrG3CnX9H48EdGzxF7skw24/RP695gv6llGe58Dqbzuv5ZXei8/80jNIDVlCpUvvE/XufIh7f6ZtL+egeT1aQ3RmsdzZ89lg72yWi1A/tcndceZDcAuuFvrSwY6kpm0H5x9Y2YE5rzB+6YsynX1NA1QPD3Qzq9hdv95A/F4AE+uGHbmEQPX12x/rWH+9Y77ysDBRfVE3Y5Z2W46f0XvbsuhnPQ9zVbWTS+szmf3vMEfs8FV/phTpbSjAViWv0xHyx5qfLah7HmmnuY+86jhlwXdboMVtOdnqHbB6N3GmNHDgrBdYFWrnlI/tX6IotCRlozrGXL5R2hLbtG6qez10vzlehba4l/rKb3ZOpp4V//T09/+9dwZQK9lBugbbCTn036k3VbRPmfgTNpfby2vaj3P0VflgpE5x2tB7ZaXNbA69PLeQVHfgtOBriV3/Dc1+xSqgd1P1Fs2CMsOP7H6UZ0eKAjLetsntCaobtrO7+TfqEM/PvwykMuyNQ1Rn9WXL7hjRdb5JmbqVAaqrQINENsHOCtzNNqRbdaXSP/Bd3dW9n9gxiCn2xtj3rQsCNsFVrd2AzB93DDj6OysSLt2F/3nKj29/eBL4Jbz9TT7L7Xnuo3u/4bWXv3hdEB0rJffnaJjXnVv6X824NxT9BTlsjo9Sy8bhIU3DN6W4785fBdN4+5w8vf6z/d44Zy/9J9fqIMv0dtAst1PlZnBAQcsKs+c+de8387Vw5RC9kSEvt2mxbLgDA3ABgtis4G22TFXLu8/Mr8x5k3PgrBd4Jk17QR8HibVjfAV25fergP03fm/esZdKq7zwxtz4xytf1rrlNpW9R4B+bQfabfdRx7U8Wjmnw7Xn6kBGWg34YI+dVpHfAYe/FbuDMH8S2NkLfoIINrdtc/7Rvb9vhHZ2qm9z9KAbKgzvoYbdHI0mXW0jgBfyDg7I2WoC1ibnbMjZy0aY940LAgrsngyzT+eW8+pezXj9w4ysGKhwpt0EMVIm9Zf/SUvyMkGYNB7bKxf9Rn/5IqlGkRlxwnKjswNeimPb2SCk4HOljvqsxqEZV1wt3bvRdvh1Tu1ID9bbD7U+CylkA2sGmZDwzBdfSM5Dk6x+QJwwjdK3QpjjDE7wYKwInt1U5ieeIqj5jYW9oRUYuAC8XVP5wbbROh3cdqjPp8ZUX2IMU+O/KxmgAbLAgUq9CyuqqbBj8wP+Z9ckOf15y4bMf90YICRykvtgrv1JINCAqsjP6vXNivmafDGGGNMhgVhRfbCei1O33vyMNem2rYS/nEZrH1cuwLnn6bXGLz/G1rInj9IaKBCi+ezJiyAIzKDhX70ER1QM5XQa+etWwzPZS6Oe/gQA4pmHfKxoYvR3/5VDdKincOvazSYcuDARfwDmXYI/O/K4ZczxhhjRoAFYUX2wtoO6sr9TB6sHsw5Pd3/2rzRkW86T8eDqm7ufwFi0EtaTDlQC6BvvUTHp8rqe8ZX016ZIEwGv85cvrddPvwyB186/DLGGGOMGZIFYUX2/Lp29pxciwzWHbb0Nrgp73IQh1+pWbHsRZyzph+mF17916e06D070v0ljw3dgPxLsRhjjDFm1LAgrIjiyTSvb+nimHmDBEKJCPz9o73n7fM+HcOpdZmO7L77yTpu1ZTMdew+tnjHGpEdkmG4q84bY4wxZpeyIKyINndGSaXdwIO0rnggM14Xeh2+Y74EFY25awF+6A7o2bbzI3pn+YJ6PcJdNY6UMcYYYwpiQVgRberUS7s01fSpB3MObslcY23KQXrdwVCfSxoFKkZuvKpCL71jjDHGmF3GgrAi2tShQdjEmj4XM+7arCPUH30VHPqJ4a8jaIwxxpg3nTc4eqgZSjYIa+obhG1eon+nHmwBmDHGGPMWZUFYEW3siFIe8FIV7BNovXgzeHwwYY/SNMwYY4wxJWdBWBFt6ozQVBPqPTzFigfh+Rv0jMeyutI1zhhjjDElZUFYEW3qiPavB1v5IIgXDvtUaRpljDHGmFHBgrAi2tQRpak678zIJ34OD/+fZsD8ocGfaIwxxpg3PQvCiiSVdmwOx2iqCeZmPvkL/RsbI9ddNMYYY0zRWBBWJK1dMVJp13uMsJrMwKupeGkaZYwxxphRw4KwItmYHSOsOq/b0ZfJip343RK0yBhjjDGjiQVhRbKx7xhhD30XXrtTL8S96KIStswYY4wxo4EFYUWyqSMCZEbLT6fgvq/pA+2rS9gqY4wxxowWFoQVyabOGAGvh/qKAHS35B7wlQ3+JGOMMca8ZRQ1CBORE0TkVRFZJiKfGeDxqSJyv4g8KyIviMhJxWzPrrSpI8KEmqAO1Nq5QWfucQacfUNpG2aMMcaYUaFoQZiIeIHrgBOB+cDZIjK/z2JXATc55/YFzgJ+Uqz27GobO6JMzI4RFt6kfw/5GIybVbpGGWOMMWbUKGYm7EBgmXNuhXMuDtwInN5nGQdUZ+7XABuK2J5danNnNFeUH868rerm0jXIGGOMMaNKMYOwScDavOl1mXn5rgbOFZF1wB3Ax4rYnl3GOcfGjrwgrH0NeANQ0VjahhljjDFm1Ch1Yf7ZwO+cc5OBk4A/iki/NonIRSKyWEQWt7S09FvJaBOOJYkl04yvyowLtvllaNgdPN7SNswYY4wxo0Yxg7D1wJS86cmZefkuAG4CcM49BoSAhr4rcs79wjm30Dm3sLFx9GeTOiMJAKpDfoh3w5rHYMIeJW6VMcYYY0aTYgZhTwGzRWSGiATQwvvb+iyzBjgGQETmoUHY6E91DaMzkgSguswHj/8E4l0w901z4qcxxhhjRkDRgjDnXBK4DLgLWIqeBfmSiHxFRE7LLPZJ4EIReR64Afigc84Vq027SjiqmbCqkB9aXoPqyTC/7zkJxhhjjHkr8xVz5c65O9CC+/x5X8y7/zJwaDHbUArhqGbCqkI+6FgHddNK3CJjjDHGjDalLsx/U+qM5tWEdayFminDPMMYY4wxbzUWhBXB9kxYAB0tv2ZyaRtkjDHGmFHHgrAi2F4TFt0ALmWj5BtjjDGmHwvCiqAzmiTo8xBoW64zxs0ubYOMMcYYM+pYEFYE4WiC6jI/tL6uMxp2K22DjDHGGDPqWBBWBJ2RpJ4Z2bYaQrVQVlfqJhljjDFmlLEgrAg6owkdI8yK8o0xxhgzCAvCiiAcTVId8kHnOqhuLnVzjDHGGDMKWRBWBJ3RhI4R1rnBgjBjjDHGDMiCsCIIR5PUBtLQ3QLVk0rdHGOMMcaMQhaEFUE4mqDZ06YTFoQZY4wxZgAWhI2weDJNNJGmSbbpDOuONMYYY8wALAgbYdnrRja6Vp1hmTBjjDHGDMCCsBHW3hMHYFyqRWdYJswYY4wxA7AgbIRt69ZMWG2yBUI1EKwscYuMMcYYMxpZEDbC2jKZsMrYFuuKNMYYY8ygLAgbYdnuyLLIJgvCjDHGGDMoC8JGWFuPdkf6ujdaPZgxxhhjBmVB2Ahr64lT4U3j6bbuSGOMMcYMzoKwEdYZSTAz1KkTlgkzxhhjzCAsCBthXbEUMwLtOlFjmTBjjDHGDMyCsBHWFU0w2WuXLDLGGGPM0CwIG2FdsSTNHrtkkTHGGGOGZkHYCOuKpZhAOwSqIFhV6uYYY4wxZpSyIGyEdcUS1NAFZbWlbooxxhhjRjELwkZYVzRJpfToJYuMMcYYYwZhQdgI646lqEh3WxBmjDHGmCFZEDaCYskU8VSa8nSXBWHGGGOMGZIFYSOoK5oEIJTqgmB1iVtjjDHGmNHMgrAR1B1LARBMWSbMGGOMMUOzIGwEhWMJhDT+RNiCMGOMMcYMyYKwEdQVTVJBFMFZEGaMMcaYIVkQNoK640mq6dEJC8KMMcYYMwQLwkZQOJqkWrJBmBXmG2OMMWZwFoSNoK5Ykmq6dcIyYcYYY4wZggVhI6g7lp8JsyDMGGOMMYOzIGwEdUWTVFlNmDHGGGMKYEHYCArHkjT6ozoRsgt4G2OMMWZwFoSNoO5YknG+iE4Eq0rbGGOMMcaMar5SN+DNpCuWpN4TAU8FeP2lbo4xxhhjRjELwkZQOJqk1hsBv9WDGWOMMWZoRe2OFJETRORVEVkmIp8ZZJn3iMjLIvKSiPy5mO0ptu5YkhrpsaJ8Y4wxxgyraJkwEfEC1wHHAeuAp0TkNufcy3nLzAY+CxzqnGsTkfHFas+u0BXLnB1pA7UaY4wxZhjFzIQdCCxzzq1wzsWBG4HT+yxzIXCdc64NwDm3pYjtKbquaJJK122ZMGOMMcYMq5hB2CRgbd70usy8fHOAOSLyiIg8LiInFLE9RdcVS1LhuiwIM8YYY8ywSl2Y7wNmA0cCk4GHRGRP51x7/kIichFwEcDUqVN3dRsL4pyjK5Yk5LMgzBhjjDHDK2YmbD0wJW96cmZevnXAbc65hHNuJfAaGpT14pz7hXNuoXNuYWNjY9Ea/EZEEinSzhFKWhBmjDHGmOEVMwh7CpgtIjNEJACcBdzWZ5lb0SwYItKAdk+uKGKbiqYrlqSMGB5SELTCfGOMMcYMbdggTEROFZEdDtacc0ngMuAuYClwk3PuJRH5ioicllnsLqBVRF4G7geudM617uhrjQZd0STVdt1IY4wxxhSokJqw9wI/EJFbgN84514pdOXOuTuAO/rM+2LefQdckbmNaV2xJNViQZgxxhhjCjNshss5dy6wL7Ac+J2IPCYiF4mIXRwxT1csSTXdOmFBmDHGGGOGUVA3o3OuE7gZHetrIvBO4BkR+VgR2zamdEXzM2G1pW2MMcYYY0a9QmrCThORvwMPAH7gQOfcicDewCeL27yxo3cmzArzjTHGGDO0QmrC3gV83zn3UP5M51yPiFxQnGaNPd2xJFUS0QnrjjTGGGPMMAoJwq4GNmYnRKQMmOCcW+Wcu7dYDRtrwvmZMBuiwhhjjDHDKKQm7K9AOm86lZln8nRFk9R6enC+EPhDpW6OMcYYY0a5QoIwX+YC3ABk7geK16SxqTuWpM4bQ4J20qgxxhhjhldIENaSN7gqInI6sLV4TRqbwtEklZ4E+MtL3RRjjDHGjAGF1IR9FPiTiPwYEGAt8P6itmoM6owmqfTGLQgzxhhjTEGGDcKcc8uBg0SkMjPdVfRWjUHhaIIKSYC/rNRNMcYYY8wYUEgmDBE5GdgDCIkIAM65rxSxXWNOZzRJhScGfqsJM8YYY8zwChms9Wfo9SM/hnZHvhuYVuR2jTnhaIIQccuEGWOMMaYghRTmH+Kcez/Q5pz7MnAwMKe4zRp7wtEkIWIWhBljjDGmIIUEYdHM3x4RaQYS6PUjTYZzjnA0QdDFrDDfGGOMMQUppCbsnyJSC3wXeAZwwC+L2qoxpjueIu0g4CwTZowxxpjCDBmEiYgHuNc51w7cIiK3AyHnXMcuad0YEY4mAPCnopYJM8YYY0xBhuyOdM6lgevypmMWgPUXjiYBhzcdtUyYMcYYYwpSSE3YvSLyLsmOTWH66Ywk8JPC41IWhBljjDGmIIUEYR9BL9gdE5FOEQmLSGeR2zWmhKNJyojphHVHGmOMMaYAhYyYb6OPDqMzO0YYWCbMGGOMMQUZNggTkcMHmu+ce2jkmzM2haNJQpINwiwTZowxxpjhFTJExZV590PAgcDTwNFFadEY1BlN5HVHWibMGGOMMcMrpDvy1PxpEZkC/KBoLRqDwtEklR4dpgKfBWHGGGOMGV4hhfl9rQPmjXRDxrJwNEF9IKkTlgkzxhhjTAEKqQn7ETpKPmjQtg86cr7JCEeT1AVSEMNqwowxxhhTkEJqwhbn3U8CNzjnHilSe8akzkiCOf5kJgizTJgxxhhjhldIEHYzEHXOpQBExCsi5c65nuI2bewIR5PU+qw70hhjjDGFK2jEfCA/sigD7ilOc8amcDRJtS9TmG/dkcYYY4wpQCFBWMg515WdyNy3SCNPOJqg9pRvKQAAGhFJREFUypsNwiwTZowxxpjhFRKEdYvIftkJEdkfiBSvSWNPZzRpQZgxxhhjdkghNWGXA38VkQ2AAE3Ae4vaqjEklXZ0xZJUSBw8fvD6S90kY4wxxowBhQzW+pSIzAV2z8x61TmXKG6zxo6umBbkl0vc6sGMMcYYU7BhuyNF5FKgwjm3xDm3BKgUkUuK37SxIRzVeLRcYtYVaYwxxpiCFVITdqFzrj074ZxrAy4sXpPGls5IJhOWCkNZbYlbY4wxxpixopAgzCsikp0QES8QKF6TxpZsJiyU6oSyuhK3xhhjjDFjRSFB2J3AX0TkGBE5BrgB+HdxmzV2hKOaCQsmLAgzxhhjTOEKOTvyf4GLgI9mpl9Az5A0QGcmE+aPt0NZfYlbY4wxxpixYthMmHMuDTwBrAIOBI4Glha3WWNHNhPmjXZYTZgxxhhjCjZoJkxE5gBnZ25bgb8AOOeO2jVNGxvC0QRB4kiyx7ojjTHGGFOwobojXwEeBk5xzi0DEJFP7JJWjSHhaJJGX+Za5haEGWOMMaZAQ3VHngFsBO4XkV9mivJliOXfkjqjCSYFozphQZgxxhhjCjRoEOacu9U5dxYwF7gfvXzReBH5qYi8vZCVi8gJIvKqiCwTkc8Msdy7RMSJyMIdfQOl1hlNMjFgQZgxxhhjdkwhhfndzrk/O+dOBSYDz6JnTA4pM57YdcCJwHzgbBGZP8ByVcDH0eL/MSccTTLeuiONMcYYs4MKGSdsO+dcm3PuF865YwpY/EBgmXNuhXMuDtwInD7Acl8Fvg1Ed6Qto0U4mqAhG4SV2xAVxhhjjCnMDgVhO2gSsDZvel1m3nYish8wxTn3ryK2o6g6IwnGebp1wjJhxhhjjClQMYOwIYmIB7gG+GQBy14kIotFZHFLS0vxG7cDwtEk9dIFHh8EKkvdHGOMMcaMEcUMwtYDU/KmJ2fmZVUBC4AHRGQVcBBw20DF+Zku0IXOuYWNjY1FbPKOC0eT1NClWTCxk0eNMcYYU5hiBmFPAbNFZIaIBICzgNuyDzrnOpxzDc656c656cDjwGnOucVFbNOISqTSRBIpqlyXdUUaY4wxZocULQhzziWBy4C70Msc3eSce0lEviIipxXrdXel7CWLKlzYgjBjjDHG7JBCLuC905xzdwB39Jn3xUGWPbKYbSmGcObi3WXJTiibXtrGGGOMMWZMKVlh/ptBR0SDsFCy0zJhxhhjjNkhFoS9AdkgLBDvgDIbI8wYY4wxhbMg7A1o70ngJ4k32W2ZMGOMMcbsEAvC3oCOSIJaunSirLa0jTHGGGPMmGJB2BvQEUlQLZnR8kM1pW2MMcYYY8YUC8LegI5IgjpvXCeCVaVtjDHGGGPGFAvC3oCOngTjg1qcb0HY/7d379FVlWcex78PSUyAAOYmt4BgZSoixAhVZKaFinZwqmKtDDrUUdS6aLVUHcei9l66lm3ttGpZLDOtF9SWUSytM2O1InRwLfECIxUEXTKIErzkHnK/PvPH3okRCeSc5GTnhN9nrSzO+5599nnO2WuHX9797r1FREQkFgphvVDV0MwJ6eFImO4bKSIiIjFQCOuF6oYWctJ0OFJERERipxDWC9UNrWSlKoSJiIhI7BTCeqG6vpmslMagoRAmIiIiMVAI64XqhhZGDWkCS4HUjKjLERERkSSiEBanlrZ26prbyBzSCOmZYBZ1SSIiIpJEFMLi1HHfyEwaIH1kxNWIiIhIslEIi1NnCGurhmG6ebeIiIjERiEsTlX1QQgb1lIJw/MirkZERESSjUJYnKrqg0tTZDRXKISJiIhIzBTC4hSMhDmpDWUwPDfqckRERCTJKITFqaqhheE0MqStUSNhIiIiEjOFsDhV1zeTZweDhkKYiIiIxEghLE6V9S2cmFEXNBTCREREJEYKYXGqamhhQnp90NCcMBEREYmRQlicquqbGZtaGzQ0EiYiIiIxUgiLU1V9C2NSwzlhwzQSJiIiIrFRCItTVUMzeVYT3LIoTTfvFhERkdgohMWpqr6FLA7CsJyoSxEREZEkpBAWh9a2dmoaWxnhtTA0K+pyREREJAkphMWh4+bdw9trFMJEREQkLgphcagKQ9jQtoMw9PiIqxEREZFkpBAWh9KaJgDSW2sgQyFMREREYqcQFoeSmibASW3WSJiIiIjERyEsDiUHG8mkAfM2jYSJiIhIXBTC4lBa20RuakPQ0EiYiIiIxEEhLA6lB5s4aVgwL0zXCRMREZF4KITFobS2iZMyaoJG5phoixEREZGkpBAWh9KaJiYc1xHCToi2GBEREUlKCmFxKK9rZmxKePNuhTARERGJg0JYjNrbnYq6ZvKoDs6MTE2PuiQRERFJQgphMapqaKGt3cnyChih+WAiIiISH4WwGJXVBmdFjmyt0KFIERERiZtCWIzKwlsWDWsuh8zREVcjIiIiySqhIczMFpjZm2a2x8xWHOb5m81sl5m9ZmbPmdmJiaynL5TVNQPOcY2lCmEiIiISt4SFMDNLAVYB5wOnApeb2amHLPYqMMvdZwDrgJ8mqp6+UlbTRCYNDGlt0OFIERERiVsiR8LOBPa4+153bwbWAgu7LuDum9y9Pmy+COQnsJ4+UVbbxJghHZen0MR8ERERiU8iQ9h4YH+XdnHY151rgD8lsJ4+UV7bzKeG1gYNjYSJiIhInFKjLgDAzL4CzALmdvP8dcB1ABMnTuzHyj6prLaJkzPqoA7NCRMREZG4JXIk7AAwoUs7P+z7GDM7F7gDuMjdmw63IncvcvdZ7j4rLy8vIcX2VFldMxPSOg5HKoSJiIhIfBIZwl4BppjZZDM7DrgMeLLrAmZWCNxHEMBKElhLnymraQpuWTQkDYZmRV2OiIiIJKmEhTB3bwVuAJ4BdgOPufvrZvZDM7soXOxnQCbwuJltN7Mnu1ndgODulNU2kWtVwXywIbrMmoiIiMQnoXPC3P0p4KlD+r7b5fG5iXz/vlbX3EZTaztZ7ZWalC8iIiK9oqGcGHRcLT+4ZZHmg4mIiEj8FMJiUF7XccuiMoUwERER6RWFsBiU1jQzhHbSmjQSJiIiIr2jEBaDstomcjiIebvmhImIiEivKITFoLy2mTFWETRG6JZFIiIiEj+FsBiU1TYxI+ODoJH7N9EWIyIiIklNISwG5XVNTE97P7hQa/ZJUZcjIiIiSUwhLAalNU1MsWLInQIpaVGXIyIiIklMISwGJTVNnNj+LuSdEnUpIiIikuQSesX8wcTdqTlYTW7K+3DC1KjLERERkSSnkbAeqmtuY2zrgaCR9+loixEREZGkpxDWQyUHG8mx6qCRqctTiIiISO8ohPVQSU0TWdQEjWE50RYjIiIiSU8hrIdKa5rIto4Qlh1tMSIiIpL0FMJ6qKSmiSyrwW0IZBwfdTkiIiKS5BTCeqikppE8q4Wh2TBEX5uIiIj0ji5R0UOlNU3MTqvDNB9MREQGoZaWFoqLi2lsbIy6lKSUkZFBfn4+aWk9v5i7QlgPlVXXUui74IR5UZciIiLS54qLixkxYgSTJk3CzKIuJ6m4O+Xl5RQXFzN58uQev07H1XoovXQnx3s1TLsk6lJERET6XGNjIzk5OQpgcTAzcnJyYh5FVAjrgcaWNlrqyoPGyPHRFiMiIpIgCmDxi+e7UwjrgX3ldYzw+qCRMTLaYkRERGRQUAjrgXfL6xlhDUEjXSFMREQkmbW2tkZdAqAQ1iPFlQ2MQCNhIiIiiXbxxRczc+ZMpk2bRlFREQBPP/00Z5xxBgUFBcyfPx+A2tpali5dyvTp05kxYwZPPPEEAJmZmZ3rWrduHVdddRUAV111FcuWLeOss87i1ltv5eWXX+bss8+msLCQOXPm8OabbwLQ1tbGLbfcwmmnncaMGTO499572bhxIxdffHHnep999lm+9KUv9fqz6uzIHiiubGB0aiNuKVjasKjLERERSagf/Ofr7HrvYJ+u89RxI/nehdOOutz9999PdnY2DQ0NfOYzn2HhwoV89atfZfPmzUyePJmKigoAfvSjHzFq1Ch27NgBQGVl5VHXXVxczAsvvEBKSgoHDx7k+eefJzU1lQ0bNnD77bfzxBNPUFRUxL59+9i+fTupqalUVFSQlZXF17/+dUpLS8nLy+OBBx7g6quv7t0XgkJYj+yvrOf09GYsZSRo0qKIiEjC3HPPPaxfvx6A/fv3U1RUxOc+97nOSz9kZwe3DtywYQNr167tfF1WVtZR171o0SJSUlIAqK6u5sorr+Stt97CzGhpaelc77Jly0hNTf3Y+11xxRU88sgjLF26lC1btrBmzZpef1aFsB4ormwgL60J0kZEXYqIiEjC9WTEKhH+8pe/sGHDBrZs2cKwYcOYN28ep59+Om+88UaP19H1LMVDLxkxfPjwzsff+c53+PznP8/69evZt28f8+bNO+J6ly5dyoUXXkhGRgaLFi3qDGm9oTlhPVBcWU9WSgOkj4q6FBERkUGrurqarKwshg0bxhtvvMGLL75IY2Mjmzdv5u233wboPBx53nnnsWrVqs7XdhyOHD16NLt376a9vb1zRK279xo/Prjs1IMPPtjZf95553Hfffd1Tt7veL9x48Yxbtw4Vq5cydKlS/vk8yqEHUV1QwvNjfVMqt8JuVOiLkdERGTQWrBgAa2trUydOpUVK1Ywe/Zs8vLyKCoq4pJLLqGgoIDFixcD8O1vf5vKykpOO+00CgoK2LRpEwB33nknF1xwAXPmzGHs2LHdvtett97KbbfdRmFh4cfOlrz22muZOHEiM2bMoKCggN/+9redzy1ZsoQJEyYwderUPvm85u59sqL+MmvWLN+6dWu/vd/OA9XctepXPHjcT+ErT8DJ5/bbe4uIiPSX3bt391m4GKxuuOEGCgsLueaaaw77/OG+QzPb5u6zDre85oQdxVslNZxmwRAo+WdGW4yIiIhEYubMmQwfPpyf//znfbZOhbCj2FF8kNmp+/DsT2G6RpiIiMgxadu2bX2+Ts0JO4qdB6o5PeUdbNzpUZciIiIig4hC2BG0tzsH3tvPCe0lMLYg6nJERERkEFEIO4K9ZXV8ue2ZoDH+sHPqREREROKiEHYEe0pquSBlC7VjzoIT50RdjoiIiAwiCmFHUFxRy4lWQuqEWbpdkYiIiPQphbAjqCl5h3RrIX20LtIqIiIykGRmZkZdQq8phB3BcSXBndkt5+SIKxEREZHBRtcJO4JZ5X+kIiWP7Imzoy5FRESk//xpBXywo2/XOWY6nH9nt0+vWLGCCRMmcP311wPw/e9/n9TUVDZt2kRlZSUtLS2sXLmShQsXHvWtamtrWbhw4WFft2bNGu666y7MjBkzZvDwww/z4YcfsmzZMvbu3QvA6tWrmTMn8XPBFcK6UV5dw4zWneyZ8GWyU9KiLkdERGRQW7x4MTfeeGNnCHvsscd45plnWL58OSNHjqSsrIzZs2dz0UUXYUeZp52RkcH69es/8bpdu3axcuVKXnjhBXJzcztvzr18+XLmzp3L+vXraWtro7a2NuGfFxTCurXzpeeYa80MPfmzUZciIiLSv44wYpUohYWFlJSU8N5771FaWkpWVhZjxozhpptuYvPmzQwZMoQDBw7w4YcfMmbMmCOuy925/fbbP/G6jRs3smjRInJzcwHIzs4GYOPGjaxZswaAlJQURo0aldgPG0poCDOzBcDdQArwa3e/85Dn04E1wEygHFjs7vsSWdPRVP31KdL++xvMbS6jhuGcdNYFUZYjIiJyzFi0aBHr1q3jgw8+YPHixTz66KOUlpaybds20tLSmDRpEo2NjUddT7yv628Jm5hvZinAKuB84FTgcjM79ZDFrgEq3f1k4BfATxJVT0/tqsngzw2f5sCQMdR/4WcMGdo/aVhERORYt3jxYtauXcu6detYtGgR1dXVnHDCCaSlpbFp0ybeeeedHq2nu9edc845PP7445SXlwN0Ho6cP38+q1evBqCtrY3q6uoEfLpPSuTZkWcCe9x9r7s3A2uBQ2fTLQQeCh+vA+bb0Q70Jti0Mz7LlGW/I/f2XYyesyTKUkRERI4p06ZNo6amhvHjxzN27FiWLFnC1q1bmT59OmvWrOGUU07p0Xq6e920adO44447mDt3LgUFBdx8880A3H333WzatInp06czc+ZMdu3albDP2JW5e2JWbHYpsMDdrw3bVwBnufsNXZbZGS5THLb/L1ymrLv1zpo1y7du3ZqQmkVERI5Vu3fvZurUqVGXkdQO9x2a2TZ3P+y9D5PiOmFmdp2ZbTWzraWlpVGXIyIiItJriZyYfwCY0KWdH/YdbpliM0sFRhFM0P8Ydy8CiiAYCUtItSIiIpJUduzYwRVXXPGxvvT0dF566aWIKopNIkPYK8AUM5tMELYuA/7pkGWeBK4EtgCXAhs9UcdHRUREZFCZPn0627dvj7qMuCUshLl7q5ndADxDcImK+939dTP7IbDV3Z8EfgM8bGZ7gAqCoCYiIiIRcPejXghVDi+eMaSEXifM3Z8Cnjqk77tdHjcCixJZg4iIiBxdRkYG5eXl5OTkKIjFyN0pLy8nIyMjptfpivkiIiJCfn4+xcXF6AS4+GRkZJCfnx/TaxTCREREhLS0NCZPnhx1GceUpLhEhYiIiMhgoxAmIiIiEgGFMBEREZEIJOy2RYliZqVAz+7gGb9coNtbJ0kktE0GJm2XgUnbZeDRNhmY+mO7nOjueYd7IulCWH8ws63d3edJoqFtMjBpuwxM2i4Dj7bJwBT1dtHhSBEREZEIKISJiIiIREAh7PCKoi5APkHbZGDSdhmYtF0GHm2TgSnS7aI5YSIiIiIR0EiYiIiISAQUwrowswVm9qaZ7TGzFVHXcywxswlmtsnMdpnZ62b2zbA/28yeNbO3wn+zwn4zs3vCbfWamZ0R7ScYvMwsxcxeNbP/CtuTzeyl8Lv/DzM7LuxPD9t7wucnRVn3YGZmx5vZOjN7w8x2m9nZ2leiZWY3hb+7dprZ78wsQ/tK/zOz+82sxMx2dumLed8wsyvD5d8ysysTVa9CWMjMUoBVwPnAqcDlZnZqtFUdU1qBf3H3U4HZwPXh978CeM7dpwDPhW0IttOU8Oc6YHX/l3zM+Cawu0v7J8Av3P1koBK4Juy/BqgM+38RLieJcTfwtLufAhQQbB/tKxExs/HAcmCWu58GpACXoX0lCg8CCw7pi2nfMLNs4HvAWcCZwPc6gltfUwj7yJnAHnff6+7NwFpgYcQ1HTPc/X13/9/wcQ3BfyrjCbbBQ+FiDwEXh48XAms88CJwvJmN7eeyBz0zywe+CPw6bBtwDrAuXOTQbdKxrdYB88PlpQ+Z2Sjgc8BvANy92d2r0L4StVRgqJmlAsOA99G+0u/cfTNQcUh3rPvG3wPPunuFu1cCz/LJYNcnFMI+Mh7Y36VdHPZJPwuH5guBl4DR7v5++NQHwOjwsbZX//glcCvQHrZzgCp3bw3bXb/3zm0SPl8dLi99azJQCjwQHib+tZkNR/tKZNz9AHAX8C5B+KoGtqF9ZaCIdd/ot31GIUwGFDPLBJ4AbnT3g12f8+BUXp3O20/M7AKgxN23RV2LfEwqcAaw2t0LgTo+OrwCaF/pb+GhqoUEAXkcMJwEjZxI7wy0fUMh7CMHgAld2vlhn/QTM0sjCGCPuvvvw+4POw6dhP+WhP3aXon3t8BFZraP4PD8OQRzkY4PD7nAx7/3zm0SPj8KKO/Pgo8RxUCxu78UttcRhDLtK9E5F3jb3UvdvQX4PcH+o31lYIh13+i3fUYh7COvAFPCs1mOI5hU+WTENR0zwvkQvwF2u/u/dXnqSaDjzJQrgT926f/n8OyW2UB1l+Fm6QPufpu757v7JIL9YaO7LwE2AZeGix26TTq21aXh8gPmL87Bwt0/APab2afDrvnALrSvROldYLaZDQt/l3VsE+0rA0Os+8YzwBfMLCsc5fxC2NfndLHWLszsHwjmwKQA97v7jyMu6ZhhZn8HPA/s4KP5R7cTzAt7DJgIvAP8o7tXhL/ofkUw5F8PLHX3rf1e+DHCzOYBt7j7BWZ2EsHIWDbwKvAVd28yswzgYYL5fBXAZe6+N6qaBzMzO53gZInjgL3AUoI/qrWvRMTMfgAsJjjT+1XgWoJ5RNpX+pGZ/Q6YB+QCHxKc5fgHYtw3zOxqgv+DAH7s7g8kpF6FMBEREZH+p8ORIiIiIhFQCBMRERGJgEKYiIiISAQUwkREREQioBAmIiIiEgGFMBFJembWZmbbu/ysOPqrerzuSWa2s6/WJyLSIfXoi4iIDHgN7n561EWIiMRCI2EiMmiZ2T4z+6mZ7TCzl83s5LB/kpltNLPXzOw5M5sY9o82s/Vm9tfwZ064qhQz+3cze93M/mxmQ8Pll5vZrnA9ayP6mCKSpBTCRGQwGHrI4cjFXZ6rdvfpBFfG/mXYdy/wkLvPAB4F7gn77wH+x90LCO7H+HrYPwVY5e7TgCrgy2H/CqAwXM+yRH04ERmcdMV8EUl6Zlbr7pmH6d8HnOPue8MbxH/g7jlmVgaMdfeWsP99d881s1Ig392buqxjEvCsu08J298C0tx9pZk9DdQS3BblD+5em+CPKiKDiEbCRGSw824ex6Kpy+M2PppP+0VgFcGo2Stmpnm2ItJjCmEiMtgt7vLvlvDxC8Bl4eMlBDePB3gO+BqAmaWY2ajuVmpmQ4AJ7r4J+BYwCvjEaJyISHf0V5uIDAZDzWx7l/bT7t5xmYosM3uNYDTr8rDvG8ADZvavQCmwNOz/JlBkZtcQjHh9DXi/m/dMAR4Jg5oB97h7VZ99IhEZ9DQnTEQGrXBO2Cx3L4u6FhGRQ+lwpIiIiEgENBImIiIiEgGNhImIiIhEQCFMREREJAIKYSIiIiIRUAgTERERiYBCmIiIiEgEFMJEREREIvD/hTmZuaQRtL4AAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 예측해보기(predict)"
      ],
      "metadata": {
        "id": "7hsZEv6au9a_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e06685b-4c91-4d7b-804a-f36f83b0daf8",
        "id": "i9_Xzkqzu9a_"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "188/188 [==============================] - 2s 9ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[44,  0,  0, ...,  0,  0,  0],\n",
              "       [ 0, 57,  0, ...,  0,  0,  0],\n",
              "       [ 0,  0, 65, ...,  0,  0,  0],\n",
              "       ...,\n",
              "       [ 0,  0,  0, ..., 68,  0,  0],\n",
              "       [ 0,  0,  0, ...,  0, 67,  0],\n",
              "       [ 0,  0,  0, ...,  0,  0, 57]])"
            ]
          },
          "metadata": {},
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_p=pd.DataFrame(y_test)\n",
        "\n",
        "for i in range(1,101):\n",
        "  y_test_p.loc[y_test_p[i] == 1.0 , 'y_test'] = i  \n",
        "y_test_p.y_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "348ddc8c-b4b0-4b45-9b9b-a83b87cbce28",
        "id": "U35lO3JDu9a_"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       59.0\n",
              "1       68.0\n",
              "2       72.0\n",
              "3        2.0\n",
              "4       57.0\n",
              "        ... \n",
              "5995     3.0\n",
              "5996    87.0\n",
              "5997    40.0\n",
              "5998    68.0\n",
              "5999    37.0\n",
              "Name: y_test, Length: 6000, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ch1 = pd.DataFrame(np.rint(y_pred))\n",
        "# for i in range(1,101):\n",
        "#   ch1.loc[ch1[i] == 1.0 , 'pred'] = i \n",
        "\n",
        " \n",
        "#predict = pd.concat([data_test,ch1['pred'], y_test_p['y_test']],axis = 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "35e79746-e41a-4713-b942-733f497e3b6c",
        "id": "zZJWJ764u9a_"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        0    1    2    3    4    5    6    7    8    9  ...   92   93   94  \\\n",
              "0     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
              "1     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
              "2     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
              "3     0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
              "4     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
              "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
              "5995  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
              "5996  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
              "5997  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
              "5998  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
              "5999  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
              "\n",
              "       95   96   97   98   99  100  pred  \n",
              "0     0.0  0.0  0.0  0.0  0.0  0.0  59.0  \n",
              "1     0.0  0.0  0.0  0.0  0.0  0.0  68.0  \n",
              "2     0.0  0.0  0.0  0.0  0.0  0.0  72.0  \n",
              "3     0.0  0.0  0.0  0.0  0.0  0.0   2.0  \n",
              "4     0.0  0.0  0.0  0.0  0.0  0.0  57.0  \n",
              "...   ...  ...  ...  ...  ...  ...   ...  \n",
              "5995  0.0  0.0  0.0  0.0  0.0  0.0   3.0  \n",
              "5996  0.0  0.0  0.0  0.0  0.0  0.0  87.0  \n",
              "5997  0.0  0.0  0.0  0.0  0.0  0.0  40.0  \n",
              "5998  0.0  0.0  0.0  0.0  0.0  0.0  68.0  \n",
              "5999  0.0  0.0  0.0  0.0  0.0  0.0  45.0  \n",
              "\n",
              "[6000 rows x 102 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d66cc86c-ae23-4a9f-8fb4-dbad40e0c5ea\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "      <th>100</th>\n",
              "      <th>pred</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>59.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>68.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>72.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>57.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5995</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5996</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>87.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5997</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5998</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>68.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5999</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>45.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6000 rows × 102 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d66cc86c-ae23-4a9f-8fb4-dbad40e0c5ea')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d66cc86c-ae23-4a9f-8fb4-dbad40e0c5ea button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d66cc86c-ae23-4a9f-8fb4-dbad40e0c5ea');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ex=np.argmax(y_pred, axis=1)\n",
        "ex=ex.astype('float64')\n",
        "ex=pd.DataFrame(ex)\n",
        "ex"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "22f2ac13-d7ce-409c-db36-bf92c9ca0cad",
        "id": "kWobvWivu9a_"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         0\n",
              "0     59.0\n",
              "1     68.0\n",
              "2     72.0\n",
              "3      2.0\n",
              "4     57.0\n",
              "...    ...\n",
              "5995   3.0\n",
              "5996  87.0\n",
              "5997  40.0\n",
              "5998  68.0\n",
              "5999  45.0\n",
              "\n",
              "[6000 rows x 1 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-33a2dfcf-6ed7-497f-9b45-70f7cc7db973\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>59.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>68.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>72.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>57.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5995</th>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5996</th>\n",
              "      <td>87.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5997</th>\n",
              "      <td>40.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5998</th>\n",
              "      <td>68.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5999</th>\n",
              "      <td>45.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6000 rows × 1 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-33a2dfcf-6ed7-497f-9b45-70f7cc7db973')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-33a2dfcf-6ed7-497f-9b45-70f7cc7db973 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-33a2dfcf-6ed7-497f-9b45-70f7cc7db973');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ch1 = pd.DataFrame(np.rint(y_pred))\n",
        "ch1['pred'] = ex\n",
        "data_test.reset_index(inplace=True, drop=True)\n",
        "predict = pd.concat([data_test,ch1['pred'], y_test_p['y_test']],axis = 1)"
      ],
      "metadata": {
        "id": "a0vVXNj2u9a_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict['compare'] = np.where(predict['pred']==predict['y_test'],'yes', 'no')"
      ],
      "metadata": {
        "id": "RuxfqPeJu9bA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_1 = predict[predict[\"compare\"]=='no']\n",
        "predict_1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "outputId": "833df983-0f21-460a-de46-668f491d18e1",
        "id": "RmByZaU2u9bA"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             0         1         2         3         4         5         6  \\\n",
              "30    0.001909  0.008317  0.014489  0.020412  0.025995  0.031137  0.035801   \n",
              "46    0.096114  0.095362  0.095047  0.095447  0.096676  0.098677  0.101247   \n",
              "54   -0.041972 -0.048778 -0.055266 -0.061636 -0.067728 -0.073065 -0.076978   \n",
              "55    0.050686  0.056807  0.061922  0.066330  0.070463  0.074831  0.079964   \n",
              "71   -0.511413 -0.507140 -0.469849 -0.408462 -0.333769 -0.256404 -0.185178   \n",
              "...        ...       ...       ...       ...       ...       ...       ...   \n",
              "5971  0.005315 -0.043084 -0.083200 -0.110473 -0.122973 -0.121490 -0.109050   \n",
              "5981 -0.005281 -0.018280 -0.031669 -0.042939 -0.050464 -0.053648 -0.052817   \n",
              "5992 -0.084734 -0.091664 -0.098196 -0.101411 -0.098148 -0.085744 -0.062829   \n",
              "5994  0.008461  0.011468  0.012707  0.012096  0.009875  0.006558  0.002841   \n",
              "5999  0.052145  0.038833  0.024907  0.010402 -0.004429 -0.019087 -0.032867   \n",
              "\n",
              "             7         8         9  ...       249       250       251  \\\n",
              "30    0.040075  0.044195  0.048538  ...  0.044631  0.050399  0.057432   \n",
              "46    0.104083  0.106844  0.109202  ... -0.004398 -0.008214 -0.013081   \n",
              "54   -0.078796 -0.078050 -0.074630  ... -0.102289 -0.108650 -0.114708   \n",
              "55    0.086349  0.094365  0.104239  ...  0.125449  0.136371  0.147691   \n",
              "71   -0.126038 -0.081724 -0.052058  ...  0.031533  0.034874  0.038518   \n",
              "...        ...       ...       ...  ...       ...       ...       ...   \n",
              "5971 -0.090022 -0.069069 -0.050204  ... -0.027012 -0.029614 -0.031356   \n",
              "5981 -0.048958 -0.043371 -0.037307  ... -0.016826 -0.013496 -0.011275   \n",
              "5992 -0.030004  0.009808  0.051498  ...  0.072150  0.081214  0.090895   \n",
              "5994 -0.000513 -0.002782 -0.003375  ... -0.048723 -0.051678 -0.052151   \n",
              "5999 -0.044991 -0.054774 -0.061789  ... -0.056943 -0.054300 -0.054324   \n",
              "\n",
              "           252       253       254       255   pred  y_test  compare  \n",
              "30    0.066046  0.076371  0.088327  0.101648   85.0    36.0       no  \n",
              "46   -0.019006 -0.025674 -0.032320 -0.037693   83.0    80.0       no  \n",
              "54   -0.119718 -0.122885 -0.123496 -0.121095   20.0    71.0       no  \n",
              "55    0.159697  0.172642  0.186692  0.201860   41.0    50.0       no  \n",
              "71    0.042180  0.045745  0.049241  0.052764    4.0    93.0       no  \n",
              "...        ...       ...       ...       ...    ...     ...      ...  \n",
              "5971 -0.033020 -0.035244 -0.038329 -0.042129   95.0    23.0       no  \n",
              "5981 -0.010594 -0.011830 -0.015208 -0.020697   52.0    40.0       no  \n",
              "5992  0.101388  0.112932  0.125713  0.139775  100.0    83.0       no  \n",
              "5994 -0.050205 -0.046318 -0.041326 -0.036264   95.0    49.0       no  \n",
              "5999 -0.058068 -0.065599 -0.075635 -0.085448   45.0    37.0       no  \n",
              "\n",
              "[545 rows x 259 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-22dffb56-3200-49eb-89c1-97f537806f07\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>249</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "      <th>pred</th>\n",
              "      <th>y_test</th>\n",
              "      <th>compare</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.001909</td>\n",
              "      <td>0.008317</td>\n",
              "      <td>0.014489</td>\n",
              "      <td>0.020412</td>\n",
              "      <td>0.025995</td>\n",
              "      <td>0.031137</td>\n",
              "      <td>0.035801</td>\n",
              "      <td>0.040075</td>\n",
              "      <td>0.044195</td>\n",
              "      <td>0.048538</td>\n",
              "      <td>...</td>\n",
              "      <td>0.044631</td>\n",
              "      <td>0.050399</td>\n",
              "      <td>0.057432</td>\n",
              "      <td>0.066046</td>\n",
              "      <td>0.076371</td>\n",
              "      <td>0.088327</td>\n",
              "      <td>0.101648</td>\n",
              "      <td>85.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>0.096114</td>\n",
              "      <td>0.095362</td>\n",
              "      <td>0.095047</td>\n",
              "      <td>0.095447</td>\n",
              "      <td>0.096676</td>\n",
              "      <td>0.098677</td>\n",
              "      <td>0.101247</td>\n",
              "      <td>0.104083</td>\n",
              "      <td>0.106844</td>\n",
              "      <td>0.109202</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.004398</td>\n",
              "      <td>-0.008214</td>\n",
              "      <td>-0.013081</td>\n",
              "      <td>-0.019006</td>\n",
              "      <td>-0.025674</td>\n",
              "      <td>-0.032320</td>\n",
              "      <td>-0.037693</td>\n",
              "      <td>83.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>-0.041972</td>\n",
              "      <td>-0.048778</td>\n",
              "      <td>-0.055266</td>\n",
              "      <td>-0.061636</td>\n",
              "      <td>-0.067728</td>\n",
              "      <td>-0.073065</td>\n",
              "      <td>-0.076978</td>\n",
              "      <td>-0.078796</td>\n",
              "      <td>-0.078050</td>\n",
              "      <td>-0.074630</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.102289</td>\n",
              "      <td>-0.108650</td>\n",
              "      <td>-0.114708</td>\n",
              "      <td>-0.119718</td>\n",
              "      <td>-0.122885</td>\n",
              "      <td>-0.123496</td>\n",
              "      <td>-0.121095</td>\n",
              "      <td>20.0</td>\n",
              "      <td>71.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>0.050686</td>\n",
              "      <td>0.056807</td>\n",
              "      <td>0.061922</td>\n",
              "      <td>0.066330</td>\n",
              "      <td>0.070463</td>\n",
              "      <td>0.074831</td>\n",
              "      <td>0.079964</td>\n",
              "      <td>0.086349</td>\n",
              "      <td>0.094365</td>\n",
              "      <td>0.104239</td>\n",
              "      <td>...</td>\n",
              "      <td>0.125449</td>\n",
              "      <td>0.136371</td>\n",
              "      <td>0.147691</td>\n",
              "      <td>0.159697</td>\n",
              "      <td>0.172642</td>\n",
              "      <td>0.186692</td>\n",
              "      <td>0.201860</td>\n",
              "      <td>41.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>-0.511413</td>\n",
              "      <td>-0.507140</td>\n",
              "      <td>-0.469849</td>\n",
              "      <td>-0.408462</td>\n",
              "      <td>-0.333769</td>\n",
              "      <td>-0.256404</td>\n",
              "      <td>-0.185178</td>\n",
              "      <td>-0.126038</td>\n",
              "      <td>-0.081724</td>\n",
              "      <td>-0.052058</td>\n",
              "      <td>...</td>\n",
              "      <td>0.031533</td>\n",
              "      <td>0.034874</td>\n",
              "      <td>0.038518</td>\n",
              "      <td>0.042180</td>\n",
              "      <td>0.045745</td>\n",
              "      <td>0.049241</td>\n",
              "      <td>0.052764</td>\n",
              "      <td>4.0</td>\n",
              "      <td>93.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5971</th>\n",
              "      <td>0.005315</td>\n",
              "      <td>-0.043084</td>\n",
              "      <td>-0.083200</td>\n",
              "      <td>-0.110473</td>\n",
              "      <td>-0.122973</td>\n",
              "      <td>-0.121490</td>\n",
              "      <td>-0.109050</td>\n",
              "      <td>-0.090022</td>\n",
              "      <td>-0.069069</td>\n",
              "      <td>-0.050204</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.027012</td>\n",
              "      <td>-0.029614</td>\n",
              "      <td>-0.031356</td>\n",
              "      <td>-0.033020</td>\n",
              "      <td>-0.035244</td>\n",
              "      <td>-0.038329</td>\n",
              "      <td>-0.042129</td>\n",
              "      <td>95.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5981</th>\n",
              "      <td>-0.005281</td>\n",
              "      <td>-0.018280</td>\n",
              "      <td>-0.031669</td>\n",
              "      <td>-0.042939</td>\n",
              "      <td>-0.050464</td>\n",
              "      <td>-0.053648</td>\n",
              "      <td>-0.052817</td>\n",
              "      <td>-0.048958</td>\n",
              "      <td>-0.043371</td>\n",
              "      <td>-0.037307</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.016826</td>\n",
              "      <td>-0.013496</td>\n",
              "      <td>-0.011275</td>\n",
              "      <td>-0.010594</td>\n",
              "      <td>-0.011830</td>\n",
              "      <td>-0.015208</td>\n",
              "      <td>-0.020697</td>\n",
              "      <td>52.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5992</th>\n",
              "      <td>-0.084734</td>\n",
              "      <td>-0.091664</td>\n",
              "      <td>-0.098196</td>\n",
              "      <td>-0.101411</td>\n",
              "      <td>-0.098148</td>\n",
              "      <td>-0.085744</td>\n",
              "      <td>-0.062829</td>\n",
              "      <td>-0.030004</td>\n",
              "      <td>0.009808</td>\n",
              "      <td>0.051498</td>\n",
              "      <td>...</td>\n",
              "      <td>0.072150</td>\n",
              "      <td>0.081214</td>\n",
              "      <td>0.090895</td>\n",
              "      <td>0.101388</td>\n",
              "      <td>0.112932</td>\n",
              "      <td>0.125713</td>\n",
              "      <td>0.139775</td>\n",
              "      <td>100.0</td>\n",
              "      <td>83.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5994</th>\n",
              "      <td>0.008461</td>\n",
              "      <td>0.011468</td>\n",
              "      <td>0.012707</td>\n",
              "      <td>0.012096</td>\n",
              "      <td>0.009875</td>\n",
              "      <td>0.006558</td>\n",
              "      <td>0.002841</td>\n",
              "      <td>-0.000513</td>\n",
              "      <td>-0.002782</td>\n",
              "      <td>-0.003375</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.048723</td>\n",
              "      <td>-0.051678</td>\n",
              "      <td>-0.052151</td>\n",
              "      <td>-0.050205</td>\n",
              "      <td>-0.046318</td>\n",
              "      <td>-0.041326</td>\n",
              "      <td>-0.036264</td>\n",
              "      <td>95.0</td>\n",
              "      <td>49.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5999</th>\n",
              "      <td>0.052145</td>\n",
              "      <td>0.038833</td>\n",
              "      <td>0.024907</td>\n",
              "      <td>0.010402</td>\n",
              "      <td>-0.004429</td>\n",
              "      <td>-0.019087</td>\n",
              "      <td>-0.032867</td>\n",
              "      <td>-0.044991</td>\n",
              "      <td>-0.054774</td>\n",
              "      <td>-0.061789</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.056943</td>\n",
              "      <td>-0.054300</td>\n",
              "      <td>-0.054324</td>\n",
              "      <td>-0.058068</td>\n",
              "      <td>-0.065599</td>\n",
              "      <td>-0.075635</td>\n",
              "      <td>-0.085448</td>\n",
              "      <td>45.0</td>\n",
              "      <td>37.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>545 rows × 259 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-22dffb56-3200-49eb-89c1-97f537806f07')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-22dffb56-3200-49eb-89c1-97f537806f07 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-22dffb56-3200-49eb-89c1-97f537806f07');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 163
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.lowpass"
      ],
      "metadata": {
        "id": "C3YzYPPIOiqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46bf1084-0e1c-4a75-faae-05630c1f105b",
        "id": "lCgPKknoOiqX"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip '/content/drive/MyDrive/model_line.zip' -d '/content/drive/MyDrive/model_line'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c6ba509-4e6d-42b3-8562-1567bcc35d70",
        "id": "iYG2TuBLOiqX"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/model_line.zip\n",
            "   creating: /content/drive/MyDrive/model_line/model_line/\n",
            "   creating: /content/drive/MyDrive/model_line/model_line/band/\n",
            "  inflating: /content/drive/MyDrive/model_line/model_line/band/test_band_line.csv  \n",
            "  inflating: /content/drive/MyDrive/model_line/model_line/band/train_band_line.csv  \n",
            "  inflating: /content/drive/MyDrive/model_line/model_line/band/valid_band_line.csv  \n",
            "   creating: /content/drive/MyDrive/model_line/model_line/high/\n",
            "  inflating: /content/drive/MyDrive/model_line/model_line/high/test_high_line.csv  \n",
            "  inflating: /content/drive/MyDrive/model_line/model_line/high/train_high_line.csv  \n",
            "  inflating: /content/drive/MyDrive/model_line/model_line/high/valid_high_line.csv  \n",
            "   creating: /content/drive/MyDrive/model_line/model_line/low/\n",
            "  inflating: /content/drive/MyDrive/model_line/model_line/low/test_low_line.csv  \n",
            "  inflating: /content/drive/MyDrive/model_line/model_line/low/train_low_line.csv  \n",
            "  inflating: /content/drive/MyDrive/model_line/model_line/low/valid_low_line.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9IwB13flOiqX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import itertools\n",
        "import time\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import (CosineAnnealingLR,\n",
        "                                      CosineAnnealingWarmRestarts,\n",
        "                                      StepLR,\n",
        "                                      ExponentialLR)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import accuracy_score, auc, f1_score, precision_score, recall_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm, re, sys\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "from keras.layers import Conv2D, Conv1D, MaxPooling2D, MaxPooling1D, Flatten, BatchNormalization, Dense\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.callbacks import CSVLogger, ModelCheckpoint"
      ],
      "metadata": {
        "id": "3omMrMIkOiqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train = pd.read_csv('/content/drive/MyDrive/model_line/model_line/low/train_low_line.csv', header=None)\n",
        "data_test = pd.read_csv('/content/drive/MyDrive/model_line/model_line/low/test_low_line.csv', header=None)\n",
        "data_val= pd.read_csv('/content/drive/MyDrive/model_line/model_line/low/valid_low_line.csv', header=None)"
      ],
      "metadata": {
        "id": "3Sqlcw_OOiqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "outputId": "e50556ee-4dcd-4ce8-ccd3-13f7e6ace318",
        "id": "WY-yP6RdOiqY"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           0         1         2         3         4         5         6    \\\n",
              "0     1.754140  1.756590  1.758298  1.759022  1.758643  1.757236  1.755108   \n",
              "1     1.590544  1.594114  1.597217  1.599791  1.601832  1.603404  1.604643   \n",
              "2     1.706668  1.708791  1.709560  1.709006  1.707349  1.704986  1.702446   \n",
              "3     2.295298  2.411576  2.487373  2.511645  2.480919  2.400025  2.281178   \n",
              "4     1.705033  1.707353  1.709552  1.711737  1.714067  1.716690  1.719673   \n",
              "...        ...       ...       ...       ...       ...       ...       ...   \n",
              "5995  1.613963  1.619528  1.627568  1.636760  1.645732  1.653393  1.659145   \n",
              "5996  1.696360  1.696858  1.698535  1.701395  1.705272  1.709867  1.714797   \n",
              "5997  1.547967  1.549320  1.550668  1.552017  1.553337  1.554553  1.555551   \n",
              "5998  1.615198  1.615168  1.615529  1.616101  1.616700  1.617167  1.617373   \n",
              "5999  1.692739  1.619039  1.551620  1.499212  1.467524  1.458448  1.470120   \n",
              "\n",
              "           7         8         9    ...       246       247       248  \\\n",
              "0     1.752784  1.750938  1.750271  ...  1.698640  1.701614  1.704703   \n",
              "1     1.605734  1.606893  1.608334  ...  1.656219  1.659175  1.662269   \n",
              "2     1.700306  1.699091  1.699173  ...  1.656174  1.659317  1.661245   \n",
              "3     2.141574  2.000075  1.873748  ...  1.688096  1.699816  1.720896   \n",
              "4     1.722960  1.726364  1.729587  ...  1.692391  1.691363  1.688594   \n",
              "...        ...       ...       ...  ...       ...       ...       ...   \n",
              "5995  1.662945  1.665238  1.666797  ...  1.784428  1.755844  1.730389   \n",
              "5996  1.719637  1.723966  1.727412  ...  1.661005  1.699782  1.744423   \n",
              "5997  1.556214  1.556469  1.556341  ...  1.596446  1.592691  1.591059   \n",
              "5998  1.617227  1.616682  1.615755  ...  1.597461  1.595455  1.593952   \n",
              "5999  1.497716  1.534729  1.574390  ...  1.790030  1.801024  1.812046   \n",
              "\n",
              "           249       250       251       252       253       254       255  \n",
              "0     1.707794  1.710813  1.713725  1.716508  1.719121  1.721462  1.723356  \n",
              "1     1.665042  1.667068  1.668064  1.667958  1.666907  1.665254  1.663453  \n",
              "2     1.662187  1.662586  1.662967  1.663795  1.665342  1.667614  1.670345  \n",
              "3     1.742862  1.759763  1.768718  1.769686  1.764689  1.756791  1.749115  \n",
              "4     1.684654  1.680340  1.676541  1.674078  1.673557  1.675256  1.679065  \n",
              "...        ...       ...       ...       ...       ...       ...       ...  \n",
              "5995  1.708044  1.688545  1.671541  1.656708  1.643799  1.632639  1.623080  \n",
              "5996  1.789141  1.826705  1.849760  1.852416  1.831714  1.788555  1.727819  \n",
              "5997  1.591456  1.593591  1.597004  1.601126  1.605314  1.608896  1.611187  \n",
              "5998  1.593380  1.593925  1.595443  1.597461  1.599283  1.600176  1.599565  \n",
              "5999  1.822807  1.833107  1.842928  1.852499  1.862298  1.872954  1.885091  \n",
              "\n",
              "[6000 rows x 256 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-67a5ee3c-a9d1-4cd7-bc6f-8b530374de78\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>246</th>\n",
              "      <th>247</th>\n",
              "      <th>248</th>\n",
              "      <th>249</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.754140</td>\n",
              "      <td>1.756590</td>\n",
              "      <td>1.758298</td>\n",
              "      <td>1.759022</td>\n",
              "      <td>1.758643</td>\n",
              "      <td>1.757236</td>\n",
              "      <td>1.755108</td>\n",
              "      <td>1.752784</td>\n",
              "      <td>1.750938</td>\n",
              "      <td>1.750271</td>\n",
              "      <td>...</td>\n",
              "      <td>1.698640</td>\n",
              "      <td>1.701614</td>\n",
              "      <td>1.704703</td>\n",
              "      <td>1.707794</td>\n",
              "      <td>1.710813</td>\n",
              "      <td>1.713725</td>\n",
              "      <td>1.716508</td>\n",
              "      <td>1.719121</td>\n",
              "      <td>1.721462</td>\n",
              "      <td>1.723356</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.590544</td>\n",
              "      <td>1.594114</td>\n",
              "      <td>1.597217</td>\n",
              "      <td>1.599791</td>\n",
              "      <td>1.601832</td>\n",
              "      <td>1.603404</td>\n",
              "      <td>1.604643</td>\n",
              "      <td>1.605734</td>\n",
              "      <td>1.606893</td>\n",
              "      <td>1.608334</td>\n",
              "      <td>...</td>\n",
              "      <td>1.656219</td>\n",
              "      <td>1.659175</td>\n",
              "      <td>1.662269</td>\n",
              "      <td>1.665042</td>\n",
              "      <td>1.667068</td>\n",
              "      <td>1.668064</td>\n",
              "      <td>1.667958</td>\n",
              "      <td>1.666907</td>\n",
              "      <td>1.665254</td>\n",
              "      <td>1.663453</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.706668</td>\n",
              "      <td>1.708791</td>\n",
              "      <td>1.709560</td>\n",
              "      <td>1.709006</td>\n",
              "      <td>1.707349</td>\n",
              "      <td>1.704986</td>\n",
              "      <td>1.702446</td>\n",
              "      <td>1.700306</td>\n",
              "      <td>1.699091</td>\n",
              "      <td>1.699173</td>\n",
              "      <td>...</td>\n",
              "      <td>1.656174</td>\n",
              "      <td>1.659317</td>\n",
              "      <td>1.661245</td>\n",
              "      <td>1.662187</td>\n",
              "      <td>1.662586</td>\n",
              "      <td>1.662967</td>\n",
              "      <td>1.663795</td>\n",
              "      <td>1.665342</td>\n",
              "      <td>1.667614</td>\n",
              "      <td>1.670345</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2.295298</td>\n",
              "      <td>2.411576</td>\n",
              "      <td>2.487373</td>\n",
              "      <td>2.511645</td>\n",
              "      <td>2.480919</td>\n",
              "      <td>2.400025</td>\n",
              "      <td>2.281178</td>\n",
              "      <td>2.141574</td>\n",
              "      <td>2.000075</td>\n",
              "      <td>1.873748</td>\n",
              "      <td>...</td>\n",
              "      <td>1.688096</td>\n",
              "      <td>1.699816</td>\n",
              "      <td>1.720896</td>\n",
              "      <td>1.742862</td>\n",
              "      <td>1.759763</td>\n",
              "      <td>1.768718</td>\n",
              "      <td>1.769686</td>\n",
              "      <td>1.764689</td>\n",
              "      <td>1.756791</td>\n",
              "      <td>1.749115</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.705033</td>\n",
              "      <td>1.707353</td>\n",
              "      <td>1.709552</td>\n",
              "      <td>1.711737</td>\n",
              "      <td>1.714067</td>\n",
              "      <td>1.716690</td>\n",
              "      <td>1.719673</td>\n",
              "      <td>1.722960</td>\n",
              "      <td>1.726364</td>\n",
              "      <td>1.729587</td>\n",
              "      <td>...</td>\n",
              "      <td>1.692391</td>\n",
              "      <td>1.691363</td>\n",
              "      <td>1.688594</td>\n",
              "      <td>1.684654</td>\n",
              "      <td>1.680340</td>\n",
              "      <td>1.676541</td>\n",
              "      <td>1.674078</td>\n",
              "      <td>1.673557</td>\n",
              "      <td>1.675256</td>\n",
              "      <td>1.679065</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5995</th>\n",
              "      <td>1.613963</td>\n",
              "      <td>1.619528</td>\n",
              "      <td>1.627568</td>\n",
              "      <td>1.636760</td>\n",
              "      <td>1.645732</td>\n",
              "      <td>1.653393</td>\n",
              "      <td>1.659145</td>\n",
              "      <td>1.662945</td>\n",
              "      <td>1.665238</td>\n",
              "      <td>1.666797</td>\n",
              "      <td>...</td>\n",
              "      <td>1.784428</td>\n",
              "      <td>1.755844</td>\n",
              "      <td>1.730389</td>\n",
              "      <td>1.708044</td>\n",
              "      <td>1.688545</td>\n",
              "      <td>1.671541</td>\n",
              "      <td>1.656708</td>\n",
              "      <td>1.643799</td>\n",
              "      <td>1.632639</td>\n",
              "      <td>1.623080</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5996</th>\n",
              "      <td>1.696360</td>\n",
              "      <td>1.696858</td>\n",
              "      <td>1.698535</td>\n",
              "      <td>1.701395</td>\n",
              "      <td>1.705272</td>\n",
              "      <td>1.709867</td>\n",
              "      <td>1.714797</td>\n",
              "      <td>1.719637</td>\n",
              "      <td>1.723966</td>\n",
              "      <td>1.727412</td>\n",
              "      <td>...</td>\n",
              "      <td>1.661005</td>\n",
              "      <td>1.699782</td>\n",
              "      <td>1.744423</td>\n",
              "      <td>1.789141</td>\n",
              "      <td>1.826705</td>\n",
              "      <td>1.849760</td>\n",
              "      <td>1.852416</td>\n",
              "      <td>1.831714</td>\n",
              "      <td>1.788555</td>\n",
              "      <td>1.727819</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5997</th>\n",
              "      <td>1.547967</td>\n",
              "      <td>1.549320</td>\n",
              "      <td>1.550668</td>\n",
              "      <td>1.552017</td>\n",
              "      <td>1.553337</td>\n",
              "      <td>1.554553</td>\n",
              "      <td>1.555551</td>\n",
              "      <td>1.556214</td>\n",
              "      <td>1.556469</td>\n",
              "      <td>1.556341</td>\n",
              "      <td>...</td>\n",
              "      <td>1.596446</td>\n",
              "      <td>1.592691</td>\n",
              "      <td>1.591059</td>\n",
              "      <td>1.591456</td>\n",
              "      <td>1.593591</td>\n",
              "      <td>1.597004</td>\n",
              "      <td>1.601126</td>\n",
              "      <td>1.605314</td>\n",
              "      <td>1.608896</td>\n",
              "      <td>1.611187</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5998</th>\n",
              "      <td>1.615198</td>\n",
              "      <td>1.615168</td>\n",
              "      <td>1.615529</td>\n",
              "      <td>1.616101</td>\n",
              "      <td>1.616700</td>\n",
              "      <td>1.617167</td>\n",
              "      <td>1.617373</td>\n",
              "      <td>1.617227</td>\n",
              "      <td>1.616682</td>\n",
              "      <td>1.615755</td>\n",
              "      <td>...</td>\n",
              "      <td>1.597461</td>\n",
              "      <td>1.595455</td>\n",
              "      <td>1.593952</td>\n",
              "      <td>1.593380</td>\n",
              "      <td>1.593925</td>\n",
              "      <td>1.595443</td>\n",
              "      <td>1.597461</td>\n",
              "      <td>1.599283</td>\n",
              "      <td>1.600176</td>\n",
              "      <td>1.599565</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5999</th>\n",
              "      <td>1.692739</td>\n",
              "      <td>1.619039</td>\n",
              "      <td>1.551620</td>\n",
              "      <td>1.499212</td>\n",
              "      <td>1.467524</td>\n",
              "      <td>1.458448</td>\n",
              "      <td>1.470120</td>\n",
              "      <td>1.497716</td>\n",
              "      <td>1.534729</td>\n",
              "      <td>1.574390</td>\n",
              "      <td>...</td>\n",
              "      <td>1.790030</td>\n",
              "      <td>1.801024</td>\n",
              "      <td>1.812046</td>\n",
              "      <td>1.822807</td>\n",
              "      <td>1.833107</td>\n",
              "      <td>1.842928</td>\n",
              "      <td>1.852499</td>\n",
              "      <td>1.862298</td>\n",
              "      <td>1.872954</td>\n",
              "      <td>1.885091</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6000 rows × 256 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-67a5ee3c-a9d1-4cd7-bc6f-8b530374de78')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-67a5ee3c-a9d1-4cd7-bc6f-8b530374de78 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-67a5ee3c-a9d1-4cd7-bc6f-8b530374de78');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a29733d-62fb-4012-c140-8fc0f17f46db",
        "id": "MarJi45zOiqY"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(24000, 256)"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 라벨생성"
      ],
      "metadata": {
        "id": "yNxaWPtzOiqY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "arr = np.array([])\n",
        "\n",
        "for i in range(1,101):\n",
        "  for j in range(240):\n",
        "    arr = np.append(arr, np.array([i]))"
      ],
      "metadata": {
        "id": "D5OBuZ1gOiqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train[256]=arr"
      ],
      "metadata": {
        "id": "Ns5w6anOOiqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "outputId": "1800d58a-d0d8-4d3d-8f1b-8f7acaf260a9",
        "id": "Pls7pK6GOiqY"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            0         1         2         3         4         5         6    \\\n",
              "0      1.697478  1.713703  1.728594  1.741144  1.750717  1.757105  1.760490   \n",
              "1      1.748610  1.747925  1.746924  1.746097  1.745874  1.746535  1.748158   \n",
              "2      1.709674  1.710317  1.711171  1.712183  1.713227  1.714166  1.714897   \n",
              "3      1.726979  1.726187  1.724966  1.723451  1.721925  1.720776  1.720402   \n",
              "4      1.889883  1.889929  1.891485  1.895719  1.903396  1.914786  1.929653   \n",
              "...         ...       ...       ...       ...       ...       ...       ...   \n",
              "23995  1.664913  1.642295  1.622705  1.606331  1.593275  1.583516  1.576874   \n",
              "23996  1.520984  1.516451  1.512894  1.510777  1.510324  1.511420  1.513570   \n",
              "23997  1.656066  1.659062  1.662113  1.665637  1.669814  1.674541  1.679471   \n",
              "23998  1.575260  1.571225  1.568130  1.566590  1.567050  1.569659  1.574187   \n",
              "23999  1.568698  1.573141  1.578992  1.585492  1.591738  1.596858  1.600196   \n",
              "\n",
              "            7         8         9    ...       247       248       249  \\\n",
              "0      1.761353  1.760349  1.758191  ...  1.733012  1.733423  1.734708   \n",
              "1      1.750626  1.753678  1.756978  ...  1.694451  1.696899  1.699745   \n",
              "2      1.715389  1.715680  1.715858  ...  1.722441  1.724729  1.726115   \n",
              "3      1.721109  1.723015  1.726001  ...  1.739826  1.739925  1.740406   \n",
              "4      1.947339  1.966906  1.987309  ...  1.914361  1.928167  1.944398   \n",
              "...         ...       ...       ...  ...       ...       ...       ...   \n",
              "23995  1.572981  1.571289  1.571116  ...  1.609903  1.611269  1.612058   \n",
              "23996  1.515967  1.517666  1.517848  ...  1.613789  1.614391  1.617356   \n",
              "23997  1.684122  1.688029  1.690891  ...  1.600670  1.597018  1.594806   \n",
              "23998  1.580032  1.586320  1.592084  ...  1.579022  1.578733  1.577374   \n",
              "23999  1.601434  1.600648  1.598281  ...  1.592516  1.596574  1.599401   \n",
              "\n",
              "            250       251       252       253       254       255    256  \n",
              "0      1.736879  1.739700  1.742739  1.745491  1.747515  1.748556    1.0  \n",
              "1      1.702543  1.704941  1.706748  1.707952  1.708690  1.709187    1.0  \n",
              "2      1.726820  1.727122  1.727259  1.727362  1.727430  1.727353    1.0  \n",
              "3      1.741383  1.742927  1.745059  1.747751  1.750910  1.754365    1.0  \n",
              "4      1.962799  1.982890  2.004016  2.025433  2.046399  2.066255    1.0  \n",
              "...         ...       ...       ...       ...       ...       ...    ...  \n",
              "23995  1.612049  1.611103  1.609213  1.606538  1.603402  1.600248  100.0  \n",
              "23996  1.622495  1.629054  1.636056  1.642622  1.648187  1.652595  100.0  \n",
              "23997  1.593269  1.591762  1.589812  1.587139  1.583680  1.579595  100.0  \n",
              "23998  1.575056  1.572128  1.569131  1.566724  1.565555  1.566133  100.0  \n",
              "23999  1.600918  1.601328  1.601033  1.600493  1.600067  1.599890  100.0  \n",
              "\n",
              "[24000 rows x 257 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-48f6dccc-83ac-43a0-8d89-812d61aa6fd3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>247</th>\n",
              "      <th>248</th>\n",
              "      <th>249</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "      <th>256</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.697478</td>\n",
              "      <td>1.713703</td>\n",
              "      <td>1.728594</td>\n",
              "      <td>1.741144</td>\n",
              "      <td>1.750717</td>\n",
              "      <td>1.757105</td>\n",
              "      <td>1.760490</td>\n",
              "      <td>1.761353</td>\n",
              "      <td>1.760349</td>\n",
              "      <td>1.758191</td>\n",
              "      <td>...</td>\n",
              "      <td>1.733012</td>\n",
              "      <td>1.733423</td>\n",
              "      <td>1.734708</td>\n",
              "      <td>1.736879</td>\n",
              "      <td>1.739700</td>\n",
              "      <td>1.742739</td>\n",
              "      <td>1.745491</td>\n",
              "      <td>1.747515</td>\n",
              "      <td>1.748556</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.748610</td>\n",
              "      <td>1.747925</td>\n",
              "      <td>1.746924</td>\n",
              "      <td>1.746097</td>\n",
              "      <td>1.745874</td>\n",
              "      <td>1.746535</td>\n",
              "      <td>1.748158</td>\n",
              "      <td>1.750626</td>\n",
              "      <td>1.753678</td>\n",
              "      <td>1.756978</td>\n",
              "      <td>...</td>\n",
              "      <td>1.694451</td>\n",
              "      <td>1.696899</td>\n",
              "      <td>1.699745</td>\n",
              "      <td>1.702543</td>\n",
              "      <td>1.704941</td>\n",
              "      <td>1.706748</td>\n",
              "      <td>1.707952</td>\n",
              "      <td>1.708690</td>\n",
              "      <td>1.709187</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.709674</td>\n",
              "      <td>1.710317</td>\n",
              "      <td>1.711171</td>\n",
              "      <td>1.712183</td>\n",
              "      <td>1.713227</td>\n",
              "      <td>1.714166</td>\n",
              "      <td>1.714897</td>\n",
              "      <td>1.715389</td>\n",
              "      <td>1.715680</td>\n",
              "      <td>1.715858</td>\n",
              "      <td>...</td>\n",
              "      <td>1.722441</td>\n",
              "      <td>1.724729</td>\n",
              "      <td>1.726115</td>\n",
              "      <td>1.726820</td>\n",
              "      <td>1.727122</td>\n",
              "      <td>1.727259</td>\n",
              "      <td>1.727362</td>\n",
              "      <td>1.727430</td>\n",
              "      <td>1.727353</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.726979</td>\n",
              "      <td>1.726187</td>\n",
              "      <td>1.724966</td>\n",
              "      <td>1.723451</td>\n",
              "      <td>1.721925</td>\n",
              "      <td>1.720776</td>\n",
              "      <td>1.720402</td>\n",
              "      <td>1.721109</td>\n",
              "      <td>1.723015</td>\n",
              "      <td>1.726001</td>\n",
              "      <td>...</td>\n",
              "      <td>1.739826</td>\n",
              "      <td>1.739925</td>\n",
              "      <td>1.740406</td>\n",
              "      <td>1.741383</td>\n",
              "      <td>1.742927</td>\n",
              "      <td>1.745059</td>\n",
              "      <td>1.747751</td>\n",
              "      <td>1.750910</td>\n",
              "      <td>1.754365</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.889883</td>\n",
              "      <td>1.889929</td>\n",
              "      <td>1.891485</td>\n",
              "      <td>1.895719</td>\n",
              "      <td>1.903396</td>\n",
              "      <td>1.914786</td>\n",
              "      <td>1.929653</td>\n",
              "      <td>1.947339</td>\n",
              "      <td>1.966906</td>\n",
              "      <td>1.987309</td>\n",
              "      <td>...</td>\n",
              "      <td>1.914361</td>\n",
              "      <td>1.928167</td>\n",
              "      <td>1.944398</td>\n",
              "      <td>1.962799</td>\n",
              "      <td>1.982890</td>\n",
              "      <td>2.004016</td>\n",
              "      <td>2.025433</td>\n",
              "      <td>2.046399</td>\n",
              "      <td>2.066255</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23995</th>\n",
              "      <td>1.664913</td>\n",
              "      <td>1.642295</td>\n",
              "      <td>1.622705</td>\n",
              "      <td>1.606331</td>\n",
              "      <td>1.593275</td>\n",
              "      <td>1.583516</td>\n",
              "      <td>1.576874</td>\n",
              "      <td>1.572981</td>\n",
              "      <td>1.571289</td>\n",
              "      <td>1.571116</td>\n",
              "      <td>...</td>\n",
              "      <td>1.609903</td>\n",
              "      <td>1.611269</td>\n",
              "      <td>1.612058</td>\n",
              "      <td>1.612049</td>\n",
              "      <td>1.611103</td>\n",
              "      <td>1.609213</td>\n",
              "      <td>1.606538</td>\n",
              "      <td>1.603402</td>\n",
              "      <td>1.600248</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23996</th>\n",
              "      <td>1.520984</td>\n",
              "      <td>1.516451</td>\n",
              "      <td>1.512894</td>\n",
              "      <td>1.510777</td>\n",
              "      <td>1.510324</td>\n",
              "      <td>1.511420</td>\n",
              "      <td>1.513570</td>\n",
              "      <td>1.515967</td>\n",
              "      <td>1.517666</td>\n",
              "      <td>1.517848</td>\n",
              "      <td>...</td>\n",
              "      <td>1.613789</td>\n",
              "      <td>1.614391</td>\n",
              "      <td>1.617356</td>\n",
              "      <td>1.622495</td>\n",
              "      <td>1.629054</td>\n",
              "      <td>1.636056</td>\n",
              "      <td>1.642622</td>\n",
              "      <td>1.648187</td>\n",
              "      <td>1.652595</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23997</th>\n",
              "      <td>1.656066</td>\n",
              "      <td>1.659062</td>\n",
              "      <td>1.662113</td>\n",
              "      <td>1.665637</td>\n",
              "      <td>1.669814</td>\n",
              "      <td>1.674541</td>\n",
              "      <td>1.679471</td>\n",
              "      <td>1.684122</td>\n",
              "      <td>1.688029</td>\n",
              "      <td>1.690891</td>\n",
              "      <td>...</td>\n",
              "      <td>1.600670</td>\n",
              "      <td>1.597018</td>\n",
              "      <td>1.594806</td>\n",
              "      <td>1.593269</td>\n",
              "      <td>1.591762</td>\n",
              "      <td>1.589812</td>\n",
              "      <td>1.587139</td>\n",
              "      <td>1.583680</td>\n",
              "      <td>1.579595</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23998</th>\n",
              "      <td>1.575260</td>\n",
              "      <td>1.571225</td>\n",
              "      <td>1.568130</td>\n",
              "      <td>1.566590</td>\n",
              "      <td>1.567050</td>\n",
              "      <td>1.569659</td>\n",
              "      <td>1.574187</td>\n",
              "      <td>1.580032</td>\n",
              "      <td>1.586320</td>\n",
              "      <td>1.592084</td>\n",
              "      <td>...</td>\n",
              "      <td>1.579022</td>\n",
              "      <td>1.578733</td>\n",
              "      <td>1.577374</td>\n",
              "      <td>1.575056</td>\n",
              "      <td>1.572128</td>\n",
              "      <td>1.569131</td>\n",
              "      <td>1.566724</td>\n",
              "      <td>1.565555</td>\n",
              "      <td>1.566133</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23999</th>\n",
              "      <td>1.568698</td>\n",
              "      <td>1.573141</td>\n",
              "      <td>1.578992</td>\n",
              "      <td>1.585492</td>\n",
              "      <td>1.591738</td>\n",
              "      <td>1.596858</td>\n",
              "      <td>1.600196</td>\n",
              "      <td>1.601434</td>\n",
              "      <td>1.600648</td>\n",
              "      <td>1.598281</td>\n",
              "      <td>...</td>\n",
              "      <td>1.592516</td>\n",
              "      <td>1.596574</td>\n",
              "      <td>1.599401</td>\n",
              "      <td>1.600918</td>\n",
              "      <td>1.601328</td>\n",
              "      <td>1.601033</td>\n",
              "      <td>1.600493</td>\n",
              "      <td>1.600067</td>\n",
              "      <td>1.599890</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>24000 rows × 257 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-48f6dccc-83ac-43a0-8d89-812d61aa6fd3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-48f6dccc-83ac-43a0-8d89-812d61aa6fd3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-48f6dccc-83ac-43a0-8d89-812d61aa6fd3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "arr_t = np.array([])\n",
        "\n",
        "for i in range(1,101):\n",
        "  for j in range(60):\n",
        "    arr_t = np.append(arr_t, np.array([i]))"
      ],
      "metadata": {
        "id": "Fwd-fa7oOiqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arr_v = np.array([])\n",
        "\n",
        "for i in range(1,101):\n",
        "  for j in range(60):\n",
        "    arr_v = np.append(arr_v, np.array([i]))"
      ],
      "metadata": {
        "id": "0Xt009g_OiqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_test[256]=arr_t\n",
        "data_val[256]=arr_v\n",
        "data_val"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "outputId": "6e0b3262-0deb-41d9-8142-95632afe948a",
        "id": "7GrCHXwzOiqZ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           0         1         2         3         4         5         6    \\\n",
              "0     1.757857  1.761049  1.763568  1.765083  1.765388  1.764480  1.762595   \n",
              "1     1.544946  1.546049  1.547770  1.549887  1.552157  1.554327  1.556169   \n",
              "2     1.695559  1.697523  1.699848  1.702404  1.704980  1.707305  1.709099   \n",
              "3     1.775055  1.770967  1.765108  1.757903  1.750143  1.742942  1.737608   \n",
              "4     1.724734  1.725498  1.725171  1.724080  1.722647  1.721315  1.720477   \n",
              "...        ...       ...       ...       ...       ...       ...       ...   \n",
              "5995  1.528888  1.529840  1.528139  1.524104  1.518376  1.511774  1.505117   \n",
              "5996  1.515056  1.511174  1.509395  1.510004  1.512908  1.517656  1.523524   \n",
              "5997  1.565344  1.564102  1.561757  1.559405  1.558051  1.558382  1.560646   \n",
              "5998  1.597548  1.595698  1.594912  1.595167  1.596213  1.597637  1.598970   \n",
              "5999  1.599834  1.599558  1.598638  1.596714  1.593627  1.589487  1.584661   \n",
              "\n",
              "           7         8         9    ...       247       248       249  \\\n",
              "0     1.760185  1.757859  1.756279  ...  1.742644  1.741366  1.740599   \n",
              "1     1.557550  1.558479  1.559129  ...  1.553053  1.556314  1.560194   \n",
              "2     1.710126  1.710271  1.709585  ...  1.678932  1.678790  1.679747   \n",
              "3     1.735426  1.737369  1.743771  ...  1.681760  1.655004  1.641810   \n",
              "4     1.720418  1.721269  1.722978  ...  1.698035  1.696608  1.695459   \n",
              "...        ...       ...       ...  ...       ...       ...       ...   \n",
              "5995  1.499054  1.493960  1.489913  ...  1.550977  1.583197  1.605661   \n",
              "5996  1.529653  1.535207  1.539537  ...  1.693020  1.696392  1.698900   \n",
              "5997  1.564660  1.569916  1.575756  ...  1.535411  1.536617  1.537876   \n",
              "5998  1.599797  1.599838  1.598980  ...  1.617056  1.620175  1.621880   \n",
              "5999  1.579703  1.575233  1.571814  ...  1.668857  1.717732  1.769180   \n",
              "\n",
              "           250       251       252       253       254       255    256  \n",
              "0     1.740582  1.741420  1.743086  1.745437  1.748252  1.751255    1.0  \n",
              "1     1.564465  1.568940  1.573478  1.577984  1.582383  1.586600    1.0  \n",
              "2     1.681958  1.685344  1.689624  1.694374  1.699106  1.703341    1.0  \n",
              "3     1.651567  1.692000  1.767089  1.875369  2.009146  2.154947    1.0  \n",
              "4     1.694902  1.695130  1.696178  1.697926  1.700150  1.702592    1.0  \n",
              "...        ...       ...       ...       ...       ...       ...    ...  \n",
              "5995  1.618126  1.622195  1.620631  1.616599  1.612957  1.611750  100.0  \n",
              "5996  1.700400  1.700869  1.700424  1.699334  1.697991  1.696852  100.0  \n",
              "5997  1.539222  1.540659  1.542157  1.543669  1.545152  1.546585  100.0  \n",
              "5998  1.622224  1.621475  1.620038  1.618362  1.616840  1.615746  100.0  \n",
              "5999  1.815834  1.849687  1.863721  1.853540  1.818573  1.762504  100.0  \n",
              "\n",
              "[6000 rows x 257 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-25cfc8d3-8be4-4440-9304-33f0c90ca935\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>247</th>\n",
              "      <th>248</th>\n",
              "      <th>249</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "      <th>256</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.757857</td>\n",
              "      <td>1.761049</td>\n",
              "      <td>1.763568</td>\n",
              "      <td>1.765083</td>\n",
              "      <td>1.765388</td>\n",
              "      <td>1.764480</td>\n",
              "      <td>1.762595</td>\n",
              "      <td>1.760185</td>\n",
              "      <td>1.757859</td>\n",
              "      <td>1.756279</td>\n",
              "      <td>...</td>\n",
              "      <td>1.742644</td>\n",
              "      <td>1.741366</td>\n",
              "      <td>1.740599</td>\n",
              "      <td>1.740582</td>\n",
              "      <td>1.741420</td>\n",
              "      <td>1.743086</td>\n",
              "      <td>1.745437</td>\n",
              "      <td>1.748252</td>\n",
              "      <td>1.751255</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.544946</td>\n",
              "      <td>1.546049</td>\n",
              "      <td>1.547770</td>\n",
              "      <td>1.549887</td>\n",
              "      <td>1.552157</td>\n",
              "      <td>1.554327</td>\n",
              "      <td>1.556169</td>\n",
              "      <td>1.557550</td>\n",
              "      <td>1.558479</td>\n",
              "      <td>1.559129</td>\n",
              "      <td>...</td>\n",
              "      <td>1.553053</td>\n",
              "      <td>1.556314</td>\n",
              "      <td>1.560194</td>\n",
              "      <td>1.564465</td>\n",
              "      <td>1.568940</td>\n",
              "      <td>1.573478</td>\n",
              "      <td>1.577984</td>\n",
              "      <td>1.582383</td>\n",
              "      <td>1.586600</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.695559</td>\n",
              "      <td>1.697523</td>\n",
              "      <td>1.699848</td>\n",
              "      <td>1.702404</td>\n",
              "      <td>1.704980</td>\n",
              "      <td>1.707305</td>\n",
              "      <td>1.709099</td>\n",
              "      <td>1.710126</td>\n",
              "      <td>1.710271</td>\n",
              "      <td>1.709585</td>\n",
              "      <td>...</td>\n",
              "      <td>1.678932</td>\n",
              "      <td>1.678790</td>\n",
              "      <td>1.679747</td>\n",
              "      <td>1.681958</td>\n",
              "      <td>1.685344</td>\n",
              "      <td>1.689624</td>\n",
              "      <td>1.694374</td>\n",
              "      <td>1.699106</td>\n",
              "      <td>1.703341</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.775055</td>\n",
              "      <td>1.770967</td>\n",
              "      <td>1.765108</td>\n",
              "      <td>1.757903</td>\n",
              "      <td>1.750143</td>\n",
              "      <td>1.742942</td>\n",
              "      <td>1.737608</td>\n",
              "      <td>1.735426</td>\n",
              "      <td>1.737369</td>\n",
              "      <td>1.743771</td>\n",
              "      <td>...</td>\n",
              "      <td>1.681760</td>\n",
              "      <td>1.655004</td>\n",
              "      <td>1.641810</td>\n",
              "      <td>1.651567</td>\n",
              "      <td>1.692000</td>\n",
              "      <td>1.767089</td>\n",
              "      <td>1.875369</td>\n",
              "      <td>2.009146</td>\n",
              "      <td>2.154947</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.724734</td>\n",
              "      <td>1.725498</td>\n",
              "      <td>1.725171</td>\n",
              "      <td>1.724080</td>\n",
              "      <td>1.722647</td>\n",
              "      <td>1.721315</td>\n",
              "      <td>1.720477</td>\n",
              "      <td>1.720418</td>\n",
              "      <td>1.721269</td>\n",
              "      <td>1.722978</td>\n",
              "      <td>...</td>\n",
              "      <td>1.698035</td>\n",
              "      <td>1.696608</td>\n",
              "      <td>1.695459</td>\n",
              "      <td>1.694902</td>\n",
              "      <td>1.695130</td>\n",
              "      <td>1.696178</td>\n",
              "      <td>1.697926</td>\n",
              "      <td>1.700150</td>\n",
              "      <td>1.702592</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5995</th>\n",
              "      <td>1.528888</td>\n",
              "      <td>1.529840</td>\n",
              "      <td>1.528139</td>\n",
              "      <td>1.524104</td>\n",
              "      <td>1.518376</td>\n",
              "      <td>1.511774</td>\n",
              "      <td>1.505117</td>\n",
              "      <td>1.499054</td>\n",
              "      <td>1.493960</td>\n",
              "      <td>1.489913</td>\n",
              "      <td>...</td>\n",
              "      <td>1.550977</td>\n",
              "      <td>1.583197</td>\n",
              "      <td>1.605661</td>\n",
              "      <td>1.618126</td>\n",
              "      <td>1.622195</td>\n",
              "      <td>1.620631</td>\n",
              "      <td>1.616599</td>\n",
              "      <td>1.612957</td>\n",
              "      <td>1.611750</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5996</th>\n",
              "      <td>1.515056</td>\n",
              "      <td>1.511174</td>\n",
              "      <td>1.509395</td>\n",
              "      <td>1.510004</td>\n",
              "      <td>1.512908</td>\n",
              "      <td>1.517656</td>\n",
              "      <td>1.523524</td>\n",
              "      <td>1.529653</td>\n",
              "      <td>1.535207</td>\n",
              "      <td>1.539537</td>\n",
              "      <td>...</td>\n",
              "      <td>1.693020</td>\n",
              "      <td>1.696392</td>\n",
              "      <td>1.698900</td>\n",
              "      <td>1.700400</td>\n",
              "      <td>1.700869</td>\n",
              "      <td>1.700424</td>\n",
              "      <td>1.699334</td>\n",
              "      <td>1.697991</td>\n",
              "      <td>1.696852</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5997</th>\n",
              "      <td>1.565344</td>\n",
              "      <td>1.564102</td>\n",
              "      <td>1.561757</td>\n",
              "      <td>1.559405</td>\n",
              "      <td>1.558051</td>\n",
              "      <td>1.558382</td>\n",
              "      <td>1.560646</td>\n",
              "      <td>1.564660</td>\n",
              "      <td>1.569916</td>\n",
              "      <td>1.575756</td>\n",
              "      <td>...</td>\n",
              "      <td>1.535411</td>\n",
              "      <td>1.536617</td>\n",
              "      <td>1.537876</td>\n",
              "      <td>1.539222</td>\n",
              "      <td>1.540659</td>\n",
              "      <td>1.542157</td>\n",
              "      <td>1.543669</td>\n",
              "      <td>1.545152</td>\n",
              "      <td>1.546585</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5998</th>\n",
              "      <td>1.597548</td>\n",
              "      <td>1.595698</td>\n",
              "      <td>1.594912</td>\n",
              "      <td>1.595167</td>\n",
              "      <td>1.596213</td>\n",
              "      <td>1.597637</td>\n",
              "      <td>1.598970</td>\n",
              "      <td>1.599797</td>\n",
              "      <td>1.599838</td>\n",
              "      <td>1.598980</td>\n",
              "      <td>...</td>\n",
              "      <td>1.617056</td>\n",
              "      <td>1.620175</td>\n",
              "      <td>1.621880</td>\n",
              "      <td>1.622224</td>\n",
              "      <td>1.621475</td>\n",
              "      <td>1.620038</td>\n",
              "      <td>1.618362</td>\n",
              "      <td>1.616840</td>\n",
              "      <td>1.615746</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5999</th>\n",
              "      <td>1.599834</td>\n",
              "      <td>1.599558</td>\n",
              "      <td>1.598638</td>\n",
              "      <td>1.596714</td>\n",
              "      <td>1.593627</td>\n",
              "      <td>1.589487</td>\n",
              "      <td>1.584661</td>\n",
              "      <td>1.579703</td>\n",
              "      <td>1.575233</td>\n",
              "      <td>1.571814</td>\n",
              "      <td>...</td>\n",
              "      <td>1.668857</td>\n",
              "      <td>1.717732</td>\n",
              "      <td>1.769180</td>\n",
              "      <td>1.815834</td>\n",
              "      <td>1.849687</td>\n",
              "      <td>1.863721</td>\n",
              "      <td>1.853540</td>\n",
              "      <td>1.818573</td>\n",
              "      <td>1.762504</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6000 rows × 257 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-25cfc8d3-8be4-4440-9304-33f0c90ca935')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-25cfc8d3-8be4-4440-9304-33f0c90ca935 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-25cfc8d3-8be4-4440-9304-33f0c90ca935');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# y_train = arr\n",
        "# y_test = arr_t\n",
        "# y_val = arr_v"
      ],
      "metadata": {
        "id": "gWJKvYGYOiqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터 전체 섞기"
      ],
      "metadata": {
        "id": "ruynxE4-OiqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_total=pd.concat([data_train, data_test, data_val])\n",
        "data_total"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "outputId": "4e310ce3-4508-4e6c-96f6-dec0a0e68a9a",
        "id": "DeXXgNCLOiqZ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           0         1         2         3         4         5         6    \\\n",
              "0     1.697478  1.713703  1.728594  1.741144  1.750717  1.757105  1.760490   \n",
              "1     1.748610  1.747925  1.746924  1.746097  1.745874  1.746535  1.748158   \n",
              "2     1.709674  1.710317  1.711171  1.712183  1.713227  1.714166  1.714897   \n",
              "3     1.726979  1.726187  1.724966  1.723451  1.721925  1.720776  1.720402   \n",
              "4     1.889883  1.889929  1.891485  1.895719  1.903396  1.914786  1.929653   \n",
              "...        ...       ...       ...       ...       ...       ...       ...   \n",
              "5995  1.528888  1.529840  1.528139  1.524104  1.518376  1.511774  1.505117   \n",
              "5996  1.515056  1.511174  1.509395  1.510004  1.512908  1.517656  1.523524   \n",
              "5997  1.565344  1.564102  1.561757  1.559405  1.558051  1.558382  1.560646   \n",
              "5998  1.597548  1.595698  1.594912  1.595167  1.596213  1.597637  1.598970   \n",
              "5999  1.599834  1.599558  1.598638  1.596714  1.593627  1.589487  1.584661   \n",
              "\n",
              "           7         8         9    ...       247       248       249  \\\n",
              "0     1.761353  1.760349  1.758191  ...  1.733012  1.733423  1.734708   \n",
              "1     1.750626  1.753678  1.756978  ...  1.694451  1.696899  1.699745   \n",
              "2     1.715389  1.715680  1.715858  ...  1.722441  1.724729  1.726115   \n",
              "3     1.721109  1.723015  1.726001  ...  1.739826  1.739925  1.740406   \n",
              "4     1.947339  1.966906  1.987309  ...  1.914361  1.928167  1.944398   \n",
              "...        ...       ...       ...  ...       ...       ...       ...   \n",
              "5995  1.499054  1.493960  1.489913  ...  1.550977  1.583197  1.605661   \n",
              "5996  1.529653  1.535207  1.539537  ...  1.693020  1.696392  1.698900   \n",
              "5997  1.564660  1.569916  1.575756  ...  1.535411  1.536617  1.537876   \n",
              "5998  1.599797  1.599838  1.598980  ...  1.617056  1.620175  1.621880   \n",
              "5999  1.579703  1.575233  1.571814  ...  1.668857  1.717732  1.769180   \n",
              "\n",
              "           250       251       252       253       254       255    256  \n",
              "0     1.736879  1.739700  1.742739  1.745491  1.747515  1.748556    1.0  \n",
              "1     1.702543  1.704941  1.706748  1.707952  1.708690  1.709187    1.0  \n",
              "2     1.726820  1.727122  1.727259  1.727362  1.727430  1.727353    1.0  \n",
              "3     1.741383  1.742927  1.745059  1.747751  1.750910  1.754365    1.0  \n",
              "4     1.962799  1.982890  2.004016  2.025433  2.046399  2.066255    1.0  \n",
              "...        ...       ...       ...       ...       ...       ...    ...  \n",
              "5995  1.618126  1.622195  1.620631  1.616599  1.612957  1.611750  100.0  \n",
              "5996  1.700400  1.700869  1.700424  1.699334  1.697991  1.696852  100.0  \n",
              "5997  1.539222  1.540659  1.542157  1.543669  1.545152  1.546585  100.0  \n",
              "5998  1.622224  1.621475  1.620038  1.618362  1.616840  1.615746  100.0  \n",
              "5999  1.815834  1.849687  1.863721  1.853540  1.818573  1.762504  100.0  \n",
              "\n",
              "[36000 rows x 257 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ba0eb768-e3a2-4a28-9209-dcf3f3ae9a67\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>247</th>\n",
              "      <th>248</th>\n",
              "      <th>249</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "      <th>256</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.697478</td>\n",
              "      <td>1.713703</td>\n",
              "      <td>1.728594</td>\n",
              "      <td>1.741144</td>\n",
              "      <td>1.750717</td>\n",
              "      <td>1.757105</td>\n",
              "      <td>1.760490</td>\n",
              "      <td>1.761353</td>\n",
              "      <td>1.760349</td>\n",
              "      <td>1.758191</td>\n",
              "      <td>...</td>\n",
              "      <td>1.733012</td>\n",
              "      <td>1.733423</td>\n",
              "      <td>1.734708</td>\n",
              "      <td>1.736879</td>\n",
              "      <td>1.739700</td>\n",
              "      <td>1.742739</td>\n",
              "      <td>1.745491</td>\n",
              "      <td>1.747515</td>\n",
              "      <td>1.748556</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.748610</td>\n",
              "      <td>1.747925</td>\n",
              "      <td>1.746924</td>\n",
              "      <td>1.746097</td>\n",
              "      <td>1.745874</td>\n",
              "      <td>1.746535</td>\n",
              "      <td>1.748158</td>\n",
              "      <td>1.750626</td>\n",
              "      <td>1.753678</td>\n",
              "      <td>1.756978</td>\n",
              "      <td>...</td>\n",
              "      <td>1.694451</td>\n",
              "      <td>1.696899</td>\n",
              "      <td>1.699745</td>\n",
              "      <td>1.702543</td>\n",
              "      <td>1.704941</td>\n",
              "      <td>1.706748</td>\n",
              "      <td>1.707952</td>\n",
              "      <td>1.708690</td>\n",
              "      <td>1.709187</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.709674</td>\n",
              "      <td>1.710317</td>\n",
              "      <td>1.711171</td>\n",
              "      <td>1.712183</td>\n",
              "      <td>1.713227</td>\n",
              "      <td>1.714166</td>\n",
              "      <td>1.714897</td>\n",
              "      <td>1.715389</td>\n",
              "      <td>1.715680</td>\n",
              "      <td>1.715858</td>\n",
              "      <td>...</td>\n",
              "      <td>1.722441</td>\n",
              "      <td>1.724729</td>\n",
              "      <td>1.726115</td>\n",
              "      <td>1.726820</td>\n",
              "      <td>1.727122</td>\n",
              "      <td>1.727259</td>\n",
              "      <td>1.727362</td>\n",
              "      <td>1.727430</td>\n",
              "      <td>1.727353</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.726979</td>\n",
              "      <td>1.726187</td>\n",
              "      <td>1.724966</td>\n",
              "      <td>1.723451</td>\n",
              "      <td>1.721925</td>\n",
              "      <td>1.720776</td>\n",
              "      <td>1.720402</td>\n",
              "      <td>1.721109</td>\n",
              "      <td>1.723015</td>\n",
              "      <td>1.726001</td>\n",
              "      <td>...</td>\n",
              "      <td>1.739826</td>\n",
              "      <td>1.739925</td>\n",
              "      <td>1.740406</td>\n",
              "      <td>1.741383</td>\n",
              "      <td>1.742927</td>\n",
              "      <td>1.745059</td>\n",
              "      <td>1.747751</td>\n",
              "      <td>1.750910</td>\n",
              "      <td>1.754365</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.889883</td>\n",
              "      <td>1.889929</td>\n",
              "      <td>1.891485</td>\n",
              "      <td>1.895719</td>\n",
              "      <td>1.903396</td>\n",
              "      <td>1.914786</td>\n",
              "      <td>1.929653</td>\n",
              "      <td>1.947339</td>\n",
              "      <td>1.966906</td>\n",
              "      <td>1.987309</td>\n",
              "      <td>...</td>\n",
              "      <td>1.914361</td>\n",
              "      <td>1.928167</td>\n",
              "      <td>1.944398</td>\n",
              "      <td>1.962799</td>\n",
              "      <td>1.982890</td>\n",
              "      <td>2.004016</td>\n",
              "      <td>2.025433</td>\n",
              "      <td>2.046399</td>\n",
              "      <td>2.066255</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5995</th>\n",
              "      <td>1.528888</td>\n",
              "      <td>1.529840</td>\n",
              "      <td>1.528139</td>\n",
              "      <td>1.524104</td>\n",
              "      <td>1.518376</td>\n",
              "      <td>1.511774</td>\n",
              "      <td>1.505117</td>\n",
              "      <td>1.499054</td>\n",
              "      <td>1.493960</td>\n",
              "      <td>1.489913</td>\n",
              "      <td>...</td>\n",
              "      <td>1.550977</td>\n",
              "      <td>1.583197</td>\n",
              "      <td>1.605661</td>\n",
              "      <td>1.618126</td>\n",
              "      <td>1.622195</td>\n",
              "      <td>1.620631</td>\n",
              "      <td>1.616599</td>\n",
              "      <td>1.612957</td>\n",
              "      <td>1.611750</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5996</th>\n",
              "      <td>1.515056</td>\n",
              "      <td>1.511174</td>\n",
              "      <td>1.509395</td>\n",
              "      <td>1.510004</td>\n",
              "      <td>1.512908</td>\n",
              "      <td>1.517656</td>\n",
              "      <td>1.523524</td>\n",
              "      <td>1.529653</td>\n",
              "      <td>1.535207</td>\n",
              "      <td>1.539537</td>\n",
              "      <td>...</td>\n",
              "      <td>1.693020</td>\n",
              "      <td>1.696392</td>\n",
              "      <td>1.698900</td>\n",
              "      <td>1.700400</td>\n",
              "      <td>1.700869</td>\n",
              "      <td>1.700424</td>\n",
              "      <td>1.699334</td>\n",
              "      <td>1.697991</td>\n",
              "      <td>1.696852</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5997</th>\n",
              "      <td>1.565344</td>\n",
              "      <td>1.564102</td>\n",
              "      <td>1.561757</td>\n",
              "      <td>1.559405</td>\n",
              "      <td>1.558051</td>\n",
              "      <td>1.558382</td>\n",
              "      <td>1.560646</td>\n",
              "      <td>1.564660</td>\n",
              "      <td>1.569916</td>\n",
              "      <td>1.575756</td>\n",
              "      <td>...</td>\n",
              "      <td>1.535411</td>\n",
              "      <td>1.536617</td>\n",
              "      <td>1.537876</td>\n",
              "      <td>1.539222</td>\n",
              "      <td>1.540659</td>\n",
              "      <td>1.542157</td>\n",
              "      <td>1.543669</td>\n",
              "      <td>1.545152</td>\n",
              "      <td>1.546585</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5998</th>\n",
              "      <td>1.597548</td>\n",
              "      <td>1.595698</td>\n",
              "      <td>1.594912</td>\n",
              "      <td>1.595167</td>\n",
              "      <td>1.596213</td>\n",
              "      <td>1.597637</td>\n",
              "      <td>1.598970</td>\n",
              "      <td>1.599797</td>\n",
              "      <td>1.599838</td>\n",
              "      <td>1.598980</td>\n",
              "      <td>...</td>\n",
              "      <td>1.617056</td>\n",
              "      <td>1.620175</td>\n",
              "      <td>1.621880</td>\n",
              "      <td>1.622224</td>\n",
              "      <td>1.621475</td>\n",
              "      <td>1.620038</td>\n",
              "      <td>1.618362</td>\n",
              "      <td>1.616840</td>\n",
              "      <td>1.615746</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5999</th>\n",
              "      <td>1.599834</td>\n",
              "      <td>1.599558</td>\n",
              "      <td>1.598638</td>\n",
              "      <td>1.596714</td>\n",
              "      <td>1.593627</td>\n",
              "      <td>1.589487</td>\n",
              "      <td>1.584661</td>\n",
              "      <td>1.579703</td>\n",
              "      <td>1.575233</td>\n",
              "      <td>1.571814</td>\n",
              "      <td>...</td>\n",
              "      <td>1.668857</td>\n",
              "      <td>1.717732</td>\n",
              "      <td>1.769180</td>\n",
              "      <td>1.815834</td>\n",
              "      <td>1.849687</td>\n",
              "      <td>1.863721</td>\n",
              "      <td>1.853540</td>\n",
              "      <td>1.818573</td>\n",
              "      <td>1.762504</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>36000 rows × 257 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ba0eb768-e3a2-4a28-9209-dcf3f3ae9a67')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ba0eb768-e3a2-4a28-9209-dcf3f3ae9a67 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ba0eb768-e3a2-4a28-9209-dcf3f3ae9a67');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_total=data_total.sample(frac=1)"
      ],
      "metadata": {
        "id": "jyie6GiXOiqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train=data_total.iloc[0:24000, :]\n",
        "data_val=data_total.iloc[24000:30000, :]\n",
        "data_test=data_total.iloc[30000:36000,:]"
      ],
      "metadata": {
        "id": "ZutkohbCOiqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "outputId": "0b6e5415-8a26-45d2-d0df-37395ab5bf20",
        "id": "Ccuz9w6EOiqZ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            0         1         2         3         4         5         6    \\\n",
              "14797  1.575498  1.563810  1.554748  1.551273  1.554579  1.564238  1.578608   \n",
              "4442   1.770798  1.776173  1.783383  1.791552  1.799648  1.806729  1.812172   \n",
              "19385  1.735351  1.746334  1.752386  1.757505  1.764550  1.774997  1.789012   \n",
              "1667   1.701525  1.681062  1.662269  1.645431  1.630848  1.618785  1.609411   \n",
              "19969  1.616300  1.620086  1.621910  1.621008  1.616957  1.609821  1.600242   \n",
              "...         ...       ...       ...       ...       ...       ...       ...   \n",
              "2564   1.569072  1.567932  1.567012  1.566737  1.567516  1.569643  1.573193   \n",
              "17925  1.733440  1.741777  1.750718  1.760091  1.769664  1.779155  1.788254   \n",
              "1922   1.625553  1.627044  1.628636  1.630064  1.631115  1.631655  1.631648   \n",
              "19182  1.829093  1.808695  1.786890  1.764763  1.743234  1.722943  1.704218   \n",
              "2007   1.929730  1.870354  1.800231  1.728651  1.664408  1.614277  1.582014   \n",
              "\n",
              "            7         8         9    ...       247       248       249  \\\n",
              "14797  1.595385  1.612176  1.626981  ...  1.496063  1.497101  1.499070   \n",
              "4442   1.815827  1.818039  1.819563  ...  1.857962  1.857110  1.857146   \n",
              "19385  1.805769  1.823886  1.841890  ...  1.584464  1.588439  1.592877   \n",
              "1667   1.602731  1.598562  1.596527  ...  1.593327  1.591406  1.589981   \n",
              "19969  1.589441  1.579115  1.571215  ...  1.652794  1.656397  1.661812   \n",
              "...         ...       ...       ...  ...       ...       ...       ...   \n",
              "2564   1.577953  1.583408  1.588808  ...  1.605560  1.596799  1.587624   \n",
              "17925  1.796650  1.804062  1.810267  ...  1.694811  1.708601  1.722008   \n",
              "1922   1.631146  1.630273  1.629189  ...  1.575915  1.575933  1.575032   \n",
              "19182  1.687110  1.671490  1.657168  ...  1.604341  1.608744  1.611719   \n",
              "2007   1.568048  1.569857  1.582870  ...  1.742550  1.741831  1.740769   \n",
              "\n",
              "            250       251       252       253       254       255   256  \n",
              "14797  1.501751  1.504783  1.507710  1.510087  1.511590  1.512108  62.0  \n",
              "4442   1.858595  1.861911  1.867374  1.874999  1.884480  1.895171  75.0  \n",
              "19385  1.597272  1.601050  1.603653  1.604633  1.603749  1.601042  81.0  \n",
              "1667   1.589109  1.588789  1.588973  1.589581  1.590503  1.591591   7.0  \n",
              "19969  1.669728  1.680872  1.695850  1.714960  1.738014  1.764247  84.0  \n",
              "...         ...       ...       ...       ...       ...       ...   ...  \n",
              "2564   1.578812  1.571024  1.564695  1.559992  1.556846  1.555039  43.0  \n",
              "17925  1.732289  1.738204  1.739854  1.738288  1.735035  1.731660  75.0  \n",
              "1922   1.573324  1.571086  1.568699  1.566545  1.564915  1.563942   9.0  \n",
              "19182  1.612854  1.612013  1.609360  1.605316  1.600460  1.595409  80.0  \n",
              "2007   1.739280  1.737119  1.733914  1.729231  1.722660  1.713918  34.0  \n",
              "\n",
              "[24000 rows x 257 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-88b119ff-d8f1-40bf-b791-06e8562fce55\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>247</th>\n",
              "      <th>248</th>\n",
              "      <th>249</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "      <th>256</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>14797</th>\n",
              "      <td>1.575498</td>\n",
              "      <td>1.563810</td>\n",
              "      <td>1.554748</td>\n",
              "      <td>1.551273</td>\n",
              "      <td>1.554579</td>\n",
              "      <td>1.564238</td>\n",
              "      <td>1.578608</td>\n",
              "      <td>1.595385</td>\n",
              "      <td>1.612176</td>\n",
              "      <td>1.626981</td>\n",
              "      <td>...</td>\n",
              "      <td>1.496063</td>\n",
              "      <td>1.497101</td>\n",
              "      <td>1.499070</td>\n",
              "      <td>1.501751</td>\n",
              "      <td>1.504783</td>\n",
              "      <td>1.507710</td>\n",
              "      <td>1.510087</td>\n",
              "      <td>1.511590</td>\n",
              "      <td>1.512108</td>\n",
              "      <td>62.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4442</th>\n",
              "      <td>1.770798</td>\n",
              "      <td>1.776173</td>\n",
              "      <td>1.783383</td>\n",
              "      <td>1.791552</td>\n",
              "      <td>1.799648</td>\n",
              "      <td>1.806729</td>\n",
              "      <td>1.812172</td>\n",
              "      <td>1.815827</td>\n",
              "      <td>1.818039</td>\n",
              "      <td>1.819563</td>\n",
              "      <td>...</td>\n",
              "      <td>1.857962</td>\n",
              "      <td>1.857110</td>\n",
              "      <td>1.857146</td>\n",
              "      <td>1.858595</td>\n",
              "      <td>1.861911</td>\n",
              "      <td>1.867374</td>\n",
              "      <td>1.874999</td>\n",
              "      <td>1.884480</td>\n",
              "      <td>1.895171</td>\n",
              "      <td>75.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19385</th>\n",
              "      <td>1.735351</td>\n",
              "      <td>1.746334</td>\n",
              "      <td>1.752386</td>\n",
              "      <td>1.757505</td>\n",
              "      <td>1.764550</td>\n",
              "      <td>1.774997</td>\n",
              "      <td>1.789012</td>\n",
              "      <td>1.805769</td>\n",
              "      <td>1.823886</td>\n",
              "      <td>1.841890</td>\n",
              "      <td>...</td>\n",
              "      <td>1.584464</td>\n",
              "      <td>1.588439</td>\n",
              "      <td>1.592877</td>\n",
              "      <td>1.597272</td>\n",
              "      <td>1.601050</td>\n",
              "      <td>1.603653</td>\n",
              "      <td>1.604633</td>\n",
              "      <td>1.603749</td>\n",
              "      <td>1.601042</td>\n",
              "      <td>81.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1667</th>\n",
              "      <td>1.701525</td>\n",
              "      <td>1.681062</td>\n",
              "      <td>1.662269</td>\n",
              "      <td>1.645431</td>\n",
              "      <td>1.630848</td>\n",
              "      <td>1.618785</td>\n",
              "      <td>1.609411</td>\n",
              "      <td>1.602731</td>\n",
              "      <td>1.598562</td>\n",
              "      <td>1.596527</td>\n",
              "      <td>...</td>\n",
              "      <td>1.593327</td>\n",
              "      <td>1.591406</td>\n",
              "      <td>1.589981</td>\n",
              "      <td>1.589109</td>\n",
              "      <td>1.588789</td>\n",
              "      <td>1.588973</td>\n",
              "      <td>1.589581</td>\n",
              "      <td>1.590503</td>\n",
              "      <td>1.591591</td>\n",
              "      <td>7.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19969</th>\n",
              "      <td>1.616300</td>\n",
              "      <td>1.620086</td>\n",
              "      <td>1.621910</td>\n",
              "      <td>1.621008</td>\n",
              "      <td>1.616957</td>\n",
              "      <td>1.609821</td>\n",
              "      <td>1.600242</td>\n",
              "      <td>1.589441</td>\n",
              "      <td>1.579115</td>\n",
              "      <td>1.571215</td>\n",
              "      <td>...</td>\n",
              "      <td>1.652794</td>\n",
              "      <td>1.656397</td>\n",
              "      <td>1.661812</td>\n",
              "      <td>1.669728</td>\n",
              "      <td>1.680872</td>\n",
              "      <td>1.695850</td>\n",
              "      <td>1.714960</td>\n",
              "      <td>1.738014</td>\n",
              "      <td>1.764247</td>\n",
              "      <td>84.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2564</th>\n",
              "      <td>1.569072</td>\n",
              "      <td>1.567932</td>\n",
              "      <td>1.567012</td>\n",
              "      <td>1.566737</td>\n",
              "      <td>1.567516</td>\n",
              "      <td>1.569643</td>\n",
              "      <td>1.573193</td>\n",
              "      <td>1.577953</td>\n",
              "      <td>1.583408</td>\n",
              "      <td>1.588808</td>\n",
              "      <td>...</td>\n",
              "      <td>1.605560</td>\n",
              "      <td>1.596799</td>\n",
              "      <td>1.587624</td>\n",
              "      <td>1.578812</td>\n",
              "      <td>1.571024</td>\n",
              "      <td>1.564695</td>\n",
              "      <td>1.559992</td>\n",
              "      <td>1.556846</td>\n",
              "      <td>1.555039</td>\n",
              "      <td>43.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17925</th>\n",
              "      <td>1.733440</td>\n",
              "      <td>1.741777</td>\n",
              "      <td>1.750718</td>\n",
              "      <td>1.760091</td>\n",
              "      <td>1.769664</td>\n",
              "      <td>1.779155</td>\n",
              "      <td>1.788254</td>\n",
              "      <td>1.796650</td>\n",
              "      <td>1.804062</td>\n",
              "      <td>1.810267</td>\n",
              "      <td>...</td>\n",
              "      <td>1.694811</td>\n",
              "      <td>1.708601</td>\n",
              "      <td>1.722008</td>\n",
              "      <td>1.732289</td>\n",
              "      <td>1.738204</td>\n",
              "      <td>1.739854</td>\n",
              "      <td>1.738288</td>\n",
              "      <td>1.735035</td>\n",
              "      <td>1.731660</td>\n",
              "      <td>75.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1922</th>\n",
              "      <td>1.625553</td>\n",
              "      <td>1.627044</td>\n",
              "      <td>1.628636</td>\n",
              "      <td>1.630064</td>\n",
              "      <td>1.631115</td>\n",
              "      <td>1.631655</td>\n",
              "      <td>1.631648</td>\n",
              "      <td>1.631146</td>\n",
              "      <td>1.630273</td>\n",
              "      <td>1.629189</td>\n",
              "      <td>...</td>\n",
              "      <td>1.575915</td>\n",
              "      <td>1.575933</td>\n",
              "      <td>1.575032</td>\n",
              "      <td>1.573324</td>\n",
              "      <td>1.571086</td>\n",
              "      <td>1.568699</td>\n",
              "      <td>1.566545</td>\n",
              "      <td>1.564915</td>\n",
              "      <td>1.563942</td>\n",
              "      <td>9.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19182</th>\n",
              "      <td>1.829093</td>\n",
              "      <td>1.808695</td>\n",
              "      <td>1.786890</td>\n",
              "      <td>1.764763</td>\n",
              "      <td>1.743234</td>\n",
              "      <td>1.722943</td>\n",
              "      <td>1.704218</td>\n",
              "      <td>1.687110</td>\n",
              "      <td>1.671490</td>\n",
              "      <td>1.657168</td>\n",
              "      <td>...</td>\n",
              "      <td>1.604341</td>\n",
              "      <td>1.608744</td>\n",
              "      <td>1.611719</td>\n",
              "      <td>1.612854</td>\n",
              "      <td>1.612013</td>\n",
              "      <td>1.609360</td>\n",
              "      <td>1.605316</td>\n",
              "      <td>1.600460</td>\n",
              "      <td>1.595409</td>\n",
              "      <td>80.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2007</th>\n",
              "      <td>1.929730</td>\n",
              "      <td>1.870354</td>\n",
              "      <td>1.800231</td>\n",
              "      <td>1.728651</td>\n",
              "      <td>1.664408</td>\n",
              "      <td>1.614277</td>\n",
              "      <td>1.582014</td>\n",
              "      <td>1.568048</td>\n",
              "      <td>1.569857</td>\n",
              "      <td>1.582870</td>\n",
              "      <td>...</td>\n",
              "      <td>1.742550</td>\n",
              "      <td>1.741831</td>\n",
              "      <td>1.740769</td>\n",
              "      <td>1.739280</td>\n",
              "      <td>1.737119</td>\n",
              "      <td>1.733914</td>\n",
              "      <td>1.729231</td>\n",
              "      <td>1.722660</td>\n",
              "      <td>1.713918</td>\n",
              "      <td>34.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>24000 rows × 257 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-88b119ff-d8f1-40bf-b791-06e8562fce55')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-88b119ff-d8f1-40bf-b791-06e8562fce55 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-88b119ff-d8f1-40bf-b791-06e8562fce55');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터 따로 섞기"
      ],
      "metadata": {
        "id": "cRGWqZVlOiqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_train = data_train.sample(frac=1).reset_index(drop=True)\n",
        "data_test = data_test.sample(frac=1).reset_index(drop=True)\n",
        "data_val = data_val.sample(frac=1).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "x7nTUyO4OiqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "outputId": "e071d1b2-f4ad-4b00-cf90-3a013213ee23",
        "id": "VC7NLriPOiqa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            0         1         2         3         4         5         6    \\\n",
              "0      0.013167  0.017490  0.018646  0.016446  0.011114  0.003245 -0.006287   \n",
              "1     -0.115512 -0.115289 -0.113396 -0.110355 -0.106877 -0.103780 -0.101890   \n",
              "2     -0.009042 -0.008103 -0.008920 -0.010897 -0.013278 -0.015248 -0.016029   \n",
              "3      0.402142  0.302911  0.185292  0.066715 -0.037182 -0.115259 -0.162345   \n",
              "4     -0.007963  0.005535  0.017452  0.026380  0.031221  0.031279  0.026293   \n",
              "...         ...       ...       ...       ...       ...       ...       ...   \n",
              "23995 -0.109838 -0.112002 -0.113641 -0.114621 -0.114863 -0.114353 -0.113177   \n",
              "23996 -0.040675 -0.054378 -0.068501 -0.081380 -0.091338 -0.096999 -0.097585   \n",
              "23997 -0.019681 -0.041208 -0.061138 -0.078153 -0.091414 -0.100656 -0.106163   \n",
              "23998  0.003074 -0.012424 -0.024224 -0.032849 -0.039069 -0.043762 -0.047765   \n",
              "23999 -0.016414 -0.019264 -0.023513 -0.029241 -0.036146 -0.043594 -0.050755   \n",
              "\n",
              "            7         8         9    ...       247       248       249  \\\n",
              "0     -0.016464 -0.026283 -0.034921  ... -0.004174 -0.002091  0.000968   \n",
              "1     -0.101913 -0.104281 -0.108999  ...  0.008242  0.022894  0.039154   \n",
              "2     -0.014967 -0.011609 -0.005777  ... -0.041403 -0.040630 -0.041345   \n",
              "3     -0.179306 -0.171868 -0.148656  ... -0.079294 -0.081055 -0.082744   \n",
              "4      0.016416  0.002152 -0.015730  ...  0.008343 -0.091526 -0.188443   \n",
              "...         ...       ...       ...  ...       ...       ...       ...   \n",
              "23995 -0.111554 -0.109847 -0.108525  ... -0.108024 -0.108699 -0.108706   \n",
              "23996 -0.093149 -0.084687 -0.074089  ...  0.001157  0.069442  0.143595   \n",
              "23997 -0.108632 -0.108992 -0.108220  ... -0.045588 -0.048637 -0.051353   \n",
              "23998 -0.051742 -0.056075 -0.060821  ... -0.119279 -0.118505 -0.116912   \n",
              "23999 -0.056785 -0.061007 -0.063049  ... -0.090826 -0.090601 -0.087764   \n",
              "\n",
              "            250       251       252       253       254       255   256  \n",
              "0      0.004741  0.008918  0.013208  0.017364  0.021204  0.024627  72.0  \n",
              "1      0.056616  0.074825  0.093340  0.111823  0.130108  0.148236  54.0  \n",
              "2     -0.043381 -0.046298 -0.049451 -0.052130 -0.053720 -0.053853  61.0  \n",
              "3     -0.083994 -0.084521 -0.084185 -0.083029 -0.081281 -0.079306  40.0  \n",
              "4     -0.269607 -0.325108 -0.349555 -0.342722 -0.309150 -0.256869   9.0  \n",
              "...         ...       ...       ...       ...       ...       ...   ...  \n",
              "23995 -0.108044 -0.106867 -0.105405 -0.103864 -0.102335 -0.100754  62.0  \n",
              "23996  0.214329  0.271583  0.306394  0.312725  0.288831  0.237771  41.0  \n",
              "23997 -0.053427 -0.054666 -0.054988 -0.054436 -0.053175 -0.051491  35.0  \n",
              "23998 -0.114928 -0.113008 -0.111522 -0.110699 -0.110611 -0.111212  98.0  \n",
              "23999 -0.082253 -0.074366 -0.064672 -0.053875 -0.042684 -0.031708  81.0  \n",
              "\n",
              "[24000 rows x 257 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-20518848-deb8-470c-bef6-2095db7f43fb\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>247</th>\n",
              "      <th>248</th>\n",
              "      <th>249</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "      <th>256</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.013167</td>\n",
              "      <td>0.017490</td>\n",
              "      <td>0.018646</td>\n",
              "      <td>0.016446</td>\n",
              "      <td>0.011114</td>\n",
              "      <td>0.003245</td>\n",
              "      <td>-0.006287</td>\n",
              "      <td>-0.016464</td>\n",
              "      <td>-0.026283</td>\n",
              "      <td>-0.034921</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.004174</td>\n",
              "      <td>-0.002091</td>\n",
              "      <td>0.000968</td>\n",
              "      <td>0.004741</td>\n",
              "      <td>0.008918</td>\n",
              "      <td>0.013208</td>\n",
              "      <td>0.017364</td>\n",
              "      <td>0.021204</td>\n",
              "      <td>0.024627</td>\n",
              "      <td>72.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.115512</td>\n",
              "      <td>-0.115289</td>\n",
              "      <td>-0.113396</td>\n",
              "      <td>-0.110355</td>\n",
              "      <td>-0.106877</td>\n",
              "      <td>-0.103780</td>\n",
              "      <td>-0.101890</td>\n",
              "      <td>-0.101913</td>\n",
              "      <td>-0.104281</td>\n",
              "      <td>-0.108999</td>\n",
              "      <td>...</td>\n",
              "      <td>0.008242</td>\n",
              "      <td>0.022894</td>\n",
              "      <td>0.039154</td>\n",
              "      <td>0.056616</td>\n",
              "      <td>0.074825</td>\n",
              "      <td>0.093340</td>\n",
              "      <td>0.111823</td>\n",
              "      <td>0.130108</td>\n",
              "      <td>0.148236</td>\n",
              "      <td>54.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.009042</td>\n",
              "      <td>-0.008103</td>\n",
              "      <td>-0.008920</td>\n",
              "      <td>-0.010897</td>\n",
              "      <td>-0.013278</td>\n",
              "      <td>-0.015248</td>\n",
              "      <td>-0.016029</td>\n",
              "      <td>-0.014967</td>\n",
              "      <td>-0.011609</td>\n",
              "      <td>-0.005777</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.041403</td>\n",
              "      <td>-0.040630</td>\n",
              "      <td>-0.041345</td>\n",
              "      <td>-0.043381</td>\n",
              "      <td>-0.046298</td>\n",
              "      <td>-0.049451</td>\n",
              "      <td>-0.052130</td>\n",
              "      <td>-0.053720</td>\n",
              "      <td>-0.053853</td>\n",
              "      <td>61.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.402142</td>\n",
              "      <td>0.302911</td>\n",
              "      <td>0.185292</td>\n",
              "      <td>0.066715</td>\n",
              "      <td>-0.037182</td>\n",
              "      <td>-0.115259</td>\n",
              "      <td>-0.162345</td>\n",
              "      <td>-0.179306</td>\n",
              "      <td>-0.171868</td>\n",
              "      <td>-0.148656</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.079294</td>\n",
              "      <td>-0.081055</td>\n",
              "      <td>-0.082744</td>\n",
              "      <td>-0.083994</td>\n",
              "      <td>-0.084521</td>\n",
              "      <td>-0.084185</td>\n",
              "      <td>-0.083029</td>\n",
              "      <td>-0.081281</td>\n",
              "      <td>-0.079306</td>\n",
              "      <td>40.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.007963</td>\n",
              "      <td>0.005535</td>\n",
              "      <td>0.017452</td>\n",
              "      <td>0.026380</td>\n",
              "      <td>0.031221</td>\n",
              "      <td>0.031279</td>\n",
              "      <td>0.026293</td>\n",
              "      <td>0.016416</td>\n",
              "      <td>0.002152</td>\n",
              "      <td>-0.015730</td>\n",
              "      <td>...</td>\n",
              "      <td>0.008343</td>\n",
              "      <td>-0.091526</td>\n",
              "      <td>-0.188443</td>\n",
              "      <td>-0.269607</td>\n",
              "      <td>-0.325108</td>\n",
              "      <td>-0.349555</td>\n",
              "      <td>-0.342722</td>\n",
              "      <td>-0.309150</td>\n",
              "      <td>-0.256869</td>\n",
              "      <td>9.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23995</th>\n",
              "      <td>-0.109838</td>\n",
              "      <td>-0.112002</td>\n",
              "      <td>-0.113641</td>\n",
              "      <td>-0.114621</td>\n",
              "      <td>-0.114863</td>\n",
              "      <td>-0.114353</td>\n",
              "      <td>-0.113177</td>\n",
              "      <td>-0.111554</td>\n",
              "      <td>-0.109847</td>\n",
              "      <td>-0.108525</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.108024</td>\n",
              "      <td>-0.108699</td>\n",
              "      <td>-0.108706</td>\n",
              "      <td>-0.108044</td>\n",
              "      <td>-0.106867</td>\n",
              "      <td>-0.105405</td>\n",
              "      <td>-0.103864</td>\n",
              "      <td>-0.102335</td>\n",
              "      <td>-0.100754</td>\n",
              "      <td>62.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23996</th>\n",
              "      <td>-0.040675</td>\n",
              "      <td>-0.054378</td>\n",
              "      <td>-0.068501</td>\n",
              "      <td>-0.081380</td>\n",
              "      <td>-0.091338</td>\n",
              "      <td>-0.096999</td>\n",
              "      <td>-0.097585</td>\n",
              "      <td>-0.093149</td>\n",
              "      <td>-0.084687</td>\n",
              "      <td>-0.074089</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001157</td>\n",
              "      <td>0.069442</td>\n",
              "      <td>0.143595</td>\n",
              "      <td>0.214329</td>\n",
              "      <td>0.271583</td>\n",
              "      <td>0.306394</td>\n",
              "      <td>0.312725</td>\n",
              "      <td>0.288831</td>\n",
              "      <td>0.237771</td>\n",
              "      <td>41.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23997</th>\n",
              "      <td>-0.019681</td>\n",
              "      <td>-0.041208</td>\n",
              "      <td>-0.061138</td>\n",
              "      <td>-0.078153</td>\n",
              "      <td>-0.091414</td>\n",
              "      <td>-0.100656</td>\n",
              "      <td>-0.106163</td>\n",
              "      <td>-0.108632</td>\n",
              "      <td>-0.108992</td>\n",
              "      <td>-0.108220</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.045588</td>\n",
              "      <td>-0.048637</td>\n",
              "      <td>-0.051353</td>\n",
              "      <td>-0.053427</td>\n",
              "      <td>-0.054666</td>\n",
              "      <td>-0.054988</td>\n",
              "      <td>-0.054436</td>\n",
              "      <td>-0.053175</td>\n",
              "      <td>-0.051491</td>\n",
              "      <td>35.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23998</th>\n",
              "      <td>0.003074</td>\n",
              "      <td>-0.012424</td>\n",
              "      <td>-0.024224</td>\n",
              "      <td>-0.032849</td>\n",
              "      <td>-0.039069</td>\n",
              "      <td>-0.043762</td>\n",
              "      <td>-0.047765</td>\n",
              "      <td>-0.051742</td>\n",
              "      <td>-0.056075</td>\n",
              "      <td>-0.060821</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.119279</td>\n",
              "      <td>-0.118505</td>\n",
              "      <td>-0.116912</td>\n",
              "      <td>-0.114928</td>\n",
              "      <td>-0.113008</td>\n",
              "      <td>-0.111522</td>\n",
              "      <td>-0.110699</td>\n",
              "      <td>-0.110611</td>\n",
              "      <td>-0.111212</td>\n",
              "      <td>98.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23999</th>\n",
              "      <td>-0.016414</td>\n",
              "      <td>-0.019264</td>\n",
              "      <td>-0.023513</td>\n",
              "      <td>-0.029241</td>\n",
              "      <td>-0.036146</td>\n",
              "      <td>-0.043594</td>\n",
              "      <td>-0.050755</td>\n",
              "      <td>-0.056785</td>\n",
              "      <td>-0.061007</td>\n",
              "      <td>-0.063049</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.090826</td>\n",
              "      <td>-0.090601</td>\n",
              "      <td>-0.087764</td>\n",
              "      <td>-0.082253</td>\n",
              "      <td>-0.074366</td>\n",
              "      <td>-0.064672</td>\n",
              "      <td>-0.053875</td>\n",
              "      <td>-0.042684</td>\n",
              "      <td>-0.031708</td>\n",
              "      <td>81.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>24000 rows × 257 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-20518848-deb8-470c-bef6-2095db7f43fb')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-20518848-deb8-470c-bef6-2095db7f43fb button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-20518848-deb8-470c-bef6-2095db7f43fb');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 전처리"
      ],
      "metadata": {
        "id": "FbtYLTXsOiqa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = to_categorical(data_train[256])\n",
        "y_test = to_categorical(data_test[256])\n",
        "y_val = to_categorical(data_val[256])\n",
        "# y_train = to_categorical(arr)\n",
        "# y_test = to_categorical(arr_t)\n",
        "# y_val = to_categorical(arr_v)\n",
        "y_train[23999]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a1bdabf-3601-4f3e-c3b3-607de465d832",
        "id": "GfabuFB_Oiqa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_train.drop([256], axis=1, inplace=True)\n",
        "data_test.drop([256], axis=1, inplace=True)\n",
        "data_val.drop([256], axis=1, inplace=True)\n",
        "data_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "outputId": "435eefd0-ae69-4f52-cd99-634f0e9d9431",
        "id": "eBZ1tR7fOiqa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py:4906: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  return super().drop(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            0         1         2         3         4         5         6    \\\n",
              "14797  1.575498  1.563810  1.554748  1.551273  1.554579  1.564238  1.578608   \n",
              "4442   1.770798  1.776173  1.783383  1.791552  1.799648  1.806729  1.812172   \n",
              "19385  1.735351  1.746334  1.752386  1.757505  1.764550  1.774997  1.789012   \n",
              "1667   1.701525  1.681062  1.662269  1.645431  1.630848  1.618785  1.609411   \n",
              "19969  1.616300  1.620086  1.621910  1.621008  1.616957  1.609821  1.600242   \n",
              "...         ...       ...       ...       ...       ...       ...       ...   \n",
              "2564   1.569072  1.567932  1.567012  1.566737  1.567516  1.569643  1.573193   \n",
              "17925  1.733440  1.741777  1.750718  1.760091  1.769664  1.779155  1.788254   \n",
              "1922   1.625553  1.627044  1.628636  1.630064  1.631115  1.631655  1.631648   \n",
              "19182  1.829093  1.808695  1.786890  1.764763  1.743234  1.722943  1.704218   \n",
              "2007   1.929730  1.870354  1.800231  1.728651  1.664408  1.614277  1.582014   \n",
              "\n",
              "            7         8         9    ...       246       247       248  \\\n",
              "14797  1.595385  1.612176  1.626981  ...  1.496059  1.496063  1.497101   \n",
              "4442   1.815827  1.818039  1.819563  ...  1.859191  1.857962  1.857110   \n",
              "19385  1.805769  1.823886  1.841890  ...  1.581357  1.584464  1.588439   \n",
              "1667   1.602731  1.598562  1.596527  ...  1.595596  1.593327  1.591406   \n",
              "19969  1.589441  1.579115  1.571215  ...  1.650422  1.652794  1.656397   \n",
              "...         ...       ...       ...  ...       ...       ...       ...   \n",
              "2564   1.577953  1.583408  1.588808  ...  1.613151  1.605560  1.596799   \n",
              "17925  1.796650  1.804062  1.810267  ...  1.684728  1.694811  1.708601   \n",
              "1922   1.631146  1.630273  1.629189  ...  1.575051  1.575915  1.575933   \n",
              "19182  1.687110  1.671490  1.657168  ...  1.599130  1.604341  1.608744   \n",
              "2007   1.568048  1.569857  1.582870  ...  1.742859  1.742550  1.741831   \n",
              "\n",
              "            249       250       251       252       253       254       255  \n",
              "14797  1.499070  1.501751  1.504783  1.507710  1.510087  1.511590  1.512108  \n",
              "4442   1.857146  1.858595  1.861911  1.867374  1.874999  1.884480  1.895171  \n",
              "19385  1.592877  1.597272  1.601050  1.603653  1.604633  1.603749  1.601042  \n",
              "1667   1.589981  1.589109  1.588789  1.588973  1.589581  1.590503  1.591591  \n",
              "19969  1.661812  1.669728  1.680872  1.695850  1.714960  1.738014  1.764247  \n",
              "...         ...       ...       ...       ...       ...       ...       ...  \n",
              "2564   1.587624  1.578812  1.571024  1.564695  1.559992  1.556846  1.555039  \n",
              "17925  1.722008  1.732289  1.738204  1.739854  1.738288  1.735035  1.731660  \n",
              "1922   1.575032  1.573324  1.571086  1.568699  1.566545  1.564915  1.563942  \n",
              "19182  1.611719  1.612854  1.612013  1.609360  1.605316  1.600460  1.595409  \n",
              "2007   1.740769  1.739280  1.737119  1.733914  1.729231  1.722660  1.713918  \n",
              "\n",
              "[24000 rows x 256 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-22967a7b-083d-4717-a982-64d254a5fa14\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>246</th>\n",
              "      <th>247</th>\n",
              "      <th>248</th>\n",
              "      <th>249</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>14797</th>\n",
              "      <td>1.575498</td>\n",
              "      <td>1.563810</td>\n",
              "      <td>1.554748</td>\n",
              "      <td>1.551273</td>\n",
              "      <td>1.554579</td>\n",
              "      <td>1.564238</td>\n",
              "      <td>1.578608</td>\n",
              "      <td>1.595385</td>\n",
              "      <td>1.612176</td>\n",
              "      <td>1.626981</td>\n",
              "      <td>...</td>\n",
              "      <td>1.496059</td>\n",
              "      <td>1.496063</td>\n",
              "      <td>1.497101</td>\n",
              "      <td>1.499070</td>\n",
              "      <td>1.501751</td>\n",
              "      <td>1.504783</td>\n",
              "      <td>1.507710</td>\n",
              "      <td>1.510087</td>\n",
              "      <td>1.511590</td>\n",
              "      <td>1.512108</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4442</th>\n",
              "      <td>1.770798</td>\n",
              "      <td>1.776173</td>\n",
              "      <td>1.783383</td>\n",
              "      <td>1.791552</td>\n",
              "      <td>1.799648</td>\n",
              "      <td>1.806729</td>\n",
              "      <td>1.812172</td>\n",
              "      <td>1.815827</td>\n",
              "      <td>1.818039</td>\n",
              "      <td>1.819563</td>\n",
              "      <td>...</td>\n",
              "      <td>1.859191</td>\n",
              "      <td>1.857962</td>\n",
              "      <td>1.857110</td>\n",
              "      <td>1.857146</td>\n",
              "      <td>1.858595</td>\n",
              "      <td>1.861911</td>\n",
              "      <td>1.867374</td>\n",
              "      <td>1.874999</td>\n",
              "      <td>1.884480</td>\n",
              "      <td>1.895171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19385</th>\n",
              "      <td>1.735351</td>\n",
              "      <td>1.746334</td>\n",
              "      <td>1.752386</td>\n",
              "      <td>1.757505</td>\n",
              "      <td>1.764550</td>\n",
              "      <td>1.774997</td>\n",
              "      <td>1.789012</td>\n",
              "      <td>1.805769</td>\n",
              "      <td>1.823886</td>\n",
              "      <td>1.841890</td>\n",
              "      <td>...</td>\n",
              "      <td>1.581357</td>\n",
              "      <td>1.584464</td>\n",
              "      <td>1.588439</td>\n",
              "      <td>1.592877</td>\n",
              "      <td>1.597272</td>\n",
              "      <td>1.601050</td>\n",
              "      <td>1.603653</td>\n",
              "      <td>1.604633</td>\n",
              "      <td>1.603749</td>\n",
              "      <td>1.601042</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1667</th>\n",
              "      <td>1.701525</td>\n",
              "      <td>1.681062</td>\n",
              "      <td>1.662269</td>\n",
              "      <td>1.645431</td>\n",
              "      <td>1.630848</td>\n",
              "      <td>1.618785</td>\n",
              "      <td>1.609411</td>\n",
              "      <td>1.602731</td>\n",
              "      <td>1.598562</td>\n",
              "      <td>1.596527</td>\n",
              "      <td>...</td>\n",
              "      <td>1.595596</td>\n",
              "      <td>1.593327</td>\n",
              "      <td>1.591406</td>\n",
              "      <td>1.589981</td>\n",
              "      <td>1.589109</td>\n",
              "      <td>1.588789</td>\n",
              "      <td>1.588973</td>\n",
              "      <td>1.589581</td>\n",
              "      <td>1.590503</td>\n",
              "      <td>1.591591</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19969</th>\n",
              "      <td>1.616300</td>\n",
              "      <td>1.620086</td>\n",
              "      <td>1.621910</td>\n",
              "      <td>1.621008</td>\n",
              "      <td>1.616957</td>\n",
              "      <td>1.609821</td>\n",
              "      <td>1.600242</td>\n",
              "      <td>1.589441</td>\n",
              "      <td>1.579115</td>\n",
              "      <td>1.571215</td>\n",
              "      <td>...</td>\n",
              "      <td>1.650422</td>\n",
              "      <td>1.652794</td>\n",
              "      <td>1.656397</td>\n",
              "      <td>1.661812</td>\n",
              "      <td>1.669728</td>\n",
              "      <td>1.680872</td>\n",
              "      <td>1.695850</td>\n",
              "      <td>1.714960</td>\n",
              "      <td>1.738014</td>\n",
              "      <td>1.764247</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2564</th>\n",
              "      <td>1.569072</td>\n",
              "      <td>1.567932</td>\n",
              "      <td>1.567012</td>\n",
              "      <td>1.566737</td>\n",
              "      <td>1.567516</td>\n",
              "      <td>1.569643</td>\n",
              "      <td>1.573193</td>\n",
              "      <td>1.577953</td>\n",
              "      <td>1.583408</td>\n",
              "      <td>1.588808</td>\n",
              "      <td>...</td>\n",
              "      <td>1.613151</td>\n",
              "      <td>1.605560</td>\n",
              "      <td>1.596799</td>\n",
              "      <td>1.587624</td>\n",
              "      <td>1.578812</td>\n",
              "      <td>1.571024</td>\n",
              "      <td>1.564695</td>\n",
              "      <td>1.559992</td>\n",
              "      <td>1.556846</td>\n",
              "      <td>1.555039</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17925</th>\n",
              "      <td>1.733440</td>\n",
              "      <td>1.741777</td>\n",
              "      <td>1.750718</td>\n",
              "      <td>1.760091</td>\n",
              "      <td>1.769664</td>\n",
              "      <td>1.779155</td>\n",
              "      <td>1.788254</td>\n",
              "      <td>1.796650</td>\n",
              "      <td>1.804062</td>\n",
              "      <td>1.810267</td>\n",
              "      <td>...</td>\n",
              "      <td>1.684728</td>\n",
              "      <td>1.694811</td>\n",
              "      <td>1.708601</td>\n",
              "      <td>1.722008</td>\n",
              "      <td>1.732289</td>\n",
              "      <td>1.738204</td>\n",
              "      <td>1.739854</td>\n",
              "      <td>1.738288</td>\n",
              "      <td>1.735035</td>\n",
              "      <td>1.731660</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1922</th>\n",
              "      <td>1.625553</td>\n",
              "      <td>1.627044</td>\n",
              "      <td>1.628636</td>\n",
              "      <td>1.630064</td>\n",
              "      <td>1.631115</td>\n",
              "      <td>1.631655</td>\n",
              "      <td>1.631648</td>\n",
              "      <td>1.631146</td>\n",
              "      <td>1.630273</td>\n",
              "      <td>1.629189</td>\n",
              "      <td>...</td>\n",
              "      <td>1.575051</td>\n",
              "      <td>1.575915</td>\n",
              "      <td>1.575933</td>\n",
              "      <td>1.575032</td>\n",
              "      <td>1.573324</td>\n",
              "      <td>1.571086</td>\n",
              "      <td>1.568699</td>\n",
              "      <td>1.566545</td>\n",
              "      <td>1.564915</td>\n",
              "      <td>1.563942</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19182</th>\n",
              "      <td>1.829093</td>\n",
              "      <td>1.808695</td>\n",
              "      <td>1.786890</td>\n",
              "      <td>1.764763</td>\n",
              "      <td>1.743234</td>\n",
              "      <td>1.722943</td>\n",
              "      <td>1.704218</td>\n",
              "      <td>1.687110</td>\n",
              "      <td>1.671490</td>\n",
              "      <td>1.657168</td>\n",
              "      <td>...</td>\n",
              "      <td>1.599130</td>\n",
              "      <td>1.604341</td>\n",
              "      <td>1.608744</td>\n",
              "      <td>1.611719</td>\n",
              "      <td>1.612854</td>\n",
              "      <td>1.612013</td>\n",
              "      <td>1.609360</td>\n",
              "      <td>1.605316</td>\n",
              "      <td>1.600460</td>\n",
              "      <td>1.595409</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2007</th>\n",
              "      <td>1.929730</td>\n",
              "      <td>1.870354</td>\n",
              "      <td>1.800231</td>\n",
              "      <td>1.728651</td>\n",
              "      <td>1.664408</td>\n",
              "      <td>1.614277</td>\n",
              "      <td>1.582014</td>\n",
              "      <td>1.568048</td>\n",
              "      <td>1.569857</td>\n",
              "      <td>1.582870</td>\n",
              "      <td>...</td>\n",
              "      <td>1.742859</td>\n",
              "      <td>1.742550</td>\n",
              "      <td>1.741831</td>\n",
              "      <td>1.740769</td>\n",
              "      <td>1.739280</td>\n",
              "      <td>1.737119</td>\n",
              "      <td>1.733914</td>\n",
              "      <td>1.729231</td>\n",
              "      <td>1.722660</td>\n",
              "      <td>1.713918</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>24000 rows × 256 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-22967a7b-083d-4717-a982-64d254a5fa14')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-22967a7b-083d-4717-a982-64d254a5fa14 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-22967a7b-083d-4717-a982-64d254a5fa14');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train=data_train.values\n",
        "X_test=data_test.values\n",
        "X_val=data_val.values"
      ],
      "metadata": {
        "id": "Rr80PXBHOiqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(X_val.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)\n",
        "print(y_val.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7ce4319-c349-489b-9569-7e7fecdd6c93",
        "id": "2OCVhzMEOiqa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(24000, 256)\n",
            "(6000, 256)\n",
            "(6000, 256)\n",
            "(24000, 101)\n",
            "(6000, 101)\n",
            "(6000, 101)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# X_train = X_train.reshape(1, 256, 24000)\n",
        "# X_test = X_test.reshape(1, 256, 6000)\n",
        "# X_val = X_val.reshape(1, 256, 6000)\n",
        "X_train = X_train.reshape(24000, 256, 1)\n",
        "X_test = X_test.reshape(6000, 256, 1)\n",
        "X_val = X_val.reshape(6000, 256, 1)\n",
        "X_train.shape, X_test.shape, X_val.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3024b6b6-e4c5-43b3-9b2a-a41d51b62e58",
        "id": "wyQIyKnkOiqa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((24000, 256, 1), (6000, 256, 1), (6000, 256, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## cnn-lstm 모델"
      ],
      "metadata": {
        "id": "GpFsZuLgOiqa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model():\n",
        "    model = Sequential()\n",
        "    model.add(Conv1D(filters=64, kernel_size=6, activation='relu', \n",
        "                    padding='same', input_shape=(256, 1)))\n",
        "    model.add(BatchNormalization())\n",
        "    \n",
        "    # adding a pooling layer\n",
        "    model.add(MaxPooling1D(pool_size=(3), strides=2, padding='same'))\n",
        "    \n",
        "    model.add(Conv1D(filters=64, kernel_size=6, activation='relu', \n",
        "                    padding='same', input_shape=(256, 1)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling1D(pool_size=(3), strides=2, padding='same'))\n",
        "    \n",
        "    model.add(Conv1D(filters=64, kernel_size=6, activation='relu', \n",
        "                    padding='same', input_shape=(256, 1)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling1D(pool_size=(3), strides=2, padding='same'))\n",
        "    \n",
        "    model.add(Flatten())\n",
        "    #model.add(Dense(64, activation='relu'))\n",
        "    #model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(101, activation='softmax'))\n",
        "    \n",
        "    model.compile(optimizer='adam', loss='mse',metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "NS-hyS2qOiqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model()\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2397946-5a7a-4f1f-e09b-f35229db7ab7",
        "id": "ZAw124CwOiqa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_18\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_17 (Conv1D)          (None, 256, 64)           448       \n",
            "                                                                 \n",
            " batch_normalization_12 (Bat  (None, 256, 64)          256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling1d_17 (MaxPoolin  (None, 128, 64)          0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_18 (Conv1D)          (None, 128, 64)           24640     \n",
            "                                                                 \n",
            " batch_normalization_13 (Bat  (None, 128, 64)          256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling1d_18 (MaxPoolin  (None, 64, 64)           0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_19 (Conv1D)          (None, 64, 64)            24640     \n",
            "                                                                 \n",
            " batch_normalization_14 (Bat  (None, 64, 64)           256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling1d_19 (MaxPoolin  (None, 32, 64)           0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " flatten_13 (Flatten)        (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_24 (Dense)            (None, 100)               204900    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 255,396\n",
            "Trainable params: 255,012\n",
            "Non-trainable params: 384\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## cnn-gru 모델"
      ],
      "metadata": {
        "id": "y3EL6TkMOiqb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Input, Embedding, GRU, LSTM, MaxPooling1D, GlobalMaxPool1D, CuDNNGRU\n",
        "from keras.layers import Dropout, Dense, Activation, Flatten, Conv1D, SpatialDropout1D\n",
        "from keras.models import Sequential\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv1D(filters=64, kernel_size=6, activation='relu', padding='same', input_shape=(256, 1)))\n",
        "model.add(SpatialDropout1D(0.2))\n",
        "model.add(MaxPooling1D(4))\n",
        "model.add(CuDNNGRU(64))\n",
        "\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Dense(101, activation='softmax'))\n",
        "model.compile(optimizer='adam', loss='mse',metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33f77877-e599-4ded-8400-373f5cec14bc",
        "id": "P1tvQQbcOiqb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_13 (Conv1D)          (None, 256, 64)           448       \n",
            "                                                                 \n",
            " spatial_dropout1d_4 (Spatia  (None, 256, 64)          0         \n",
            " lDropout1D)                                                     \n",
            "                                                                 \n",
            " max_pooling1d_13 (MaxPoolin  (None, 64, 64)           0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " cu_dnngru_4 (CuDNNGRU)      (None, 64)                24960     \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 101)               6565      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 31,973\n",
            "Trainable params: 31,973\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## gru모델"
      ],
      "metadata": {
        "id": "mnDw7nHbOiqb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(GRU(units=100, return_sequences=True, input_shape=(256,1))) #shape[1]=열\n",
        "model.add(GRU(units=50, return_sequences=True))\n",
        "model.add(GRU(units=50, return_sequences=True))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Dense(101, activation='softmax'))\n",
        "model.compile(optimizer='adam', loss='mse',metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60136cb5-1a96-431e-dc97-9ca7f73002f6",
        "id": "DzBk-L37Oiqb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " gru_37 (GRU)                (None, 256, 100)          30900     \n",
            "                                                                 \n",
            " gru_38 (GRU)                (None, 256, 50)           22800     \n",
            "                                                                 \n",
            " gru_39 (GRU)                (None, 256, 50)           15300     \n",
            "                                                                 \n",
            " flatten_8 (Flatten)         (None, 12800)             0         \n",
            "                                                                 \n",
            " dropout_16 (Dropout)        (None, 12800)             0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 101)               1292901   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,361,901\n",
            "Trainable params: 1,361,901\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSTM"
      ],
      "metadata": {
        "id": "zKaqqdRXOiqb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(units=100, return_sequences=True, input_shape=(256,1))) #shape[1]=열\n",
        "model.add(LSTM(units=50, return_sequences=True))\n",
        "model.add(LSTM(units=50, return_sequences=True))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Dense(101, activation='softmax'))\n",
        "model.compile(optimizer='adam', loss='mse',metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8055881e-f18e-4941-e7d3-1d853b39d2e3",
        "id": "FMLEPXX0Oiqb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_3 (LSTM)               (None, 256, 100)          40800     \n",
            "                                                                 \n",
            " lstm_4 (LSTM)               (None, 256, 50)           30200     \n",
            "                                                                 \n",
            " lstm_5 (LSTM)               (None, 256, 50)           20200     \n",
            "                                                                 \n",
            " flatten_6 (Flatten)         (None, 12800)             0         \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 12800)             0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 101)               1292901   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,384,101\n",
            "Trainable params: 1,384,101\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RNN모델"
      ],
      "metadata": {
        "id": "EdnsP_BpOiqb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Input, Embedding, GRU, LSTM, SimpleRNN, MaxPooling1D, GlobalMaxPool1D, CuDNNGRU\n",
        "from keras.layers import Dropout, Dense, Activation, Flatten, Conv1D, SpatialDropout1D\n",
        "from keras.models import Sequential\n",
        "\n",
        "model = Sequential()\n",
        "model.add(SimpleRNN(units=100, return_sequences=True, input_shape=(256,1))) #shape[1]=열\n",
        "model.add(SimpleRNN(units=50, return_sequences=True))\n",
        "model.add(SimpleRNN(units=50, return_sequences=True))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(101, activation='softmax'))\n",
        "model.compile(optimizer='adam', loss='mse',metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f56aa04-4c34-4860-fc44-299596d4f96f",
        "id": "HPYhoYU4Oiqb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " simple_rnn_12 (SimpleRNN)   (None, 256, 100)          10200     \n",
            "                                                                 \n",
            " simple_rnn_13 (SimpleRNN)   (None, 256, 50)           7550      \n",
            "                                                                 \n",
            " simple_rnn_14 (SimpleRNN)   (None, 256, 50)           5050      \n",
            "                                                                 \n",
            " flatten_4 (Flatten)         (None, 12800)             0         \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 12800)             0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 101)               1292901   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,315,701\n",
            "Trainable params: 1,315,701\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNN"
      ],
      "metadata": {
        "id": "udFX1BhNOiqb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model():\n",
        "  model = Sequential()\n",
        "  model.add(Conv1D(256, 8, padding='same', activation='relu', input_shape= (256,1)))\n",
        "  model.add(MaxPooling1D())\n",
        "  model.add(Conv1D(256, 8, padding='same', activation='relu'))\n",
        "  model.add(MaxPooling1D())\n",
        "  model.add(Conv1D(64, 8, padding='same',activation='relu'))\n",
        "  model.add(MaxPooling1D())\n",
        "  model.add(Conv1D(64, 8, padding='same', activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(101, activation='softmax')) # activation='softmax'\n",
        "\n",
        "  model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
        "  model.summary()\n",
        "  return model"
      ],
      "metadata": {
        "id": "S6on6HqgOiqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fc01b9a-a5a8-4976-c711-e65c2587ea5d",
        "id": "d4SYc-WbOiqb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_4 (Conv1D)           (None, 256, 256)          2304      \n",
            "                                                                 \n",
            " max_pooling1d_3 (MaxPooling  (None, 128, 256)         0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " conv1d_5 (Conv1D)           (None, 128, 256)          524544    \n",
            "                                                                 \n",
            " max_pooling1d_4 (MaxPooling  (None, 64, 256)          0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " conv1d_6 (Conv1D)           (None, 64, 64)            131136    \n",
            "                                                                 \n",
            " max_pooling1d_5 (MaxPooling  (None, 32, 64)           0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " conv1d_7 (Conv1D)           (None, 32, 64)            32832     \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 32, 64)           256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 101)               206949    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 898,021\n",
            "Trainable params: 897,893\n",
            "Non-trainable params: 128\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델학습/평가"
      ],
      "metadata": {
        "id": "brTFW1xjOiqb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train, y_train, batch_size = 512, epochs = 1000, verbose = 1, validation_data=(X_val, y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a84bc478-6ee0-437c-9452-60ab1c4ffbdd",
        "id": "t-st2iE7Oiqb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "47/47 [==============================] - 9s 99ms/step - loss: 0.0098 - accuracy: 0.0089 - val_loss: 0.0098 - val_accuracy: 0.0112\n",
            "Epoch 2/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 0.0098 - accuracy: 0.0105 - val_loss: 0.0098 - val_accuracy: 0.0092\n",
            "Epoch 3/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 0.0098 - accuracy: 0.0109 - val_loss: 0.0098 - val_accuracy: 0.0137\n",
            "Epoch 4/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 0.0098 - accuracy: 0.0138 - val_loss: 0.0098 - val_accuracy: 0.0095\n",
            "Epoch 5/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 0.0098 - accuracy: 0.0145 - val_loss: 0.0098 - val_accuracy: 0.0097\n",
            "Epoch 6/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 0.0098 - accuracy: 0.0105 - val_loss: 0.0098 - val_accuracy: 0.0122\n",
            "Epoch 7/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 0.0098 - accuracy: 0.0125 - val_loss: 0.0098 - val_accuracy: 0.0097\n",
            "Epoch 8/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 0.0098 - accuracy: 0.0170 - val_loss: 0.0098 - val_accuracy: 0.0205\n",
            "Epoch 9/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0098 - accuracy: 0.0192 - val_loss: 0.0098 - val_accuracy: 0.0193\n",
            "Epoch 10/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0098 - accuracy: 0.0225 - val_loss: 0.0098 - val_accuracy: 0.0247\n",
            "Epoch 11/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0098 - accuracy: 0.0300 - val_loss: 0.0098 - val_accuracy: 0.0248\n",
            "Epoch 12/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0097 - accuracy: 0.0345 - val_loss: 0.0098 - val_accuracy: 0.0342\n",
            "Epoch 13/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0097 - accuracy: 0.0412 - val_loss: 0.0098 - val_accuracy: 0.0363\n",
            "Epoch 14/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0097 - accuracy: 0.0497 - val_loss: 0.0097 - val_accuracy: 0.0438\n",
            "Epoch 15/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0097 - accuracy: 0.0560 - val_loss: 0.0097 - val_accuracy: 0.0438\n",
            "Epoch 16/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0097 - accuracy: 0.0617 - val_loss: 0.0097 - val_accuracy: 0.0475\n",
            "Epoch 17/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0097 - accuracy: 0.0741 - val_loss: 0.0097 - val_accuracy: 0.0475\n",
            "Epoch 18/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0096 - accuracy: 0.0758 - val_loss: 0.0097 - val_accuracy: 0.0565\n",
            "Epoch 19/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0096 - accuracy: 0.0871 - val_loss: 0.0097 - val_accuracy: 0.0632\n",
            "Epoch 20/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0096 - accuracy: 0.0963 - val_loss: 0.0097 - val_accuracy: 0.0810\n",
            "Epoch 21/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0096 - accuracy: 0.1046 - val_loss: 0.0097 - val_accuracy: 0.0850\n",
            "Epoch 22/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0095 - accuracy: 0.1145 - val_loss: 0.0097 - val_accuracy: 0.0887\n",
            "Epoch 23/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0094 - accuracy: 0.1278 - val_loss: 0.0096 - val_accuracy: 0.1043\n",
            "Epoch 24/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0094 - accuracy: 0.1459 - val_loss: 0.0095 - val_accuracy: 0.1125\n",
            "Epoch 25/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0093 - accuracy: 0.1616 - val_loss: 0.0095 - val_accuracy: 0.1348\n",
            "Epoch 26/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0092 - accuracy: 0.1793 - val_loss: 0.0095 - val_accuracy: 0.1345\n",
            "Epoch 27/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0091 - accuracy: 0.1939 - val_loss: 0.0094 - val_accuracy: 0.1553\n",
            "Epoch 28/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0090 - accuracy: 0.2077 - val_loss: 0.0093 - val_accuracy: 0.1588\n",
            "Epoch 29/1000\n",
            "47/47 [==============================] - 3s 75ms/step - loss: 0.0089 - accuracy: 0.2162 - val_loss: 0.0093 - val_accuracy: 0.1713\n",
            "Epoch 30/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0088 - accuracy: 0.2330 - val_loss: 0.0093 - val_accuracy: 0.1848\n",
            "Epoch 31/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0087 - accuracy: 0.2488 - val_loss: 0.0091 - val_accuracy: 0.2008\n",
            "Epoch 32/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 0.0086 - accuracy: 0.2620 - val_loss: 0.0091 - val_accuracy: 0.2065\n",
            "Epoch 33/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0085 - accuracy: 0.2813 - val_loss: 0.0090 - val_accuracy: 0.2310\n",
            "Epoch 34/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0083 - accuracy: 0.2993 - val_loss: 0.0088 - val_accuracy: 0.2517\n",
            "Epoch 35/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0082 - accuracy: 0.3160 - val_loss: 0.0088 - val_accuracy: 0.2592\n",
            "Epoch 36/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0080 - accuracy: 0.3355 - val_loss: 0.0087 - val_accuracy: 0.2810\n",
            "Epoch 37/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0079 - accuracy: 0.3517 - val_loss: 0.0086 - val_accuracy: 0.2935\n",
            "Epoch 38/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0077 - accuracy: 0.3726 - val_loss: 0.0085 - val_accuracy: 0.3032\n",
            "Epoch 39/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0076 - accuracy: 0.3892 - val_loss: 0.0083 - val_accuracy: 0.3188\n",
            "Epoch 40/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0073 - accuracy: 0.4132 - val_loss: 0.0082 - val_accuracy: 0.3438\n",
            "Epoch 41/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0072 - accuracy: 0.4312 - val_loss: 0.0080 - val_accuracy: 0.3567\n",
            "Epoch 42/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0070 - accuracy: 0.4546 - val_loss: 0.0080 - val_accuracy: 0.3723\n",
            "Epoch 43/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0068 - accuracy: 0.4780 - val_loss: 0.0077 - val_accuracy: 0.4018\n",
            "Epoch 44/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0065 - accuracy: 0.5049 - val_loss: 0.0077 - val_accuracy: 0.4025\n",
            "Epoch 45/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0064 - accuracy: 0.5167 - val_loss: 0.0075 - val_accuracy: 0.4115\n",
            "Epoch 46/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0060 - accuracy: 0.5410 - val_loss: 0.0073 - val_accuracy: 0.4457\n",
            "Epoch 47/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0058 - accuracy: 0.5600 - val_loss: 0.0071 - val_accuracy: 0.4612\n",
            "Epoch 48/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0056 - accuracy: 0.5818 - val_loss: 0.0069 - val_accuracy: 0.4860\n",
            "Epoch 49/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0053 - accuracy: 0.6052 - val_loss: 0.0067 - val_accuracy: 0.5023\n",
            "Epoch 50/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0052 - accuracy: 0.6186 - val_loss: 0.0065 - val_accuracy: 0.5138\n",
            "Epoch 51/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0049 - accuracy: 0.6396 - val_loss: 0.0064 - val_accuracy: 0.5308\n",
            "Epoch 52/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0047 - accuracy: 0.6575 - val_loss: 0.0062 - val_accuracy: 0.5470\n",
            "Epoch 53/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0045 - accuracy: 0.6751 - val_loss: 0.0059 - val_accuracy: 0.5683\n",
            "Epoch 54/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0043 - accuracy: 0.6929 - val_loss: 0.0055 - val_accuracy: 0.6000\n",
            "Epoch 55/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0040 - accuracy: 0.7143 - val_loss: 0.0055 - val_accuracy: 0.6085\n",
            "Epoch 56/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0038 - accuracy: 0.7316 - val_loss: 0.0053 - val_accuracy: 0.6200\n",
            "Epoch 57/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0036 - accuracy: 0.7453 - val_loss: 0.0053 - val_accuracy: 0.6228\n",
            "Epoch 58/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0035 - accuracy: 0.7519 - val_loss: 0.0048 - val_accuracy: 0.6607\n",
            "Epoch 59/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0033 - accuracy: 0.7668 - val_loss: 0.0047 - val_accuracy: 0.6613\n",
            "Epoch 60/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0031 - accuracy: 0.7835 - val_loss: 0.0046 - val_accuracy: 0.6732\n",
            "Epoch 61/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0029 - accuracy: 0.7958 - val_loss: 0.0045 - val_accuracy: 0.6827\n",
            "Epoch 62/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0029 - accuracy: 0.7993 - val_loss: 0.0045 - val_accuracy: 0.6863\n",
            "Epoch 63/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0027 - accuracy: 0.8110 - val_loss: 0.0043 - val_accuracy: 0.7000\n",
            "Epoch 64/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0026 - accuracy: 0.8183 - val_loss: 0.0043 - val_accuracy: 0.6975\n",
            "Epoch 65/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0025 - accuracy: 0.8258 - val_loss: 0.0041 - val_accuracy: 0.7142\n",
            "Epoch 66/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0024 - accuracy: 0.8350 - val_loss: 0.0039 - val_accuracy: 0.7272\n",
            "Epoch 67/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0023 - accuracy: 0.8440 - val_loss: 0.0038 - val_accuracy: 0.7323\n",
            "Epoch 68/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0022 - accuracy: 0.8500 - val_loss: 0.0038 - val_accuracy: 0.7352\n",
            "Epoch 69/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0021 - accuracy: 0.8563 - val_loss: 0.0037 - val_accuracy: 0.7463\n",
            "Epoch 70/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0020 - accuracy: 0.8617 - val_loss: 0.0036 - val_accuracy: 0.7565\n",
            "Epoch 71/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0020 - accuracy: 0.8643 - val_loss: 0.0036 - val_accuracy: 0.7508\n",
            "Epoch 72/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0019 - accuracy: 0.8720 - val_loss: 0.0035 - val_accuracy: 0.7607\n",
            "Epoch 73/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0018 - accuracy: 0.8767 - val_loss: 0.0034 - val_accuracy: 0.7657\n",
            "Epoch 74/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0018 - accuracy: 0.8802 - val_loss: 0.0035 - val_accuracy: 0.7620\n",
            "Epoch 75/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0017 - accuracy: 0.8831 - val_loss: 0.0034 - val_accuracy: 0.7688\n",
            "Epoch 76/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 0.0016 - accuracy: 0.8886 - val_loss: 0.0035 - val_accuracy: 0.7612\n",
            "Epoch 77/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0016 - accuracy: 0.8878 - val_loss: 0.0033 - val_accuracy: 0.7748\n",
            "Epoch 78/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0016 - accuracy: 0.8926 - val_loss: 0.0033 - val_accuracy: 0.7795\n",
            "Epoch 79/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0015 - accuracy: 0.8976 - val_loss: 0.0031 - val_accuracy: 0.7870\n",
            "Epoch 80/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0014 - accuracy: 0.9018 - val_loss: 0.0032 - val_accuracy: 0.7822\n",
            "Epoch 81/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0014 - accuracy: 0.9043 - val_loss: 0.0031 - val_accuracy: 0.7880\n",
            "Epoch 82/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0013 - accuracy: 0.9078 - val_loss: 0.0031 - val_accuracy: 0.7915\n",
            "Epoch 83/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0013 - accuracy: 0.9100 - val_loss: 0.0032 - val_accuracy: 0.7873\n",
            "Epoch 84/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0013 - accuracy: 0.9101 - val_loss: 0.0031 - val_accuracy: 0.7947\n",
            "Epoch 85/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0013 - accuracy: 0.9116 - val_loss: 0.0032 - val_accuracy: 0.7828\n",
            "Epoch 86/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0013 - accuracy: 0.9148 - val_loss: 0.0030 - val_accuracy: 0.7940\n",
            "Epoch 87/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0012 - accuracy: 0.9187 - val_loss: 0.0030 - val_accuracy: 0.8028\n",
            "Epoch 88/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0012 - accuracy: 0.9215 - val_loss: 0.0029 - val_accuracy: 0.8050\n",
            "Epoch 89/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0011 - accuracy: 0.9258 - val_loss: 0.0028 - val_accuracy: 0.8143\n",
            "Epoch 90/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0010 - accuracy: 0.9288 - val_loss: 0.0029 - val_accuracy: 0.8048\n",
            "Epoch 91/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0010 - accuracy: 0.9300 - val_loss: 0.0027 - val_accuracy: 0.8165\n",
            "Epoch 92/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0010 - accuracy: 0.9314 - val_loss: 0.0029 - val_accuracy: 0.8108\n",
            "Epoch 93/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0010 - accuracy: 0.9321 - val_loss: 0.0028 - val_accuracy: 0.8142\n",
            "Epoch 94/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.8530e-04 - accuracy: 0.9339 - val_loss: 0.0027 - val_accuracy: 0.8175\n",
            "Epoch 95/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.6037e-04 - accuracy: 0.9348 - val_loss: 0.0026 - val_accuracy: 0.8248\n",
            "Epoch 96/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.3638e-04 - accuracy: 0.9376 - val_loss: 0.0027 - val_accuracy: 0.8227\n",
            "Epoch 97/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.1995e-04 - accuracy: 0.9388 - val_loss: 0.0027 - val_accuracy: 0.8197\n",
            "Epoch 98/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.1754e-04 - accuracy: 0.9389 - val_loss: 0.0028 - val_accuracy: 0.8162\n",
            "Epoch 99/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.7136e-04 - accuracy: 0.9420 - val_loss: 0.0025 - val_accuracy: 0.8302\n",
            "Epoch 100/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.9106e-04 - accuracy: 0.9473 - val_loss: 0.0026 - val_accuracy: 0.8325\n",
            "Epoch 101/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.9243e-04 - accuracy: 0.9470 - val_loss: 0.0025 - val_accuracy: 0.8337\n",
            "Epoch 102/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.3716e-04 - accuracy: 0.9502 - val_loss: 0.0025 - val_accuracy: 0.8327\n",
            "Epoch 103/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.1944e-04 - accuracy: 0.9512 - val_loss: 0.0025 - val_accuracy: 0.8352\n",
            "Epoch 104/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.3837e-04 - accuracy: 0.9502 - val_loss: 0.0024 - val_accuracy: 0.8388\n",
            "Epoch 105/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.0659e-04 - accuracy: 0.9523 - val_loss: 0.0024 - val_accuracy: 0.8390\n",
            "Epoch 106/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 6.7221e-04 - accuracy: 0.9546 - val_loss: 0.0024 - val_accuracy: 0.8382\n",
            "Epoch 107/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.8317e-04 - accuracy: 0.9541 - val_loss: 0.0024 - val_accuracy: 0.8402\n",
            "Epoch 108/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.7168e-04 - accuracy: 0.9546 - val_loss: 0.0024 - val_accuracy: 0.8440\n",
            "Epoch 109/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.6043e-04 - accuracy: 0.9558 - val_loss: 0.0024 - val_accuracy: 0.8443\n",
            "Epoch 110/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.7339e-04 - accuracy: 0.9547 - val_loss: 0.0026 - val_accuracy: 0.8302\n",
            "Epoch 111/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.4527e-04 - accuracy: 0.9502 - val_loss: 0.0025 - val_accuracy: 0.8343\n",
            "Epoch 112/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.0148e-04 - accuracy: 0.9531 - val_loss: 0.0025 - val_accuracy: 0.8328\n",
            "Epoch 113/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.7353e-04 - accuracy: 0.9554 - val_loss: 0.0025 - val_accuracy: 0.8360\n",
            "Epoch 114/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.4132e-04 - accuracy: 0.9573 - val_loss: 0.0024 - val_accuracy: 0.8417\n",
            "Epoch 115/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.0448e-04 - accuracy: 0.9600 - val_loss: 0.0022 - val_accuracy: 0.8535\n",
            "Epoch 116/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.6624e-04 - accuracy: 0.9627 - val_loss: 0.0023 - val_accuracy: 0.8522\n",
            "Epoch 117/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.5607e-04 - accuracy: 0.9629 - val_loss: 0.0022 - val_accuracy: 0.8575\n",
            "Epoch 118/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.2401e-04 - accuracy: 0.9650 - val_loss: 0.0023 - val_accuracy: 0.8555\n",
            "Epoch 119/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.3566e-04 - accuracy: 0.9643 - val_loss: 0.0023 - val_accuracy: 0.8453\n",
            "Epoch 120/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.4579e-04 - accuracy: 0.9644 - val_loss: 0.0023 - val_accuracy: 0.8517\n",
            "Epoch 121/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.9285e-04 - accuracy: 0.9671 - val_loss: 0.0022 - val_accuracy: 0.8567\n",
            "Epoch 122/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.0639e-04 - accuracy: 0.9663 - val_loss: 0.0023 - val_accuracy: 0.8482\n",
            "Epoch 123/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.0124e-04 - accuracy: 0.9670 - val_loss: 0.0021 - val_accuracy: 0.8610\n",
            "Epoch 124/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.8784e-04 - accuracy: 0.9673 - val_loss: 0.0021 - val_accuracy: 0.8628\n",
            "Epoch 125/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.6374e-04 - accuracy: 0.9690 - val_loss: 0.0021 - val_accuracy: 0.8610\n",
            "Epoch 126/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.9002e-04 - accuracy: 0.9677 - val_loss: 0.0023 - val_accuracy: 0.8513\n",
            "Epoch 127/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.3811e-04 - accuracy: 0.9640 - val_loss: 0.0022 - val_accuracy: 0.8632\n",
            "Epoch 128/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.7448e-04 - accuracy: 0.9683 - val_loss: 0.0024 - val_accuracy: 0.8452\n",
            "Epoch 129/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.0849e-04 - accuracy: 0.9674 - val_loss: 0.0022 - val_accuracy: 0.8563\n",
            "Epoch 130/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.6832e-04 - accuracy: 0.9695 - val_loss: 0.0022 - val_accuracy: 0.8567\n",
            "Epoch 131/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.7121e-04 - accuracy: 0.9694 - val_loss: 0.0023 - val_accuracy: 0.8555\n",
            "Epoch 132/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.9900e-04 - accuracy: 0.9670 - val_loss: 0.0022 - val_accuracy: 0.8565\n",
            "Epoch 133/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.8753e-04 - accuracy: 0.9680 - val_loss: 0.0023 - val_accuracy: 0.8540\n",
            "Epoch 134/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.3735e-04 - accuracy: 0.9712 - val_loss: 0.0022 - val_accuracy: 0.8575\n",
            "Epoch 135/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.0359e-04 - accuracy: 0.9733 - val_loss: 0.0021 - val_accuracy: 0.8667\n",
            "Epoch 136/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.2251e-04 - accuracy: 0.9723 - val_loss: 0.0020 - val_accuracy: 0.8710\n",
            "Epoch 137/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.0121e-04 - accuracy: 0.9737 - val_loss: 0.0021 - val_accuracy: 0.8693\n",
            "Epoch 138/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.1016e-04 - accuracy: 0.9723 - val_loss: 0.0021 - val_accuracy: 0.8683\n",
            "Epoch 139/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.1006e-04 - accuracy: 0.9728 - val_loss: 0.0021 - val_accuracy: 0.8695\n",
            "Epoch 140/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.4258e-04 - accuracy: 0.9709 - val_loss: 0.0021 - val_accuracy: 0.8672\n",
            "Epoch 141/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.1841e-04 - accuracy: 0.9725 - val_loss: 0.0021 - val_accuracy: 0.8620\n",
            "Epoch 142/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.9058e-04 - accuracy: 0.9745 - val_loss: 0.0021 - val_accuracy: 0.8638\n",
            "Epoch 143/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.3957e-04 - accuracy: 0.9713 - val_loss: 0.0021 - val_accuracy: 0.8647\n",
            "Epoch 144/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.0425e-04 - accuracy: 0.9743 - val_loss: 0.0021 - val_accuracy: 0.8710\n",
            "Epoch 145/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.2505e-04 - accuracy: 0.9726 - val_loss: 0.0021 - val_accuracy: 0.8688\n",
            "Epoch 146/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.9461e-04 - accuracy: 0.9745 - val_loss: 0.0020 - val_accuracy: 0.8748\n",
            "Epoch 147/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.6591e-04 - accuracy: 0.9759 - val_loss: 0.0021 - val_accuracy: 0.8663\n",
            "Epoch 148/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.7667e-04 - accuracy: 0.9752 - val_loss: 0.0019 - val_accuracy: 0.8735\n",
            "Epoch 149/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.5264e-04 - accuracy: 0.9766 - val_loss: 0.0019 - val_accuracy: 0.8767\n",
            "Epoch 150/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 3.6154e-04 - accuracy: 0.9763 - val_loss: 0.0021 - val_accuracy: 0.8657\n",
            "Epoch 151/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.5757e-04 - accuracy: 0.9765 - val_loss: 0.0020 - val_accuracy: 0.8748\n",
            "Epoch 152/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.4223e-04 - accuracy: 0.9775 - val_loss: 0.0019 - val_accuracy: 0.8807\n",
            "Epoch 153/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.3548e-04 - accuracy: 0.9779 - val_loss: 0.0018 - val_accuracy: 0.8860\n",
            "Epoch 154/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.4029e-04 - accuracy: 0.9773 - val_loss: 0.0019 - val_accuracy: 0.8753\n",
            "Epoch 155/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.4848e-04 - accuracy: 0.9769 - val_loss: 0.0019 - val_accuracy: 0.8763\n",
            "Epoch 156/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.8151e-04 - accuracy: 0.9751 - val_loss: 0.0022 - val_accuracy: 0.8608\n",
            "Epoch 157/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.1859e-04 - accuracy: 0.9728 - val_loss: 0.0022 - val_accuracy: 0.8623\n",
            "Epoch 158/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.1186e-04 - accuracy: 0.9735 - val_loss: 0.0020 - val_accuracy: 0.8742\n",
            "Epoch 159/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.6662e-04 - accuracy: 0.9762 - val_loss: 0.0020 - val_accuracy: 0.8728\n",
            "Epoch 160/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.7469e-04 - accuracy: 0.9756 - val_loss: 0.0020 - val_accuracy: 0.8710\n",
            "Epoch 161/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.5508e-04 - accuracy: 0.9771 - val_loss: 0.0019 - val_accuracy: 0.8805\n",
            "Epoch 162/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.4416e-04 - accuracy: 0.9777 - val_loss: 0.0019 - val_accuracy: 0.8760\n",
            "Epoch 163/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.5647e-04 - accuracy: 0.9762 - val_loss: 0.0019 - val_accuracy: 0.8795\n",
            "Epoch 164/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.2158e-04 - accuracy: 0.9727 - val_loss: 0.0020 - val_accuracy: 0.8697\n",
            "Epoch 165/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.7422e-04 - accuracy: 0.9760 - val_loss: 0.0020 - val_accuracy: 0.8727\n",
            "Epoch 166/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.8776e-04 - accuracy: 0.9747 - val_loss: 0.0021 - val_accuracy: 0.8703\n",
            "Epoch 167/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.6791e-04 - accuracy: 0.9765 - val_loss: 0.0020 - val_accuracy: 0.8755\n",
            "Epoch 168/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.3217e-04 - accuracy: 0.9790 - val_loss: 0.0020 - val_accuracy: 0.8732\n",
            "Epoch 169/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.3497e-04 - accuracy: 0.9784 - val_loss: 0.0020 - val_accuracy: 0.8750\n",
            "Epoch 170/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.5662e-04 - accuracy: 0.9767 - val_loss: 0.0020 - val_accuracy: 0.8697\n",
            "Epoch 171/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.5634e-04 - accuracy: 0.9768 - val_loss: 0.0020 - val_accuracy: 0.8742\n",
            "Epoch 172/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.6275e-04 - accuracy: 0.9766 - val_loss: 0.0019 - val_accuracy: 0.8813\n",
            "Epoch 173/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.3599e-04 - accuracy: 0.9784 - val_loss: 0.0019 - val_accuracy: 0.8807\n",
            "Epoch 174/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.6232e-04 - accuracy: 0.9771 - val_loss: 0.0019 - val_accuracy: 0.8780\n",
            "Epoch 175/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.2441e-04 - accuracy: 0.9787 - val_loss: 0.0018 - val_accuracy: 0.8858\n",
            "Epoch 176/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.9663e-04 - accuracy: 0.9806 - val_loss: 0.0019 - val_accuracy: 0.8783\n",
            "Epoch 177/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.1961e-04 - accuracy: 0.9797 - val_loss: 0.0019 - val_accuracy: 0.8797\n",
            "Epoch 178/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.9877e-04 - accuracy: 0.9808 - val_loss: 0.0019 - val_accuracy: 0.8792\n",
            "Epoch 179/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.9609e-04 - accuracy: 0.9805 - val_loss: 0.0018 - val_accuracy: 0.8848\n",
            "Epoch 180/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.0448e-04 - accuracy: 0.9804 - val_loss: 0.0019 - val_accuracy: 0.8805\n",
            "Epoch 181/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 2.8955e-04 - accuracy: 0.9818 - val_loss: 0.0018 - val_accuracy: 0.8848\n",
            "Epoch 182/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 3.1941e-04 - accuracy: 0.9797 - val_loss: 0.0019 - val_accuracy: 0.8832\n",
            "Epoch 183/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.9092e-04 - accuracy: 0.9818 - val_loss: 0.0017 - val_accuracy: 0.8933\n",
            "Epoch 184/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.6479e-04 - accuracy: 0.9834 - val_loss: 0.0017 - val_accuracy: 0.8930\n",
            "Epoch 185/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.2105e-04 - accuracy: 0.9797 - val_loss: 0.0019 - val_accuracy: 0.8825\n",
            "Epoch 186/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.0577e-04 - accuracy: 0.9803 - val_loss: 0.0018 - val_accuracy: 0.8840\n",
            "Epoch 187/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.8259e-04 - accuracy: 0.9821 - val_loss: 0.0018 - val_accuracy: 0.8848\n",
            "Epoch 188/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.4593e-04 - accuracy: 0.9845 - val_loss: 0.0018 - val_accuracy: 0.8905\n",
            "Epoch 189/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 2.5664e-04 - accuracy: 0.9841 - val_loss: 0.0019 - val_accuracy: 0.8795\n",
            "Epoch 190/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.6548e-04 - accuracy: 0.9835 - val_loss: 0.0017 - val_accuracy: 0.8913\n",
            "Epoch 191/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.6040e-04 - accuracy: 0.9837 - val_loss: 0.0018 - val_accuracy: 0.8873\n",
            "Epoch 192/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.5778e-04 - accuracy: 0.9835 - val_loss: 0.0017 - val_accuracy: 0.8903\n",
            "Epoch 193/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.5127e-04 - accuracy: 0.9843 - val_loss: 0.0017 - val_accuracy: 0.8937\n",
            "Epoch 194/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.5107e-04 - accuracy: 0.9838 - val_loss: 0.0017 - val_accuracy: 0.8942\n",
            "Epoch 195/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.5094e-04 - accuracy: 0.9847 - val_loss: 0.0020 - val_accuracy: 0.8798\n",
            "Epoch 196/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.6697e-04 - accuracy: 0.9835 - val_loss: 0.0017 - val_accuracy: 0.8920\n",
            "Epoch 197/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3551e-04 - accuracy: 0.9855 - val_loss: 0.0017 - val_accuracy: 0.8945\n",
            "Epoch 198/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3858e-04 - accuracy: 0.9852 - val_loss: 0.0016 - val_accuracy: 0.8998\n",
            "Epoch 199/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.2851e-04 - accuracy: 0.9860 - val_loss: 0.0017 - val_accuracy: 0.8948\n",
            "Epoch 200/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3888e-04 - accuracy: 0.9850 - val_loss: 0.0017 - val_accuracy: 0.8917\n",
            "Epoch 201/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.4135e-04 - accuracy: 0.9852 - val_loss: 0.0019 - val_accuracy: 0.8848\n",
            "Epoch 202/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.4203e-04 - accuracy: 0.9850 - val_loss: 0.0016 - val_accuracy: 0.8967\n",
            "Epoch 203/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.5230e-04 - accuracy: 0.9840 - val_loss: 0.0019 - val_accuracy: 0.8820\n",
            "Epoch 204/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.4368e-04 - accuracy: 0.9847 - val_loss: 0.0017 - val_accuracy: 0.8970\n",
            "Epoch 205/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3397e-04 - accuracy: 0.9857 - val_loss: 0.0016 - val_accuracy: 0.8967\n",
            "Epoch 206/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.1547e-04 - accuracy: 0.9808 - val_loss: 0.0019 - val_accuracy: 0.8833\n",
            "Epoch 207/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.7720e-04 - accuracy: 0.9824 - val_loss: 0.0018 - val_accuracy: 0.8883\n",
            "Epoch 208/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.4352e-04 - accuracy: 0.9846 - val_loss: 0.0019 - val_accuracy: 0.8838\n",
            "Epoch 209/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3590e-04 - accuracy: 0.9851 - val_loss: 0.0016 - val_accuracy: 0.8967\n",
            "Epoch 210/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.4241e-04 - accuracy: 0.9848 - val_loss: 0.0019 - val_accuracy: 0.8875\n",
            "Epoch 211/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.8273e-04 - accuracy: 0.9822 - val_loss: 0.0017 - val_accuracy: 0.8952\n",
            "Epoch 212/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.7011e-04 - accuracy: 0.9827 - val_loss: 0.0018 - val_accuracy: 0.8925\n",
            "Epoch 213/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.7202e-04 - accuracy: 0.9830 - val_loss: 0.0019 - val_accuracy: 0.8840\n",
            "Epoch 214/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.4548e-04 - accuracy: 0.9782 - val_loss: 0.0019 - val_accuracy: 0.8800\n",
            "Epoch 215/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.9440e-04 - accuracy: 0.9812 - val_loss: 0.0018 - val_accuracy: 0.8892\n",
            "Epoch 216/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.8235e-04 - accuracy: 0.9824 - val_loss: 0.0018 - val_accuracy: 0.8912\n",
            "Epoch 217/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.5654e-04 - accuracy: 0.9839 - val_loss: 0.0018 - val_accuracy: 0.8927\n",
            "Epoch 218/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.4466e-04 - accuracy: 0.9848 - val_loss: 0.0016 - val_accuracy: 0.9022\n",
            "Epoch 219/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3129e-04 - accuracy: 0.9856 - val_loss: 0.0016 - val_accuracy: 0.9028\n",
            "Epoch 220/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3081e-04 - accuracy: 0.9860 - val_loss: 0.0016 - val_accuracy: 0.8988\n",
            "Epoch 221/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.2650e-04 - accuracy: 0.9862 - val_loss: 0.0017 - val_accuracy: 0.8970\n",
            "Epoch 222/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 2.1425e-04 - accuracy: 0.9869 - val_loss: 0.0015 - val_accuracy: 0.9048\n",
            "Epoch 223/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 2.0847e-04 - accuracy: 0.9875 - val_loss: 0.0016 - val_accuracy: 0.9042\n",
            "Epoch 224/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.1172e-04 - accuracy: 0.9870 - val_loss: 0.0016 - val_accuracy: 0.9023\n",
            "Epoch 225/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.1094e-04 - accuracy: 0.9868 - val_loss: 0.0016 - val_accuracy: 0.8998\n",
            "Epoch 226/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.8153e-04 - accuracy: 0.9824 - val_loss: 0.0021 - val_accuracy: 0.8685\n",
            "Epoch 227/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.9846e-04 - accuracy: 0.9810 - val_loss: 0.0017 - val_accuracy: 0.8957\n",
            "Epoch 228/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3543e-04 - accuracy: 0.9856 - val_loss: 0.0017 - val_accuracy: 0.8920\n",
            "Epoch 229/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.1990e-04 - accuracy: 0.9865 - val_loss: 0.0017 - val_accuracy: 0.8923\n",
            "Epoch 230/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.1376e-04 - accuracy: 0.9869 - val_loss: 0.0016 - val_accuracy: 0.9012\n",
            "Epoch 231/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.2995e-04 - accuracy: 0.9858 - val_loss: 0.0018 - val_accuracy: 0.8932\n",
            "Epoch 232/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.5614e-04 - accuracy: 0.9839 - val_loss: 0.0017 - val_accuracy: 0.8973\n",
            "Epoch 233/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3604e-04 - accuracy: 0.9853 - val_loss: 0.0017 - val_accuracy: 0.8932\n",
            "Epoch 234/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3142e-04 - accuracy: 0.9854 - val_loss: 0.0017 - val_accuracy: 0.8968\n",
            "Epoch 235/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.2967e-04 - accuracy: 0.9860 - val_loss: 0.0016 - val_accuracy: 0.9018\n",
            "Epoch 236/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.2283e-04 - accuracy: 0.9861 - val_loss: 0.0016 - val_accuracy: 0.9032\n",
            "Epoch 237/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.2276e-04 - accuracy: 0.9863 - val_loss: 0.0017 - val_accuracy: 0.8977\n",
            "Epoch 238/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.2789e-04 - accuracy: 0.9859 - val_loss: 0.0017 - val_accuracy: 0.8968\n",
            "Epoch 239/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 2.5838e-04 - accuracy: 0.9843 - val_loss: 0.0016 - val_accuracy: 0.9000\n",
            "Epoch 240/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.5298e-04 - accuracy: 0.9845 - val_loss: 0.0016 - val_accuracy: 0.9023\n",
            "Epoch 241/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3149e-04 - accuracy: 0.9860 - val_loss: 0.0015 - val_accuracy: 0.9057\n",
            "Epoch 242/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.6319e-04 - accuracy: 0.9841 - val_loss: 0.0017 - val_accuracy: 0.8978\n",
            "Epoch 243/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.0986e-04 - accuracy: 0.9810 - val_loss: 0.0017 - val_accuracy: 0.8975\n",
            "Epoch 244/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3203e-04 - accuracy: 0.9861 - val_loss: 0.0018 - val_accuracy: 0.8883\n",
            "Epoch 245/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 2.3101e-04 - accuracy: 0.9858 - val_loss: 0.0018 - val_accuracy: 0.8912\n",
            "Epoch 246/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.1981e-04 - accuracy: 0.9865 - val_loss: 0.0016 - val_accuracy: 0.9020\n",
            "Epoch 247/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.5738e-04 - accuracy: 0.9840 - val_loss: 0.0018 - val_accuracy: 0.8950\n",
            "Epoch 248/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.5225e-04 - accuracy: 0.9845 - val_loss: 0.0017 - val_accuracy: 0.8958\n",
            "Epoch 249/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.8230e-04 - accuracy: 0.9827 - val_loss: 0.0016 - val_accuracy: 0.9040\n",
            "Epoch 250/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3666e-04 - accuracy: 0.9856 - val_loss: 0.0017 - val_accuracy: 0.8947\n",
            "Epoch 251/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 2.7106e-04 - accuracy: 0.9828 - val_loss: 0.0017 - val_accuracy: 0.8937\n",
            "Epoch 252/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 2.3333e-04 - accuracy: 0.9856 - val_loss: 0.0018 - val_accuracy: 0.8928\n",
            "Epoch 253/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 2.0621e-04 - accuracy: 0.9875 - val_loss: 0.0018 - val_accuracy: 0.8940\n",
            "Epoch 254/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 2.2688e-04 - accuracy: 0.9859 - val_loss: 0.0017 - val_accuracy: 0.8970\n",
            "Epoch 255/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 2.7564e-04 - accuracy: 0.9830 - val_loss: 0.0020 - val_accuracy: 0.8820\n",
            "Epoch 256/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.8962e-04 - accuracy: 0.9818 - val_loss: 0.0018 - val_accuracy: 0.8903\n",
            "Epoch 257/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.5788e-04 - accuracy: 0.9843 - val_loss: 0.0016 - val_accuracy: 0.9020\n",
            "Epoch 258/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.2197e-04 - accuracy: 0.9865 - val_loss: 0.0017 - val_accuracy: 0.8993\n",
            "Epoch 259/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.1309e-04 - accuracy: 0.9871 - val_loss: 0.0016 - val_accuracy: 0.9045\n",
            "Epoch 260/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.0096e-04 - accuracy: 0.9880 - val_loss: 0.0016 - val_accuracy: 0.9055\n",
            "Epoch 261/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.0373e-04 - accuracy: 0.9877 - val_loss: 0.0016 - val_accuracy: 0.9032\n",
            "Epoch 262/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.0906e-04 - accuracy: 0.9874 - val_loss: 0.0015 - val_accuracy: 0.9082\n",
            "Epoch 263/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.0801e-04 - accuracy: 0.9875 - val_loss: 0.0015 - val_accuracy: 0.9102\n",
            "Epoch 264/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3843e-04 - accuracy: 0.9853 - val_loss: 0.0017 - val_accuracy: 0.8972\n",
            "Epoch 265/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.2784e-04 - accuracy: 0.9860 - val_loss: 0.0016 - val_accuracy: 0.9047\n",
            "Epoch 266/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.4002e-04 - accuracy: 0.9855 - val_loss: 0.0017 - val_accuracy: 0.8995\n",
            "Epoch 267/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.7639e-04 - accuracy: 0.9823 - val_loss: 0.0015 - val_accuracy: 0.9098\n",
            "Epoch 268/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.5447e-04 - accuracy: 0.9845 - val_loss: 0.0018 - val_accuracy: 0.8928\n",
            "Epoch 269/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3284e-04 - accuracy: 0.9856 - val_loss: 0.0017 - val_accuracy: 0.8975\n",
            "Epoch 270/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.4523e-04 - accuracy: 0.9848 - val_loss: 0.0016 - val_accuracy: 0.9030\n",
            "Epoch 271/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.4544e-04 - accuracy: 0.9850 - val_loss: 0.0016 - val_accuracy: 0.9062\n",
            "Epoch 272/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3729e-04 - accuracy: 0.9856 - val_loss: 0.0019 - val_accuracy: 0.8912\n",
            "Epoch 273/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3188e-04 - accuracy: 0.9859 - val_loss: 0.0015 - val_accuracy: 0.9082\n",
            "Epoch 274/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 2.1121e-04 - accuracy: 0.9874 - val_loss: 0.0015 - val_accuracy: 0.9085\n",
            "Epoch 275/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.1262e-04 - accuracy: 0.9872 - val_loss: 0.0016 - val_accuracy: 0.9033\n",
            "Epoch 276/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.9576e-04 - accuracy: 0.9819 - val_loss: 0.0017 - val_accuracy: 0.8970\n",
            "Epoch 277/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.4366e-04 - accuracy: 0.9858 - val_loss: 0.0017 - val_accuracy: 0.9015\n",
            "Epoch 278/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.0922e-04 - accuracy: 0.9875 - val_loss: 0.0017 - val_accuracy: 0.9005\n",
            "Epoch 279/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.0660e-04 - accuracy: 0.9876 - val_loss: 0.0015 - val_accuracy: 0.9118\n",
            "Epoch 280/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 2.0704e-04 - accuracy: 0.9874 - val_loss: 0.0015 - val_accuracy: 0.9085\n",
            "Epoch 281/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 2.0929e-04 - accuracy: 0.9873 - val_loss: 0.0017 - val_accuracy: 0.8962\n",
            "Epoch 282/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 2.1040e-04 - accuracy: 0.9876 - val_loss: 0.0015 - val_accuracy: 0.9127\n",
            "Epoch 283/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 2.0395e-04 - accuracy: 0.9877 - val_loss: 0.0016 - val_accuracy: 0.9057\n",
            "Epoch 284/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 2.0218e-04 - accuracy: 0.9878 - val_loss: 0.0016 - val_accuracy: 0.9033\n",
            "Epoch 285/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.0378e-04 - accuracy: 0.9875 - val_loss: 0.0014 - val_accuracy: 0.9138\n",
            "Epoch 286/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.9631e-04 - accuracy: 0.9880 - val_loss: 0.0015 - val_accuracy: 0.9118\n",
            "Epoch 287/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.9821e-04 - accuracy: 0.9880 - val_loss: 0.0016 - val_accuracy: 0.9063\n",
            "Epoch 288/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.1305e-04 - accuracy: 0.9870 - val_loss: 0.0018 - val_accuracy: 0.8883\n",
            "Epoch 289/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.4351e-04 - accuracy: 0.9851 - val_loss: 0.0015 - val_accuracy: 0.9102\n",
            "Epoch 290/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.5311e-04 - accuracy: 0.9845 - val_loss: 0.0016 - val_accuracy: 0.9057\n",
            "Epoch 291/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.4411e-04 - accuracy: 0.9847 - val_loss: 0.0016 - val_accuracy: 0.9038\n",
            "Epoch 292/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.6230e-04 - accuracy: 0.9836 - val_loss: 0.0019 - val_accuracy: 0.8880\n",
            "Epoch 293/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3455e-04 - accuracy: 0.9858 - val_loss: 0.0016 - val_accuracy: 0.9078\n",
            "Epoch 294/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.1188e-04 - accuracy: 0.9870 - val_loss: 0.0016 - val_accuracy: 0.9053\n",
            "Epoch 295/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 2.3233e-04 - accuracy: 0.9860 - val_loss: 0.0016 - val_accuracy: 0.9013\n",
            "Epoch 296/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.2515e-04 - accuracy: 0.9864 - val_loss: 0.0016 - val_accuracy: 0.9022\n",
            "Epoch 297/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.1057e-04 - accuracy: 0.9871 - val_loss: 0.0017 - val_accuracy: 0.8998\n",
            "Epoch 298/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3214e-04 - accuracy: 0.9859 - val_loss: 0.0017 - val_accuracy: 0.9012\n",
            "Epoch 299/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 2.2669e-04 - accuracy: 0.9866 - val_loss: 0.0014 - val_accuracy: 0.9153\n",
            "Epoch 300/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 2.0929e-04 - accuracy: 0.9876 - val_loss: 0.0015 - val_accuracy: 0.9082\n",
            "Epoch 301/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 2.1352e-04 - accuracy: 0.9873 - val_loss: 0.0016 - val_accuracy: 0.9047\n",
            "Epoch 302/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.9753e-04 - accuracy: 0.9883 - val_loss: 0.0016 - val_accuracy: 0.9060\n",
            "Epoch 303/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.8843e-04 - accuracy: 0.9891 - val_loss: 0.0017 - val_accuracy: 0.9028\n",
            "Epoch 304/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.0847e-04 - accuracy: 0.9875 - val_loss: 0.0015 - val_accuracy: 0.9103\n",
            "Epoch 305/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.0406e-04 - accuracy: 0.9880 - val_loss: 0.0016 - val_accuracy: 0.9043\n",
            "Epoch 306/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.2131e-04 - accuracy: 0.9868 - val_loss: 0.0015 - val_accuracy: 0.9105\n",
            "Epoch 307/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.9709e-04 - accuracy: 0.9880 - val_loss: 0.0016 - val_accuracy: 0.9055\n",
            "Epoch 308/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.0132e-04 - accuracy: 0.9877 - val_loss: 0.0016 - val_accuracy: 0.9047\n",
            "Epoch 309/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.9408e-04 - accuracy: 0.9885 - val_loss: 0.0016 - val_accuracy: 0.9028\n",
            "Epoch 310/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.0147e-04 - accuracy: 0.9879 - val_loss: 0.0018 - val_accuracy: 0.8928\n",
            "Epoch 311/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.0710e-04 - accuracy: 0.9877 - val_loss: 0.0015 - val_accuracy: 0.9128\n",
            "Epoch 312/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.2232e-04 - accuracy: 0.9865 - val_loss: 0.0020 - val_accuracy: 0.8817\n",
            "Epoch 313/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.4653e-04 - accuracy: 0.9850 - val_loss: 0.0018 - val_accuracy: 0.8898\n",
            "Epoch 314/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.9114e-04 - accuracy: 0.9887 - val_loss: 0.0015 - val_accuracy: 0.9093\n",
            "Epoch 315/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.8193e-04 - accuracy: 0.9892 - val_loss: 0.0015 - val_accuracy: 0.9110\n",
            "Epoch 316/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.9734e-04 - accuracy: 0.9885 - val_loss: 0.0016 - val_accuracy: 0.9078\n",
            "Epoch 317/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.9180e-04 - accuracy: 0.9888 - val_loss: 0.0014 - val_accuracy: 0.9167\n",
            "Epoch 318/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.9750e-04 - accuracy: 0.9883 - val_loss: 0.0015 - val_accuracy: 0.9087\n",
            "Epoch 319/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.8793e-04 - accuracy: 0.9889 - val_loss: 0.0015 - val_accuracy: 0.9093\n",
            "Epoch 320/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.9741e-04 - accuracy: 0.9885 - val_loss: 0.0017 - val_accuracy: 0.9020\n",
            "Epoch 321/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.0375e-04 - accuracy: 0.9877 - val_loss: 0.0015 - val_accuracy: 0.9100\n",
            "Epoch 322/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.0248e-04 - accuracy: 0.9878 - val_loss: 0.0019 - val_accuracy: 0.8893\n",
            "Epoch 323/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 2.3338e-04 - accuracy: 0.9860 - val_loss: 0.0016 - val_accuracy: 0.9007\n",
            "Epoch 324/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 2.4404e-04 - accuracy: 0.9854 - val_loss: 0.0016 - val_accuracy: 0.9072\n",
            "Epoch 325/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 2.0189e-04 - accuracy: 0.9880 - val_loss: 0.0017 - val_accuracy: 0.8975\n",
            "Epoch 326/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 2.3337e-04 - accuracy: 0.9860 - val_loss: 0.0018 - val_accuracy: 0.8952\n",
            "Epoch 327/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 3.0421e-04 - accuracy: 0.9815 - val_loss: 0.0017 - val_accuracy: 0.8997\n",
            "Epoch 328/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 2.4815e-04 - accuracy: 0.9850 - val_loss: 0.0016 - val_accuracy: 0.9082\n",
            "Epoch 329/1000\n",
            "47/47 [==============================] - 4s 76ms/step - loss: 2.1749e-04 - accuracy: 0.9870 - val_loss: 0.0015 - val_accuracy: 0.9118\n",
            "Epoch 330/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.9451e-04 - accuracy: 0.9886 - val_loss: 0.0014 - val_accuracy: 0.9137\n",
            "Epoch 331/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.8316e-04 - accuracy: 0.9890 - val_loss: 0.0015 - val_accuracy: 0.9088\n",
            "Epoch 332/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 2.0898e-04 - accuracy: 0.9871 - val_loss: 0.0016 - val_accuracy: 0.9065\n",
            "Epoch 333/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 2.0856e-04 - accuracy: 0.9873 - val_loss: 0.0016 - val_accuracy: 0.9042\n",
            "Epoch 334/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.9964e-04 - accuracy: 0.9882 - val_loss: 0.0016 - val_accuracy: 0.9063\n",
            "Epoch 335/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 2.3528e-04 - accuracy: 0.9854 - val_loss: 0.0017 - val_accuracy: 0.9017\n",
            "Epoch 336/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 2.4597e-04 - accuracy: 0.9849 - val_loss: 0.0016 - val_accuracy: 0.9035\n",
            "Epoch 337/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 2.2225e-04 - accuracy: 0.9866 - val_loss: 0.0015 - val_accuracy: 0.9095\n",
            "Epoch 338/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 2.0245e-04 - accuracy: 0.9880 - val_loss: 0.0015 - val_accuracy: 0.9095\n",
            "Epoch 339/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.8160e-04 - accuracy: 0.9896 - val_loss: 0.0014 - val_accuracy: 0.9155\n",
            "Epoch 340/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.8430e-04 - accuracy: 0.9894 - val_loss: 0.0016 - val_accuracy: 0.9040\n",
            "Epoch 341/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.9187e-04 - accuracy: 0.9890 - val_loss: 0.0015 - val_accuracy: 0.9095\n",
            "Epoch 342/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.8471e-04 - accuracy: 0.9894 - val_loss: 0.0016 - val_accuracy: 0.9048\n",
            "Epoch 343/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.7879e-04 - accuracy: 0.9893 - val_loss: 0.0015 - val_accuracy: 0.9083\n",
            "Epoch 344/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.7982e-04 - accuracy: 0.9895 - val_loss: 0.0015 - val_accuracy: 0.9118\n",
            "Epoch 345/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.8850e-04 - accuracy: 0.9893 - val_loss: 0.0016 - val_accuracy: 0.9028\n",
            "Epoch 346/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.8338e-04 - accuracy: 0.9894 - val_loss: 0.0016 - val_accuracy: 0.9088\n",
            "Epoch 347/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.6904e-04 - accuracy: 0.9900 - val_loss: 0.0015 - val_accuracy: 0.9115\n",
            "Epoch 348/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.8192e-04 - accuracy: 0.9896 - val_loss: 0.0015 - val_accuracy: 0.9108\n",
            "Epoch 349/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3071e-04 - accuracy: 0.9862 - val_loss: 0.0017 - val_accuracy: 0.9017\n",
            "Epoch 350/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 2.1710e-04 - accuracy: 0.9872 - val_loss: 0.0015 - val_accuracy: 0.9078\n",
            "Epoch 351/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.0173e-04 - accuracy: 0.9882 - val_loss: 0.0014 - val_accuracy: 0.9152\n",
            "Epoch 352/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.8382e-04 - accuracy: 0.9890 - val_loss: 0.0014 - val_accuracy: 0.9180\n",
            "Epoch 353/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.7557e-04 - accuracy: 0.9898 - val_loss: 0.0014 - val_accuracy: 0.9180\n",
            "Epoch 354/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.9991e-04 - accuracy: 0.9883 - val_loss: 0.0014 - val_accuracy: 0.9163\n",
            "Epoch 355/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.8504e-04 - accuracy: 0.9893 - val_loss: 0.0015 - val_accuracy: 0.9140\n",
            "Epoch 356/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.8408e-04 - accuracy: 0.9894 - val_loss: 0.0015 - val_accuracy: 0.9105\n",
            "Epoch 357/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.8106e-04 - accuracy: 0.9897 - val_loss: 0.0014 - val_accuracy: 0.9160\n",
            "Epoch 358/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.7470e-04 - accuracy: 0.9900 - val_loss: 0.0014 - val_accuracy: 0.9142\n",
            "Epoch 359/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.6577e-04 - accuracy: 0.9903 - val_loss: 0.0014 - val_accuracy: 0.9170\n",
            "Epoch 360/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.5901e-04 - accuracy: 0.9911 - val_loss: 0.0014 - val_accuracy: 0.9183\n",
            "Epoch 361/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.6815e-04 - accuracy: 0.9905 - val_loss: 0.0013 - val_accuracy: 0.9238\n",
            "Epoch 362/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.7319e-04 - accuracy: 0.9900 - val_loss: 0.0013 - val_accuracy: 0.9225\n",
            "Epoch 363/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.6706e-04 - accuracy: 0.9905 - val_loss: 0.0014 - val_accuracy: 0.9160\n",
            "Epoch 364/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.6618e-04 - accuracy: 0.9905 - val_loss: 0.0014 - val_accuracy: 0.9152\n",
            "Epoch 365/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.7445e-04 - accuracy: 0.9899 - val_loss: 0.0015 - val_accuracy: 0.9118\n",
            "Epoch 366/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.7420e-04 - accuracy: 0.9897 - val_loss: 0.0015 - val_accuracy: 0.9085\n",
            "Epoch 367/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.9168e-04 - accuracy: 0.9888 - val_loss: 0.0015 - val_accuracy: 0.9117\n",
            "Epoch 368/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.9297e-04 - accuracy: 0.9889 - val_loss: 0.0015 - val_accuracy: 0.9108\n",
            "Epoch 369/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.1346e-04 - accuracy: 0.9872 - val_loss: 0.0015 - val_accuracy: 0.9133\n",
            "Epoch 370/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.2091e-04 - accuracy: 0.9868 - val_loss: 0.0016 - val_accuracy: 0.9087\n",
            "Epoch 371/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.0587e-04 - accuracy: 0.9877 - val_loss: 0.0015 - val_accuracy: 0.9098\n",
            "Epoch 372/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.6218e-04 - accuracy: 0.9840 - val_loss: 0.0019 - val_accuracy: 0.8908\n",
            "Epoch 373/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.9341e-04 - accuracy: 0.9885 - val_loss: 0.0014 - val_accuracy: 0.9152\n",
            "Epoch 374/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.8246e-04 - accuracy: 0.9893 - val_loss: 0.0017 - val_accuracy: 0.9002\n",
            "Epoch 375/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 2.0762e-04 - accuracy: 0.9879 - val_loss: 0.0014 - val_accuracy: 0.9135\n",
            "Epoch 376/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.9584e-04 - accuracy: 0.9885 - val_loss: 0.0015 - val_accuracy: 0.9130\n",
            "Epoch 377/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.7723e-04 - accuracy: 0.9895 - val_loss: 0.0015 - val_accuracy: 0.9130\n",
            "Epoch 378/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.7253e-04 - accuracy: 0.9898 - val_loss: 0.0013 - val_accuracy: 0.9225\n",
            "Epoch 379/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.6687e-04 - accuracy: 0.9906 - val_loss: 0.0016 - val_accuracy: 0.9062\n",
            "Epoch 380/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.7148e-04 - accuracy: 0.9900 - val_loss: 0.0014 - val_accuracy: 0.9203\n",
            "Epoch 381/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.7557e-04 - accuracy: 0.9900 - val_loss: 0.0017 - val_accuracy: 0.9007\n",
            "Epoch 382/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 2.2569e-04 - accuracy: 0.9865 - val_loss: 0.0018 - val_accuracy: 0.8930\n",
            "Epoch 383/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 2.1039e-04 - accuracy: 0.9874 - val_loss: 0.0015 - val_accuracy: 0.9137\n",
            "Epoch 384/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.0144e-04 - accuracy: 0.9882 - val_loss: 0.0012 - val_accuracy: 0.9278\n",
            "Epoch 385/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.8513e-04 - accuracy: 0.9891 - val_loss: 0.0014 - val_accuracy: 0.9167\n",
            "Epoch 386/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.7851e-04 - accuracy: 0.9896 - val_loss: 0.0015 - val_accuracy: 0.9118\n",
            "Epoch 387/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.6560e-04 - accuracy: 0.9905 - val_loss: 0.0014 - val_accuracy: 0.9205\n",
            "Epoch 388/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.7447e-04 - accuracy: 0.9898 - val_loss: 0.0014 - val_accuracy: 0.9173\n",
            "Epoch 389/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.6890e-04 - accuracy: 0.9903 - val_loss: 0.0014 - val_accuracy: 0.9188\n",
            "Epoch 390/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.6793e-04 - accuracy: 0.9902 - val_loss: 0.0014 - val_accuracy: 0.9177\n",
            "Epoch 391/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.9007e-04 - accuracy: 0.9890 - val_loss: 0.0016 - val_accuracy: 0.9060\n",
            "Epoch 392/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.9308e-04 - accuracy: 0.9889 - val_loss: 0.0014 - val_accuracy: 0.9160\n",
            "Epoch 393/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.7896e-04 - accuracy: 0.9898 - val_loss: 0.0014 - val_accuracy: 0.9202\n",
            "Epoch 394/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.6107e-04 - accuracy: 0.9907 - val_loss: 0.0014 - val_accuracy: 0.9195\n",
            "Epoch 395/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.6285e-04 - accuracy: 0.9905 - val_loss: 0.0013 - val_accuracy: 0.9240\n",
            "Epoch 396/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.5332e-04 - accuracy: 0.9912 - val_loss: 0.0013 - val_accuracy: 0.9222\n",
            "Epoch 397/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.6101e-04 - accuracy: 0.9907 - val_loss: 0.0014 - val_accuracy: 0.9203\n",
            "Epoch 398/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.5849e-04 - accuracy: 0.9909 - val_loss: 0.0014 - val_accuracy: 0.9203\n",
            "Epoch 399/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.5825e-04 - accuracy: 0.9911 - val_loss: 0.0013 - val_accuracy: 0.9262\n",
            "Epoch 400/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.6329e-04 - accuracy: 0.9906 - val_loss: 0.0013 - val_accuracy: 0.9237\n",
            "Epoch 401/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.5869e-04 - accuracy: 0.9909 - val_loss: 0.0013 - val_accuracy: 0.9213\n",
            "Epoch 402/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.7692e-04 - accuracy: 0.9899 - val_loss: 0.0013 - val_accuracy: 0.9227\n",
            "Epoch 403/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.7880e-04 - accuracy: 0.9899 - val_loss: 0.0013 - val_accuracy: 0.9245\n",
            "Epoch 404/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.6161e-04 - accuracy: 0.9910 - val_loss: 0.0013 - val_accuracy: 0.9222\n",
            "Epoch 405/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.7992e-04 - accuracy: 0.9895 - val_loss: 0.0013 - val_accuracy: 0.9253\n",
            "Epoch 406/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.8641e-04 - accuracy: 0.9889 - val_loss: 0.0014 - val_accuracy: 0.9200\n",
            "Epoch 407/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 2.1955e-04 - accuracy: 0.9870 - val_loss: 0.0015 - val_accuracy: 0.9122\n",
            "Epoch 408/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.4596e-04 - accuracy: 0.9857 - val_loss: 0.0020 - val_accuracy: 0.8838\n",
            "Epoch 409/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 2.0712e-04 - accuracy: 0.9877 - val_loss: 0.0015 - val_accuracy: 0.9148\n",
            "Epoch 410/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.8100e-04 - accuracy: 0.9893 - val_loss: 0.0015 - val_accuracy: 0.9148\n",
            "Epoch 411/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.1986e-04 - accuracy: 0.9869 - val_loss: 0.0020 - val_accuracy: 0.8840\n",
            "Epoch 412/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.0764e-04 - accuracy: 0.9878 - val_loss: 0.0014 - val_accuracy: 0.9227\n",
            "Epoch 413/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.2336e-04 - accuracy: 0.9868 - val_loss: 0.0014 - val_accuracy: 0.9217\n",
            "Epoch 414/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.9342e-04 - accuracy: 0.9886 - val_loss: 0.0014 - val_accuracy: 0.9220\n",
            "Epoch 415/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.8022e-04 - accuracy: 0.9896 - val_loss: 0.0013 - val_accuracy: 0.9225\n",
            "Epoch 416/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.0609e-04 - accuracy: 0.9878 - val_loss: 0.0016 - val_accuracy: 0.9080\n",
            "Epoch 417/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.0243e-04 - accuracy: 0.9881 - val_loss: 0.0013 - val_accuracy: 0.9223\n",
            "Epoch 418/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.8040e-04 - accuracy: 0.9896 - val_loss: 0.0013 - val_accuracy: 0.9240\n",
            "Epoch 419/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.5379e-04 - accuracy: 0.9913 - val_loss: 0.0014 - val_accuracy: 0.9205\n",
            "Epoch 420/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.5195e-04 - accuracy: 0.9914 - val_loss: 0.0012 - val_accuracy: 0.9282\n",
            "Epoch 421/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.5463e-04 - accuracy: 0.9911 - val_loss: 0.0014 - val_accuracy: 0.9192\n",
            "Epoch 422/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.6640e-04 - accuracy: 0.9846 - val_loss: 0.0016 - val_accuracy: 0.9090\n",
            "Epoch 423/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.2279e-04 - accuracy: 0.9871 - val_loss: 0.0014 - val_accuracy: 0.9185\n",
            "Epoch 424/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.8670e-04 - accuracy: 0.9895 - val_loss: 0.0016 - val_accuracy: 0.9085\n",
            "Epoch 425/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.0572e-04 - accuracy: 0.9880 - val_loss: 0.0017 - val_accuracy: 0.9015\n",
            "Epoch 426/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.9847e-04 - accuracy: 0.9884 - val_loss: 0.0014 - val_accuracy: 0.9217\n",
            "Epoch 427/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.7448e-04 - accuracy: 0.9899 - val_loss: 0.0014 - val_accuracy: 0.9200\n",
            "Epoch 428/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.6366e-04 - accuracy: 0.9908 - val_loss: 0.0013 - val_accuracy: 0.9257\n",
            "Epoch 429/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.4459e-04 - accuracy: 0.9918 - val_loss: 0.0012 - val_accuracy: 0.9270\n",
            "Epoch 430/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.5671e-04 - accuracy: 0.9912 - val_loss: 0.0013 - val_accuracy: 0.9230\n",
            "Epoch 431/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.4869e-04 - accuracy: 0.9917 - val_loss: 0.0013 - val_accuracy: 0.9253\n",
            "Epoch 432/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.4681e-04 - accuracy: 0.9916 - val_loss: 0.0013 - val_accuracy: 0.9238\n",
            "Epoch 433/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.4913e-04 - accuracy: 0.9915 - val_loss: 0.0014 - val_accuracy: 0.9212\n",
            "Epoch 434/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.6608e-04 - accuracy: 0.9905 - val_loss: 0.0013 - val_accuracy: 0.9243\n",
            "Epoch 435/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.5977e-04 - accuracy: 0.9910 - val_loss: 0.0013 - val_accuracy: 0.9270\n",
            "Epoch 436/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.5638e-04 - accuracy: 0.9913 - val_loss: 0.0012 - val_accuracy: 0.9283\n",
            "Epoch 437/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.5224e-04 - accuracy: 0.9915 - val_loss: 0.0012 - val_accuracy: 0.9293\n",
            "Epoch 438/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.4430e-04 - accuracy: 0.9920 - val_loss: 0.0012 - val_accuracy: 0.9303\n",
            "Epoch 439/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.5412e-04 - accuracy: 0.9913 - val_loss: 0.0013 - val_accuracy: 0.9235\n",
            "Epoch 440/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.7686e-04 - accuracy: 0.9899 - val_loss: 0.0013 - val_accuracy: 0.9238\n",
            "Epoch 441/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.7075e-04 - accuracy: 0.9904 - val_loss: 0.0012 - val_accuracy: 0.9327\n",
            "Epoch 442/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.7061e-04 - accuracy: 0.9902 - val_loss: 0.0014 - val_accuracy: 0.9210\n",
            "Epoch 443/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.7072e-04 - accuracy: 0.9904 - val_loss: 0.0013 - val_accuracy: 0.9285\n",
            "Epoch 444/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.5313e-04 - accuracy: 0.9914 - val_loss: 0.0013 - val_accuracy: 0.9267\n",
            "Epoch 445/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.7139e-04 - accuracy: 0.9904 - val_loss: 0.0013 - val_accuracy: 0.9252\n",
            "Epoch 446/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.8174e-04 - accuracy: 0.9897 - val_loss: 0.0014 - val_accuracy: 0.9180\n",
            "Epoch 447/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.7716e-04 - accuracy: 0.9902 - val_loss: 0.0013 - val_accuracy: 0.9247\n",
            "Epoch 448/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.6056e-04 - accuracy: 0.9911 - val_loss: 0.0014 - val_accuracy: 0.9183\n",
            "Epoch 449/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.7256e-04 - accuracy: 0.9903 - val_loss: 0.0014 - val_accuracy: 0.9207\n",
            "Epoch 450/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 2.2549e-04 - accuracy: 0.9869 - val_loss: 0.0014 - val_accuracy: 0.9182\n",
            "Epoch 451/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 2.2020e-04 - accuracy: 0.9869 - val_loss: 0.0017 - val_accuracy: 0.9028\n",
            "Epoch 452/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.9259e-04 - accuracy: 0.9887 - val_loss: 0.0015 - val_accuracy: 0.9172\n",
            "Epoch 453/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.8614e-04 - accuracy: 0.9893 - val_loss: 0.0013 - val_accuracy: 0.9252\n",
            "Epoch 454/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.7648e-04 - accuracy: 0.9899 - val_loss: 0.0015 - val_accuracy: 0.9102\n",
            "Epoch 455/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.7713e-04 - accuracy: 0.9898 - val_loss: 0.0013 - val_accuracy: 0.9228\n",
            "Epoch 456/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.7775e-04 - accuracy: 0.9897 - val_loss: 0.0013 - val_accuracy: 0.9235\n",
            "Epoch 457/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.7735e-04 - accuracy: 0.9898 - val_loss: 0.0013 - val_accuracy: 0.9223\n",
            "Epoch 458/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.8757e-04 - accuracy: 0.9893 - val_loss: 0.0016 - val_accuracy: 0.9083\n",
            "Epoch 459/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.0904e-04 - accuracy: 0.9879 - val_loss: 0.0013 - val_accuracy: 0.9255\n",
            "Epoch 460/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.6753e-04 - accuracy: 0.9903 - val_loss: 0.0012 - val_accuracy: 0.9292\n",
            "Epoch 461/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.5330e-04 - accuracy: 0.9912 - val_loss: 0.0013 - val_accuracy: 0.9295\n",
            "Epoch 462/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.4887e-04 - accuracy: 0.9916 - val_loss: 0.0014 - val_accuracy: 0.9210\n",
            "Epoch 463/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.6385e-04 - accuracy: 0.9908 - val_loss: 0.0015 - val_accuracy: 0.9162\n",
            "Epoch 464/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.6895e-04 - accuracy: 0.9904 - val_loss: 0.0016 - val_accuracy: 0.9093\n",
            "Epoch 465/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.9222e-04 - accuracy: 0.9890 - val_loss: 0.0016 - val_accuracy: 0.9092\n",
            "Epoch 466/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.8539e-04 - accuracy: 0.9895 - val_loss: 0.0014 - val_accuracy: 0.9207\n",
            "Epoch 467/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.4960e-04 - accuracy: 0.9918 - val_loss: 0.0015 - val_accuracy: 0.9138\n",
            "Epoch 468/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.7137e-04 - accuracy: 0.9905 - val_loss: 0.0014 - val_accuracy: 0.9207\n",
            "Epoch 469/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.7324e-04 - accuracy: 0.9899 - val_loss: 0.0014 - val_accuracy: 0.9185\n",
            "Epoch 470/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.7234e-04 - accuracy: 0.9901 - val_loss: 0.0013 - val_accuracy: 0.9272\n",
            "Epoch 471/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.4541e-04 - accuracy: 0.9918 - val_loss: 0.0012 - val_accuracy: 0.9295\n",
            "Epoch 472/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.6112e-04 - accuracy: 0.9909 - val_loss: 0.0013 - val_accuracy: 0.9285\n",
            "Epoch 473/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.0341e-04 - accuracy: 0.9885 - val_loss: 0.0015 - val_accuracy: 0.9125\n",
            "Epoch 474/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.5354e-04 - accuracy: 0.9915 - val_loss: 0.0014 - val_accuracy: 0.9168\n",
            "Epoch 475/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.5564e-04 - accuracy: 0.9912 - val_loss: 0.0013 - val_accuracy: 0.9268\n",
            "Epoch 476/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.6397e-04 - accuracy: 0.9905 - val_loss: 0.0013 - val_accuracy: 0.9243\n",
            "Epoch 477/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.4157e-04 - accuracy: 0.9920 - val_loss: 0.0013 - val_accuracy: 0.9228\n",
            "Epoch 478/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.6097e-04 - accuracy: 0.9910 - val_loss: 0.0014 - val_accuracy: 0.9210\n",
            "Epoch 479/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.5258e-04 - accuracy: 0.9913 - val_loss: 0.0014 - val_accuracy: 0.9218\n",
            "Epoch 480/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.5073e-04 - accuracy: 0.9915 - val_loss: 0.0012 - val_accuracy: 0.9342\n",
            "Epoch 481/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.4133e-04 - accuracy: 0.9920 - val_loss: 0.0013 - val_accuracy: 0.9268\n",
            "Epoch 482/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.3749e-04 - accuracy: 0.9922 - val_loss: 0.0013 - val_accuracy: 0.9280\n",
            "Epoch 483/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.4530e-04 - accuracy: 0.9919 - val_loss: 0.0014 - val_accuracy: 0.9212\n",
            "Epoch 484/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.3940e-04 - accuracy: 0.9920 - val_loss: 0.0013 - val_accuracy: 0.9260\n",
            "Epoch 485/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.3979e-04 - accuracy: 0.9921 - val_loss: 0.0012 - val_accuracy: 0.9330\n",
            "Epoch 486/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.4186e-04 - accuracy: 0.9920 - val_loss: 0.0013 - val_accuracy: 0.9250\n",
            "Epoch 487/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.7837e-04 - accuracy: 0.9896 - val_loss: 0.0015 - val_accuracy: 0.9107\n",
            "Epoch 488/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.5073e-04 - accuracy: 0.9913 - val_loss: 0.0014 - val_accuracy: 0.9215\n",
            "Epoch 489/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.3857e-04 - accuracy: 0.9922 - val_loss: 0.0013 - val_accuracy: 0.9270\n",
            "Epoch 490/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.4585e-04 - accuracy: 0.9917 - val_loss: 0.0013 - val_accuracy: 0.9217\n",
            "Epoch 491/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.5732e-04 - accuracy: 0.9908 - val_loss: 0.0014 - val_accuracy: 0.9208\n",
            "Epoch 492/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.5137e-04 - accuracy: 0.9914 - val_loss: 0.0014 - val_accuracy: 0.9192\n",
            "Epoch 493/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.2256e-04 - accuracy: 0.9931 - val_loss: 0.0014 - val_accuracy: 0.9180\n",
            "Epoch 494/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.1459e-04 - accuracy: 0.9936 - val_loss: 0.0013 - val_accuracy: 0.9253\n",
            "Epoch 495/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.1274e-04 - accuracy: 0.9935 - val_loss: 0.0012 - val_accuracy: 0.9288\n",
            "Epoch 496/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.0524e-04 - accuracy: 0.9940 - val_loss: 0.0012 - val_accuracy: 0.9332\n",
            "Epoch 497/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.7421e-05 - accuracy: 0.9945 - val_loss: 0.0013 - val_accuracy: 0.9258\n",
            "Epoch 498/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.1340e-04 - accuracy: 0.9937 - val_loss: 0.0012 - val_accuracy: 0.9328\n",
            "Epoch 499/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.2126e-04 - accuracy: 0.9931 - val_loss: 0.0013 - val_accuracy: 0.9275\n",
            "Epoch 500/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.3592e-04 - accuracy: 0.9924 - val_loss: 0.0012 - val_accuracy: 0.9297\n",
            "Epoch 501/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.1679e-04 - accuracy: 0.9934 - val_loss: 0.0014 - val_accuracy: 0.9163\n",
            "Epoch 502/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.0924e-04 - accuracy: 0.9939 - val_loss: 0.0011 - val_accuracy: 0.9382\n",
            "Epoch 503/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.5908e-05 - accuracy: 0.9948 - val_loss: 0.0012 - val_accuracy: 0.9307\n",
            "Epoch 504/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.8602e-05 - accuracy: 0.9945 - val_loss: 0.0012 - val_accuracy: 0.9333\n",
            "Epoch 505/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.8212e-05 - accuracy: 0.9952 - val_loss: 0.0011 - val_accuracy: 0.9352\n",
            "Epoch 506/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.6054e-05 - accuracy: 0.9948 - val_loss: 0.0012 - val_accuracy: 0.9277\n",
            "Epoch 507/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.1118e-04 - accuracy: 0.9938 - val_loss: 0.0012 - val_accuracy: 0.9322\n",
            "Epoch 508/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.0640e-04 - accuracy: 0.9940 - val_loss: 0.0012 - val_accuracy: 0.9290\n",
            "Epoch 509/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.0582e-04 - accuracy: 0.9942 - val_loss: 0.0014 - val_accuracy: 0.9205\n",
            "Epoch 510/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.2501e-04 - accuracy: 0.9927 - val_loss: 0.0013 - val_accuracy: 0.9260\n",
            "Epoch 511/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.3125e-04 - accuracy: 0.9925 - val_loss: 0.0012 - val_accuracy: 0.9307\n",
            "Epoch 512/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.0804e-04 - accuracy: 0.9940 - val_loss: 0.0013 - val_accuracy: 0.9275\n",
            "Epoch 513/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.0816e-04 - accuracy: 0.9939 - val_loss: 0.0011 - val_accuracy: 0.9343\n",
            "Epoch 514/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.9831e-05 - accuracy: 0.9942 - val_loss: 0.0012 - val_accuracy: 0.9333\n",
            "Epoch 515/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.1337e-04 - accuracy: 0.9935 - val_loss: 0.0012 - val_accuracy: 0.9300\n",
            "Epoch 516/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.0631e-04 - accuracy: 0.9940 - val_loss: 0.0012 - val_accuracy: 0.9318\n",
            "Epoch 517/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.3143e-05 - accuracy: 0.9947 - val_loss: 0.0011 - val_accuracy: 0.9332\n",
            "Epoch 518/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.0014e-04 - accuracy: 0.9944 - val_loss: 0.0014 - val_accuracy: 0.9185\n",
            "Epoch 519/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.2093e-04 - accuracy: 0.9930 - val_loss: 0.0013 - val_accuracy: 0.9240\n",
            "Epoch 520/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.1184e-04 - accuracy: 0.9934 - val_loss: 0.0011 - val_accuracy: 0.9360\n",
            "Epoch 521/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.2304e-04 - accuracy: 0.9929 - val_loss: 0.0013 - val_accuracy: 0.9257\n",
            "Epoch 522/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.0121e-04 - accuracy: 0.9881 - val_loss: 0.0013 - val_accuracy: 0.9268\n",
            "Epoch 523/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.3609e-04 - accuracy: 0.9921 - val_loss: 0.0015 - val_accuracy: 0.9148\n",
            "Epoch 524/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.1763e-04 - accuracy: 0.9933 - val_loss: 0.0013 - val_accuracy: 0.9230\n",
            "Epoch 525/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.0427e-04 - accuracy: 0.9940 - val_loss: 0.0011 - val_accuracy: 0.9327\n",
            "Epoch 526/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.1578e-04 - accuracy: 0.9933 - val_loss: 0.0014 - val_accuracy: 0.9182\n",
            "Epoch 527/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.7508e-04 - accuracy: 0.9898 - val_loss: 0.0014 - val_accuracy: 0.9203\n",
            "Epoch 528/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.2967e-04 - accuracy: 0.9923 - val_loss: 0.0013 - val_accuracy: 0.9227\n",
            "Epoch 529/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.1848e-04 - accuracy: 0.9931 - val_loss: 0.0013 - val_accuracy: 0.9262\n",
            "Epoch 530/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.7733e-05 - accuracy: 0.9944 - val_loss: 0.0012 - val_accuracy: 0.9307\n",
            "Epoch 531/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.7776e-05 - accuracy: 0.9951 - val_loss: 0.0012 - val_accuracy: 0.9328\n",
            "Epoch 532/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.6362e-05 - accuracy: 0.9952 - val_loss: 0.0012 - val_accuracy: 0.9348\n",
            "Epoch 533/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.3632e-04 - accuracy: 0.9918 - val_loss: 0.0013 - val_accuracy: 0.9275\n",
            "Epoch 534/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.3099e-04 - accuracy: 0.9923 - val_loss: 0.0013 - val_accuracy: 0.9280\n",
            "Epoch 535/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.7449e-05 - accuracy: 0.9945 - val_loss: 0.0013 - val_accuracy: 0.9278\n",
            "Epoch 536/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.2235e-05 - accuracy: 0.9948 - val_loss: 0.0012 - val_accuracy: 0.9318\n",
            "Epoch 537/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 8.4635e-05 - accuracy: 0.9953 - val_loss: 0.0012 - val_accuracy: 0.9322\n",
            "Epoch 538/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.1821e-05 - accuracy: 0.9956 - val_loss: 0.0012 - val_accuracy: 0.9313\n",
            "Epoch 539/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.6814e-05 - accuracy: 0.9958 - val_loss: 0.0012 - val_accuracy: 0.9328\n",
            "Epoch 540/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.9070e-05 - accuracy: 0.9957 - val_loss: 0.0011 - val_accuracy: 0.9352\n",
            "Epoch 541/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.8983e-05 - accuracy: 0.9956 - val_loss: 0.0011 - val_accuracy: 0.9368\n",
            "Epoch 542/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.7955e-05 - accuracy: 0.9956 - val_loss: 0.0011 - val_accuracy: 0.9350\n",
            "Epoch 543/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.9400e-05 - accuracy: 0.9957 - val_loss: 0.0011 - val_accuracy: 0.9373\n",
            "Epoch 544/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.3483e-05 - accuracy: 0.9952 - val_loss: 0.0012 - val_accuracy: 0.9302\n",
            "Epoch 545/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.7933e-05 - accuracy: 0.9955 - val_loss: 0.0013 - val_accuracy: 0.9280\n",
            "Epoch 546/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.6197e-05 - accuracy: 0.9946 - val_loss: 0.0011 - val_accuracy: 0.9367\n",
            "Epoch 547/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.1836e-05 - accuracy: 0.9949 - val_loss: 0.0011 - val_accuracy: 0.9368\n",
            "Epoch 548/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.5455e-05 - accuracy: 0.9952 - val_loss: 0.0011 - val_accuracy: 0.9358\n",
            "Epoch 549/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.4215e-05 - accuracy: 0.9954 - val_loss: 0.0012 - val_accuracy: 0.9323\n",
            "Epoch 550/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.0214e-04 - accuracy: 0.9943 - val_loss: 0.0015 - val_accuracy: 0.9177\n",
            "Epoch 551/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.2041e-04 - accuracy: 0.9933 - val_loss: 0.0013 - val_accuracy: 0.9258\n",
            "Epoch 552/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.3861e-04 - accuracy: 0.9918 - val_loss: 0.0013 - val_accuracy: 0.9232\n",
            "Epoch 553/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.1353e-04 - accuracy: 0.9934 - val_loss: 0.0013 - val_accuracy: 0.9250\n",
            "Epoch 554/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.0584e-04 - accuracy: 0.9940 - val_loss: 0.0013 - val_accuracy: 0.9255\n",
            "Epoch 555/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.2343e-04 - accuracy: 0.9928 - val_loss: 0.0012 - val_accuracy: 0.9313\n",
            "Epoch 556/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.7942e-05 - accuracy: 0.9950 - val_loss: 0.0012 - val_accuracy: 0.9293\n",
            "Epoch 557/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.5381e-05 - accuracy: 0.9952 - val_loss: 0.0011 - val_accuracy: 0.9397\n",
            "Epoch 558/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.8088e-05 - accuracy: 0.9955 - val_loss: 0.0013 - val_accuracy: 0.9245\n",
            "Epoch 559/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.9101e-05 - accuracy: 0.9949 - val_loss: 0.0013 - val_accuracy: 0.9258\n",
            "Epoch 560/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.5525e-05 - accuracy: 0.9958 - val_loss: 0.0012 - val_accuracy: 0.9305\n",
            "Epoch 561/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.4129e-05 - accuracy: 0.9958 - val_loss: 0.0011 - val_accuracy: 0.9362\n",
            "Epoch 562/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.8985e-05 - accuracy: 0.9955 - val_loss: 0.0013 - val_accuracy: 0.9237\n",
            "Epoch 563/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.2215e-04 - accuracy: 0.9929 - val_loss: 0.0014 - val_accuracy: 0.9215\n",
            "Epoch 564/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.0763e-04 - accuracy: 0.9935 - val_loss: 0.0012 - val_accuracy: 0.9292\n",
            "Epoch 565/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.3382e-05 - accuracy: 0.9951 - val_loss: 0.0013 - val_accuracy: 0.9278\n",
            "Epoch 566/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.6037e-05 - accuracy: 0.9945 - val_loss: 0.0013 - val_accuracy: 0.9247\n",
            "Epoch 567/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.9887e-05 - accuracy: 0.9948 - val_loss: 0.0012 - val_accuracy: 0.9303\n",
            "Epoch 568/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.2365e-05 - accuracy: 0.9960 - val_loss: 0.0012 - val_accuracy: 0.9310\n",
            "Epoch 569/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.8133e-05 - accuracy: 0.9962 - val_loss: 0.0012 - val_accuracy: 0.9297\n",
            "Epoch 570/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.1243e-05 - accuracy: 0.9967 - val_loss: 0.0011 - val_accuracy: 0.9340\n",
            "Epoch 571/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.7341e-05 - accuracy: 0.9963 - val_loss: 0.0011 - val_accuracy: 0.9368\n",
            "Epoch 572/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.5824e-05 - accuracy: 0.9963 - val_loss: 0.0011 - val_accuracy: 0.9355\n",
            "Epoch 573/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.4612e-05 - accuracy: 0.9958 - val_loss: 0.0013 - val_accuracy: 0.9277\n",
            "Epoch 574/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.7452e-05 - accuracy: 0.9962 - val_loss: 0.0011 - val_accuracy: 0.9338\n",
            "Epoch 575/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.2048e-05 - accuracy: 0.9961 - val_loss: 0.0011 - val_accuracy: 0.9362\n",
            "Epoch 576/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.1708e-05 - accuracy: 0.9953 - val_loss: 0.0012 - val_accuracy: 0.9327\n",
            "Epoch 577/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.4098e-05 - accuracy: 0.9960 - val_loss: 0.0012 - val_accuracy: 0.9345\n",
            "Epoch 578/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.2964e-05 - accuracy: 0.9959 - val_loss: 0.0013 - val_accuracy: 0.9247\n",
            "Epoch 579/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.2008e-04 - accuracy: 0.9930 - val_loss: 0.0013 - val_accuracy: 0.9260\n",
            "Epoch 580/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.5284e-05 - accuracy: 0.9951 - val_loss: 0.0013 - val_accuracy: 0.9267\n",
            "Epoch 581/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.3354e-05 - accuracy: 0.9952 - val_loss: 0.0013 - val_accuracy: 0.9292\n",
            "Epoch 582/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 8.9298e-05 - accuracy: 0.9948 - val_loss: 0.0012 - val_accuracy: 0.9297\n",
            "Epoch 583/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.0824e-05 - accuracy: 0.9946 - val_loss: 0.0014 - val_accuracy: 0.9207\n",
            "Epoch 584/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.4924e-05 - accuracy: 0.9951 - val_loss: 0.0012 - val_accuracy: 0.9320\n",
            "Epoch 585/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.1951e-05 - accuracy: 0.9955 - val_loss: 0.0012 - val_accuracy: 0.9310\n",
            "Epoch 586/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.9880e-05 - accuracy: 0.9955 - val_loss: 0.0013 - val_accuracy: 0.9277\n",
            "Epoch 587/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.6789e-05 - accuracy: 0.9941 - val_loss: 0.0012 - val_accuracy: 0.9295\n",
            "Epoch 588/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.9502e-05 - accuracy: 0.9960 - val_loss: 0.0011 - val_accuracy: 0.9388\n",
            "Epoch 589/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.0515e-05 - accuracy: 0.9960 - val_loss: 0.0011 - val_accuracy: 0.9352\n",
            "Epoch 590/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.2283e-05 - accuracy: 0.9952 - val_loss: 0.0011 - val_accuracy: 0.9382\n",
            "Epoch 591/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.1506e-05 - accuracy: 0.9954 - val_loss: 0.0015 - val_accuracy: 0.9170\n",
            "Epoch 592/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.1870e-05 - accuracy: 0.9958 - val_loss: 0.0012 - val_accuracy: 0.9337\n",
            "Epoch 593/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.1888e-05 - accuracy: 0.9960 - val_loss: 0.0011 - val_accuracy: 0.9395\n",
            "Epoch 594/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.5984e-05 - accuracy: 0.9963 - val_loss: 0.0012 - val_accuracy: 0.9322\n",
            "Epoch 595/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.2728e-05 - accuracy: 0.9966 - val_loss: 0.0011 - val_accuracy: 0.9375\n",
            "Epoch 596/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.2113e-05 - accuracy: 0.9952 - val_loss: 0.0013 - val_accuracy: 0.9265\n",
            "Epoch 597/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.3522e-05 - accuracy: 0.9944 - val_loss: 0.0012 - val_accuracy: 0.9290\n",
            "Epoch 598/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.8021e-05 - accuracy: 0.9948 - val_loss: 0.0013 - val_accuracy: 0.9240\n",
            "Epoch 599/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.8433e-05 - accuracy: 0.9941 - val_loss: 0.0011 - val_accuracy: 0.9348\n",
            "Epoch 600/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.4721e-05 - accuracy: 0.9965 - val_loss: 0.0012 - val_accuracy: 0.9318\n",
            "Epoch 601/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.1052e-05 - accuracy: 0.9970 - val_loss: 0.0010 - val_accuracy: 0.9412\n",
            "Epoch 602/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.4622e-05 - accuracy: 0.9975 - val_loss: 0.0010 - val_accuracy: 0.9423\n",
            "Epoch 603/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.1702e-05 - accuracy: 0.9964 - val_loss: 0.0013 - val_accuracy: 0.9253\n",
            "Epoch 604/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.9905e-05 - accuracy: 0.9966 - val_loss: 0.0013 - val_accuracy: 0.9257\n",
            "Epoch 605/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.0439e-05 - accuracy: 0.9973 - val_loss: 0.0010 - val_accuracy: 0.9427\n",
            "Epoch 606/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.0130e-05 - accuracy: 0.9977 - val_loss: 0.0011 - val_accuracy: 0.9393\n",
            "Epoch 607/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.0891e-05 - accuracy: 0.9978 - val_loss: 0.0011 - val_accuracy: 0.9372\n",
            "Epoch 608/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.1351e-05 - accuracy: 0.9977 - val_loss: 0.0012 - val_accuracy: 0.9328\n",
            "Epoch 609/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.4009e-05 - accuracy: 0.9976 - val_loss: 0.0011 - val_accuracy: 0.9385\n",
            "Epoch 610/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.8992e-05 - accuracy: 0.9967 - val_loss: 0.0011 - val_accuracy: 0.9385\n",
            "Epoch 611/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.8840e-05 - accuracy: 0.9967 - val_loss: 0.0012 - val_accuracy: 0.9323\n",
            "Epoch 612/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 4.4343e-05 - accuracy: 0.9975 - val_loss: 0.0012 - val_accuracy: 0.9323\n",
            "Epoch 613/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 4.1690e-05 - accuracy: 0.9977 - val_loss: 0.0011 - val_accuracy: 0.9402\n",
            "Epoch 614/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.8941e-05 - accuracy: 0.9979 - val_loss: 0.0010 - val_accuracy: 0.9427\n",
            "Epoch 615/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.6263e-05 - accuracy: 0.9981 - val_loss: 0.0010 - val_accuracy: 0.9430\n",
            "Epoch 616/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.1489e-05 - accuracy: 0.9977 - val_loss: 0.0011 - val_accuracy: 0.9385\n",
            "Epoch 617/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.0220e-05 - accuracy: 0.9966 - val_loss: 0.0012 - val_accuracy: 0.9322\n",
            "Epoch 618/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.7290e-05 - accuracy: 0.9955 - val_loss: 0.0011 - val_accuracy: 0.9363\n",
            "Epoch 619/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.5473e-05 - accuracy: 0.9967 - val_loss: 0.0012 - val_accuracy: 0.9330\n",
            "Epoch 620/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.6897e-05 - accuracy: 0.9967 - val_loss: 0.0011 - val_accuracy: 0.9368\n",
            "Epoch 621/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.8516e-05 - accuracy: 0.9960 - val_loss: 0.0011 - val_accuracy: 0.9393\n",
            "Epoch 622/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.3115e-05 - accuracy: 0.9975 - val_loss: 9.7231e-04 - val_accuracy: 0.9448\n",
            "Epoch 623/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.9202e-05 - accuracy: 0.9965 - val_loss: 0.0012 - val_accuracy: 0.9280\n",
            "Epoch 624/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.0071e-05 - accuracy: 0.9953 - val_loss: 0.0012 - val_accuracy: 0.9317\n",
            "Epoch 625/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.6767e-05 - accuracy: 0.9963 - val_loss: 0.0012 - val_accuracy: 0.9320\n",
            "Epoch 626/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.6151e-05 - accuracy: 0.9963 - val_loss: 0.0012 - val_accuracy: 0.9330\n",
            "Epoch 627/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.1834e-05 - accuracy: 0.9971 - val_loss: 0.0012 - val_accuracy: 0.9343\n",
            "Epoch 628/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.2001e-05 - accuracy: 0.9969 - val_loss: 0.0012 - val_accuracy: 0.9328\n",
            "Epoch 629/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.9859e-05 - accuracy: 0.9965 - val_loss: 0.0013 - val_accuracy: 0.9275\n",
            "Epoch 630/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.6756e-05 - accuracy: 0.9973 - val_loss: 0.0011 - val_accuracy: 0.9388\n",
            "Epoch 631/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.9620e-05 - accuracy: 0.9966 - val_loss: 0.0011 - val_accuracy: 0.9362\n",
            "Epoch 632/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.0296e-05 - accuracy: 0.9967 - val_loss: 0.0011 - val_accuracy: 0.9352\n",
            "Epoch 633/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.5323e-05 - accuracy: 0.9980 - val_loss: 0.0013 - val_accuracy: 0.9243\n",
            "Epoch 634/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.8540e-05 - accuracy: 0.9960 - val_loss: 0.0013 - val_accuracy: 0.9240\n",
            "Epoch 635/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.1779e-05 - accuracy: 0.9964 - val_loss: 0.0011 - val_accuracy: 0.9357\n",
            "Epoch 636/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.7897e-05 - accuracy: 0.9973 - val_loss: 0.0012 - val_accuracy: 0.9320\n",
            "Epoch 637/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.1378e-05 - accuracy: 0.9977 - val_loss: 0.0011 - val_accuracy: 0.9385\n",
            "Epoch 638/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.2381e-05 - accuracy: 0.9977 - val_loss: 0.0011 - val_accuracy: 0.9338\n",
            "Epoch 639/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.2228e-04 - accuracy: 0.9927 - val_loss: 0.0013 - val_accuracy: 0.9253\n",
            "Epoch 640/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.0174e-05 - accuracy: 0.9957 - val_loss: 0.0011 - val_accuracy: 0.9353\n",
            "Epoch 641/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.6610e-05 - accuracy: 0.9943 - val_loss: 0.0013 - val_accuracy: 0.9228\n",
            "Epoch 642/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.1495e-05 - accuracy: 0.9951 - val_loss: 0.0014 - val_accuracy: 0.9233\n",
            "Epoch 643/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.6603e-05 - accuracy: 0.9974 - val_loss: 0.0011 - val_accuracy: 0.9368\n",
            "Epoch 644/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.7414e-05 - accuracy: 0.9974 - val_loss: 0.0012 - val_accuracy: 0.9353\n",
            "Epoch 645/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.7645e-05 - accuracy: 0.9972 - val_loss: 0.0011 - val_accuracy: 0.9342\n",
            "Epoch 646/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.3190e-05 - accuracy: 0.9976 - val_loss: 0.0011 - val_accuracy: 0.9372\n",
            "Epoch 647/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.1313e-05 - accuracy: 0.9978 - val_loss: 0.0010 - val_accuracy: 0.9418\n",
            "Epoch 648/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.3376e-05 - accuracy: 0.9981 - val_loss: 0.0011 - val_accuracy: 0.9397\n",
            "Epoch 649/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.8316e-05 - accuracy: 0.9978 - val_loss: 0.0010 - val_accuracy: 0.9422\n",
            "Epoch 650/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.2753e-05 - accuracy: 0.9976 - val_loss: 0.0011 - val_accuracy: 0.9398\n",
            "Epoch 651/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.6300e-05 - accuracy: 0.9979 - val_loss: 0.0011 - val_accuracy: 0.9360\n",
            "Epoch 652/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.6004e-05 - accuracy: 0.9980 - val_loss: 0.0011 - val_accuracy: 0.9362\n",
            "Epoch 653/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.5321e-05 - accuracy: 0.9982 - val_loss: 0.0010 - val_accuracy: 0.9413\n",
            "Epoch 654/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.0000e-05 - accuracy: 0.9952 - val_loss: 0.0010 - val_accuracy: 0.9430\n",
            "Epoch 655/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.4106e-05 - accuracy: 0.9963 - val_loss: 0.0012 - val_accuracy: 0.9317\n",
            "Epoch 656/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.1889e-05 - accuracy: 0.9952 - val_loss: 0.0011 - val_accuracy: 0.9347\n",
            "Epoch 657/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.7475e-05 - accuracy: 0.9949 - val_loss: 0.0013 - val_accuracy: 0.9285\n",
            "Epoch 658/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.4148e-04 - accuracy: 0.9919 - val_loss: 0.0011 - val_accuracy: 0.9370\n",
            "Epoch 659/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.7688e-05 - accuracy: 0.9955 - val_loss: 0.0011 - val_accuracy: 0.9358\n",
            "Epoch 660/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.6881e-05 - accuracy: 0.9968 - val_loss: 0.0010 - val_accuracy: 0.9407\n",
            "Epoch 661/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.7151e-05 - accuracy: 0.9957 - val_loss: 0.0011 - val_accuracy: 0.9372\n",
            "Epoch 662/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.8079e-05 - accuracy: 0.9968 - val_loss: 0.0011 - val_accuracy: 0.9372\n",
            "Epoch 663/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.4831e-05 - accuracy: 0.9975 - val_loss: 0.0011 - val_accuracy: 0.9377\n",
            "Epoch 664/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.8780e-05 - accuracy: 0.9961 - val_loss: 0.0015 - val_accuracy: 0.9152\n",
            "Epoch 665/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.2144e-04 - accuracy: 0.9931 - val_loss: 0.0012 - val_accuracy: 0.9328\n",
            "Epoch 666/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.8181e-05 - accuracy: 0.9966 - val_loss: 0.0012 - val_accuracy: 0.9348\n",
            "Epoch 667/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.2966e-05 - accuracy: 0.9970 - val_loss: 0.0011 - val_accuracy: 0.9377\n",
            "Epoch 668/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.5284e-05 - accuracy: 0.9973 - val_loss: 0.0014 - val_accuracy: 0.9233\n",
            "Epoch 669/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.6197e-05 - accuracy: 0.9975 - val_loss: 0.0011 - val_accuracy: 0.9362\n",
            "Epoch 670/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.0967e-05 - accuracy: 0.9977 - val_loss: 0.0011 - val_accuracy: 0.9397\n",
            "Epoch 671/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.0324e-05 - accuracy: 0.9979 - val_loss: 0.0010 - val_accuracy: 0.9408\n",
            "Epoch 672/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.8341e-05 - accuracy: 0.9979 - val_loss: 0.0010 - val_accuracy: 0.9418\n",
            "Epoch 673/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.1238e-05 - accuracy: 0.9977 - val_loss: 0.0011 - val_accuracy: 0.9353\n",
            "Epoch 674/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.8021e-05 - accuracy: 0.9973 - val_loss: 9.8343e-04 - val_accuracy: 0.9443\n",
            "Epoch 675/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.4385e-05 - accuracy: 0.9974 - val_loss: 9.8307e-04 - val_accuracy: 0.9445\n",
            "Epoch 676/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.6429e-05 - accuracy: 0.9980 - val_loss: 0.0011 - val_accuracy: 0.9393\n",
            "Epoch 677/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.4957e-05 - accuracy: 0.9980 - val_loss: 0.0010 - val_accuracy: 0.9417\n",
            "Epoch 678/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.8383e-05 - accuracy: 0.9978 - val_loss: 0.0010 - val_accuracy: 0.9428\n",
            "Epoch 679/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.2837e-05 - accuracy: 0.9970 - val_loss: 0.0010 - val_accuracy: 0.9420\n",
            "Epoch 680/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.9624e-05 - accuracy: 0.9967 - val_loss: 0.0011 - val_accuracy: 0.9395\n",
            "Epoch 681/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.0403e-05 - accuracy: 0.9967 - val_loss: 0.0010 - val_accuracy: 0.9413\n",
            "Epoch 682/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.7464e-05 - accuracy: 0.9979 - val_loss: 0.0011 - val_accuracy: 0.9410\n",
            "Epoch 683/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.8939e-05 - accuracy: 0.9979 - val_loss: 0.0010 - val_accuracy: 0.9410\n",
            "Epoch 684/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.4731e-05 - accuracy: 0.9981 - val_loss: 0.0011 - val_accuracy: 0.9393\n",
            "Epoch 685/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.1357e-05 - accuracy: 0.9977 - val_loss: 9.7767e-04 - val_accuracy: 0.9452\n",
            "Epoch 686/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.2104e-05 - accuracy: 0.9972 - val_loss: 0.0016 - val_accuracy: 0.9113\n",
            "Epoch 687/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.0510e-05 - accuracy: 0.9955 - val_loss: 0.0012 - val_accuracy: 0.9315\n",
            "Epoch 688/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.8809e-05 - accuracy: 0.9949 - val_loss: 0.0015 - val_accuracy: 0.9172\n",
            "Epoch 689/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.4410e-05 - accuracy: 0.9947 - val_loss: 0.0011 - val_accuracy: 0.9395\n",
            "Epoch 690/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.5128e-05 - accuracy: 0.9968 - val_loss: 0.0011 - val_accuracy: 0.9368\n",
            "Epoch 691/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.2201e-05 - accuracy: 0.9970 - val_loss: 0.0011 - val_accuracy: 0.9368\n",
            "Epoch 692/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.0864e-05 - accuracy: 0.9966 - val_loss: 0.0012 - val_accuracy: 0.9332\n",
            "Epoch 693/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.7576e-05 - accuracy: 0.9968 - val_loss: 0.0012 - val_accuracy: 0.9343\n",
            "Epoch 694/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.3572e-05 - accuracy: 0.9969 - val_loss: 0.0011 - val_accuracy: 0.9385\n",
            "Epoch 695/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.2194e-05 - accuracy: 0.9964 - val_loss: 0.0014 - val_accuracy: 0.9218\n",
            "Epoch 696/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.6503e-05 - accuracy: 0.9968 - val_loss: 0.0010 - val_accuracy: 0.9415\n",
            "Epoch 697/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.7072e-05 - accuracy: 0.9967 - val_loss: 0.0012 - val_accuracy: 0.9342\n",
            "Epoch 698/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.7409e-05 - accuracy: 0.9966 - val_loss: 0.0010 - val_accuracy: 0.9420\n",
            "Epoch 699/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.8052e-05 - accuracy: 0.9973 - val_loss: 9.7889e-04 - val_accuracy: 0.9450\n",
            "Epoch 700/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.0089e-05 - accuracy: 0.9969 - val_loss: 0.0010 - val_accuracy: 0.9422\n",
            "Epoch 701/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 4.4015e-05 - accuracy: 0.9976 - val_loss: 0.0011 - val_accuracy: 0.9367\n",
            "Epoch 702/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.5249e-05 - accuracy: 0.9981 - val_loss: 0.0010 - val_accuracy: 0.9412\n",
            "Epoch 703/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.7421e-05 - accuracy: 0.9979 - val_loss: 0.0010 - val_accuracy: 0.9422\n",
            "Epoch 704/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.9309e-05 - accuracy: 0.9977 - val_loss: 9.0465e-04 - val_accuracy: 0.9497\n",
            "Epoch 705/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.7882e-05 - accuracy: 0.9975 - val_loss: 0.0010 - val_accuracy: 0.9415\n",
            "Epoch 706/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.5510e-05 - accuracy: 0.9980 - val_loss: 0.0010 - val_accuracy: 0.9437\n",
            "Epoch 707/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.2644e-05 - accuracy: 0.9976 - val_loss: 0.0010 - val_accuracy: 0.9427\n",
            "Epoch 708/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.1858e-05 - accuracy: 0.9977 - val_loss: 0.0011 - val_accuracy: 0.9382\n",
            "Epoch 709/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.7387e-05 - accuracy: 0.9980 - val_loss: 0.0010 - val_accuracy: 0.9432\n",
            "Epoch 710/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.0983e-05 - accuracy: 0.9977 - val_loss: 0.0010 - val_accuracy: 0.9417\n",
            "Epoch 711/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.0552e-05 - accuracy: 0.9984 - val_loss: 0.0011 - val_accuracy: 0.9393\n",
            "Epoch 712/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.6037e-05 - accuracy: 0.9975 - val_loss: 0.0012 - val_accuracy: 0.9320\n",
            "Epoch 713/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.3740e-05 - accuracy: 0.9975 - val_loss: 0.0011 - val_accuracy: 0.9360\n",
            "Epoch 714/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.6512e-05 - accuracy: 0.9962 - val_loss: 0.0010 - val_accuracy: 0.9423\n",
            "Epoch 715/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.5009e-05 - accuracy: 0.9963 - val_loss: 0.0011 - val_accuracy: 0.9383\n",
            "Epoch 716/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.2442e-05 - accuracy: 0.9971 - val_loss: 0.0012 - val_accuracy: 0.9317\n",
            "Epoch 717/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.6960e-05 - accuracy: 0.9968 - val_loss: 0.0012 - val_accuracy: 0.9307\n",
            "Epoch 718/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.6880e-05 - accuracy: 0.9975 - val_loss: 0.0013 - val_accuracy: 0.9268\n",
            "Epoch 719/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.2975e-05 - accuracy: 0.9965 - val_loss: 0.0012 - val_accuracy: 0.9363\n",
            "Epoch 720/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.1433e-05 - accuracy: 0.9969 - val_loss: 0.0011 - val_accuracy: 0.9385\n",
            "Epoch 721/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.1211e-05 - accuracy: 0.9971 - val_loss: 0.0010 - val_accuracy: 0.9437\n",
            "Epoch 722/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.8120e-05 - accuracy: 0.9979 - val_loss: 9.9966e-04 - val_accuracy: 0.9432\n",
            "Epoch 723/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.0249e-05 - accuracy: 0.9972 - val_loss: 9.7714e-04 - val_accuracy: 0.9455\n",
            "Epoch 724/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.7931e-05 - accuracy: 0.9979 - val_loss: 0.0011 - val_accuracy: 0.9372\n",
            "Epoch 725/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.5971e-05 - accuracy: 0.9972 - val_loss: 0.0013 - val_accuracy: 0.9240\n",
            "Epoch 726/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.2036e-05 - accuracy: 0.9964 - val_loss: 0.0011 - val_accuracy: 0.9412\n",
            "Epoch 727/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.7508e-05 - accuracy: 0.9972 - val_loss: 0.0011 - val_accuracy: 0.9392\n",
            "Epoch 728/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.8383e-05 - accuracy: 0.9961 - val_loss: 0.0011 - val_accuracy: 0.9372\n",
            "Epoch 729/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.4991e-05 - accuracy: 0.9975 - val_loss: 9.9750e-04 - val_accuracy: 0.9442\n",
            "Epoch 730/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.6286e-05 - accuracy: 0.9975 - val_loss: 0.0010 - val_accuracy: 0.9425\n",
            "Epoch 731/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.8813e-05 - accuracy: 0.9972 - val_loss: 0.0012 - val_accuracy: 0.9322\n",
            "Epoch 732/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.8917e-05 - accuracy: 0.9966 - val_loss: 0.0011 - val_accuracy: 0.9347\n",
            "Epoch 733/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.0789e-05 - accuracy: 0.9972 - val_loss: 0.0011 - val_accuracy: 0.9390\n",
            "Epoch 734/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.6034e-05 - accuracy: 0.9981 - val_loss: 0.0010 - val_accuracy: 0.9413\n",
            "Epoch 735/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.9710e-05 - accuracy: 0.9978 - val_loss: 0.0012 - val_accuracy: 0.9315\n",
            "Epoch 736/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.4267e-05 - accuracy: 0.9976 - val_loss: 9.4494e-04 - val_accuracy: 0.9478\n",
            "Epoch 737/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.9964e-05 - accuracy: 0.9978 - val_loss: 0.0010 - val_accuracy: 0.9432\n",
            "Epoch 738/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.3641e-05 - accuracy: 0.9982 - val_loss: 0.0011 - val_accuracy: 0.9408\n",
            "Epoch 739/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.9521e-05 - accuracy: 0.9970 - val_loss: 0.0011 - val_accuracy: 0.9378\n",
            "Epoch 740/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.8676e-05 - accuracy: 0.9967 - val_loss: 0.0012 - val_accuracy: 0.9323\n",
            "Epoch 741/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.3599e-05 - accuracy: 0.9965 - val_loss: 0.0011 - val_accuracy: 0.9367\n",
            "Epoch 742/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.5590e-05 - accuracy: 0.9969 - val_loss: 0.0011 - val_accuracy: 0.9400\n",
            "Epoch 743/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.4985e-05 - accuracy: 0.9968 - val_loss: 0.0011 - val_accuracy: 0.9383\n",
            "Epoch 744/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.5777e-05 - accuracy: 0.9968 - val_loss: 0.0011 - val_accuracy: 0.9407\n",
            "Epoch 745/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.2842e-05 - accuracy: 0.9970 - val_loss: 0.0010 - val_accuracy: 0.9430\n",
            "Epoch 746/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.1283e-05 - accuracy: 0.9978 - val_loss: 0.0010 - val_accuracy: 0.9432\n",
            "Epoch 747/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.5347e-05 - accuracy: 0.9982 - val_loss: 0.0011 - val_accuracy: 0.9390\n",
            "Epoch 748/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.0258e-05 - accuracy: 0.9973 - val_loss: 0.0012 - val_accuracy: 0.9347\n",
            "Epoch 749/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 5.7478e-05 - accuracy: 0.9967 - val_loss: 0.0012 - val_accuracy: 0.9335\n",
            "Epoch 750/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.7311e-05 - accuracy: 0.9968 - val_loss: 0.0011 - val_accuracy: 0.9398\n",
            "Epoch 751/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.9680e-05 - accuracy: 0.9978 - val_loss: 0.0011 - val_accuracy: 0.9387\n",
            "Epoch 752/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.8349e-05 - accuracy: 0.9979 - val_loss: 0.0011 - val_accuracy: 0.9363\n",
            "Epoch 753/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.4002e-05 - accuracy: 0.9970 - val_loss: 0.0012 - val_accuracy: 0.9337\n",
            "Epoch 754/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.7525e-05 - accuracy: 0.9974 - val_loss: 0.0014 - val_accuracy: 0.9220\n",
            "Epoch 755/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.6031e-05 - accuracy: 0.9968 - val_loss: 0.0011 - val_accuracy: 0.9370\n",
            "Epoch 756/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.5814e-05 - accuracy: 0.9968 - val_loss: 0.0013 - val_accuracy: 0.9275\n",
            "Epoch 757/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.7857e-05 - accuracy: 0.9950 - val_loss: 0.0011 - val_accuracy: 0.9380\n",
            "Epoch 758/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.4168e-04 - accuracy: 0.9917 - val_loss: 0.0012 - val_accuracy: 0.9365\n",
            "Epoch 759/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.5879e-05 - accuracy: 0.9962 - val_loss: 0.0011 - val_accuracy: 0.9380\n",
            "Epoch 760/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.1275e-05 - accuracy: 0.9958 - val_loss: 0.0011 - val_accuracy: 0.9387\n",
            "Epoch 761/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.4725e-05 - accuracy: 0.9982 - val_loss: 9.4092e-04 - val_accuracy: 0.9473\n",
            "Epoch 762/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.0744e-05 - accuracy: 0.9977 - val_loss: 0.0012 - val_accuracy: 0.9358\n",
            "Epoch 763/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.1272e-05 - accuracy: 0.9972 - val_loss: 0.0010 - val_accuracy: 0.9428\n",
            "Epoch 764/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.2959e-05 - accuracy: 0.9976 - val_loss: 0.0011 - val_accuracy: 0.9400\n",
            "Epoch 765/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.6460e-05 - accuracy: 0.9980 - val_loss: 0.0010 - val_accuracy: 0.9428\n",
            "Epoch 766/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.3877e-05 - accuracy: 0.9975 - val_loss: 0.0012 - val_accuracy: 0.9343\n",
            "Epoch 767/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.9050e-05 - accuracy: 0.9979 - val_loss: 0.0010 - val_accuracy: 0.9437\n",
            "Epoch 768/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.3678e-05 - accuracy: 0.9976 - val_loss: 0.0015 - val_accuracy: 0.9178\n",
            "Epoch 769/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.5708e-05 - accuracy: 0.9969 - val_loss: 0.0011 - val_accuracy: 0.9388\n",
            "Epoch 770/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.4097e-04 - accuracy: 0.9918 - val_loss: 0.0013 - val_accuracy: 0.9257\n",
            "Epoch 771/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.0193e-04 - accuracy: 0.9939 - val_loss: 0.0013 - val_accuracy: 0.9263\n",
            "Epoch 772/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.5787e-05 - accuracy: 0.9951 - val_loss: 0.0011 - val_accuracy: 0.9363\n",
            "Epoch 773/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.8043e-05 - accuracy: 0.9973 - val_loss: 0.0010 - val_accuracy: 0.9443\n",
            "Epoch 774/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.0748e-05 - accuracy: 0.9977 - val_loss: 0.0013 - val_accuracy: 0.9265\n",
            "Epoch 775/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.5976e-05 - accuracy: 0.9980 - val_loss: 9.8216e-04 - val_accuracy: 0.9455\n",
            "Epoch 776/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.3528e-05 - accuracy: 0.9983 - val_loss: 9.4516e-04 - val_accuracy: 0.9477\n",
            "Epoch 777/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.9045e-05 - accuracy: 0.9985 - val_loss: 9.1797e-04 - val_accuracy: 0.9480\n",
            "Epoch 778/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.8418e-05 - accuracy: 0.9985 - val_loss: 9.7369e-04 - val_accuracy: 0.9452\n",
            "Epoch 779/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.0983e-05 - accuracy: 0.9984 - val_loss: 0.0012 - val_accuracy: 0.9352\n",
            "Epoch 780/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.1559e-05 - accuracy: 0.9965 - val_loss: 0.0011 - val_accuracy: 0.9372\n",
            "Epoch 781/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.7004e-05 - accuracy: 0.9979 - val_loss: 0.0013 - val_accuracy: 0.9283\n",
            "Epoch 782/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.2299e-05 - accuracy: 0.9969 - val_loss: 0.0012 - val_accuracy: 0.9338\n",
            "Epoch 783/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.1516e-05 - accuracy: 0.9972 - val_loss: 0.0011 - val_accuracy: 0.9407\n",
            "Epoch 784/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.9767e-05 - accuracy: 0.9977 - val_loss: 0.0011 - val_accuracy: 0.9408\n",
            "Epoch 785/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.5305e-05 - accuracy: 0.9981 - val_loss: 0.0012 - val_accuracy: 0.9318\n",
            "Epoch 786/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.7782e-05 - accuracy: 0.9980 - val_loss: 0.0010 - val_accuracy: 0.9440\n",
            "Epoch 787/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.2257e-05 - accuracy: 0.9983 - val_loss: 9.9698e-04 - val_accuracy: 0.9453\n",
            "Epoch 788/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.6929e-05 - accuracy: 0.9970 - val_loss: 0.0012 - val_accuracy: 0.9307\n",
            "Epoch 789/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.1166e-05 - accuracy: 0.9959 - val_loss: 0.0011 - val_accuracy: 0.9363\n",
            "Epoch 790/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.0419e-04 - accuracy: 0.9942 - val_loss: 0.0011 - val_accuracy: 0.9372\n",
            "Epoch 791/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.1236e-05 - accuracy: 0.9959 - val_loss: 0.0013 - val_accuracy: 0.9297\n",
            "Epoch 792/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.7932e-05 - accuracy: 0.9974 - val_loss: 0.0013 - val_accuracy: 0.9292\n",
            "Epoch 793/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.9113e-05 - accuracy: 0.9961 - val_loss: 0.0011 - val_accuracy: 0.9392\n",
            "Epoch 794/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.9641e-05 - accuracy: 0.9972 - val_loss: 0.0010 - val_accuracy: 0.9422\n",
            "Epoch 795/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.7032e-05 - accuracy: 0.9974 - val_loss: 0.0011 - val_accuracy: 0.9372\n",
            "Epoch 796/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.1019e-05 - accuracy: 0.9977 - val_loss: 0.0010 - val_accuracy: 0.9422\n",
            "Epoch 797/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 5.8347e-05 - accuracy: 0.9966 - val_loss: 0.0013 - val_accuracy: 0.9297\n",
            "Epoch 798/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.2773e-05 - accuracy: 0.9970 - val_loss: 0.0011 - val_accuracy: 0.9397\n",
            "Epoch 799/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.5166e-05 - accuracy: 0.9981 - val_loss: 0.0010 - val_accuracy: 0.9422\n",
            "Epoch 800/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.9416e-05 - accuracy: 0.9985 - val_loss: 0.0010 - val_accuracy: 0.9428\n",
            "Epoch 801/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.6224e-05 - accuracy: 0.9980 - val_loss: 0.0010 - val_accuracy: 0.9425\n",
            "Epoch 802/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.3893e-05 - accuracy: 0.9981 - val_loss: 0.0011 - val_accuracy: 0.9392\n",
            "Epoch 803/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.7977e-05 - accuracy: 0.9978 - val_loss: 0.0012 - val_accuracy: 0.9305\n",
            "Epoch 804/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.2382e-05 - accuracy: 0.9977 - val_loss: 0.0013 - val_accuracy: 0.9262\n",
            "Epoch 805/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.7205e-05 - accuracy: 0.9973 - val_loss: 0.0011 - val_accuracy: 0.9377\n",
            "Epoch 806/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.1102e-05 - accuracy: 0.9947 - val_loss: 0.0013 - val_accuracy: 0.9277\n",
            "Epoch 807/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.5765e-05 - accuracy: 0.9975 - val_loss: 0.0011 - val_accuracy: 0.9410\n",
            "Epoch 808/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.3967e-05 - accuracy: 0.9974 - val_loss: 0.0015 - val_accuracy: 0.9162\n",
            "Epoch 809/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.9188e-05 - accuracy: 0.9949 - val_loss: 0.0012 - val_accuracy: 0.9315\n",
            "Epoch 810/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.7527e-05 - accuracy: 0.9960 - val_loss: 0.0011 - val_accuracy: 0.9403\n",
            "Epoch 811/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.3949e-05 - accuracy: 0.9969 - val_loss: 0.0012 - val_accuracy: 0.9360\n",
            "Epoch 812/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.8706e-05 - accuracy: 0.9966 - val_loss: 0.0010 - val_accuracy: 0.9430\n",
            "Epoch 813/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.1684e-05 - accuracy: 0.9977 - val_loss: 0.0010 - val_accuracy: 0.9415\n",
            "Epoch 814/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.0018e-05 - accuracy: 0.9978 - val_loss: 0.0011 - val_accuracy: 0.9403\n",
            "Epoch 815/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.4725e-05 - accuracy: 0.9982 - val_loss: 9.9461e-04 - val_accuracy: 0.9452\n",
            "Epoch 816/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.6366e-05 - accuracy: 0.9980 - val_loss: 9.4478e-04 - val_accuracy: 0.9470\n",
            "Epoch 817/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.2692e-05 - accuracy: 0.9983 - val_loss: 9.8515e-04 - val_accuracy: 0.9442\n",
            "Epoch 818/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.6769e-05 - accuracy: 0.9986 - val_loss: 0.0010 - val_accuracy: 0.9432\n",
            "Epoch 819/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.2335e-05 - accuracy: 0.9982 - val_loss: 0.0010 - val_accuracy: 0.9420\n",
            "Epoch 820/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.6550e-05 - accuracy: 0.9972 - val_loss: 0.0010 - val_accuracy: 0.9415\n",
            "Epoch 821/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.7454e-05 - accuracy: 0.9979 - val_loss: 9.8409e-04 - val_accuracy: 0.9457\n",
            "Epoch 822/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.3912e-05 - accuracy: 0.9975 - val_loss: 9.7000e-04 - val_accuracy: 0.9475\n",
            "Epoch 823/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.8078e-05 - accuracy: 0.9967 - val_loss: 0.0011 - val_accuracy: 0.9383\n",
            "Epoch 824/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.8943e-05 - accuracy: 0.9954 - val_loss: 0.0011 - val_accuracy: 0.9388\n",
            "Epoch 825/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.8035e-05 - accuracy: 0.9972 - val_loss: 0.0011 - val_accuracy: 0.9385\n",
            "Epoch 826/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.5188e-05 - accuracy: 0.9981 - val_loss: 9.6904e-04 - val_accuracy: 0.9462\n",
            "Epoch 827/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.0382e-05 - accuracy: 0.9983 - val_loss: 0.0011 - val_accuracy: 0.9392\n",
            "Epoch 828/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.7713e-05 - accuracy: 0.9974 - val_loss: 0.0011 - val_accuracy: 0.9400\n",
            "Epoch 829/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.1780e-05 - accuracy: 0.9983 - val_loss: 0.0011 - val_accuracy: 0.9407\n",
            "Epoch 830/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.4599e-05 - accuracy: 0.9974 - val_loss: 9.6986e-04 - val_accuracy: 0.9448\n",
            "Epoch 831/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.6477e-05 - accuracy: 0.9971 - val_loss: 0.0011 - val_accuracy: 0.9368\n",
            "Epoch 832/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.3109e-05 - accuracy: 0.9971 - val_loss: 0.0011 - val_accuracy: 0.9365\n",
            "Epoch 833/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.3412e-05 - accuracy: 0.9964 - val_loss: 0.0013 - val_accuracy: 0.9272\n",
            "Epoch 834/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.6261e-05 - accuracy: 0.9962 - val_loss: 0.0010 - val_accuracy: 0.9417\n",
            "Epoch 835/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.3121e-05 - accuracy: 0.9957 - val_loss: 0.0011 - val_accuracy: 0.9347\n",
            "Epoch 836/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.6252e-05 - accuracy: 0.9974 - val_loss: 9.6605e-04 - val_accuracy: 0.9465\n",
            "Epoch 837/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.8100e-05 - accuracy: 0.9973 - val_loss: 0.0011 - val_accuracy: 0.9405\n",
            "Epoch 838/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.0091e-05 - accuracy: 0.9977 - val_loss: 0.0011 - val_accuracy: 0.9408\n",
            "Epoch 839/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.0109e-05 - accuracy: 0.9972 - val_loss: 0.0013 - val_accuracy: 0.9298\n",
            "Epoch 840/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.4484e-05 - accuracy: 0.9975 - val_loss: 0.0011 - val_accuracy: 0.9380\n",
            "Epoch 841/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.3827e-05 - accuracy: 0.9970 - val_loss: 0.0013 - val_accuracy: 0.9262\n",
            "Epoch 842/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.4944e-05 - accuracy: 0.9981 - val_loss: 9.7347e-04 - val_accuracy: 0.9463\n",
            "Epoch 843/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.9223e-05 - accuracy: 0.9985 - val_loss: 0.0010 - val_accuracy: 0.9448\n",
            "Epoch 844/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.5578e-05 - accuracy: 0.9981 - val_loss: 0.0011 - val_accuracy: 0.9373\n",
            "Epoch 845/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.3044e-05 - accuracy: 0.9982 - val_loss: 0.0012 - val_accuracy: 0.9328\n",
            "Epoch 846/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.5863e-05 - accuracy: 0.9980 - val_loss: 0.0011 - val_accuracy: 0.9363\n",
            "Epoch 847/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.2574e-04 - accuracy: 0.9926 - val_loss: 0.0014 - val_accuracy: 0.9215\n",
            "Epoch 848/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.9113e-05 - accuracy: 0.9940 - val_loss: 0.0011 - val_accuracy: 0.9368\n",
            "Epoch 849/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.5328e-05 - accuracy: 0.9958 - val_loss: 9.5385e-04 - val_accuracy: 0.9467\n",
            "Epoch 850/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.7563e-05 - accuracy: 0.9980 - val_loss: 0.0010 - val_accuracy: 0.9427\n",
            "Epoch 851/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.6247e-05 - accuracy: 0.9980 - val_loss: 0.0010 - val_accuracy: 0.9437\n",
            "Epoch 852/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.3272e-05 - accuracy: 0.9983 - val_loss: 9.2135e-04 - val_accuracy: 0.9487\n",
            "Epoch 853/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.6260e-05 - accuracy: 0.9987 - val_loss: 9.1465e-04 - val_accuracy: 0.9488\n",
            "Epoch 854/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.7439e-05 - accuracy: 0.9986 - val_loss: 8.6384e-04 - val_accuracy: 0.9532\n",
            "Epoch 855/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.2882e-05 - accuracy: 0.9988 - val_loss: 9.4744e-04 - val_accuracy: 0.9480\n",
            "Epoch 856/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3309e-05 - accuracy: 0.9988 - val_loss: 8.8539e-04 - val_accuracy: 0.9510\n",
            "Epoch 857/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3033e-05 - accuracy: 0.9988 - val_loss: 8.7205e-04 - val_accuracy: 0.9525\n",
            "Epoch 858/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3148e-05 - accuracy: 0.9988 - val_loss: 9.3413e-04 - val_accuracy: 0.9482\n",
            "Epoch 859/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3245e-05 - accuracy: 0.9988 - val_loss: 8.3571e-04 - val_accuracy: 0.9547\n",
            "Epoch 860/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3119e-05 - accuracy: 0.9988 - val_loss: 8.2863e-04 - val_accuracy: 0.9550\n",
            "Epoch 861/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3048e-05 - accuracy: 0.9988 - val_loss: 8.2228e-04 - val_accuracy: 0.9557\n",
            "Epoch 862/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3119e-05 - accuracy: 0.9988 - val_loss: 8.2163e-04 - val_accuracy: 0.9550\n",
            "Epoch 863/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3101e-05 - accuracy: 0.9988 - val_loss: 8.2185e-04 - val_accuracy: 0.9548\n",
            "Epoch 864/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3111e-05 - accuracy: 0.9988 - val_loss: 8.2069e-04 - val_accuracy: 0.9550\n",
            "Epoch 865/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3101e-05 - accuracy: 0.9988 - val_loss: 8.1999e-04 - val_accuracy: 0.9552\n",
            "Epoch 866/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3098e-05 - accuracy: 0.9988 - val_loss: 8.2075e-04 - val_accuracy: 0.9553\n",
            "Epoch 867/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.2821e-05 - accuracy: 0.9988 - val_loss: 8.7419e-04 - val_accuracy: 0.9517\n",
            "Epoch 868/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.4147e-05 - accuracy: 0.9988 - val_loss: 9.0634e-04 - val_accuracy: 0.9495\n",
            "Epoch 869/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3246e-05 - accuracy: 0.9988 - val_loss: 9.0296e-04 - val_accuracy: 0.9495\n",
            "Epoch 870/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3802e-05 - accuracy: 0.9988 - val_loss: 8.7295e-04 - val_accuracy: 0.9520\n",
            "Epoch 871/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3281e-05 - accuracy: 0.9988 - val_loss: 8.4781e-04 - val_accuracy: 0.9533\n",
            "Epoch 872/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3771e-05 - accuracy: 0.9988 - val_loss: 8.4619e-04 - val_accuracy: 0.9532\n",
            "Epoch 873/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3137e-05 - accuracy: 0.9988 - val_loss: 8.5238e-04 - val_accuracy: 0.9532\n",
            "Epoch 874/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3088e-05 - accuracy: 0.9988 - val_loss: 8.5175e-04 - val_accuracy: 0.9528\n",
            "Epoch 875/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3095e-05 - accuracy: 0.9988 - val_loss: 8.5271e-04 - val_accuracy: 0.9527\n",
            "Epoch 876/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3015e-05 - accuracy: 0.9988 - val_loss: 8.6339e-04 - val_accuracy: 0.9515\n",
            "Epoch 877/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3142e-05 - accuracy: 0.9988 - val_loss: 8.4451e-04 - val_accuracy: 0.9520\n",
            "Epoch 878/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.2954e-05 - accuracy: 0.9988 - val_loss: 8.4641e-04 - val_accuracy: 0.9520\n",
            "Epoch 879/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3457e-05 - accuracy: 0.9988 - val_loss: 8.4113e-04 - val_accuracy: 0.9533\n",
            "Epoch 880/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.8327e-05 - accuracy: 0.9985 - val_loss: 0.0011 - val_accuracy: 0.9390\n",
            "Epoch 881/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 5.2739e-05 - accuracy: 0.9970 - val_loss: 0.0012 - val_accuracy: 0.9348\n",
            "Epoch 882/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.4228e-04 - accuracy: 0.9920 - val_loss: 0.0014 - val_accuracy: 0.9227\n",
            "Epoch 883/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.4648e-04 - accuracy: 0.9916 - val_loss: 0.0013 - val_accuracy: 0.9290\n",
            "Epoch 884/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.5913e-04 - accuracy: 0.9906 - val_loss: 0.0014 - val_accuracy: 0.9230\n",
            "Epoch 885/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.2725e-04 - accuracy: 0.9927 - val_loss: 0.0011 - val_accuracy: 0.9375\n",
            "Epoch 886/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.9562e-05 - accuracy: 0.9961 - val_loss: 0.0013 - val_accuracy: 0.9313\n",
            "Epoch 887/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.7527e-05 - accuracy: 0.9950 - val_loss: 0.0014 - val_accuracy: 0.9237\n",
            "Epoch 888/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.2200e-05 - accuracy: 0.9955 - val_loss: 0.0011 - val_accuracy: 0.9405\n",
            "Epoch 889/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.4515e-05 - accuracy: 0.9970 - val_loss: 0.0010 - val_accuracy: 0.9447\n",
            "Epoch 890/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.5851e-05 - accuracy: 0.9975 - val_loss: 0.0010 - val_accuracy: 0.9418\n",
            "Epoch 891/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.9275e-05 - accuracy: 0.9976 - val_loss: 0.0011 - val_accuracy: 0.9397\n",
            "Epoch 892/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.8814e-05 - accuracy: 0.9972 - val_loss: 0.0010 - val_accuracy: 0.9450\n",
            "Epoch 893/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.3709e-05 - accuracy: 0.9981 - val_loss: 0.0010 - val_accuracy: 0.9417\n",
            "Epoch 894/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.3587e-05 - accuracy: 0.9981 - val_loss: 9.1138e-04 - val_accuracy: 0.9495\n",
            "Epoch 895/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.1307e-05 - accuracy: 0.9983 - val_loss: 0.0010 - val_accuracy: 0.9423\n",
            "Epoch 896/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.6739e-05 - accuracy: 0.9979 - val_loss: 0.0011 - val_accuracy: 0.9420\n",
            "Epoch 897/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.0975e-05 - accuracy: 0.9971 - val_loss: 0.0012 - val_accuracy: 0.9358\n",
            "Epoch 898/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.8054e-05 - accuracy: 0.9967 - val_loss: 9.8545e-04 - val_accuracy: 0.9453\n",
            "Epoch 899/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.7022e-05 - accuracy: 0.9980 - val_loss: 0.0010 - val_accuracy: 0.9450\n",
            "Epoch 900/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.6979e-05 - accuracy: 0.9975 - val_loss: 0.0011 - val_accuracy: 0.9405\n",
            "Epoch 901/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 3.7960e-05 - accuracy: 0.9980 - val_loss: 0.0011 - val_accuracy: 0.9403\n",
            "Epoch 902/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.9672e-05 - accuracy: 0.9977 - val_loss: 0.0011 - val_accuracy: 0.9377\n",
            "Epoch 903/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.1005e-05 - accuracy: 0.9977 - val_loss: 0.0011 - val_accuracy: 0.9402\n",
            "Epoch 904/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.4916e-05 - accuracy: 0.9980 - val_loss: 0.0010 - val_accuracy: 0.9415\n",
            "Epoch 905/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 3.7127e-05 - accuracy: 0.9978 - val_loss: 9.7983e-04 - val_accuracy: 0.9442\n",
            "Epoch 906/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.1333e-05 - accuracy: 0.9982 - val_loss: 9.2107e-04 - val_accuracy: 0.9478\n",
            "Epoch 907/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.7665e-05 - accuracy: 0.9985 - val_loss: 8.8048e-04 - val_accuracy: 0.9498\n",
            "Epoch 908/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.4645e-05 - accuracy: 0.9987 - val_loss: 9.2506e-04 - val_accuracy: 0.9492\n",
            "Epoch 909/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.6967e-05 - accuracy: 0.9986 - val_loss: 9.3922e-04 - val_accuracy: 0.9480\n",
            "Epoch 910/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.7762e-05 - accuracy: 0.9973 - val_loss: 0.0012 - val_accuracy: 0.9327\n",
            "Epoch 911/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.5917e-05 - accuracy: 0.9973 - val_loss: 9.1630e-04 - val_accuracy: 0.9498\n",
            "Epoch 912/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.0112e-05 - accuracy: 0.9977 - val_loss: 0.0010 - val_accuracy: 0.9417\n",
            "Epoch 913/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.4971e-05 - accuracy: 0.9967 - val_loss: 0.0012 - val_accuracy: 0.9357\n",
            "Epoch 914/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.3630e-05 - accuracy: 0.9958 - val_loss: 0.0010 - val_accuracy: 0.9432\n",
            "Epoch 915/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.8037e-05 - accuracy: 0.9974 - val_loss: 8.9811e-04 - val_accuracy: 0.9498\n",
            "Epoch 916/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.1884e-05 - accuracy: 0.9977 - val_loss: 0.0010 - val_accuracy: 0.9415\n",
            "Epoch 917/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.3318e-05 - accuracy: 0.9977 - val_loss: 9.6285e-04 - val_accuracy: 0.9477\n",
            "Epoch 918/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.6705e-05 - accuracy: 0.9981 - val_loss: 0.0011 - val_accuracy: 0.9418\n",
            "Epoch 919/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.9910e-05 - accuracy: 0.9978 - val_loss: 0.0011 - val_accuracy: 0.9415\n",
            "Epoch 920/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.0656e-05 - accuracy: 0.9973 - val_loss: 9.5155e-04 - val_accuracy: 0.9463\n",
            "Epoch 921/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.7187e-05 - accuracy: 0.9979 - val_loss: 8.8377e-04 - val_accuracy: 0.9517\n",
            "Epoch 922/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 3.0596e-05 - accuracy: 0.9984 - val_loss: 8.8639e-04 - val_accuracy: 0.9502\n",
            "Epoch 923/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.2286e-05 - accuracy: 0.9983 - val_loss: 8.6760e-04 - val_accuracy: 0.9515\n",
            "Epoch 924/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 3.0286e-05 - accuracy: 0.9984 - val_loss: 9.3801e-04 - val_accuracy: 0.9475\n",
            "Epoch 925/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.6477e-05 - accuracy: 0.9980 - val_loss: 0.0012 - val_accuracy: 0.9350\n",
            "Epoch 926/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 6.4678e-05 - accuracy: 0.9964 - val_loss: 0.0011 - val_accuracy: 0.9405\n",
            "Epoch 927/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.2123e-05 - accuracy: 0.9955 - val_loss: 0.0016 - val_accuracy: 0.9137\n",
            "Epoch 928/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.2850e-05 - accuracy: 0.9952 - val_loss: 0.0011 - val_accuracy: 0.9402\n",
            "Epoch 929/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.9464e-05 - accuracy: 0.9962 - val_loss: 0.0014 - val_accuracy: 0.9243\n",
            "Epoch 930/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.4980e-05 - accuracy: 0.9968 - val_loss: 9.7936e-04 - val_accuracy: 0.9462\n",
            "Epoch 931/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.4860e-05 - accuracy: 0.9970 - val_loss: 0.0012 - val_accuracy: 0.9345\n",
            "Epoch 932/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.3999e-05 - accuracy: 0.9975 - val_loss: 0.0010 - val_accuracy: 0.9435\n",
            "Epoch 933/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.5907e-05 - accuracy: 0.9975 - val_loss: 9.8348e-04 - val_accuracy: 0.9452\n",
            "Epoch 934/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 3.6037e-05 - accuracy: 0.9980 - val_loss: 0.0010 - val_accuracy: 0.9445\n",
            "Epoch 935/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.9121e-05 - accuracy: 0.9984 - val_loss: 8.8806e-04 - val_accuracy: 0.9512\n",
            "Epoch 936/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.3558e-05 - accuracy: 0.9975 - val_loss: 9.9807e-04 - val_accuracy: 0.9453\n",
            "Epoch 937/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.1737e-05 - accuracy: 0.9965 - val_loss: 0.0011 - val_accuracy: 0.9372\n",
            "Epoch 938/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 6.0842e-05 - accuracy: 0.9964 - val_loss: 0.0012 - val_accuracy: 0.9322\n",
            "Epoch 939/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.5004e-05 - accuracy: 0.9975 - val_loss: 0.0011 - val_accuracy: 0.9387\n",
            "Epoch 940/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 3.6728e-05 - accuracy: 0.9980 - val_loss: 0.0012 - val_accuracy: 0.9330\n",
            "Epoch 941/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.0287e-05 - accuracy: 0.9977 - val_loss: 0.0012 - val_accuracy: 0.9350\n",
            "Epoch 942/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.0744e-05 - accuracy: 0.9978 - val_loss: 9.6405e-04 - val_accuracy: 0.9472\n",
            "Epoch 943/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.7200e-05 - accuracy: 0.9986 - val_loss: 9.2243e-04 - val_accuracy: 0.9502\n",
            "Epoch 944/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.3263e-05 - accuracy: 0.9982 - val_loss: 9.4763e-04 - val_accuracy: 0.9473\n",
            "Epoch 945/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.9857e-05 - accuracy: 0.9977 - val_loss: 0.0010 - val_accuracy: 0.9445\n",
            "Epoch 946/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.2324e-05 - accuracy: 0.9977 - val_loss: 9.2171e-04 - val_accuracy: 0.9490\n",
            "Epoch 947/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 2.9716e-05 - accuracy: 0.9985 - val_loss: 9.5418e-04 - val_accuracy: 0.9477\n",
            "Epoch 948/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 2.4065e-05 - accuracy: 0.9988 - val_loss: 8.9727e-04 - val_accuracy: 0.9508\n",
            "Epoch 949/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 2.3093e-05 - accuracy: 0.9988 - val_loss: 8.9699e-04 - val_accuracy: 0.9507\n",
            "Epoch 950/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 2.3103e-05 - accuracy: 0.9988 - val_loss: 8.9756e-04 - val_accuracy: 0.9507\n",
            "Epoch 951/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 2.5140e-05 - accuracy: 0.9987 - val_loss: 9.5281e-04 - val_accuracy: 0.9477\n",
            "Epoch 952/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 2.4944e-05 - accuracy: 0.9987 - val_loss: 8.7457e-04 - val_accuracy: 0.9512\n",
            "Epoch 953/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.6422e-05 - accuracy: 0.9986 - val_loss: 9.1052e-04 - val_accuracy: 0.9497\n",
            "Epoch 954/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.4746e-05 - accuracy: 0.9987 - val_loss: 9.5131e-04 - val_accuracy: 0.9478\n",
            "Epoch 955/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.1602e-05 - accuracy: 0.9966 - val_loss: 0.0010 - val_accuracy: 0.9422\n",
            "Epoch 956/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.5538e-05 - accuracy: 0.9963 - val_loss: 0.0013 - val_accuracy: 0.9272\n",
            "Epoch 957/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.1941e-04 - accuracy: 0.9933 - val_loss: 0.0013 - val_accuracy: 0.9272\n",
            "Epoch 958/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.0256e-05 - accuracy: 0.9961 - val_loss: 0.0010 - val_accuracy: 0.9440\n",
            "Epoch 959/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.2583e-05 - accuracy: 0.9983 - val_loss: 0.0010 - val_accuracy: 0.9447\n",
            "Epoch 960/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.1637e-05 - accuracy: 0.9982 - val_loss: 0.0010 - val_accuracy: 0.9435\n",
            "Epoch 961/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.3567e-05 - accuracy: 0.9982 - val_loss: 8.9539e-04 - val_accuracy: 0.9502\n",
            "Epoch 962/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 3.0119e-05 - accuracy: 0.9984 - val_loss: 9.6518e-04 - val_accuracy: 0.9458\n",
            "Epoch 963/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.9448e-05 - accuracy: 0.9985 - val_loss: 0.0010 - val_accuracy: 0.9458\n",
            "Epoch 964/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.4581e-05 - accuracy: 0.9981 - val_loss: 9.1865e-04 - val_accuracy: 0.9502\n",
            "Epoch 965/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.8789e-05 - accuracy: 0.9979 - val_loss: 0.0015 - val_accuracy: 0.9145\n",
            "Epoch 966/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.0757e-04 - accuracy: 0.9936 - val_loss: 0.0013 - val_accuracy: 0.9305\n",
            "Epoch 967/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.1467e-05 - accuracy: 0.9953 - val_loss: 0.0011 - val_accuracy: 0.9402\n",
            "Epoch 968/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.5851e-05 - accuracy: 0.9981 - val_loss: 0.0011 - val_accuracy: 0.9398\n",
            "Epoch 969/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 9.4068e-05 - accuracy: 0.9947 - val_loss: 0.0018 - val_accuracy: 0.9035\n",
            "Epoch 970/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.7182e-05 - accuracy: 0.9949 - val_loss: 0.0011 - val_accuracy: 0.9403\n",
            "Epoch 971/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.9076e-05 - accuracy: 0.9973 - val_loss: 0.0011 - val_accuracy: 0.9420\n",
            "Epoch 972/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.1757e-05 - accuracy: 0.9975 - val_loss: 9.4334e-04 - val_accuracy: 0.9475\n",
            "Epoch 973/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.7136e-05 - accuracy: 0.9986 - val_loss: 9.2836e-04 - val_accuracy: 0.9482\n",
            "Epoch 974/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 2.8312e-05 - accuracy: 0.9985 - val_loss: 9.0936e-04 - val_accuracy: 0.9497\n",
            "Epoch 975/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.3990e-05 - accuracy: 0.9981 - val_loss: 9.1127e-04 - val_accuracy: 0.9497\n",
            "Epoch 976/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.1927e-05 - accuracy: 0.9977 - val_loss: 0.0010 - val_accuracy: 0.9432\n",
            "Epoch 977/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.5418e-05 - accuracy: 0.9973 - val_loss: 0.0012 - val_accuracy: 0.9332\n",
            "Epoch 978/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.6733e-05 - accuracy: 0.9968 - val_loss: 0.0011 - val_accuracy: 0.9395\n",
            "Epoch 979/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.8828e-05 - accuracy: 0.9950 - val_loss: 0.0013 - val_accuracy: 0.9283\n",
            "Epoch 980/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.0255e-05 - accuracy: 0.9973 - val_loss: 0.0012 - val_accuracy: 0.9347\n",
            "Epoch 981/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 3.9101e-05 - accuracy: 0.9978 - val_loss: 0.0011 - val_accuracy: 0.9397\n",
            "Epoch 982/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 3.8757e-05 - accuracy: 0.9979 - val_loss: 9.7921e-04 - val_accuracy: 0.9463\n",
            "Epoch 983/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.4044e-05 - accuracy: 0.9982 - val_loss: 9.6120e-04 - val_accuracy: 0.9475\n",
            "Epoch 984/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 3.7827e-05 - accuracy: 0.9980 - val_loss: 0.0011 - val_accuracy: 0.9370\n",
            "Epoch 985/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.7880e-05 - accuracy: 0.9980 - val_loss: 9.9317e-04 - val_accuracy: 0.9455\n",
            "Epoch 986/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.2685e-05 - accuracy: 0.9982 - val_loss: 0.0010 - val_accuracy: 0.9450\n",
            "Epoch 987/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.1957e-05 - accuracy: 0.9977 - val_loss: 9.3875e-04 - val_accuracy: 0.9478\n",
            "Epoch 988/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.1365e-05 - accuracy: 0.9983 - val_loss: 0.0011 - val_accuracy: 0.9410\n",
            "Epoch 989/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.0489e-05 - accuracy: 0.9977 - val_loss: 0.0010 - val_accuracy: 0.9447\n",
            "Epoch 990/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.3817e-05 - accuracy: 0.9981 - val_loss: 0.0011 - val_accuracy: 0.9397\n",
            "Epoch 991/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.4880e-05 - accuracy: 0.9969 - val_loss: 0.0013 - val_accuracy: 0.9275\n",
            "Epoch 992/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.6603e-05 - accuracy: 0.9973 - val_loss: 0.0010 - val_accuracy: 0.9435\n",
            "Epoch 993/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.8893e-05 - accuracy: 0.9979 - val_loss: 9.1736e-04 - val_accuracy: 0.9500\n",
            "Epoch 994/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.0473e-05 - accuracy: 0.9983 - val_loss: 0.0010 - val_accuracy: 0.9430\n",
            "Epoch 995/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.5711e-05 - accuracy: 0.9981 - val_loss: 0.0011 - val_accuracy: 0.9392\n",
            "Epoch 996/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.6832e-05 - accuracy: 0.9980 - val_loss: 0.0011 - val_accuracy: 0.9398\n",
            "Epoch 997/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.0882e-05 - accuracy: 0.9984 - val_loss: 9.5843e-04 - val_accuracy: 0.9473\n",
            "Epoch 998/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.2086e-05 - accuracy: 0.9977 - val_loss: 9.8889e-04 - val_accuracy: 0.9460\n",
            "Epoch 999/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.0516e-05 - accuracy: 0.9978 - val_loss: 0.0011 - val_accuracy: 0.9415\n",
            "Epoch 1000/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.0768e-05 - accuracy: 0.9976 - val_loss: 0.0011 - val_accuracy: 0.9368\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7708c09-3e74-4fce-acd1-f7077df418aa",
        "id": "XmPbTOUqOiqb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "188/188 [==============================] - 2s 9ms/step - loss: 0.0011 - accuracy: 0.9410\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.0010692388750612736, 0.9409999847412109]"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('lowpass_1s.h5')"
      ],
      "metadata": {
        "id": "2HyH71npFn2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 그래프 분석"
      ],
      "metadata": {
        "id": "hY4i-LmaOiqc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history=history.history\n",
        "epochs = range(1, len(history['loss']) + 1)\n",
        "acc = history['accuracy']\n",
        "loss = history['loss']\n",
        "val_acc = history['val_accuracy']\n",
        "val_loss = history['val_loss']\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.plot(epochs, acc, label='accuracy')\n",
        "plt.plot(epochs, val_acc, label='val_acc')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.plot(epochs, loss, label='loss', color='g')\n",
        "plt.plot(epochs, val_loss, label='val_loss', color='r')\n",
        "plt.legend()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 701
        },
        "outputId": "a11fd5d5-1d19-44a4-9349-9d2845878798",
        "id": "bo9BenMoOiqc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f11c467f370>"
            ]
          },
          "metadata": {},
          "execution_count": 108
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 예측해보기(predict)"
      ],
      "metadata": {
        "id": "0kZeUf4HOiqc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e06685b-4c91-4d7b-804a-f36f83b0daf8",
        "id": "Ywbc7dwdOiqc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "188/188 [==============================] - 2s 9ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[44,  0,  0, ...,  0,  0,  0],\n",
              "       [ 0, 57,  0, ...,  0,  0,  0],\n",
              "       [ 0,  0, 65, ...,  0,  0,  0],\n",
              "       ...,\n",
              "       [ 0,  0,  0, ..., 68,  0,  0],\n",
              "       [ 0,  0,  0, ...,  0, 67,  0],\n",
              "       [ 0,  0,  0, ...,  0,  0, 57]])"
            ]
          },
          "metadata": {},
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_p=pd.DataFrame(y_test)\n",
        "\n",
        "for i in range(1,101):\n",
        "  y_test_p.loc[y_test_p[i] == 1.0 , 'y_test'] = i  \n",
        "y_test_p.y_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "348ddc8c-b4b0-4b45-9b9b-a83b87cbce28",
        "id": "UznLC4h1Oiqc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       59.0\n",
              "1       68.0\n",
              "2       72.0\n",
              "3        2.0\n",
              "4       57.0\n",
              "        ... \n",
              "5995     3.0\n",
              "5996    87.0\n",
              "5997    40.0\n",
              "5998    68.0\n",
              "5999    37.0\n",
              "Name: y_test, Length: 6000, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ch1 = pd.DataFrame(np.rint(y_pred))\n",
        "# for i in range(1,101):\n",
        "#   ch1.loc[ch1[i] == 1.0 , 'pred'] = i \n",
        "\n",
        " \n",
        "#predict = pd.concat([data_test,ch1['pred'], y_test_p['y_test']],axis = 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "35e79746-e41a-4713-b942-733f497e3b6c",
        "id": "i1KFHlM9Oiqc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        0    1    2    3    4    5    6    7    8    9  ...   92   93   94  \\\n",
              "0     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
              "1     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
              "2     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
              "3     0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
              "4     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
              "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
              "5995  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
              "5996  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
              "5997  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
              "5998  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
              "5999  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
              "\n",
              "       95   96   97   98   99  100  pred  \n",
              "0     0.0  0.0  0.0  0.0  0.0  0.0  59.0  \n",
              "1     0.0  0.0  0.0  0.0  0.0  0.0  68.0  \n",
              "2     0.0  0.0  0.0  0.0  0.0  0.0  72.0  \n",
              "3     0.0  0.0  0.0  0.0  0.0  0.0   2.0  \n",
              "4     0.0  0.0  0.0  0.0  0.0  0.0  57.0  \n",
              "...   ...  ...  ...  ...  ...  ...   ...  \n",
              "5995  0.0  0.0  0.0  0.0  0.0  0.0   3.0  \n",
              "5996  0.0  0.0  0.0  0.0  0.0  0.0  87.0  \n",
              "5997  0.0  0.0  0.0  0.0  0.0  0.0  40.0  \n",
              "5998  0.0  0.0  0.0  0.0  0.0  0.0  68.0  \n",
              "5999  0.0  0.0  0.0  0.0  0.0  0.0  45.0  \n",
              "\n",
              "[6000 rows x 102 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d66cc86c-ae23-4a9f-8fb4-dbad40e0c5ea\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "      <th>100</th>\n",
              "      <th>pred</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>59.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>68.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>72.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>57.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5995</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5996</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>87.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5997</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5998</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>68.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5999</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>45.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6000 rows × 102 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d66cc86c-ae23-4a9f-8fb4-dbad40e0c5ea')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d66cc86c-ae23-4a9f-8fb4-dbad40e0c5ea button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d66cc86c-ae23-4a9f-8fb4-dbad40e0c5ea');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ex=np.argmax(y_pred, axis=1)\n",
        "ex=ex.astype('float64')\n",
        "ex=pd.DataFrame(ex)\n",
        "ex"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "22f2ac13-d7ce-409c-db36-bf92c9ca0cad",
        "id": "8dvHH9kjOiqc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         0\n",
              "0     59.0\n",
              "1     68.0\n",
              "2     72.0\n",
              "3      2.0\n",
              "4     57.0\n",
              "...    ...\n",
              "5995   3.0\n",
              "5996  87.0\n",
              "5997  40.0\n",
              "5998  68.0\n",
              "5999  45.0\n",
              "\n",
              "[6000 rows x 1 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-33a2dfcf-6ed7-497f-9b45-70f7cc7db973\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>59.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>68.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>72.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>57.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5995</th>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5996</th>\n",
              "      <td>87.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5997</th>\n",
              "      <td>40.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5998</th>\n",
              "      <td>68.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5999</th>\n",
              "      <td>45.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6000 rows × 1 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-33a2dfcf-6ed7-497f-9b45-70f7cc7db973')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-33a2dfcf-6ed7-497f-9b45-70f7cc7db973 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-33a2dfcf-6ed7-497f-9b45-70f7cc7db973');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ch1 = pd.DataFrame(np.rint(y_pred))\n",
        "ch1['pred'] = ex\n",
        "data_test.reset_index(inplace=True, drop=True)\n",
        "predict = pd.concat([data_test,ch1['pred'], y_test_p['y_test']],axis = 1)"
      ],
      "metadata": {
        "id": "Ak1wLqaVOiqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict['compare'] = np.where(predict['pred']==predict['y_test'],'yes', 'no')"
      ],
      "metadata": {
        "id": "A7edBwKcOiqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_1 = predict[predict[\"compare\"]=='no']\n",
        "predict_1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "outputId": "833df983-0f21-460a-de46-668f491d18e1",
        "id": "_M8Qra9qOiqc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             0         1         2         3         4         5         6  \\\n",
              "30    0.001909  0.008317  0.014489  0.020412  0.025995  0.031137  0.035801   \n",
              "46    0.096114  0.095362  0.095047  0.095447  0.096676  0.098677  0.101247   \n",
              "54   -0.041972 -0.048778 -0.055266 -0.061636 -0.067728 -0.073065 -0.076978   \n",
              "55    0.050686  0.056807  0.061922  0.066330  0.070463  0.074831  0.079964   \n",
              "71   -0.511413 -0.507140 -0.469849 -0.408462 -0.333769 -0.256404 -0.185178   \n",
              "...        ...       ...       ...       ...       ...       ...       ...   \n",
              "5971  0.005315 -0.043084 -0.083200 -0.110473 -0.122973 -0.121490 -0.109050   \n",
              "5981 -0.005281 -0.018280 -0.031669 -0.042939 -0.050464 -0.053648 -0.052817   \n",
              "5992 -0.084734 -0.091664 -0.098196 -0.101411 -0.098148 -0.085744 -0.062829   \n",
              "5994  0.008461  0.011468  0.012707  0.012096  0.009875  0.006558  0.002841   \n",
              "5999  0.052145  0.038833  0.024907  0.010402 -0.004429 -0.019087 -0.032867   \n",
              "\n",
              "             7         8         9  ...       249       250       251  \\\n",
              "30    0.040075  0.044195  0.048538  ...  0.044631  0.050399  0.057432   \n",
              "46    0.104083  0.106844  0.109202  ... -0.004398 -0.008214 -0.013081   \n",
              "54   -0.078796 -0.078050 -0.074630  ... -0.102289 -0.108650 -0.114708   \n",
              "55    0.086349  0.094365  0.104239  ...  0.125449  0.136371  0.147691   \n",
              "71   -0.126038 -0.081724 -0.052058  ...  0.031533  0.034874  0.038518   \n",
              "...        ...       ...       ...  ...       ...       ...       ...   \n",
              "5971 -0.090022 -0.069069 -0.050204  ... -0.027012 -0.029614 -0.031356   \n",
              "5981 -0.048958 -0.043371 -0.037307  ... -0.016826 -0.013496 -0.011275   \n",
              "5992 -0.030004  0.009808  0.051498  ...  0.072150  0.081214  0.090895   \n",
              "5994 -0.000513 -0.002782 -0.003375  ... -0.048723 -0.051678 -0.052151   \n",
              "5999 -0.044991 -0.054774 -0.061789  ... -0.056943 -0.054300 -0.054324   \n",
              "\n",
              "           252       253       254       255   pred  y_test  compare  \n",
              "30    0.066046  0.076371  0.088327  0.101648   85.0    36.0       no  \n",
              "46   -0.019006 -0.025674 -0.032320 -0.037693   83.0    80.0       no  \n",
              "54   -0.119718 -0.122885 -0.123496 -0.121095   20.0    71.0       no  \n",
              "55    0.159697  0.172642  0.186692  0.201860   41.0    50.0       no  \n",
              "71    0.042180  0.045745  0.049241  0.052764    4.0    93.0       no  \n",
              "...        ...       ...       ...       ...    ...     ...      ...  \n",
              "5971 -0.033020 -0.035244 -0.038329 -0.042129   95.0    23.0       no  \n",
              "5981 -0.010594 -0.011830 -0.015208 -0.020697   52.0    40.0       no  \n",
              "5992  0.101388  0.112932  0.125713  0.139775  100.0    83.0       no  \n",
              "5994 -0.050205 -0.046318 -0.041326 -0.036264   95.0    49.0       no  \n",
              "5999 -0.058068 -0.065599 -0.075635 -0.085448   45.0    37.0       no  \n",
              "\n",
              "[545 rows x 259 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-22dffb56-3200-49eb-89c1-97f537806f07\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>249</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "      <th>pred</th>\n",
              "      <th>y_test</th>\n",
              "      <th>compare</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.001909</td>\n",
              "      <td>0.008317</td>\n",
              "      <td>0.014489</td>\n",
              "      <td>0.020412</td>\n",
              "      <td>0.025995</td>\n",
              "      <td>0.031137</td>\n",
              "      <td>0.035801</td>\n",
              "      <td>0.040075</td>\n",
              "      <td>0.044195</td>\n",
              "      <td>0.048538</td>\n",
              "      <td>...</td>\n",
              "      <td>0.044631</td>\n",
              "      <td>0.050399</td>\n",
              "      <td>0.057432</td>\n",
              "      <td>0.066046</td>\n",
              "      <td>0.076371</td>\n",
              "      <td>0.088327</td>\n",
              "      <td>0.101648</td>\n",
              "      <td>85.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>0.096114</td>\n",
              "      <td>0.095362</td>\n",
              "      <td>0.095047</td>\n",
              "      <td>0.095447</td>\n",
              "      <td>0.096676</td>\n",
              "      <td>0.098677</td>\n",
              "      <td>0.101247</td>\n",
              "      <td>0.104083</td>\n",
              "      <td>0.106844</td>\n",
              "      <td>0.109202</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.004398</td>\n",
              "      <td>-0.008214</td>\n",
              "      <td>-0.013081</td>\n",
              "      <td>-0.019006</td>\n",
              "      <td>-0.025674</td>\n",
              "      <td>-0.032320</td>\n",
              "      <td>-0.037693</td>\n",
              "      <td>83.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>-0.041972</td>\n",
              "      <td>-0.048778</td>\n",
              "      <td>-0.055266</td>\n",
              "      <td>-0.061636</td>\n",
              "      <td>-0.067728</td>\n",
              "      <td>-0.073065</td>\n",
              "      <td>-0.076978</td>\n",
              "      <td>-0.078796</td>\n",
              "      <td>-0.078050</td>\n",
              "      <td>-0.074630</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.102289</td>\n",
              "      <td>-0.108650</td>\n",
              "      <td>-0.114708</td>\n",
              "      <td>-0.119718</td>\n",
              "      <td>-0.122885</td>\n",
              "      <td>-0.123496</td>\n",
              "      <td>-0.121095</td>\n",
              "      <td>20.0</td>\n",
              "      <td>71.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>0.050686</td>\n",
              "      <td>0.056807</td>\n",
              "      <td>0.061922</td>\n",
              "      <td>0.066330</td>\n",
              "      <td>0.070463</td>\n",
              "      <td>0.074831</td>\n",
              "      <td>0.079964</td>\n",
              "      <td>0.086349</td>\n",
              "      <td>0.094365</td>\n",
              "      <td>0.104239</td>\n",
              "      <td>...</td>\n",
              "      <td>0.125449</td>\n",
              "      <td>0.136371</td>\n",
              "      <td>0.147691</td>\n",
              "      <td>0.159697</td>\n",
              "      <td>0.172642</td>\n",
              "      <td>0.186692</td>\n",
              "      <td>0.201860</td>\n",
              "      <td>41.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>-0.511413</td>\n",
              "      <td>-0.507140</td>\n",
              "      <td>-0.469849</td>\n",
              "      <td>-0.408462</td>\n",
              "      <td>-0.333769</td>\n",
              "      <td>-0.256404</td>\n",
              "      <td>-0.185178</td>\n",
              "      <td>-0.126038</td>\n",
              "      <td>-0.081724</td>\n",
              "      <td>-0.052058</td>\n",
              "      <td>...</td>\n",
              "      <td>0.031533</td>\n",
              "      <td>0.034874</td>\n",
              "      <td>0.038518</td>\n",
              "      <td>0.042180</td>\n",
              "      <td>0.045745</td>\n",
              "      <td>0.049241</td>\n",
              "      <td>0.052764</td>\n",
              "      <td>4.0</td>\n",
              "      <td>93.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5971</th>\n",
              "      <td>0.005315</td>\n",
              "      <td>-0.043084</td>\n",
              "      <td>-0.083200</td>\n",
              "      <td>-0.110473</td>\n",
              "      <td>-0.122973</td>\n",
              "      <td>-0.121490</td>\n",
              "      <td>-0.109050</td>\n",
              "      <td>-0.090022</td>\n",
              "      <td>-0.069069</td>\n",
              "      <td>-0.050204</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.027012</td>\n",
              "      <td>-0.029614</td>\n",
              "      <td>-0.031356</td>\n",
              "      <td>-0.033020</td>\n",
              "      <td>-0.035244</td>\n",
              "      <td>-0.038329</td>\n",
              "      <td>-0.042129</td>\n",
              "      <td>95.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5981</th>\n",
              "      <td>-0.005281</td>\n",
              "      <td>-0.018280</td>\n",
              "      <td>-0.031669</td>\n",
              "      <td>-0.042939</td>\n",
              "      <td>-0.050464</td>\n",
              "      <td>-0.053648</td>\n",
              "      <td>-0.052817</td>\n",
              "      <td>-0.048958</td>\n",
              "      <td>-0.043371</td>\n",
              "      <td>-0.037307</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.016826</td>\n",
              "      <td>-0.013496</td>\n",
              "      <td>-0.011275</td>\n",
              "      <td>-0.010594</td>\n",
              "      <td>-0.011830</td>\n",
              "      <td>-0.015208</td>\n",
              "      <td>-0.020697</td>\n",
              "      <td>52.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5992</th>\n",
              "      <td>-0.084734</td>\n",
              "      <td>-0.091664</td>\n",
              "      <td>-0.098196</td>\n",
              "      <td>-0.101411</td>\n",
              "      <td>-0.098148</td>\n",
              "      <td>-0.085744</td>\n",
              "      <td>-0.062829</td>\n",
              "      <td>-0.030004</td>\n",
              "      <td>0.009808</td>\n",
              "      <td>0.051498</td>\n",
              "      <td>...</td>\n",
              "      <td>0.072150</td>\n",
              "      <td>0.081214</td>\n",
              "      <td>0.090895</td>\n",
              "      <td>0.101388</td>\n",
              "      <td>0.112932</td>\n",
              "      <td>0.125713</td>\n",
              "      <td>0.139775</td>\n",
              "      <td>100.0</td>\n",
              "      <td>83.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5994</th>\n",
              "      <td>0.008461</td>\n",
              "      <td>0.011468</td>\n",
              "      <td>0.012707</td>\n",
              "      <td>0.012096</td>\n",
              "      <td>0.009875</td>\n",
              "      <td>0.006558</td>\n",
              "      <td>0.002841</td>\n",
              "      <td>-0.000513</td>\n",
              "      <td>-0.002782</td>\n",
              "      <td>-0.003375</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.048723</td>\n",
              "      <td>-0.051678</td>\n",
              "      <td>-0.052151</td>\n",
              "      <td>-0.050205</td>\n",
              "      <td>-0.046318</td>\n",
              "      <td>-0.041326</td>\n",
              "      <td>-0.036264</td>\n",
              "      <td>95.0</td>\n",
              "      <td>49.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5999</th>\n",
              "      <td>0.052145</td>\n",
              "      <td>0.038833</td>\n",
              "      <td>0.024907</td>\n",
              "      <td>0.010402</td>\n",
              "      <td>-0.004429</td>\n",
              "      <td>-0.019087</td>\n",
              "      <td>-0.032867</td>\n",
              "      <td>-0.044991</td>\n",
              "      <td>-0.054774</td>\n",
              "      <td>-0.061789</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.056943</td>\n",
              "      <td>-0.054300</td>\n",
              "      <td>-0.054324</td>\n",
              "      <td>-0.058068</td>\n",
              "      <td>-0.065599</td>\n",
              "      <td>-0.075635</td>\n",
              "      <td>-0.085448</td>\n",
              "      <td>45.0</td>\n",
              "      <td>37.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>545 rows × 259 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-22dffb56-3200-49eb-89c1-97f537806f07')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-22dffb56-3200-49eb-89c1-97f537806f07 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-22dffb56-3200-49eb-89c1-97f537806f07');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 163
        }
      ]
    }
  ]
}