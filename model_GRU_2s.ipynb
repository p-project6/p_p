{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [
        "Lzt1SMEoSshL",
        "EgfaelmnXe04",
        "m-Hm3lsWXkbE",
        "Na73oRb9Xq69",
        "DBnTIBCDGX94",
        "mLO6GGJ5Nn9w",
        "2aZgEs15Kln5"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "cXWIFrrheAli",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc7c23aa-e4e1-4093-ee8d-c8fa8626ac0b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.bandpass"
      ],
      "metadata": {
        "id": "Nize6zVDXND_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Nhn3soxJQYzl"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import itertools\n",
        "import time\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import (CosineAnnealingLR,\n",
        "                                      CosineAnnealingWarmRestarts,\n",
        "                                      StepLR,\n",
        "                                      ExponentialLR)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import accuracy_score, auc, f1_score, precision_score, recall_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm, re, sys\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "from keras.layers import Conv2D, Conv1D, MaxPooling2D, MaxPooling1D, Flatten, BatchNormalization, Dense\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.callbacks import CSVLogger, ModelCheckpoint"
      ],
      "metadata": {
        "id": "88FIZSPSbA_H"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_total = pd.read_csv('/content/drive/MyDrive/model_2s/total_band.csv', header=None)"
      ],
      "metadata": {
        "id": "Fo2pj1pSfN4N"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_total"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "jYUIMIVKd3yW",
        "outputId": "99b641ba-af51-4ea5-d4a4-a22c38591a83"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            0         1         2         3         4         5         6    \\\n",
              "0     -0.031820 -0.015874 -0.001172  0.011249  0.020713  0.026974  0.030190   \n",
              "1     -0.057454 -0.056822 -0.055984 -0.054985 -0.053945 -0.052999 -0.052250   \n",
              "2     -0.054004 -0.050825 -0.048322 -0.046824 -0.046532 -0.047445 -0.049330   \n",
              "3      0.053295  0.053439  0.055066  0.059409  0.067278  0.078961  0.094216   \n",
              "4      0.134007  0.088232  0.042803 -0.000932 -0.041686 -0.078336 -0.110032   \n",
              "...         ...       ...       ...       ...       ...       ...       ...   \n",
              "17995 -0.087605 -0.069234 -0.058444 -0.053914 -0.053427 -0.054525 -0.055053   \n",
              "17996 -0.033347 -0.035287 -0.036169 -0.036014 -0.035070 -0.033744 -0.032503   \n",
              "17997 -0.076867 -0.083002 -0.088128 -0.091454 -0.092582 -0.091623 -0.089189   \n",
              "17998 -0.079288 -0.083358 -0.086464 -0.087995 -0.087513 -0.084880 -0.080336   \n",
              "17999 -0.041533 -0.041870 -0.042836 -0.044782 -0.047871 -0.052004 -0.056828   \n",
              "\n",
              "            7         8         9    ...       503       504       505  \\\n",
              "0      0.030831  0.029559  0.027102  ... -0.072854 -0.070401 -0.067539   \n",
              "1     -0.051733 -0.051414 -0.051213  ... -0.071806 -0.071741 -0.071302   \n",
              "2     -0.051735 -0.054058 -0.055645  ...  0.009824  0.013030  0.016239   \n",
              "3      0.112361  0.132422  0.153321  ...  0.388900  0.382609  0.370062   \n",
              "4     -0.136271 -0.156943 -0.172313  ... -0.113220 -0.141579 -0.164040   \n",
              "...         ...       ...       ...  ...       ...       ...       ...   \n",
              "17995 -0.053525 -0.049262 -0.042344  ... -0.020100 -0.018824 -0.018132   \n",
              "17996 -0.031760 -0.031802 -0.032746  ... -0.060069 -0.061489 -0.061986   \n",
              "17997 -0.086281 -0.084051 -0.083490  ... -0.053679 -0.057318 -0.059515   \n",
              "17998 -0.074488 -0.068211 -0.062467  ... -0.048461 -0.044455 -0.041670   \n",
              "17999 -0.061806 -0.066332 -0.069845  ...  0.136895  0.147996  0.158825   \n",
              "\n",
              "            506       507       508       509       510       511    512  \n",
              "0     -0.064716 -0.062285 -0.060443 -0.059208 -0.058448 -0.057940    1.0  \n",
              "1     -0.070367 -0.068861 -0.066758 -0.064085 -0.060936 -0.057487    1.0  \n",
              "2      0.019380  0.022418  0.025327  0.028062  0.030516  0.032514    1.0  \n",
              "3      0.351265  0.326472  0.296135  0.260876  0.221465  0.178820    1.0  \n",
              "4     -0.180986 -0.193086 -0.201181 -0.206146 -0.208783 -0.209743    1.0  \n",
              "...         ...       ...       ...       ...       ...       ...    ...  \n",
              "17995 -0.018247 -0.019303 -0.021303 -0.024082 -0.027314 -0.030559  100.0  \n",
              "17996 -0.061385 -0.059834 -0.057803 -0.055972 -0.055062 -0.055631  100.0  \n",
              "17997 -0.061043 -0.062554 -0.064528 -0.067243 -0.070758 -0.074902  100.0  \n",
              "17998 -0.040185 -0.039799 -0.040120 -0.040694 -0.041169 -0.041409  100.0  \n",
              "17999  0.169190  0.179084  0.188748  0.198658  0.209439  0.221705  100.0  \n",
              "\n",
              "[18000 rows x 513 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0fc7b503-7ff4-4e28-88ba-0002858c3182\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>503</th>\n",
              "      <th>504</th>\n",
              "      <th>505</th>\n",
              "      <th>506</th>\n",
              "      <th>507</th>\n",
              "      <th>508</th>\n",
              "      <th>509</th>\n",
              "      <th>510</th>\n",
              "      <th>511</th>\n",
              "      <th>512</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.031820</td>\n",
              "      <td>-0.015874</td>\n",
              "      <td>-0.001172</td>\n",
              "      <td>0.011249</td>\n",
              "      <td>0.020713</td>\n",
              "      <td>0.026974</td>\n",
              "      <td>0.030190</td>\n",
              "      <td>0.030831</td>\n",
              "      <td>0.029559</td>\n",
              "      <td>0.027102</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.072854</td>\n",
              "      <td>-0.070401</td>\n",
              "      <td>-0.067539</td>\n",
              "      <td>-0.064716</td>\n",
              "      <td>-0.062285</td>\n",
              "      <td>-0.060443</td>\n",
              "      <td>-0.059208</td>\n",
              "      <td>-0.058448</td>\n",
              "      <td>-0.057940</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.057454</td>\n",
              "      <td>-0.056822</td>\n",
              "      <td>-0.055984</td>\n",
              "      <td>-0.054985</td>\n",
              "      <td>-0.053945</td>\n",
              "      <td>-0.052999</td>\n",
              "      <td>-0.052250</td>\n",
              "      <td>-0.051733</td>\n",
              "      <td>-0.051414</td>\n",
              "      <td>-0.051213</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.071806</td>\n",
              "      <td>-0.071741</td>\n",
              "      <td>-0.071302</td>\n",
              "      <td>-0.070367</td>\n",
              "      <td>-0.068861</td>\n",
              "      <td>-0.066758</td>\n",
              "      <td>-0.064085</td>\n",
              "      <td>-0.060936</td>\n",
              "      <td>-0.057487</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.054004</td>\n",
              "      <td>-0.050825</td>\n",
              "      <td>-0.048322</td>\n",
              "      <td>-0.046824</td>\n",
              "      <td>-0.046532</td>\n",
              "      <td>-0.047445</td>\n",
              "      <td>-0.049330</td>\n",
              "      <td>-0.051735</td>\n",
              "      <td>-0.054058</td>\n",
              "      <td>-0.055645</td>\n",
              "      <td>...</td>\n",
              "      <td>0.009824</td>\n",
              "      <td>0.013030</td>\n",
              "      <td>0.016239</td>\n",
              "      <td>0.019380</td>\n",
              "      <td>0.022418</td>\n",
              "      <td>0.025327</td>\n",
              "      <td>0.028062</td>\n",
              "      <td>0.030516</td>\n",
              "      <td>0.032514</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.053295</td>\n",
              "      <td>0.053439</td>\n",
              "      <td>0.055066</td>\n",
              "      <td>0.059409</td>\n",
              "      <td>0.067278</td>\n",
              "      <td>0.078961</td>\n",
              "      <td>0.094216</td>\n",
              "      <td>0.112361</td>\n",
              "      <td>0.132422</td>\n",
              "      <td>0.153321</td>\n",
              "      <td>...</td>\n",
              "      <td>0.388900</td>\n",
              "      <td>0.382609</td>\n",
              "      <td>0.370062</td>\n",
              "      <td>0.351265</td>\n",
              "      <td>0.326472</td>\n",
              "      <td>0.296135</td>\n",
              "      <td>0.260876</td>\n",
              "      <td>0.221465</td>\n",
              "      <td>0.178820</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.134007</td>\n",
              "      <td>0.088232</td>\n",
              "      <td>0.042803</td>\n",
              "      <td>-0.000932</td>\n",
              "      <td>-0.041686</td>\n",
              "      <td>-0.078336</td>\n",
              "      <td>-0.110032</td>\n",
              "      <td>-0.136271</td>\n",
              "      <td>-0.156943</td>\n",
              "      <td>-0.172313</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.113220</td>\n",
              "      <td>-0.141579</td>\n",
              "      <td>-0.164040</td>\n",
              "      <td>-0.180986</td>\n",
              "      <td>-0.193086</td>\n",
              "      <td>-0.201181</td>\n",
              "      <td>-0.206146</td>\n",
              "      <td>-0.208783</td>\n",
              "      <td>-0.209743</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17995</th>\n",
              "      <td>-0.087605</td>\n",
              "      <td>-0.069234</td>\n",
              "      <td>-0.058444</td>\n",
              "      <td>-0.053914</td>\n",
              "      <td>-0.053427</td>\n",
              "      <td>-0.054525</td>\n",
              "      <td>-0.055053</td>\n",
              "      <td>-0.053525</td>\n",
              "      <td>-0.049262</td>\n",
              "      <td>-0.042344</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.020100</td>\n",
              "      <td>-0.018824</td>\n",
              "      <td>-0.018132</td>\n",
              "      <td>-0.018247</td>\n",
              "      <td>-0.019303</td>\n",
              "      <td>-0.021303</td>\n",
              "      <td>-0.024082</td>\n",
              "      <td>-0.027314</td>\n",
              "      <td>-0.030559</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17996</th>\n",
              "      <td>-0.033347</td>\n",
              "      <td>-0.035287</td>\n",
              "      <td>-0.036169</td>\n",
              "      <td>-0.036014</td>\n",
              "      <td>-0.035070</td>\n",
              "      <td>-0.033744</td>\n",
              "      <td>-0.032503</td>\n",
              "      <td>-0.031760</td>\n",
              "      <td>-0.031802</td>\n",
              "      <td>-0.032746</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.060069</td>\n",
              "      <td>-0.061489</td>\n",
              "      <td>-0.061986</td>\n",
              "      <td>-0.061385</td>\n",
              "      <td>-0.059834</td>\n",
              "      <td>-0.057803</td>\n",
              "      <td>-0.055972</td>\n",
              "      <td>-0.055062</td>\n",
              "      <td>-0.055631</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17997</th>\n",
              "      <td>-0.076867</td>\n",
              "      <td>-0.083002</td>\n",
              "      <td>-0.088128</td>\n",
              "      <td>-0.091454</td>\n",
              "      <td>-0.092582</td>\n",
              "      <td>-0.091623</td>\n",
              "      <td>-0.089189</td>\n",
              "      <td>-0.086281</td>\n",
              "      <td>-0.084051</td>\n",
              "      <td>-0.083490</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.053679</td>\n",
              "      <td>-0.057318</td>\n",
              "      <td>-0.059515</td>\n",
              "      <td>-0.061043</td>\n",
              "      <td>-0.062554</td>\n",
              "      <td>-0.064528</td>\n",
              "      <td>-0.067243</td>\n",
              "      <td>-0.070758</td>\n",
              "      <td>-0.074902</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17998</th>\n",
              "      <td>-0.079288</td>\n",
              "      <td>-0.083358</td>\n",
              "      <td>-0.086464</td>\n",
              "      <td>-0.087995</td>\n",
              "      <td>-0.087513</td>\n",
              "      <td>-0.084880</td>\n",
              "      <td>-0.080336</td>\n",
              "      <td>-0.074488</td>\n",
              "      <td>-0.068211</td>\n",
              "      <td>-0.062467</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.048461</td>\n",
              "      <td>-0.044455</td>\n",
              "      <td>-0.041670</td>\n",
              "      <td>-0.040185</td>\n",
              "      <td>-0.039799</td>\n",
              "      <td>-0.040120</td>\n",
              "      <td>-0.040694</td>\n",
              "      <td>-0.041169</td>\n",
              "      <td>-0.041409</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17999</th>\n",
              "      <td>-0.041533</td>\n",
              "      <td>-0.041870</td>\n",
              "      <td>-0.042836</td>\n",
              "      <td>-0.044782</td>\n",
              "      <td>-0.047871</td>\n",
              "      <td>-0.052004</td>\n",
              "      <td>-0.056828</td>\n",
              "      <td>-0.061806</td>\n",
              "      <td>-0.066332</td>\n",
              "      <td>-0.069845</td>\n",
              "      <td>...</td>\n",
              "      <td>0.136895</td>\n",
              "      <td>0.147996</td>\n",
              "      <td>0.158825</td>\n",
              "      <td>0.169190</td>\n",
              "      <td>0.179084</td>\n",
              "      <td>0.188748</td>\n",
              "      <td>0.198658</td>\n",
              "      <td>0.209439</td>\n",
              "      <td>0.221705</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>18000 rows × 513 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0fc7b503-7ff4-4e28-88ba-0002858c3182')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0fc7b503-7ff4-4e28-88ba-0002858c3182 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0fc7b503-7ff4-4e28-88ba-0002858c3182');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터 전체 섞기"
      ],
      "metadata": {
        "id": "I7BNW6vLSn40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_total=data_total.sample(frac=1)"
      ],
      "metadata": {
        "id": "zuAljzhlSxO0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train=data_total.iloc[0:14400, :]\n",
        "data_val=data_total.iloc[14400:16200, :]\n",
        "data_test=data_total.iloc[16200:18000,:]"
      ],
      "metadata": {
        "id": "dAO2XTRKS938"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "R03ForgAUEFw",
        "outputId": "2b0f34ed-6157-42a9-fb42-155328f0e677"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            0         1         2         3         4         5         6    \\\n",
              "7907  -0.017906 -0.016111 -0.015265 -0.015048 -0.015061 -0.014937 -0.014443   \n",
              "11143 -0.118654 -0.121925 -0.126439 -0.131408 -0.135973 -0.139351 -0.140978   \n",
              "5909  -0.081073 -0.083194 -0.084811 -0.085956 -0.086619 -0.086742 -0.086232   \n",
              "11356 -0.165740 -0.152825 -0.133079 -0.111981 -0.093823 -0.081138 -0.074583   \n",
              "8679  -0.008112 -0.001494  0.002451  0.001650 -0.004908 -0.016943 -0.032950   \n",
              "...         ...       ...       ...       ...       ...       ...       ...   \n",
              "8649   0.000466  0.005334  0.009596  0.012806  0.014792  0.015685  0.015858   \n",
              "938   -0.032009 -0.027885 -0.021962 -0.014614 -0.006394  0.002082  0.010239   \n",
              "15953  0.363541  0.358640  0.350826  0.340031  0.326239  0.309534  0.290130   \n",
              "1287   0.023535  0.018301  0.013760  0.010491  0.008849  0.008923  0.010539   \n",
              "13182 -0.025937 -0.028203 -0.029994 -0.030886 -0.030562 -0.028843 -0.025717   \n",
              "\n",
              "            7         8         9    ...       503       504       505  \\\n",
              "7907  -0.013542 -0.012380 -0.011228  ...  0.017852  0.005363 -0.004584   \n",
              "11143 -0.140594 -0.138273 -0.134396  ...  0.459992  0.467328  0.469964   \n",
              "5909  -0.084994 -0.082948 -0.080048  ... -0.037322 -0.035238 -0.036388   \n",
              "11356 -0.073233 -0.075129 -0.077930  ... -0.055745 -0.066144 -0.071713   \n",
              "8679  -0.050519 -0.066840 -0.079277  ... -0.067563 -0.069860 -0.071013   \n",
              "...         ...       ...       ...  ...       ...       ...       ...   \n",
              "8649   0.015806  0.015992  0.016739  ... -0.040887 -0.048713 -0.057635   \n",
              "938    0.017626  0.023943  0.029020  ... -0.022558  0.029429  0.089977   \n",
              "15953  0.268378  0.244732  0.219710  ... -0.053166 -0.051262 -0.051134   \n",
              "1287   0.013317  0.016758  0.020343  ...  0.110350  0.105924  0.099017   \n",
              "13182 -0.021346 -0.016065 -0.010357  ... -0.430318 -0.397804 -0.342009   \n",
              "\n",
              "            506       507       508       509       510       511   512  \n",
              "7907  -0.011813 -0.016382 -0.018636 -0.019204 -0.018925 -0.018720  44.0  \n",
              "11143  0.467341  0.458892  0.444103  0.422623  0.394386  0.359716  62.0  \n",
              "5909  -0.040418 -0.046553 -0.053785 -0.061092 -0.067619 -0.072789  33.0  \n",
              "11356 -0.067951 -0.050901 -0.018156  0.030236  0.091215  0.158715  64.0  \n",
              "8679  -0.071196 -0.070569 -0.069267 -0.067397 -0.065056 -0.062354  49.0  \n",
              "...         ...       ...       ...       ...       ...       ...   ...  \n",
              "8649  -0.066352 -0.073594 -0.078350 -0.080013 -0.078441 -0.073937  49.0  \n",
              "938    0.151508  0.205117  0.242187  0.256085  0.243552  0.205435   6.0  \n",
              "15953 -0.053391 -0.057984 -0.064026 -0.069738 -0.072595 -0.069675  89.0  \n",
              "1287   0.089660  0.078087  0.064757  0.050327  0.035595  0.021386   8.0  \n",
              "13182 -0.275152 -0.208358 -0.150023 -0.104997 -0.074593 -0.057260  74.0  \n",
              "\n",
              "[14400 rows x 513 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-61dda1fb-281c-4101-967d-da3e252e7ab8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>503</th>\n",
              "      <th>504</th>\n",
              "      <th>505</th>\n",
              "      <th>506</th>\n",
              "      <th>507</th>\n",
              "      <th>508</th>\n",
              "      <th>509</th>\n",
              "      <th>510</th>\n",
              "      <th>511</th>\n",
              "      <th>512</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7907</th>\n",
              "      <td>-0.017906</td>\n",
              "      <td>-0.016111</td>\n",
              "      <td>-0.015265</td>\n",
              "      <td>-0.015048</td>\n",
              "      <td>-0.015061</td>\n",
              "      <td>-0.014937</td>\n",
              "      <td>-0.014443</td>\n",
              "      <td>-0.013542</td>\n",
              "      <td>-0.012380</td>\n",
              "      <td>-0.011228</td>\n",
              "      <td>...</td>\n",
              "      <td>0.017852</td>\n",
              "      <td>0.005363</td>\n",
              "      <td>-0.004584</td>\n",
              "      <td>-0.011813</td>\n",
              "      <td>-0.016382</td>\n",
              "      <td>-0.018636</td>\n",
              "      <td>-0.019204</td>\n",
              "      <td>-0.018925</td>\n",
              "      <td>-0.018720</td>\n",
              "      <td>44.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11143</th>\n",
              "      <td>-0.118654</td>\n",
              "      <td>-0.121925</td>\n",
              "      <td>-0.126439</td>\n",
              "      <td>-0.131408</td>\n",
              "      <td>-0.135973</td>\n",
              "      <td>-0.139351</td>\n",
              "      <td>-0.140978</td>\n",
              "      <td>-0.140594</td>\n",
              "      <td>-0.138273</td>\n",
              "      <td>-0.134396</td>\n",
              "      <td>...</td>\n",
              "      <td>0.459992</td>\n",
              "      <td>0.467328</td>\n",
              "      <td>0.469964</td>\n",
              "      <td>0.467341</td>\n",
              "      <td>0.458892</td>\n",
              "      <td>0.444103</td>\n",
              "      <td>0.422623</td>\n",
              "      <td>0.394386</td>\n",
              "      <td>0.359716</td>\n",
              "      <td>62.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5909</th>\n",
              "      <td>-0.081073</td>\n",
              "      <td>-0.083194</td>\n",
              "      <td>-0.084811</td>\n",
              "      <td>-0.085956</td>\n",
              "      <td>-0.086619</td>\n",
              "      <td>-0.086742</td>\n",
              "      <td>-0.086232</td>\n",
              "      <td>-0.084994</td>\n",
              "      <td>-0.082948</td>\n",
              "      <td>-0.080048</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.037322</td>\n",
              "      <td>-0.035238</td>\n",
              "      <td>-0.036388</td>\n",
              "      <td>-0.040418</td>\n",
              "      <td>-0.046553</td>\n",
              "      <td>-0.053785</td>\n",
              "      <td>-0.061092</td>\n",
              "      <td>-0.067619</td>\n",
              "      <td>-0.072789</td>\n",
              "      <td>33.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11356</th>\n",
              "      <td>-0.165740</td>\n",
              "      <td>-0.152825</td>\n",
              "      <td>-0.133079</td>\n",
              "      <td>-0.111981</td>\n",
              "      <td>-0.093823</td>\n",
              "      <td>-0.081138</td>\n",
              "      <td>-0.074583</td>\n",
              "      <td>-0.073233</td>\n",
              "      <td>-0.075129</td>\n",
              "      <td>-0.077930</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.055745</td>\n",
              "      <td>-0.066144</td>\n",
              "      <td>-0.071713</td>\n",
              "      <td>-0.067951</td>\n",
              "      <td>-0.050901</td>\n",
              "      <td>-0.018156</td>\n",
              "      <td>0.030236</td>\n",
              "      <td>0.091215</td>\n",
              "      <td>0.158715</td>\n",
              "      <td>64.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8679</th>\n",
              "      <td>-0.008112</td>\n",
              "      <td>-0.001494</td>\n",
              "      <td>0.002451</td>\n",
              "      <td>0.001650</td>\n",
              "      <td>-0.004908</td>\n",
              "      <td>-0.016943</td>\n",
              "      <td>-0.032950</td>\n",
              "      <td>-0.050519</td>\n",
              "      <td>-0.066840</td>\n",
              "      <td>-0.079277</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.067563</td>\n",
              "      <td>-0.069860</td>\n",
              "      <td>-0.071013</td>\n",
              "      <td>-0.071196</td>\n",
              "      <td>-0.070569</td>\n",
              "      <td>-0.069267</td>\n",
              "      <td>-0.067397</td>\n",
              "      <td>-0.065056</td>\n",
              "      <td>-0.062354</td>\n",
              "      <td>49.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8649</th>\n",
              "      <td>0.000466</td>\n",
              "      <td>0.005334</td>\n",
              "      <td>0.009596</td>\n",
              "      <td>0.012806</td>\n",
              "      <td>0.014792</td>\n",
              "      <td>0.015685</td>\n",
              "      <td>0.015858</td>\n",
              "      <td>0.015806</td>\n",
              "      <td>0.015992</td>\n",
              "      <td>0.016739</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.040887</td>\n",
              "      <td>-0.048713</td>\n",
              "      <td>-0.057635</td>\n",
              "      <td>-0.066352</td>\n",
              "      <td>-0.073594</td>\n",
              "      <td>-0.078350</td>\n",
              "      <td>-0.080013</td>\n",
              "      <td>-0.078441</td>\n",
              "      <td>-0.073937</td>\n",
              "      <td>49.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>938</th>\n",
              "      <td>-0.032009</td>\n",
              "      <td>-0.027885</td>\n",
              "      <td>-0.021962</td>\n",
              "      <td>-0.014614</td>\n",
              "      <td>-0.006394</td>\n",
              "      <td>0.002082</td>\n",
              "      <td>0.010239</td>\n",
              "      <td>0.017626</td>\n",
              "      <td>0.023943</td>\n",
              "      <td>0.029020</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.022558</td>\n",
              "      <td>0.029429</td>\n",
              "      <td>0.089977</td>\n",
              "      <td>0.151508</td>\n",
              "      <td>0.205117</td>\n",
              "      <td>0.242187</td>\n",
              "      <td>0.256085</td>\n",
              "      <td>0.243552</td>\n",
              "      <td>0.205435</td>\n",
              "      <td>6.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15953</th>\n",
              "      <td>0.363541</td>\n",
              "      <td>0.358640</td>\n",
              "      <td>0.350826</td>\n",
              "      <td>0.340031</td>\n",
              "      <td>0.326239</td>\n",
              "      <td>0.309534</td>\n",
              "      <td>0.290130</td>\n",
              "      <td>0.268378</td>\n",
              "      <td>0.244732</td>\n",
              "      <td>0.219710</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.053166</td>\n",
              "      <td>-0.051262</td>\n",
              "      <td>-0.051134</td>\n",
              "      <td>-0.053391</td>\n",
              "      <td>-0.057984</td>\n",
              "      <td>-0.064026</td>\n",
              "      <td>-0.069738</td>\n",
              "      <td>-0.072595</td>\n",
              "      <td>-0.069675</td>\n",
              "      <td>89.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1287</th>\n",
              "      <td>0.023535</td>\n",
              "      <td>0.018301</td>\n",
              "      <td>0.013760</td>\n",
              "      <td>0.010491</td>\n",
              "      <td>0.008849</td>\n",
              "      <td>0.008923</td>\n",
              "      <td>0.010539</td>\n",
              "      <td>0.013317</td>\n",
              "      <td>0.016758</td>\n",
              "      <td>0.020343</td>\n",
              "      <td>...</td>\n",
              "      <td>0.110350</td>\n",
              "      <td>0.105924</td>\n",
              "      <td>0.099017</td>\n",
              "      <td>0.089660</td>\n",
              "      <td>0.078087</td>\n",
              "      <td>0.064757</td>\n",
              "      <td>0.050327</td>\n",
              "      <td>0.035595</td>\n",
              "      <td>0.021386</td>\n",
              "      <td>8.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13182</th>\n",
              "      <td>-0.025937</td>\n",
              "      <td>-0.028203</td>\n",
              "      <td>-0.029994</td>\n",
              "      <td>-0.030886</td>\n",
              "      <td>-0.030562</td>\n",
              "      <td>-0.028843</td>\n",
              "      <td>-0.025717</td>\n",
              "      <td>-0.021346</td>\n",
              "      <td>-0.016065</td>\n",
              "      <td>-0.010357</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.430318</td>\n",
              "      <td>-0.397804</td>\n",
              "      <td>-0.342009</td>\n",
              "      <td>-0.275152</td>\n",
              "      <td>-0.208358</td>\n",
              "      <td>-0.150023</td>\n",
              "      <td>-0.104997</td>\n",
              "      <td>-0.074593</td>\n",
              "      <td>-0.057260</td>\n",
              "      <td>74.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>14400 rows × 513 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-61dda1fb-281c-4101-967d-da3e252e7ab8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-61dda1fb-281c-4101-967d-da3e252e7ab8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-61dda1fb-281c-4101-967d-da3e252e7ab8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 전처리"
      ],
      "metadata": {
        "id": "JQBBj0ypT8T-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = to_categorical(data_train[512])\n",
        "y_test = to_categorical(data_test[512])\n",
        "y_val = to_categorical(data_val[512])\n",
        "# y_train = to_categorical(arr)\n",
        "# y_test = to_categorical(arr_t)\n",
        "# y_val = to_categorical(arr_v)\n",
        "y_train[14399]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "koQxh6-XfrG1",
        "outputId": "eee3b76e-9106-4b6b-f240-605b5f99ffa3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_train.drop([512], axis=1, inplace=True)\n",
        "data_test.drop([512], axis=1, inplace=True)\n",
        "data_val.drop([512], axis=1, inplace=True)\n",
        "data_train"
      ],
      "metadata": {
        "id": "QJUFXwCRkGhC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "outputId": "46dd6c46-ffcd-481b-84a8-1cec90333f4e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py:4906: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  return super().drop(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            0         1         2         3         4         5         6    \\\n",
              "7907  -0.017906 -0.016111 -0.015265 -0.015048 -0.015061 -0.014937 -0.014443   \n",
              "11143 -0.118654 -0.121925 -0.126439 -0.131408 -0.135973 -0.139351 -0.140978   \n",
              "5909  -0.081073 -0.083194 -0.084811 -0.085956 -0.086619 -0.086742 -0.086232   \n",
              "11356 -0.165740 -0.152825 -0.133079 -0.111981 -0.093823 -0.081138 -0.074583   \n",
              "8679  -0.008112 -0.001494  0.002451  0.001650 -0.004908 -0.016943 -0.032950   \n",
              "...         ...       ...       ...       ...       ...       ...       ...   \n",
              "8649   0.000466  0.005334  0.009596  0.012806  0.014792  0.015685  0.015858   \n",
              "938   -0.032009 -0.027885 -0.021962 -0.014614 -0.006394  0.002082  0.010239   \n",
              "15953  0.363541  0.358640  0.350826  0.340031  0.326239  0.309534  0.290130   \n",
              "1287   0.023535  0.018301  0.013760  0.010491  0.008849  0.008923  0.010539   \n",
              "13182 -0.025937 -0.028203 -0.029994 -0.030886 -0.030562 -0.028843 -0.025717   \n",
              "\n",
              "            7         8         9    ...       502       503       504  \\\n",
              "7907  -0.013542 -0.012380 -0.011228  ...  0.032553  0.017852  0.005363   \n",
              "11143 -0.140594 -0.138273 -0.134396  ...  0.448493  0.459992  0.467328   \n",
              "5909  -0.084994 -0.082948 -0.080048  ... -0.042474 -0.037322 -0.035238   \n",
              "11356 -0.073233 -0.075129 -0.077930  ... -0.044651 -0.055745 -0.066144   \n",
              "8679  -0.050519 -0.066840 -0.079277  ... -0.063964 -0.067563 -0.069860   \n",
              "...         ...       ...       ...  ...       ...       ...       ...   \n",
              "8649   0.015806  0.015992  0.016739  ... -0.035229 -0.040887 -0.048713   \n",
              "938    0.017626  0.023943  0.029020  ... -0.060928 -0.022558  0.029429   \n",
              "15953  0.268378  0.244732  0.219710  ... -0.055847 -0.053166 -0.051262   \n",
              "1287   0.013317  0.016758  0.020343  ...  0.112414  0.110350  0.105924   \n",
              "13182 -0.021346 -0.016065 -0.010357  ... -0.428491 -0.430318 -0.397804   \n",
              "\n",
              "            505       506       507       508       509       510       511  \n",
              "7907  -0.004584 -0.011813 -0.016382 -0.018636 -0.019204 -0.018925 -0.018720  \n",
              "11143  0.469964  0.467341  0.458892  0.444103  0.422623  0.394386  0.359716  \n",
              "5909  -0.036388 -0.040418 -0.046553 -0.053785 -0.061092 -0.067619 -0.072789  \n",
              "11356 -0.071713 -0.067951 -0.050901 -0.018156  0.030236  0.091215  0.158715  \n",
              "8679  -0.071013 -0.071196 -0.070569 -0.069267 -0.067397 -0.065056 -0.062354  \n",
              "...         ...       ...       ...       ...       ...       ...       ...  \n",
              "8649  -0.057635 -0.066352 -0.073594 -0.078350 -0.080013 -0.078441 -0.073937  \n",
              "938    0.089977  0.151508  0.205117  0.242187  0.256085  0.243552  0.205435  \n",
              "15953 -0.051134 -0.053391 -0.057984 -0.064026 -0.069738 -0.072595 -0.069675  \n",
              "1287   0.099017  0.089660  0.078087  0.064757  0.050327  0.035595  0.021386  \n",
              "13182 -0.342009 -0.275152 -0.208358 -0.150023 -0.104997 -0.074593 -0.057260  \n",
              "\n",
              "[14400 rows x 512 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9723ce54-1861-49ef-8390-d3ab347c9ebc\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>502</th>\n",
              "      <th>503</th>\n",
              "      <th>504</th>\n",
              "      <th>505</th>\n",
              "      <th>506</th>\n",
              "      <th>507</th>\n",
              "      <th>508</th>\n",
              "      <th>509</th>\n",
              "      <th>510</th>\n",
              "      <th>511</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7907</th>\n",
              "      <td>-0.017906</td>\n",
              "      <td>-0.016111</td>\n",
              "      <td>-0.015265</td>\n",
              "      <td>-0.015048</td>\n",
              "      <td>-0.015061</td>\n",
              "      <td>-0.014937</td>\n",
              "      <td>-0.014443</td>\n",
              "      <td>-0.013542</td>\n",
              "      <td>-0.012380</td>\n",
              "      <td>-0.011228</td>\n",
              "      <td>...</td>\n",
              "      <td>0.032553</td>\n",
              "      <td>0.017852</td>\n",
              "      <td>0.005363</td>\n",
              "      <td>-0.004584</td>\n",
              "      <td>-0.011813</td>\n",
              "      <td>-0.016382</td>\n",
              "      <td>-0.018636</td>\n",
              "      <td>-0.019204</td>\n",
              "      <td>-0.018925</td>\n",
              "      <td>-0.018720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11143</th>\n",
              "      <td>-0.118654</td>\n",
              "      <td>-0.121925</td>\n",
              "      <td>-0.126439</td>\n",
              "      <td>-0.131408</td>\n",
              "      <td>-0.135973</td>\n",
              "      <td>-0.139351</td>\n",
              "      <td>-0.140978</td>\n",
              "      <td>-0.140594</td>\n",
              "      <td>-0.138273</td>\n",
              "      <td>-0.134396</td>\n",
              "      <td>...</td>\n",
              "      <td>0.448493</td>\n",
              "      <td>0.459992</td>\n",
              "      <td>0.467328</td>\n",
              "      <td>0.469964</td>\n",
              "      <td>0.467341</td>\n",
              "      <td>0.458892</td>\n",
              "      <td>0.444103</td>\n",
              "      <td>0.422623</td>\n",
              "      <td>0.394386</td>\n",
              "      <td>0.359716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5909</th>\n",
              "      <td>-0.081073</td>\n",
              "      <td>-0.083194</td>\n",
              "      <td>-0.084811</td>\n",
              "      <td>-0.085956</td>\n",
              "      <td>-0.086619</td>\n",
              "      <td>-0.086742</td>\n",
              "      <td>-0.086232</td>\n",
              "      <td>-0.084994</td>\n",
              "      <td>-0.082948</td>\n",
              "      <td>-0.080048</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.042474</td>\n",
              "      <td>-0.037322</td>\n",
              "      <td>-0.035238</td>\n",
              "      <td>-0.036388</td>\n",
              "      <td>-0.040418</td>\n",
              "      <td>-0.046553</td>\n",
              "      <td>-0.053785</td>\n",
              "      <td>-0.061092</td>\n",
              "      <td>-0.067619</td>\n",
              "      <td>-0.072789</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11356</th>\n",
              "      <td>-0.165740</td>\n",
              "      <td>-0.152825</td>\n",
              "      <td>-0.133079</td>\n",
              "      <td>-0.111981</td>\n",
              "      <td>-0.093823</td>\n",
              "      <td>-0.081138</td>\n",
              "      <td>-0.074583</td>\n",
              "      <td>-0.073233</td>\n",
              "      <td>-0.075129</td>\n",
              "      <td>-0.077930</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.044651</td>\n",
              "      <td>-0.055745</td>\n",
              "      <td>-0.066144</td>\n",
              "      <td>-0.071713</td>\n",
              "      <td>-0.067951</td>\n",
              "      <td>-0.050901</td>\n",
              "      <td>-0.018156</td>\n",
              "      <td>0.030236</td>\n",
              "      <td>0.091215</td>\n",
              "      <td>0.158715</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8679</th>\n",
              "      <td>-0.008112</td>\n",
              "      <td>-0.001494</td>\n",
              "      <td>0.002451</td>\n",
              "      <td>0.001650</td>\n",
              "      <td>-0.004908</td>\n",
              "      <td>-0.016943</td>\n",
              "      <td>-0.032950</td>\n",
              "      <td>-0.050519</td>\n",
              "      <td>-0.066840</td>\n",
              "      <td>-0.079277</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.063964</td>\n",
              "      <td>-0.067563</td>\n",
              "      <td>-0.069860</td>\n",
              "      <td>-0.071013</td>\n",
              "      <td>-0.071196</td>\n",
              "      <td>-0.070569</td>\n",
              "      <td>-0.069267</td>\n",
              "      <td>-0.067397</td>\n",
              "      <td>-0.065056</td>\n",
              "      <td>-0.062354</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8649</th>\n",
              "      <td>0.000466</td>\n",
              "      <td>0.005334</td>\n",
              "      <td>0.009596</td>\n",
              "      <td>0.012806</td>\n",
              "      <td>0.014792</td>\n",
              "      <td>0.015685</td>\n",
              "      <td>0.015858</td>\n",
              "      <td>0.015806</td>\n",
              "      <td>0.015992</td>\n",
              "      <td>0.016739</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.035229</td>\n",
              "      <td>-0.040887</td>\n",
              "      <td>-0.048713</td>\n",
              "      <td>-0.057635</td>\n",
              "      <td>-0.066352</td>\n",
              "      <td>-0.073594</td>\n",
              "      <td>-0.078350</td>\n",
              "      <td>-0.080013</td>\n",
              "      <td>-0.078441</td>\n",
              "      <td>-0.073937</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>938</th>\n",
              "      <td>-0.032009</td>\n",
              "      <td>-0.027885</td>\n",
              "      <td>-0.021962</td>\n",
              "      <td>-0.014614</td>\n",
              "      <td>-0.006394</td>\n",
              "      <td>0.002082</td>\n",
              "      <td>0.010239</td>\n",
              "      <td>0.017626</td>\n",
              "      <td>0.023943</td>\n",
              "      <td>0.029020</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.060928</td>\n",
              "      <td>-0.022558</td>\n",
              "      <td>0.029429</td>\n",
              "      <td>0.089977</td>\n",
              "      <td>0.151508</td>\n",
              "      <td>0.205117</td>\n",
              "      <td>0.242187</td>\n",
              "      <td>0.256085</td>\n",
              "      <td>0.243552</td>\n",
              "      <td>0.205435</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15953</th>\n",
              "      <td>0.363541</td>\n",
              "      <td>0.358640</td>\n",
              "      <td>0.350826</td>\n",
              "      <td>0.340031</td>\n",
              "      <td>0.326239</td>\n",
              "      <td>0.309534</td>\n",
              "      <td>0.290130</td>\n",
              "      <td>0.268378</td>\n",
              "      <td>0.244732</td>\n",
              "      <td>0.219710</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.055847</td>\n",
              "      <td>-0.053166</td>\n",
              "      <td>-0.051262</td>\n",
              "      <td>-0.051134</td>\n",
              "      <td>-0.053391</td>\n",
              "      <td>-0.057984</td>\n",
              "      <td>-0.064026</td>\n",
              "      <td>-0.069738</td>\n",
              "      <td>-0.072595</td>\n",
              "      <td>-0.069675</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1287</th>\n",
              "      <td>0.023535</td>\n",
              "      <td>0.018301</td>\n",
              "      <td>0.013760</td>\n",
              "      <td>0.010491</td>\n",
              "      <td>0.008849</td>\n",
              "      <td>0.008923</td>\n",
              "      <td>0.010539</td>\n",
              "      <td>0.013317</td>\n",
              "      <td>0.016758</td>\n",
              "      <td>0.020343</td>\n",
              "      <td>...</td>\n",
              "      <td>0.112414</td>\n",
              "      <td>0.110350</td>\n",
              "      <td>0.105924</td>\n",
              "      <td>0.099017</td>\n",
              "      <td>0.089660</td>\n",
              "      <td>0.078087</td>\n",
              "      <td>0.064757</td>\n",
              "      <td>0.050327</td>\n",
              "      <td>0.035595</td>\n",
              "      <td>0.021386</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13182</th>\n",
              "      <td>-0.025937</td>\n",
              "      <td>-0.028203</td>\n",
              "      <td>-0.029994</td>\n",
              "      <td>-0.030886</td>\n",
              "      <td>-0.030562</td>\n",
              "      <td>-0.028843</td>\n",
              "      <td>-0.025717</td>\n",
              "      <td>-0.021346</td>\n",
              "      <td>-0.016065</td>\n",
              "      <td>-0.010357</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.428491</td>\n",
              "      <td>-0.430318</td>\n",
              "      <td>-0.397804</td>\n",
              "      <td>-0.342009</td>\n",
              "      <td>-0.275152</td>\n",
              "      <td>-0.208358</td>\n",
              "      <td>-0.150023</td>\n",
              "      <td>-0.104997</td>\n",
              "      <td>-0.074593</td>\n",
              "      <td>-0.057260</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>14400 rows × 512 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9723ce54-1861-49ef-8390-d3ab347c9ebc')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9723ce54-1861-49ef-8390-d3ab347c9ebc button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9723ce54-1861-49ef-8390-d3ab347c9ebc');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train=data_train.values\n",
        "X_test=data_test.values\n",
        "X_val=data_val.values"
      ],
      "metadata": {
        "id": "EK7Q96RZtwZl"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(X_val.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)\n",
        "print(y_val.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwfh9AwhjkMM",
        "outputId": "574828e2-3aab-4413-fd87-04f85c185f19"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(14400, 512)\n",
            "(1800, 512)\n",
            "(1800, 512)\n",
            "(14400, 101)\n",
            "(1800, 101)\n",
            "(1800, 101)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# X_train = X_train.reshape(1, 256, 24000)\n",
        "# X_test = X_test.reshape(1, 256, 6000)\n",
        "# X_val = X_val.reshape(1, 256, 6000)\n",
        "X_train = X_train.reshape(14400, 512, 1)\n",
        "X_test = X_test.reshape(1800, 512, 1)\n",
        "X_val = X_val.reshape(1800, 512, 1)\n",
        "X_train.shape, X_test.shape, X_val.shape"
      ],
      "metadata": {
        "id": "50YyZlL9htW3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4064a75b-fdc8-48ae-fc71-1c74f3c8258b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((14400, 512, 1), (1800, 512, 1), (1800, 512, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## cnn-lstm 모델"
      ],
      "metadata": {
        "id": "EgfaelmnXe04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model():\n",
        "    model = Sequential()\n",
        "    model.add(Conv1D(filters=64, kernel_size=6, activation='relu', \n",
        "                    padding='same', input_shape=(256, 1)))\n",
        "    model.add(BatchNormalization())\n",
        "    \n",
        "    # adding a pooling layer\n",
        "    model.add(MaxPooling1D(pool_size=(3), strides=2, padding='same'))\n",
        "    \n",
        "    model.add(Conv1D(filters=64, kernel_size=6, activation='relu', \n",
        "                    padding='same', input_shape=(256, 1)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling1D(pool_size=(3), strides=2, padding='same'))\n",
        "    \n",
        "    model.add(Conv1D(filters=64, kernel_size=6, activation='relu', \n",
        "                    padding='same', input_shape=(256, 1)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling1D(pool_size=(3), strides=2, padding='same'))\n",
        "    \n",
        "    model.add(Flatten())\n",
        "    #model.add(Dense(64, activation='relu'))\n",
        "    #model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(101, activation='softmax'))\n",
        "    \n",
        "    model.compile(optimizer='adam', loss='mse',metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "DqFKm7PSQS3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model()\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "wum-FyK7QVUe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2397946-5a7a-4f1f-e09b-f35229db7ab7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_18\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_17 (Conv1D)          (None, 256, 64)           448       \n",
            "                                                                 \n",
            " batch_normalization_12 (Bat  (None, 256, 64)          256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling1d_17 (MaxPoolin  (None, 128, 64)          0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_18 (Conv1D)          (None, 128, 64)           24640     \n",
            "                                                                 \n",
            " batch_normalization_13 (Bat  (None, 128, 64)          256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling1d_18 (MaxPoolin  (None, 64, 64)           0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_19 (Conv1D)          (None, 64, 64)            24640     \n",
            "                                                                 \n",
            " batch_normalization_14 (Bat  (None, 64, 64)           256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling1d_19 (MaxPoolin  (None, 32, 64)           0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " flatten_13 (Flatten)        (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_24 (Dense)            (None, 100)               204900    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 255,396\n",
            "Trainable params: 255,012\n",
            "Non-trainable params: 384\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## cnn-gru 모델"
      ],
      "metadata": {
        "id": "m-Hm3lsWXkbE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Input, Embedding, GRU, LSTM, MaxPooling1D, GlobalMaxPool1D, CuDNNGRU\n",
        "from keras.layers import Dropout, Dense, Activation, Flatten, Conv1D, SpatialDropout1D\n",
        "from keras.models import Sequential\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv1D(filters=64, kernel_size=6, activation='relu', padding='same', input_shape=(256, 1)))\n",
        "model.add(SpatialDropout1D(0.2))\n",
        "model.add(MaxPooling1D(4))\n",
        "model.add(CuDNNGRU(64))\n",
        "\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Dense(101, activation='softmax'))\n",
        "model.compile(optimizer='adam', loss='mse',metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXzfgBF75uk-",
        "outputId": "33f77877-e599-4ded-8400-373f5cec14bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_13 (Conv1D)          (None, 256, 64)           448       \n",
            "                                                                 \n",
            " spatial_dropout1d_4 (Spatia  (None, 256, 64)          0         \n",
            " lDropout1D)                                                     \n",
            "                                                                 \n",
            " max_pooling1d_13 (MaxPoolin  (None, 64, 64)           0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " cu_dnngru_4 (CuDNNGRU)      (None, 64)                24960     \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 101)               6565      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 31,973\n",
            "Trainable params: 31,973\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## gru모델"
      ],
      "metadata": {
        "id": "Na73oRb9Xq69"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Input, Embedding, GRU, LSTM, SimpleRNN, MaxPooling1D, GlobalMaxPool1D, CuDNNGRU\n",
        "from keras.layers import Dropout, Dense, Activation, Flatten, Conv1D, SpatialDropout1D\n",
        "from keras.models import Sequential\n",
        "\n",
        "model = Sequential()\n",
        "model.add(GRU(units=150, return_sequences=True, input_shape=(512,1))) #shape[1]=열\n",
        "model.add(GRU(units=50, return_sequences=True))\n",
        "model.add(GRU(units=50, return_sequences=True))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(101, activation='softmax'))\n",
        "model.compile(optimizer='adam', loss='mse',metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWyVmfGGXar5",
        "outputId": "f0674bba-b327-47c8-cfd6-55018aec61df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " gru (GRU)                   (None, 512, 150)          68850     \n",
            "                                                                 \n",
            " gru_1 (GRU)                 (None, 512, 50)           30300     \n",
            "                                                                 \n",
            " gru_2 (GRU)                 (None, 512, 50)           15300     \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 25600)             0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 25600)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 101)               2585701   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,700,151\n",
            "Trainable params: 2,700,151\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSTM"
      ],
      "metadata": {
        "id": "DBnTIBCDGX94"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(units=100, return_sequences=True, input_shape=(256,1))) #shape[1]=열\n",
        "model.add(LSTM(units=50, return_sequences=True))\n",
        "model.add(LSTM(units=50, return_sequences=True))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Dense(101, activation='softmax'))\n",
        "model.compile(optimizer='adam', loss='mse',metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jbh5mP0GWh5",
        "outputId": "8055881e-f18e-4941-e7d3-1d853b39d2e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_3 (LSTM)               (None, 256, 100)          40800     \n",
            "                                                                 \n",
            " lstm_4 (LSTM)               (None, 256, 50)           30200     \n",
            "                                                                 \n",
            " lstm_5 (LSTM)               (None, 256, 50)           20200     \n",
            "                                                                 \n",
            " flatten_6 (Flatten)         (None, 12800)             0         \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 12800)             0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 101)               1292901   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,384,101\n",
            "Trainable params: 1,384,101\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RNN모델"
      ],
      "metadata": {
        "id": "mLO6GGJ5Nn9w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Input, Embedding, GRU, LSTM, SimpleRNN, MaxPooling1D, GlobalMaxPool1D, CuDNNGRU\n",
        "from keras.layers import Dropout, Dense, Activation, Flatten, Conv1D, SpatialDropout1D\n",
        "from keras.models import Sequential\n",
        "\n",
        "model = Sequential()\n",
        "model.add(SimpleRNN(units=100, return_sequences=True, input_shape=(256,1))) #shape[1]=열\n",
        "model.add(SimpleRNN(units=50, return_sequences=True))\n",
        "model.add(SimpleRNN(units=50, return_sequences=True))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(101, activation='softmax'))\n",
        "model.compile(optimizer='adam', loss='mse',metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHOArXJVNnd0",
        "outputId": "4f56aa04-4c34-4860-fc44-299596d4f96f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " simple_rnn_12 (SimpleRNN)   (None, 256, 100)          10200     \n",
            "                                                                 \n",
            " simple_rnn_13 (SimpleRNN)   (None, 256, 50)           7550      \n",
            "                                                                 \n",
            " simple_rnn_14 (SimpleRNN)   (None, 256, 50)           5050      \n",
            "                                                                 \n",
            " flatten_4 (Flatten)         (None, 12800)             0         \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 12800)             0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 101)               1292901   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,315,701\n",
            "Trainable params: 1,315,701\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNN"
      ],
      "metadata": {
        "id": "rtiJLZsQtOYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model():\n",
        "  model = Sequential()\n",
        "  model.add(Conv1D(512, 8, padding='same', activation='relu', input_shape= (512,1)))\n",
        "  model.add(MaxPooling1D())\n",
        "  model.add(Conv1D(512, 8, padding='same', activation='relu'))\n",
        "  model.add(MaxPooling1D())\n",
        "  model.add(Conv1D(128, 8, padding='same',activation='relu'))\n",
        "  model.add(MaxPooling1D())\n",
        "  model.add(Conv1D(64, 8, padding='same',activation='relu'))\n",
        "  model.add(MaxPooling1D())\n",
        "  model.add(Conv1D(64, 8, padding='same', activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(101, activation='softmax')) # activation='softmax'\n",
        "\n",
        "  model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
        "  model.summary()\n",
        "  return model"
      ],
      "metadata": {
        "id": "ANc7SszVtQbd"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yEmn1q3x9YQ",
        "outputId": "59681572-8011-463f-e344-7eba66118699"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_17 (Conv1D)          (None, 512, 512)          4608      \n",
            "                                                                 \n",
            " max_pooling1d_13 (MaxPoolin  (None, 256, 512)         0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_18 (Conv1D)          (None, 256, 512)          2097664   \n",
            "                                                                 \n",
            " max_pooling1d_14 (MaxPoolin  (None, 128, 512)         0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_19 (Conv1D)          (None, 128, 128)          524416    \n",
            "                                                                 \n",
            " max_pooling1d_15 (MaxPoolin  (None, 64, 128)          0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_20 (Conv1D)          (None, 64, 64)            65600     \n",
            "                                                                 \n",
            " max_pooling1d_16 (MaxPoolin  (None, 32, 64)           0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_21 (Conv1D)          (None, 32, 64)            32832     \n",
            "                                                                 \n",
            " batch_normalization_4 (Batc  (None, 32, 64)           256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " flatten_4 (Flatten)         (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 101)               206949    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,932,325\n",
            "Trainable params: 2,932,197\n",
            "Non-trainable params: 128\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델학습/평가"
      ],
      "metadata": {
        "id": "bWixFuhvKbcz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train, y_train, batch_size =1024, epochs = 1000, verbose = 1, validation_data=(X_val, y_val))"
      ],
      "metadata": {
        "id": "t-UtiKqRQYVv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2883ae0f-8758-4076-f165-254943b46d65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "15/15 [==============================] - 16s 1s/step - loss: 0.0098 - accuracy: 0.0172 - val_loss: 0.0098 - val_accuracy: 0.0083\n",
            "Epoch 2/1000\n",
            "15/15 [==============================] - 12s 809ms/step - loss: 0.0097 - accuracy: 0.0334 - val_loss: 0.0098 - val_accuracy: 0.0333\n",
            "Epoch 3/1000\n",
            "15/15 [==============================] - 12s 822ms/step - loss: 0.0094 - accuracy: 0.0719 - val_loss: 0.0098 - val_accuracy: 0.0328\n",
            "Epoch 4/1000\n",
            "15/15 [==============================] - 13s 836ms/step - loss: 0.0087 - accuracy: 0.1956 - val_loss: 0.0097 - val_accuracy: 0.0133\n",
            "Epoch 5/1000\n",
            "15/15 [==============================] - 13s 832ms/step - loss: 0.0076 - accuracy: 0.3649 - val_loss: 0.0099 - val_accuracy: 0.0267\n",
            "Epoch 6/1000\n",
            "15/15 [==============================] - 12s 821ms/step - loss: 0.0064 - accuracy: 0.4980 - val_loss: 0.0100 - val_accuracy: 0.0472\n",
            "Epoch 7/1000\n",
            "15/15 [==============================] - 12s 814ms/step - loss: 0.0052 - accuracy: 0.6268 - val_loss: 0.0101 - val_accuracy: 0.0422\n",
            "Epoch 8/1000\n",
            "15/15 [==============================] - 12s 814ms/step - loss: 0.0035 - accuracy: 0.7723 - val_loss: 0.0108 - val_accuracy: 0.0528\n",
            "Epoch 9/1000\n",
            "15/15 [==============================] - 12s 817ms/step - loss: 0.0023 - accuracy: 0.8631 - val_loss: 0.0095 - val_accuracy: 0.1422\n",
            "Epoch 10/1000\n",
            "15/15 [==============================] - 12s 822ms/step - loss: 0.0014 - accuracy: 0.9214 - val_loss: 0.0099 - val_accuracy: 0.1611\n",
            "Epoch 11/1000\n",
            "15/15 [==============================] - 12s 824ms/step - loss: 8.9560e-04 - accuracy: 0.9538 - val_loss: 0.0088 - val_accuracy: 0.2694\n",
            "Epoch 12/1000\n",
            "15/15 [==============================] - 12s 824ms/step - loss: 5.3863e-04 - accuracy: 0.9744 - val_loss: 0.0080 - val_accuracy: 0.3389\n",
            "Epoch 13/1000\n",
            "15/15 [==============================] - 12s 823ms/step - loss: 3.5527e-04 - accuracy: 0.9853 - val_loss: 0.0058 - val_accuracy: 0.5450\n",
            "Epoch 14/1000\n",
            "15/15 [==============================] - 12s 824ms/step - loss: 2.1454e-04 - accuracy: 0.9925 - val_loss: 0.0043 - val_accuracy: 0.7100\n",
            "Epoch 15/1000\n",
            "15/15 [==============================] - 12s 822ms/step - loss: 1.5050e-04 - accuracy: 0.9948 - val_loss: 0.0037 - val_accuracy: 0.7667\n",
            "Epoch 16/1000\n",
            "15/15 [==============================] - 12s 822ms/step - loss: 1.0028e-04 - accuracy: 0.9972 - val_loss: 0.0027 - val_accuracy: 0.8572\n",
            "Epoch 17/1000\n",
            "15/15 [==============================] - 12s 821ms/step - loss: 1.0010e-04 - accuracy: 0.9972 - val_loss: 0.0027 - val_accuracy: 0.8461\n",
            "Epoch 18/1000\n",
            "15/15 [==============================] - 12s 820ms/step - loss: 8.4513e-05 - accuracy: 0.9978 - val_loss: 0.0021 - val_accuracy: 0.8822\n",
            "Epoch 19/1000\n",
            "15/15 [==============================] - 12s 820ms/step - loss: 5.0931e-05 - accuracy: 0.9985 - val_loss: 9.8900e-04 - val_accuracy: 0.9667\n",
            "Epoch 20/1000\n",
            "15/15 [==============================] - 12s 820ms/step - loss: 4.0747e-05 - accuracy: 0.9987 - val_loss: 7.3770e-04 - val_accuracy: 0.9750\n",
            "Epoch 21/1000\n",
            "15/15 [==============================] - 12s 821ms/step - loss: 3.9297e-05 - accuracy: 0.9987 - val_loss: 6.9418e-04 - val_accuracy: 0.9728\n",
            "Epoch 22/1000\n",
            "15/15 [==============================] - 12s 820ms/step - loss: 3.1564e-05 - accuracy: 0.9990 - val_loss: 5.7050e-04 - val_accuracy: 0.9822\n",
            "Epoch 23/1000\n",
            "15/15 [==============================] - 12s 820ms/step - loss: 3.1110e-05 - accuracy: 0.9988 - val_loss: 7.0214e-04 - val_accuracy: 0.9711\n",
            "Epoch 24/1000\n",
            "15/15 [==============================] - 12s 820ms/step - loss: 4.4574e-05 - accuracy: 0.9985 - val_loss: 7.2658e-04 - val_accuracy: 0.9661\n",
            "Epoch 25/1000\n",
            "15/15 [==============================] - 12s 820ms/step - loss: 4.6215e-05 - accuracy: 0.9989 - val_loss: 6.0681e-04 - val_accuracy: 0.9711\n",
            "Epoch 26/1000\n",
            "15/15 [==============================] - 12s 821ms/step - loss: 6.1800e-05 - accuracy: 0.9981 - val_loss: 6.0901e-04 - val_accuracy: 0.9656\n",
            "Epoch 27/1000\n",
            "15/15 [==============================] - 12s 823ms/step - loss: 4.1042e-05 - accuracy: 0.9990 - val_loss: 3.4912e-04 - val_accuracy: 0.9828\n",
            "Epoch 28/1000\n",
            "15/15 [==============================] - 12s 823ms/step - loss: 3.4799e-05 - accuracy: 0.9992 - val_loss: 4.3519e-04 - val_accuracy: 0.9783\n",
            "Epoch 29/1000\n",
            "15/15 [==============================] - 12s 822ms/step - loss: 2.6458e-05 - accuracy: 0.9992 - val_loss: 4.2679e-04 - val_accuracy: 0.9756\n",
            "Epoch 30/1000\n",
            "15/15 [==============================] - 12s 822ms/step - loss: 2.1801e-05 - accuracy: 0.9994 - val_loss: 2.7051e-04 - val_accuracy: 0.9878\n",
            "Epoch 31/1000\n",
            "15/15 [==============================] - 12s 822ms/step - loss: 1.6782e-05 - accuracy: 0.9994 - val_loss: 2.3390e-04 - val_accuracy: 0.9878\n",
            "Epoch 32/1000\n",
            "15/15 [==============================] - 12s 825ms/step - loss: 1.3544e-05 - accuracy: 0.9995 - val_loss: 1.9149e-04 - val_accuracy: 0.9900\n",
            "Epoch 33/1000\n",
            "15/15 [==============================] - 12s 823ms/step - loss: 1.2185e-05 - accuracy: 0.9995 - val_loss: 1.8694e-04 - val_accuracy: 0.9928\n",
            "Epoch 34/1000\n",
            "15/15 [==============================] - 12s 822ms/step - loss: 1.2058e-05 - accuracy: 0.9995 - val_loss: 2.1425e-04 - val_accuracy: 0.9883\n",
            "Epoch 35/1000\n",
            "15/15 [==============================] - 12s 820ms/step - loss: 1.1714e-05 - accuracy: 0.9995 - val_loss: 2.0225e-04 - val_accuracy: 0.9872\n",
            "Epoch 36/1000\n",
            "15/15 [==============================] - 12s 820ms/step - loss: 1.1417e-05 - accuracy: 0.9995 - val_loss: 1.9991e-04 - val_accuracy: 0.9900\n",
            "Epoch 37/1000\n",
            "15/15 [==============================] - 12s 819ms/step - loss: 1.2017e-05 - accuracy: 0.9995 - val_loss: 2.0374e-04 - val_accuracy: 0.9894\n",
            "Epoch 38/1000\n",
            "15/15 [==============================] - 12s 819ms/step - loss: 1.1589e-05 - accuracy: 0.9995 - val_loss: 1.8395e-04 - val_accuracy: 0.9889\n",
            "Epoch 39/1000\n",
            "15/15 [==============================] - 12s 818ms/step - loss: 1.0985e-05 - accuracy: 0.9995 - val_loss: 2.2625e-04 - val_accuracy: 0.9856\n",
            "Epoch 40/1000\n",
            "15/15 [==============================] - 12s 819ms/step - loss: 1.6843e-05 - accuracy: 0.9994 - val_loss: 6.1353e-04 - val_accuracy: 0.9567\n",
            "Epoch 41/1000\n",
            "15/15 [==============================] - 12s 821ms/step - loss: 1.6849e-05 - accuracy: 0.9994 - val_loss: 2.7336e-04 - val_accuracy: 0.9844\n",
            "Epoch 42/1000\n",
            "15/15 [==============================] - 12s 822ms/step - loss: 1.5372e-05 - accuracy: 0.9994 - val_loss: 2.4308e-04 - val_accuracy: 0.9856\n",
            "Epoch 43/1000\n",
            "15/15 [==============================] - 12s 823ms/step - loss: 1.3059e-05 - accuracy: 0.9995 - val_loss: 2.6091e-04 - val_accuracy: 0.9844\n",
            "Epoch 44/1000\n",
            "15/15 [==============================] - 12s 822ms/step - loss: 1.1749e-05 - accuracy: 0.9994 - val_loss: 2.8659e-04 - val_accuracy: 0.9800\n",
            "Epoch 45/1000\n",
            "15/15 [==============================] - 12s 821ms/step - loss: 3.4428e-05 - accuracy: 0.9984 - val_loss: 3.0713e-04 - val_accuracy: 0.9817\n",
            "Epoch 46/1000\n",
            "15/15 [==============================] - 12s 819ms/step - loss: 2.1574e-05 - accuracy: 0.9993 - val_loss: 5.6521e-04 - val_accuracy: 0.9639\n",
            "Epoch 47/1000\n",
            "15/15 [==============================] - 12s 819ms/step - loss: 1.9015e-05 - accuracy: 0.9995 - val_loss: 2.8170e-04 - val_accuracy: 0.9850\n",
            "Epoch 48/1000\n",
            "15/15 [==============================] - 12s 821ms/step - loss: 1.2386e-05 - accuracy: 0.9995 - val_loss: 1.7383e-04 - val_accuracy: 0.9900\n",
            "Epoch 49/1000\n",
            "15/15 [==============================] - 12s 823ms/step - loss: 1.0807e-05 - accuracy: 0.9995 - val_loss: 2.1622e-04 - val_accuracy: 0.9850\n",
            "Epoch 50/1000\n",
            "15/15 [==============================] - 12s 822ms/step - loss: 1.0626e-05 - accuracy: 0.9995 - val_loss: 1.5765e-04 - val_accuracy: 0.9906\n",
            "Epoch 51/1000\n",
            "15/15 [==============================] - 12s 820ms/step - loss: 1.0050e-05 - accuracy: 0.9995 - val_loss: 1.5821e-04 - val_accuracy: 0.9906\n",
            "Epoch 52/1000\n",
            "15/15 [==============================] - 12s 819ms/step - loss: 1.0288e-05 - accuracy: 0.9994 - val_loss: 1.5093e-04 - val_accuracy: 0.9922\n",
            "Epoch 53/1000\n",
            "15/15 [==============================] - 12s 818ms/step - loss: 1.0074e-05 - accuracy: 0.9995 - val_loss: 1.5934e-04 - val_accuracy: 0.9917\n",
            "Epoch 54/1000\n",
            "15/15 [==============================] - 12s 819ms/step - loss: 9.9833e-06 - accuracy: 0.9995 - val_loss: 1.6377e-04 - val_accuracy: 0.9917\n",
            "Epoch 55/1000\n",
            "15/15 [==============================] - 12s 818ms/step - loss: 9.9211e-06 - accuracy: 0.9995 - val_loss: 2.7924e-04 - val_accuracy: 0.9817\n",
            "Epoch 56/1000\n",
            "15/15 [==============================] - 12s 819ms/step - loss: 1.6385e-05 - accuracy: 0.9994 - val_loss: 2.3361e-04 - val_accuracy: 0.9850\n",
            "Epoch 57/1000\n",
            "15/15 [==============================] - 12s 819ms/step - loss: 1.1685e-05 - accuracy: 0.9994 - val_loss: 0.0019 - val_accuracy: 0.8733\n",
            "Epoch 58/1000\n",
            "15/15 [==============================] - 12s 820ms/step - loss: 1.0058e-04 - accuracy: 0.9953 - val_loss: 0.0019 - val_accuracy: 0.8667\n",
            "Epoch 59/1000\n",
            "15/15 [==============================] - 12s 820ms/step - loss: 2.2128e-04 - accuracy: 0.9890 - val_loss: 0.0053 - val_accuracy: 0.6550\n",
            "Epoch 60/1000\n",
            "15/15 [==============================] - 12s 820ms/step - loss: 1.7850e-04 - accuracy: 0.9917 - val_loss: 0.0014 - val_accuracy: 0.9100\n",
            "Epoch 61/1000\n",
            "15/15 [==============================] - 12s 818ms/step - loss: 7.5507e-05 - accuracy: 0.9969 - val_loss: 0.0034 - val_accuracy: 0.7767\n",
            "Epoch 62/1000\n",
            "15/15 [==============================] - 12s 819ms/step - loss: 5.7934e-05 - accuracy: 0.9974 - val_loss: 8.6905e-04 - val_accuracy: 0.9400\n",
            "Epoch 63/1000\n",
            "15/15 [==============================] - 12s 818ms/step - loss: 2.9807e-05 - accuracy: 0.9992 - val_loss: 6.8163e-04 - val_accuracy: 0.9528\n",
            "Epoch 64/1000\n",
            "15/15 [==============================] - 12s 820ms/step - loss: 2.0140e-05 - accuracy: 0.9992 - val_loss: 7.4372e-04 - val_accuracy: 0.9500\n",
            "Epoch 65/1000\n",
            "15/15 [==============================] - 12s 818ms/step - loss: 4.6346e-05 - accuracy: 0.9980 - val_loss: 0.0020 - val_accuracy: 0.8539\n",
            "Epoch 66/1000\n",
            "15/15 [==============================] - 12s 817ms/step - loss: 5.7652e-05 - accuracy: 0.9975 - val_loss: 0.0018 - val_accuracy: 0.8750\n",
            "Epoch 67/1000\n",
            "15/15 [==============================] - 12s 819ms/step - loss: 2.8418e-05 - accuracy: 0.9989 - val_loss: 4.5465e-04 - val_accuracy: 0.9694\n",
            "Epoch 68/1000\n",
            "15/15 [==============================] - 12s 818ms/step - loss: 1.3690e-05 - accuracy: 0.9994 - val_loss: 2.7192e-04 - val_accuracy: 0.9822\n",
            "Epoch 69/1000\n",
            "15/15 [==============================] - 12s 817ms/step - loss: 1.1352e-05 - accuracy: 0.9995 - val_loss: 1.9214e-04 - val_accuracy: 0.9867\n",
            "Epoch 70/1000\n",
            "15/15 [==============================] - 12s 818ms/step - loss: 1.0186e-05 - accuracy: 0.9995 - val_loss: 1.4523e-04 - val_accuracy: 0.9922\n",
            "Epoch 71/1000\n",
            "15/15 [==============================] - 12s 817ms/step - loss: 1.6522e-05 - accuracy: 0.9992 - val_loss: 1.9145e-04 - val_accuracy: 0.9900\n",
            "Epoch 72/1000\n",
            "15/15 [==============================] - 12s 817ms/step - loss: 1.0809e-05 - accuracy: 0.9995 - val_loss: 1.8200e-04 - val_accuracy: 0.9900\n",
            "Epoch 73/1000\n",
            "15/15 [==============================] - 12s 818ms/step - loss: 1.4769e-05 - accuracy: 0.9993 - val_loss: 2.0082e-04 - val_accuracy: 0.9878\n",
            "Epoch 74/1000\n",
            "15/15 [==============================] - 12s 819ms/step - loss: 1.0602e-05 - accuracy: 0.9995 - val_loss: 1.6514e-04 - val_accuracy: 0.9900\n",
            "Epoch 75/1000\n",
            "15/15 [==============================] - 12s 818ms/step - loss: 9.2186e-06 - accuracy: 0.9995 - val_loss: 1.4780e-04 - val_accuracy: 0.9911\n",
            "Epoch 76/1000\n",
            "15/15 [==============================] - 12s 817ms/step - loss: 9.0317e-06 - accuracy: 0.9995 - val_loss: 1.2975e-04 - val_accuracy: 0.9911\n",
            "Epoch 77/1000\n",
            "15/15 [==============================] - 12s 818ms/step - loss: 9.0430e-06 - accuracy: 0.9995 - val_loss: 1.4364e-04 - val_accuracy: 0.9906\n",
            "Epoch 78/1000\n",
            "15/15 [==============================] - 12s 819ms/step - loss: 2.1440e-05 - accuracy: 0.9991 - val_loss: 8.8294e-04 - val_accuracy: 0.9378\n",
            "Epoch 79/1000\n",
            "15/15 [==============================] - 12s 819ms/step - loss: 5.7855e-05 - accuracy: 0.9975 - val_loss: 0.0124 - val_accuracy: 0.2711\n",
            "Epoch 80/1000\n",
            "15/15 [==============================] - 12s 818ms/step - loss: 2.5518e-04 - accuracy: 0.9855 - val_loss: 0.0043 - val_accuracy: 0.7156\n",
            "Epoch 81/1000\n",
            "15/15 [==============================] - 12s 817ms/step - loss: 2.1993e-04 - accuracy: 0.9881 - val_loss: 0.0054 - val_accuracy: 0.6433\n",
            "Epoch 82/1000\n",
            "15/15 [==============================] - 12s 818ms/step - loss: 8.2646e-05 - accuracy: 0.9955 - val_loss: 0.0040 - val_accuracy: 0.7389\n",
            "Epoch 83/1000\n",
            "15/15 [==============================] - 12s 817ms/step - loss: 1.0879e-04 - accuracy: 0.9948 - val_loss: 0.0018 - val_accuracy: 0.8794\n",
            "Epoch 84/1000\n",
            "15/15 [==============================] - 12s 816ms/step - loss: 8.5435e-05 - accuracy: 0.9957 - val_loss: 0.0034 - val_accuracy: 0.7728\n",
            "Epoch 85/1000\n",
            "15/15 [==============================] - 12s 815ms/step - loss: 4.6448e-05 - accuracy: 0.9979 - val_loss: 0.0028 - val_accuracy: 0.8094\n",
            "Epoch 86/1000\n",
            "15/15 [==============================] - 12s 818ms/step - loss: 8.8691e-05 - accuracy: 0.9956 - val_loss: 0.0014 - val_accuracy: 0.9033\n",
            "Epoch 87/1000\n",
            "15/15 [==============================] - 12s 819ms/step - loss: 2.4703e-05 - accuracy: 0.9990 - val_loss: 0.0014 - val_accuracy: 0.9011\n",
            "Epoch 88/1000\n",
            "15/15 [==============================] - 12s 819ms/step - loss: 2.5774e-05 - accuracy: 0.9987 - val_loss: 6.1594e-04 - val_accuracy: 0.9567\n",
            "Epoch 89/1000\n",
            "15/15 [==============================] - 12s 817ms/step - loss: 1.6496e-05 - accuracy: 0.9992 - val_loss: 1.8120e-04 - val_accuracy: 0.9883\n",
            "Epoch 90/1000\n",
            "15/15 [==============================] - 12s 817ms/step - loss: 1.3248e-05 - accuracy: 0.9993 - val_loss: 2.2074e-04 - val_accuracy: 0.9850\n",
            "Epoch 91/1000\n",
            "15/15 [==============================] - 12s 816ms/step - loss: 1.1588e-05 - accuracy: 0.9994 - val_loss: 1.8194e-04 - val_accuracy: 0.9878\n",
            "Epoch 92/1000\n",
            "15/15 [==============================] - 12s 815ms/step - loss: 1.0515e-05 - accuracy: 0.9994 - val_loss: 1.2725e-04 - val_accuracy: 0.9928\n",
            "Epoch 93/1000\n",
            "15/15 [==============================] - 12s 818ms/step - loss: 1.0292e-05 - accuracy: 0.9994 - val_loss: 1.3324e-04 - val_accuracy: 0.9917\n",
            "Epoch 94/1000\n",
            "15/15 [==============================] - 12s 819ms/step - loss: 1.5581e-05 - accuracy: 0.9991 - val_loss: 3.5059e-04 - val_accuracy: 0.9772\n",
            "Epoch 95/1000\n",
            "15/15 [==============================] - 12s 819ms/step - loss: 1.1245e-05 - accuracy: 0.9994 - val_loss: 1.7509e-04 - val_accuracy: 0.9872\n",
            "Epoch 96/1000\n",
            "15/15 [==============================] - 12s 817ms/step - loss: 1.0850e-05 - accuracy: 0.9994 - val_loss: 1.2960e-04 - val_accuracy: 0.9922\n",
            "Epoch 97/1000\n",
            "15/15 [==============================] - 12s 817ms/step - loss: 1.1421e-05 - accuracy: 0.9994 - val_loss: 1.3091e-04 - val_accuracy: 0.9917\n",
            "Epoch 98/1000\n",
            "15/15 [==============================] - 12s 816ms/step - loss: 1.0402e-05 - accuracy: 0.9994 - val_loss: 1.1973e-04 - val_accuracy: 0.9922\n",
            "Epoch 99/1000\n",
            "15/15 [==============================] - 12s 817ms/step - loss: 1.0475e-05 - accuracy: 0.9994 - val_loss: 1.0575e-04 - val_accuracy: 0.9933\n",
            "Epoch 100/1000\n",
            "15/15 [==============================] - 12s 817ms/step - loss: 9.9026e-06 - accuracy: 0.9994 - val_loss: 1.0477e-04 - val_accuracy: 0.9922\n",
            "Epoch 101/1000\n",
            "15/15 [==============================] - 12s 817ms/step - loss: 9.9990e-06 - accuracy: 0.9994 - val_loss: 1.0183e-04 - val_accuracy: 0.9939\n",
            "Epoch 102/1000\n",
            "15/15 [==============================] - 12s 817ms/step - loss: 8.8793e-06 - accuracy: 0.9995 - val_loss: 1.1879e-04 - val_accuracy: 0.9911\n",
            "Epoch 103/1000\n",
            "15/15 [==============================] - 12s 816ms/step - loss: 8.8469e-06 - accuracy: 0.9995 - val_loss: 1.2547e-04 - val_accuracy: 0.9917\n",
            "Epoch 104/1000\n",
            "15/15 [==============================] - 12s 817ms/step - loss: 8.8375e-06 - accuracy: 0.9995 - val_loss: 1.2846e-04 - val_accuracy: 0.9911\n",
            "Epoch 105/1000\n",
            "15/15 [==============================] - 12s 816ms/step - loss: 8.7404e-06 - accuracy: 0.9994 - val_loss: 1.0797e-04 - val_accuracy: 0.9933\n",
            "Epoch 106/1000\n",
            "15/15 [==============================] - 12s 818ms/step - loss: 8.8577e-06 - accuracy: 0.9995 - val_loss: 1.1864e-04 - val_accuracy: 0.9928\n",
            "Epoch 107/1000\n",
            "15/15 [==============================] - 12s 817ms/step - loss: 1.0246e-05 - accuracy: 0.9994 - val_loss: 1.5620e-04 - val_accuracy: 0.9911\n",
            "Epoch 108/1000\n",
            "15/15 [==============================] - 12s 818ms/step - loss: 9.5108e-06 - accuracy: 0.9995 - val_loss: 1.4450e-04 - val_accuracy: 0.9900\n",
            "Epoch 109/1000\n",
            "15/15 [==============================] - 12s 818ms/step - loss: 8.4782e-06 - accuracy: 0.9995 - val_loss: 1.3394e-04 - val_accuracy: 0.9900\n",
            "Epoch 110/1000\n",
            "15/15 [==============================] - 12s 818ms/step - loss: 1.2069e-05 - accuracy: 0.9993 - val_loss: 1.7237e-04 - val_accuracy: 0.9900\n",
            "Epoch 111/1000\n",
            "15/15 [==============================] - 12s 818ms/step - loss: 1.0925e-05 - accuracy: 0.9994 - val_loss: 1.9619e-04 - val_accuracy: 0.9867\n",
            "Epoch 112/1000\n",
            "15/15 [==============================] - 12s 818ms/step - loss: 1.0560e-05 - accuracy: 0.9994 - val_loss: 1.3431e-04 - val_accuracy: 0.9917\n",
            "Epoch 113/1000\n",
            "15/15 [==============================] - 12s 817ms/step - loss: 9.4939e-06 - accuracy: 0.9994 - val_loss: 1.4649e-04 - val_accuracy: 0.9906\n",
            "Epoch 114/1000\n",
            "15/15 [==============================] - 12s 818ms/step - loss: 9.3307e-06 - accuracy: 0.9994 - val_loss: 1.3553e-04 - val_accuracy: 0.9906\n",
            "Epoch 115/1000\n",
            "15/15 [==============================] - 12s 818ms/step - loss: 9.0081e-06 - accuracy: 0.9994 - val_loss: 1.1322e-04 - val_accuracy: 0.9917\n",
            "Epoch 116/1000\n",
            "15/15 [==============================] - 12s 818ms/step - loss: 9.3818e-06 - accuracy: 0.9994 - val_loss: 1.0069e-04 - val_accuracy: 0.9928\n",
            "Epoch 117/1000\n",
            "15/15 [==============================] - 12s 819ms/step - loss: 8.8283e-06 - accuracy: 0.9994 - val_loss: 9.9162e-05 - val_accuracy: 0.9928\n",
            "Epoch 118/1000\n",
            "15/15 [==============================] - 12s 818ms/step - loss: 8.8270e-06 - accuracy: 0.9994 - val_loss: 1.0105e-04 - val_accuracy: 0.9928\n",
            "Epoch 119/1000\n",
            "15/15 [==============================] - 12s 819ms/step - loss: 8.7006e-06 - accuracy: 0.9994 - val_loss: 1.0639e-04 - val_accuracy: 0.9917\n",
            "Epoch 120/1000\n",
            "15/15 [==============================] - 12s 820ms/step - loss: 8.8695e-06 - accuracy: 0.9994 - val_loss: 1.0947e-04 - val_accuracy: 0.9917\n",
            "Epoch 121/1000\n",
            "15/15 [==============================] - 12s 819ms/step - loss: 9.2539e-06 - accuracy: 0.9994 - val_loss: 1.2686e-04 - val_accuracy: 0.9906\n",
            "Epoch 122/1000\n",
            "15/15 [==============================] - 12s 817ms/step - loss: 8.9991e-06 - accuracy: 0.9994 - val_loss: 1.1971e-04 - val_accuracy: 0.9906\n",
            "Epoch 123/1000\n",
            "15/15 [==============================] - 12s 817ms/step - loss: 8.9951e-06 - accuracy: 0.9994 - val_loss: 1.1671e-04 - val_accuracy: 0.9933\n",
            "Epoch 124/1000\n",
            "15/15 [==============================] - 12s 818ms/step - loss: 8.9346e-06 - accuracy: 0.9994 - val_loss: 1.3336e-04 - val_accuracy: 0.9917\n",
            "Epoch 125/1000\n",
            "15/15 [==============================] - 12s 819ms/step - loss: 8.8787e-06 - accuracy: 0.9994 - val_loss: 1.1363e-04 - val_accuracy: 0.9928\n",
            "Epoch 126/1000\n",
            "15/15 [==============================] - 12s 819ms/step - loss: 9.0190e-06 - accuracy: 0.9994 - val_loss: 1.2185e-04 - val_accuracy: 0.9894\n",
            "Epoch 127/1000\n",
            "15/15 [==============================] - 12s 818ms/step - loss: 8.8063e-06 - accuracy: 0.9994 - val_loss: 1.1043e-04 - val_accuracy: 0.9911\n",
            "Epoch 128/1000\n",
            "15/15 [==============================] - 12s 816ms/step - loss: 9.5609e-06 - accuracy: 0.9994 - val_loss: 1.0154e-04 - val_accuracy: 0.9944\n",
            "Epoch 129/1000\n",
            "15/15 [==============================] - 12s 814ms/step - loss: 8.6971e-06 - accuracy: 0.9994 - val_loss: 1.0268e-04 - val_accuracy: 0.9944\n",
            "Epoch 130/1000\n",
            "15/15 [==============================] - 12s 815ms/step - loss: 8.8257e-06 - accuracy: 0.9994 - val_loss: 1.0690e-04 - val_accuracy: 0.9928\n",
            "Epoch 131/1000\n",
            "15/15 [==============================] - 12s 816ms/step - loss: 8.8100e-06 - accuracy: 0.9994 - val_loss: 1.2149e-04 - val_accuracy: 0.9911\n",
            "Epoch 132/1000\n",
            "15/15 [==============================] - 12s 817ms/step - loss: 1.1493e-05 - accuracy: 0.9992 - val_loss: 3.5156e-04 - val_accuracy: 0.9756\n",
            "Epoch 133/1000\n",
            "15/15 [==============================] - 12s 818ms/step - loss: 9.4795e-06 - accuracy: 0.9994 - val_loss: 1.7557e-04 - val_accuracy: 0.9878\n",
            "Epoch 134/1000\n",
            "15/15 [==============================] - 12s 818ms/step - loss: 8.9514e-06 - accuracy: 0.9994 - val_loss: 1.3788e-04 - val_accuracy: 0.9889\n",
            "Epoch 135/1000\n",
            "15/15 [==============================] - 12s 818ms/step - loss: 8.9733e-06 - accuracy: 0.9994 - val_loss: 1.1446e-04 - val_accuracy: 0.9922\n",
            "Epoch 136/1000\n",
            "15/15 [==============================] - 12s 818ms/step - loss: 8.7861e-06 - accuracy: 0.9994 - val_loss: 1.1278e-04 - val_accuracy: 0.9928\n",
            "Epoch 137/1000\n",
            "15/15 [==============================] - 12s 819ms/step - loss: 1.0705e-05 - accuracy: 0.9993 - val_loss: 2.9791e-04 - val_accuracy: 0.9817\n",
            "Epoch 138/1000\n",
            "15/15 [==============================] - 12s 818ms/step - loss: 9.5488e-06 - accuracy: 0.9994 - val_loss: 1.5001e-04 - val_accuracy: 0.9894\n",
            "Epoch 139/1000\n",
            "15/15 [==============================] - 12s 818ms/step - loss: 9.2935e-06 - accuracy: 0.9994 - val_loss: 1.6311e-04 - val_accuracy: 0.9878\n",
            "Epoch 140/1000\n",
            "15/15 [==============================] - 12s 817ms/step - loss: 8.9219e-06 - accuracy: 0.9994 - val_loss: 1.6264e-04 - val_accuracy: 0.9883\n",
            "Epoch 141/1000\n",
            " 6/15 [===========>..................] - ETA: 7s - loss: 1.2153e-05 - accuracy: 0.9992"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "id": "HVvegAfYQjT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('cnn_bandpass_2s.h5')"
      ],
      "metadata": {
        "id": "QUI82zYxY17r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 그래프 분석"
      ],
      "metadata": {
        "id": "uLvm3XtMKYKg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history=history.history\n",
        "epochs = range(1, len(history['loss']) + 1)\n",
        "acc = history['accuracy']\n",
        "loss = history['loss']\n",
        "val_acc = history['val_accuracy']\n",
        "val_loss = history['val_loss']\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.plot(epochs, acc, label='accuracy')\n",
        "plt.plot(epochs, val_acc, label='val_acc')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.plot(epochs, loss, label='loss', color='g')\n",
        "plt.plot(epochs, val_loss, label='val_loss', color='r')\n",
        "plt.legend()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 701
        },
        "id": "7YRFsECl7Qp0",
        "outputId": "e26ade9a-295a-409d-f83d-cfed635b150b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fd57c0320d0>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 예측해보기(predict)"
      ],
      "metadata": {
        "id": "B7id_mnNKhHg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKnoTLlLBPPN",
        "outputId": "8e06685b-4c91-4d7b-804a-f36f83b0daf8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "188/188 [==============================] - 2s 9ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[44,  0,  0, ...,  0,  0,  0],\n",
              "       [ 0, 57,  0, ...,  0,  0,  0],\n",
              "       [ 0,  0, 65, ...,  0,  0,  0],\n",
              "       ...,\n",
              "       [ 0,  0,  0, ..., 68,  0,  0],\n",
              "       [ 0,  0,  0, ...,  0, 67,  0],\n",
              "       [ 0,  0,  0, ...,  0,  0, 57]])"
            ]
          },
          "metadata": {},
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_p=pd.DataFrame(y_test)\n",
        "\n",
        "for i in range(1,101):\n",
        "  y_test_p.loc[y_test_p[i] == 1.0 , 'y_test'] = i  \n",
        "y_test_p.y_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8EWDyMdHpWR",
        "outputId": "348ddc8c-b4b0-4b45-9b9b-a83b87cbce28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       59.0\n",
              "1       68.0\n",
              "2       72.0\n",
              "3        2.0\n",
              "4       57.0\n",
              "        ... \n",
              "5995     3.0\n",
              "5996    87.0\n",
              "5997    40.0\n",
              "5998    68.0\n",
              "5999    37.0\n",
              "Name: y_test, Length: 6000, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ch1 = pd.DataFrame(np.rint(y_pred))\n",
        "# for i in range(1,101):\n",
        "#   ch1.loc[ch1[i] == 1.0 , 'pred'] = i \n",
        "\n",
        " \n",
        "#predict = pd.concat([data_test,ch1['pred'], y_test_p['y_test']],axis = 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "U6a3cpwDCkHl",
        "outputId": "35e79746-e41a-4713-b942-733f497e3b6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        0    1    2    3    4    5    6    7    8    9  ...   92   93   94  \\\n",
              "0     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
              "1     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
              "2     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
              "3     0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
              "4     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
              "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
              "5995  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
              "5996  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
              "5997  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
              "5998  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
              "5999  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
              "\n",
              "       95   96   97   98   99  100  pred  \n",
              "0     0.0  0.0  0.0  0.0  0.0  0.0  59.0  \n",
              "1     0.0  0.0  0.0  0.0  0.0  0.0  68.0  \n",
              "2     0.0  0.0  0.0  0.0  0.0  0.0  72.0  \n",
              "3     0.0  0.0  0.0  0.0  0.0  0.0   2.0  \n",
              "4     0.0  0.0  0.0  0.0  0.0  0.0  57.0  \n",
              "...   ...  ...  ...  ...  ...  ...   ...  \n",
              "5995  0.0  0.0  0.0  0.0  0.0  0.0   3.0  \n",
              "5996  0.0  0.0  0.0  0.0  0.0  0.0  87.0  \n",
              "5997  0.0  0.0  0.0  0.0  0.0  0.0  40.0  \n",
              "5998  0.0  0.0  0.0  0.0  0.0  0.0  68.0  \n",
              "5999  0.0  0.0  0.0  0.0  0.0  0.0  45.0  \n",
              "\n",
              "[6000 rows x 102 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d66cc86c-ae23-4a9f-8fb4-dbad40e0c5ea\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "      <th>100</th>\n",
              "      <th>pred</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>59.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>68.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>72.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>57.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5995</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5996</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>87.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5997</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5998</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>68.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5999</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>45.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6000 rows × 102 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d66cc86c-ae23-4a9f-8fb4-dbad40e0c5ea')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d66cc86c-ae23-4a9f-8fb4-dbad40e0c5ea button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d66cc86c-ae23-4a9f-8fb4-dbad40e0c5ea');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ex=np.argmax(y_pred, axis=1)\n",
        "ex=ex.astype('float64')\n",
        "ex=pd.DataFrame(ex)\n",
        "ex"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "lI3myxchcv-Q",
        "outputId": "22f2ac13-d7ce-409c-db36-bf92c9ca0cad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         0\n",
              "0     59.0\n",
              "1     68.0\n",
              "2     72.0\n",
              "3      2.0\n",
              "4     57.0\n",
              "...    ...\n",
              "5995   3.0\n",
              "5996  87.0\n",
              "5997  40.0\n",
              "5998  68.0\n",
              "5999  45.0\n",
              "\n",
              "[6000 rows x 1 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-33a2dfcf-6ed7-497f-9b45-70f7cc7db973\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>59.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>68.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>72.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>57.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5995</th>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5996</th>\n",
              "      <td>87.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5997</th>\n",
              "      <td>40.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5998</th>\n",
              "      <td>68.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5999</th>\n",
              "      <td>45.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6000 rows × 1 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-33a2dfcf-6ed7-497f-9b45-70f7cc7db973')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-33a2dfcf-6ed7-497f-9b45-70f7cc7db973 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-33a2dfcf-6ed7-497f-9b45-70f7cc7db973');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ch1 = pd.DataFrame(np.rint(y_pred))\n",
        "ch1['pred'] = ex\n",
        "data_test.reset_index(inplace=True, drop=True)\n",
        "predict = pd.concat([data_test,ch1['pred'], y_test_p['y_test']],axis = 1)"
      ],
      "metadata": {
        "id": "HSPb21Y4eMAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict['compare'] = np.where(predict['pred']==predict['y_test'],'yes', 'no')"
      ],
      "metadata": {
        "id": "yhe54j6XJzhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_1 = predict[predict[\"compare\"]=='no']\n",
        "predict_1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "mdgfuEehKIXX",
        "outputId": "833df983-0f21-460a-de46-668f491d18e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             0         1         2         3         4         5         6  \\\n",
              "30    0.001909  0.008317  0.014489  0.020412  0.025995  0.031137  0.035801   \n",
              "46    0.096114  0.095362  0.095047  0.095447  0.096676  0.098677  0.101247   \n",
              "54   -0.041972 -0.048778 -0.055266 -0.061636 -0.067728 -0.073065 -0.076978   \n",
              "55    0.050686  0.056807  0.061922  0.066330  0.070463  0.074831  0.079964   \n",
              "71   -0.511413 -0.507140 -0.469849 -0.408462 -0.333769 -0.256404 -0.185178   \n",
              "...        ...       ...       ...       ...       ...       ...       ...   \n",
              "5971  0.005315 -0.043084 -0.083200 -0.110473 -0.122973 -0.121490 -0.109050   \n",
              "5981 -0.005281 -0.018280 -0.031669 -0.042939 -0.050464 -0.053648 -0.052817   \n",
              "5992 -0.084734 -0.091664 -0.098196 -0.101411 -0.098148 -0.085744 -0.062829   \n",
              "5994  0.008461  0.011468  0.012707  0.012096  0.009875  0.006558  0.002841   \n",
              "5999  0.052145  0.038833  0.024907  0.010402 -0.004429 -0.019087 -0.032867   \n",
              "\n",
              "             7         8         9  ...       249       250       251  \\\n",
              "30    0.040075  0.044195  0.048538  ...  0.044631  0.050399  0.057432   \n",
              "46    0.104083  0.106844  0.109202  ... -0.004398 -0.008214 -0.013081   \n",
              "54   -0.078796 -0.078050 -0.074630  ... -0.102289 -0.108650 -0.114708   \n",
              "55    0.086349  0.094365  0.104239  ...  0.125449  0.136371  0.147691   \n",
              "71   -0.126038 -0.081724 -0.052058  ...  0.031533  0.034874  0.038518   \n",
              "...        ...       ...       ...  ...       ...       ...       ...   \n",
              "5971 -0.090022 -0.069069 -0.050204  ... -0.027012 -0.029614 -0.031356   \n",
              "5981 -0.048958 -0.043371 -0.037307  ... -0.016826 -0.013496 -0.011275   \n",
              "5992 -0.030004  0.009808  0.051498  ...  0.072150  0.081214  0.090895   \n",
              "5994 -0.000513 -0.002782 -0.003375  ... -0.048723 -0.051678 -0.052151   \n",
              "5999 -0.044991 -0.054774 -0.061789  ... -0.056943 -0.054300 -0.054324   \n",
              "\n",
              "           252       253       254       255   pred  y_test  compare  \n",
              "30    0.066046  0.076371  0.088327  0.101648   85.0    36.0       no  \n",
              "46   -0.019006 -0.025674 -0.032320 -0.037693   83.0    80.0       no  \n",
              "54   -0.119718 -0.122885 -0.123496 -0.121095   20.0    71.0       no  \n",
              "55    0.159697  0.172642  0.186692  0.201860   41.0    50.0       no  \n",
              "71    0.042180  0.045745  0.049241  0.052764    4.0    93.0       no  \n",
              "...        ...       ...       ...       ...    ...     ...      ...  \n",
              "5971 -0.033020 -0.035244 -0.038329 -0.042129   95.0    23.0       no  \n",
              "5981 -0.010594 -0.011830 -0.015208 -0.020697   52.0    40.0       no  \n",
              "5992  0.101388  0.112932  0.125713  0.139775  100.0    83.0       no  \n",
              "5994 -0.050205 -0.046318 -0.041326 -0.036264   95.0    49.0       no  \n",
              "5999 -0.058068 -0.065599 -0.075635 -0.085448   45.0    37.0       no  \n",
              "\n",
              "[545 rows x 259 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-22dffb56-3200-49eb-89c1-97f537806f07\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>249</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "      <th>pred</th>\n",
              "      <th>y_test</th>\n",
              "      <th>compare</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.001909</td>\n",
              "      <td>0.008317</td>\n",
              "      <td>0.014489</td>\n",
              "      <td>0.020412</td>\n",
              "      <td>0.025995</td>\n",
              "      <td>0.031137</td>\n",
              "      <td>0.035801</td>\n",
              "      <td>0.040075</td>\n",
              "      <td>0.044195</td>\n",
              "      <td>0.048538</td>\n",
              "      <td>...</td>\n",
              "      <td>0.044631</td>\n",
              "      <td>0.050399</td>\n",
              "      <td>0.057432</td>\n",
              "      <td>0.066046</td>\n",
              "      <td>0.076371</td>\n",
              "      <td>0.088327</td>\n",
              "      <td>0.101648</td>\n",
              "      <td>85.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>0.096114</td>\n",
              "      <td>0.095362</td>\n",
              "      <td>0.095047</td>\n",
              "      <td>0.095447</td>\n",
              "      <td>0.096676</td>\n",
              "      <td>0.098677</td>\n",
              "      <td>0.101247</td>\n",
              "      <td>0.104083</td>\n",
              "      <td>0.106844</td>\n",
              "      <td>0.109202</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.004398</td>\n",
              "      <td>-0.008214</td>\n",
              "      <td>-0.013081</td>\n",
              "      <td>-0.019006</td>\n",
              "      <td>-0.025674</td>\n",
              "      <td>-0.032320</td>\n",
              "      <td>-0.037693</td>\n",
              "      <td>83.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>-0.041972</td>\n",
              "      <td>-0.048778</td>\n",
              "      <td>-0.055266</td>\n",
              "      <td>-0.061636</td>\n",
              "      <td>-0.067728</td>\n",
              "      <td>-0.073065</td>\n",
              "      <td>-0.076978</td>\n",
              "      <td>-0.078796</td>\n",
              "      <td>-0.078050</td>\n",
              "      <td>-0.074630</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.102289</td>\n",
              "      <td>-0.108650</td>\n",
              "      <td>-0.114708</td>\n",
              "      <td>-0.119718</td>\n",
              "      <td>-0.122885</td>\n",
              "      <td>-0.123496</td>\n",
              "      <td>-0.121095</td>\n",
              "      <td>20.0</td>\n",
              "      <td>71.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>0.050686</td>\n",
              "      <td>0.056807</td>\n",
              "      <td>0.061922</td>\n",
              "      <td>0.066330</td>\n",
              "      <td>0.070463</td>\n",
              "      <td>0.074831</td>\n",
              "      <td>0.079964</td>\n",
              "      <td>0.086349</td>\n",
              "      <td>0.094365</td>\n",
              "      <td>0.104239</td>\n",
              "      <td>...</td>\n",
              "      <td>0.125449</td>\n",
              "      <td>0.136371</td>\n",
              "      <td>0.147691</td>\n",
              "      <td>0.159697</td>\n",
              "      <td>0.172642</td>\n",
              "      <td>0.186692</td>\n",
              "      <td>0.201860</td>\n",
              "      <td>41.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>-0.511413</td>\n",
              "      <td>-0.507140</td>\n",
              "      <td>-0.469849</td>\n",
              "      <td>-0.408462</td>\n",
              "      <td>-0.333769</td>\n",
              "      <td>-0.256404</td>\n",
              "      <td>-0.185178</td>\n",
              "      <td>-0.126038</td>\n",
              "      <td>-0.081724</td>\n",
              "      <td>-0.052058</td>\n",
              "      <td>...</td>\n",
              "      <td>0.031533</td>\n",
              "      <td>0.034874</td>\n",
              "      <td>0.038518</td>\n",
              "      <td>0.042180</td>\n",
              "      <td>0.045745</td>\n",
              "      <td>0.049241</td>\n",
              "      <td>0.052764</td>\n",
              "      <td>4.0</td>\n",
              "      <td>93.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5971</th>\n",
              "      <td>0.005315</td>\n",
              "      <td>-0.043084</td>\n",
              "      <td>-0.083200</td>\n",
              "      <td>-0.110473</td>\n",
              "      <td>-0.122973</td>\n",
              "      <td>-0.121490</td>\n",
              "      <td>-0.109050</td>\n",
              "      <td>-0.090022</td>\n",
              "      <td>-0.069069</td>\n",
              "      <td>-0.050204</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.027012</td>\n",
              "      <td>-0.029614</td>\n",
              "      <td>-0.031356</td>\n",
              "      <td>-0.033020</td>\n",
              "      <td>-0.035244</td>\n",
              "      <td>-0.038329</td>\n",
              "      <td>-0.042129</td>\n",
              "      <td>95.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5981</th>\n",
              "      <td>-0.005281</td>\n",
              "      <td>-0.018280</td>\n",
              "      <td>-0.031669</td>\n",
              "      <td>-0.042939</td>\n",
              "      <td>-0.050464</td>\n",
              "      <td>-0.053648</td>\n",
              "      <td>-0.052817</td>\n",
              "      <td>-0.048958</td>\n",
              "      <td>-0.043371</td>\n",
              "      <td>-0.037307</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.016826</td>\n",
              "      <td>-0.013496</td>\n",
              "      <td>-0.011275</td>\n",
              "      <td>-0.010594</td>\n",
              "      <td>-0.011830</td>\n",
              "      <td>-0.015208</td>\n",
              "      <td>-0.020697</td>\n",
              "      <td>52.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5992</th>\n",
              "      <td>-0.084734</td>\n",
              "      <td>-0.091664</td>\n",
              "      <td>-0.098196</td>\n",
              "      <td>-0.101411</td>\n",
              "      <td>-0.098148</td>\n",
              "      <td>-0.085744</td>\n",
              "      <td>-0.062829</td>\n",
              "      <td>-0.030004</td>\n",
              "      <td>0.009808</td>\n",
              "      <td>0.051498</td>\n",
              "      <td>...</td>\n",
              "      <td>0.072150</td>\n",
              "      <td>0.081214</td>\n",
              "      <td>0.090895</td>\n",
              "      <td>0.101388</td>\n",
              "      <td>0.112932</td>\n",
              "      <td>0.125713</td>\n",
              "      <td>0.139775</td>\n",
              "      <td>100.0</td>\n",
              "      <td>83.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5994</th>\n",
              "      <td>0.008461</td>\n",
              "      <td>0.011468</td>\n",
              "      <td>0.012707</td>\n",
              "      <td>0.012096</td>\n",
              "      <td>0.009875</td>\n",
              "      <td>0.006558</td>\n",
              "      <td>0.002841</td>\n",
              "      <td>-0.000513</td>\n",
              "      <td>-0.002782</td>\n",
              "      <td>-0.003375</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.048723</td>\n",
              "      <td>-0.051678</td>\n",
              "      <td>-0.052151</td>\n",
              "      <td>-0.050205</td>\n",
              "      <td>-0.046318</td>\n",
              "      <td>-0.041326</td>\n",
              "      <td>-0.036264</td>\n",
              "      <td>95.0</td>\n",
              "      <td>49.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5999</th>\n",
              "      <td>0.052145</td>\n",
              "      <td>0.038833</td>\n",
              "      <td>0.024907</td>\n",
              "      <td>0.010402</td>\n",
              "      <td>-0.004429</td>\n",
              "      <td>-0.019087</td>\n",
              "      <td>-0.032867</td>\n",
              "      <td>-0.044991</td>\n",
              "      <td>-0.054774</td>\n",
              "      <td>-0.061789</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.056943</td>\n",
              "      <td>-0.054300</td>\n",
              "      <td>-0.054324</td>\n",
              "      <td>-0.058068</td>\n",
              "      <td>-0.065599</td>\n",
              "      <td>-0.075635</td>\n",
              "      <td>-0.085448</td>\n",
              "      <td>45.0</td>\n",
              "      <td>37.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>545 rows × 259 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-22dffb56-3200-49eb-89c1-97f537806f07')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-22dffb56-3200-49eb-89c1-97f537806f07 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-22dffb56-3200-49eb-89c1-97f537806f07');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 163
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.highpass"
      ],
      "metadata": {
        "id": "Lghovm--XQw8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cqgt0YGaXQw9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import itertools\n",
        "import time\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import (CosineAnnealingLR,\n",
        "                                      CosineAnnealingWarmRestarts,\n",
        "                                      StepLR,\n",
        "                                      ExponentialLR)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import accuracy_score, auc, f1_score, precision_score, recall_score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XasqFkvNXQw9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm, re, sys\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "from keras.layers import Conv2D, Conv1D, MaxPooling2D, MaxPooling1D, Flatten, BatchNormalization, Dense\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.callbacks import CSVLogger, ModelCheckpoint"
      ],
      "metadata": {
        "id": "QkszYRwkXQw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_total = pd.read_csv('/content/drive/MyDrive/model_2s/total_band.csv', header=None)"
      ],
      "metadata": {
        "id": "a7yxW8Q8XQw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_total"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "outputId": "663239a1-1b07-4904-c8e1-952a25f4da50",
        "id": "4i2gD7zKXQw9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            0         1         2         3         4         5         6    \\\n",
              "0     -0.031820 -0.015874 -0.001172  0.011249  0.020713  0.026974  0.030190   \n",
              "1     -0.057454 -0.056822 -0.055984 -0.054985 -0.053945 -0.052999 -0.052250   \n",
              "2     -0.054004 -0.050825 -0.048322 -0.046824 -0.046532 -0.047445 -0.049330   \n",
              "3      0.053295  0.053439  0.055066  0.059409  0.067278  0.078961  0.094216   \n",
              "4      0.134007  0.088232  0.042803 -0.000932 -0.041686 -0.078336 -0.110032   \n",
              "...         ...       ...       ...       ...       ...       ...       ...   \n",
              "17995 -0.087605 -0.069234 -0.058444 -0.053914 -0.053427 -0.054525 -0.055053   \n",
              "17996 -0.033347 -0.035287 -0.036169 -0.036014 -0.035070 -0.033744 -0.032503   \n",
              "17997 -0.076867 -0.083002 -0.088128 -0.091454 -0.092582 -0.091623 -0.089189   \n",
              "17998 -0.079288 -0.083358 -0.086464 -0.087995 -0.087513 -0.084880 -0.080336   \n",
              "17999 -0.041533 -0.041870 -0.042836 -0.044782 -0.047871 -0.052004 -0.056828   \n",
              "\n",
              "            7         8         9    ...       503       504       505  \\\n",
              "0      0.030831  0.029559  0.027102  ... -0.072854 -0.070401 -0.067539   \n",
              "1     -0.051733 -0.051414 -0.051213  ... -0.071806 -0.071741 -0.071302   \n",
              "2     -0.051735 -0.054058 -0.055645  ...  0.009824  0.013030  0.016239   \n",
              "3      0.112361  0.132422  0.153321  ...  0.388900  0.382609  0.370062   \n",
              "4     -0.136271 -0.156943 -0.172313  ... -0.113220 -0.141579 -0.164040   \n",
              "...         ...       ...       ...  ...       ...       ...       ...   \n",
              "17995 -0.053525 -0.049262 -0.042344  ... -0.020100 -0.018824 -0.018132   \n",
              "17996 -0.031760 -0.031802 -0.032746  ... -0.060069 -0.061489 -0.061986   \n",
              "17997 -0.086281 -0.084051 -0.083490  ... -0.053679 -0.057318 -0.059515   \n",
              "17998 -0.074488 -0.068211 -0.062467  ... -0.048461 -0.044455 -0.041670   \n",
              "17999 -0.061806 -0.066332 -0.069845  ...  0.136895  0.147996  0.158825   \n",
              "\n",
              "            506       507       508       509       510       511    512  \n",
              "0     -0.064716 -0.062285 -0.060443 -0.059208 -0.058448 -0.057940    1.0  \n",
              "1     -0.070367 -0.068861 -0.066758 -0.064085 -0.060936 -0.057487    1.0  \n",
              "2      0.019380  0.022418  0.025327  0.028062  0.030516  0.032514    1.0  \n",
              "3      0.351265  0.326472  0.296135  0.260876  0.221465  0.178820    1.0  \n",
              "4     -0.180986 -0.193086 -0.201181 -0.206146 -0.208783 -0.209743    1.0  \n",
              "...         ...       ...       ...       ...       ...       ...    ...  \n",
              "17995 -0.018247 -0.019303 -0.021303 -0.024082 -0.027314 -0.030559  100.0  \n",
              "17996 -0.061385 -0.059834 -0.057803 -0.055972 -0.055062 -0.055631  100.0  \n",
              "17997 -0.061043 -0.062554 -0.064528 -0.067243 -0.070758 -0.074902  100.0  \n",
              "17998 -0.040185 -0.039799 -0.040120 -0.040694 -0.041169 -0.041409  100.0  \n",
              "17999  0.169190  0.179084  0.188748  0.198658  0.209439  0.221705  100.0  \n",
              "\n",
              "[18000 rows x 513 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b940805d-3827-4d7f-9e69-c5013a7aab61\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>503</th>\n",
              "      <th>504</th>\n",
              "      <th>505</th>\n",
              "      <th>506</th>\n",
              "      <th>507</th>\n",
              "      <th>508</th>\n",
              "      <th>509</th>\n",
              "      <th>510</th>\n",
              "      <th>511</th>\n",
              "      <th>512</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.031820</td>\n",
              "      <td>-0.015874</td>\n",
              "      <td>-0.001172</td>\n",
              "      <td>0.011249</td>\n",
              "      <td>0.020713</td>\n",
              "      <td>0.026974</td>\n",
              "      <td>0.030190</td>\n",
              "      <td>0.030831</td>\n",
              "      <td>0.029559</td>\n",
              "      <td>0.027102</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.072854</td>\n",
              "      <td>-0.070401</td>\n",
              "      <td>-0.067539</td>\n",
              "      <td>-0.064716</td>\n",
              "      <td>-0.062285</td>\n",
              "      <td>-0.060443</td>\n",
              "      <td>-0.059208</td>\n",
              "      <td>-0.058448</td>\n",
              "      <td>-0.057940</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.057454</td>\n",
              "      <td>-0.056822</td>\n",
              "      <td>-0.055984</td>\n",
              "      <td>-0.054985</td>\n",
              "      <td>-0.053945</td>\n",
              "      <td>-0.052999</td>\n",
              "      <td>-0.052250</td>\n",
              "      <td>-0.051733</td>\n",
              "      <td>-0.051414</td>\n",
              "      <td>-0.051213</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.071806</td>\n",
              "      <td>-0.071741</td>\n",
              "      <td>-0.071302</td>\n",
              "      <td>-0.070367</td>\n",
              "      <td>-0.068861</td>\n",
              "      <td>-0.066758</td>\n",
              "      <td>-0.064085</td>\n",
              "      <td>-0.060936</td>\n",
              "      <td>-0.057487</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.054004</td>\n",
              "      <td>-0.050825</td>\n",
              "      <td>-0.048322</td>\n",
              "      <td>-0.046824</td>\n",
              "      <td>-0.046532</td>\n",
              "      <td>-0.047445</td>\n",
              "      <td>-0.049330</td>\n",
              "      <td>-0.051735</td>\n",
              "      <td>-0.054058</td>\n",
              "      <td>-0.055645</td>\n",
              "      <td>...</td>\n",
              "      <td>0.009824</td>\n",
              "      <td>0.013030</td>\n",
              "      <td>0.016239</td>\n",
              "      <td>0.019380</td>\n",
              "      <td>0.022418</td>\n",
              "      <td>0.025327</td>\n",
              "      <td>0.028062</td>\n",
              "      <td>0.030516</td>\n",
              "      <td>0.032514</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.053295</td>\n",
              "      <td>0.053439</td>\n",
              "      <td>0.055066</td>\n",
              "      <td>0.059409</td>\n",
              "      <td>0.067278</td>\n",
              "      <td>0.078961</td>\n",
              "      <td>0.094216</td>\n",
              "      <td>0.112361</td>\n",
              "      <td>0.132422</td>\n",
              "      <td>0.153321</td>\n",
              "      <td>...</td>\n",
              "      <td>0.388900</td>\n",
              "      <td>0.382609</td>\n",
              "      <td>0.370062</td>\n",
              "      <td>0.351265</td>\n",
              "      <td>0.326472</td>\n",
              "      <td>0.296135</td>\n",
              "      <td>0.260876</td>\n",
              "      <td>0.221465</td>\n",
              "      <td>0.178820</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.134007</td>\n",
              "      <td>0.088232</td>\n",
              "      <td>0.042803</td>\n",
              "      <td>-0.000932</td>\n",
              "      <td>-0.041686</td>\n",
              "      <td>-0.078336</td>\n",
              "      <td>-0.110032</td>\n",
              "      <td>-0.136271</td>\n",
              "      <td>-0.156943</td>\n",
              "      <td>-0.172313</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.113220</td>\n",
              "      <td>-0.141579</td>\n",
              "      <td>-0.164040</td>\n",
              "      <td>-0.180986</td>\n",
              "      <td>-0.193086</td>\n",
              "      <td>-0.201181</td>\n",
              "      <td>-0.206146</td>\n",
              "      <td>-0.208783</td>\n",
              "      <td>-0.209743</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17995</th>\n",
              "      <td>-0.087605</td>\n",
              "      <td>-0.069234</td>\n",
              "      <td>-0.058444</td>\n",
              "      <td>-0.053914</td>\n",
              "      <td>-0.053427</td>\n",
              "      <td>-0.054525</td>\n",
              "      <td>-0.055053</td>\n",
              "      <td>-0.053525</td>\n",
              "      <td>-0.049262</td>\n",
              "      <td>-0.042344</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.020100</td>\n",
              "      <td>-0.018824</td>\n",
              "      <td>-0.018132</td>\n",
              "      <td>-0.018247</td>\n",
              "      <td>-0.019303</td>\n",
              "      <td>-0.021303</td>\n",
              "      <td>-0.024082</td>\n",
              "      <td>-0.027314</td>\n",
              "      <td>-0.030559</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17996</th>\n",
              "      <td>-0.033347</td>\n",
              "      <td>-0.035287</td>\n",
              "      <td>-0.036169</td>\n",
              "      <td>-0.036014</td>\n",
              "      <td>-0.035070</td>\n",
              "      <td>-0.033744</td>\n",
              "      <td>-0.032503</td>\n",
              "      <td>-0.031760</td>\n",
              "      <td>-0.031802</td>\n",
              "      <td>-0.032746</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.060069</td>\n",
              "      <td>-0.061489</td>\n",
              "      <td>-0.061986</td>\n",
              "      <td>-0.061385</td>\n",
              "      <td>-0.059834</td>\n",
              "      <td>-0.057803</td>\n",
              "      <td>-0.055972</td>\n",
              "      <td>-0.055062</td>\n",
              "      <td>-0.055631</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17997</th>\n",
              "      <td>-0.076867</td>\n",
              "      <td>-0.083002</td>\n",
              "      <td>-0.088128</td>\n",
              "      <td>-0.091454</td>\n",
              "      <td>-0.092582</td>\n",
              "      <td>-0.091623</td>\n",
              "      <td>-0.089189</td>\n",
              "      <td>-0.086281</td>\n",
              "      <td>-0.084051</td>\n",
              "      <td>-0.083490</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.053679</td>\n",
              "      <td>-0.057318</td>\n",
              "      <td>-0.059515</td>\n",
              "      <td>-0.061043</td>\n",
              "      <td>-0.062554</td>\n",
              "      <td>-0.064528</td>\n",
              "      <td>-0.067243</td>\n",
              "      <td>-0.070758</td>\n",
              "      <td>-0.074902</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17998</th>\n",
              "      <td>-0.079288</td>\n",
              "      <td>-0.083358</td>\n",
              "      <td>-0.086464</td>\n",
              "      <td>-0.087995</td>\n",
              "      <td>-0.087513</td>\n",
              "      <td>-0.084880</td>\n",
              "      <td>-0.080336</td>\n",
              "      <td>-0.074488</td>\n",
              "      <td>-0.068211</td>\n",
              "      <td>-0.062467</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.048461</td>\n",
              "      <td>-0.044455</td>\n",
              "      <td>-0.041670</td>\n",
              "      <td>-0.040185</td>\n",
              "      <td>-0.039799</td>\n",
              "      <td>-0.040120</td>\n",
              "      <td>-0.040694</td>\n",
              "      <td>-0.041169</td>\n",
              "      <td>-0.041409</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17999</th>\n",
              "      <td>-0.041533</td>\n",
              "      <td>-0.041870</td>\n",
              "      <td>-0.042836</td>\n",
              "      <td>-0.044782</td>\n",
              "      <td>-0.047871</td>\n",
              "      <td>-0.052004</td>\n",
              "      <td>-0.056828</td>\n",
              "      <td>-0.061806</td>\n",
              "      <td>-0.066332</td>\n",
              "      <td>-0.069845</td>\n",
              "      <td>...</td>\n",
              "      <td>0.136895</td>\n",
              "      <td>0.147996</td>\n",
              "      <td>0.158825</td>\n",
              "      <td>0.169190</td>\n",
              "      <td>0.179084</td>\n",
              "      <td>0.188748</td>\n",
              "      <td>0.198658</td>\n",
              "      <td>0.209439</td>\n",
              "      <td>0.221705</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>18000 rows × 513 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b940805d-3827-4d7f-9e69-c5013a7aab61')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b940805d-3827-4d7f-9e69-c5013a7aab61 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b940805d-3827-4d7f-9e69-c5013a7aab61');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터 전체 섞기"
      ],
      "metadata": {
        "id": "Y9JcGPPTXQw-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_total=data_total.sample(frac=1)"
      ],
      "metadata": {
        "id": "rQoeiTSdXQw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train=data_total.iloc[0:14400, :]\n",
        "data_val=data_total.iloc[14400:16200, :]\n",
        "data_test=data_total.iloc[16200:18000,:]"
      ],
      "metadata": {
        "id": "wqjWqLYkXQw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "outputId": "c2fd17df-b966-4239-b14c-c0ba66c4cdb6",
        "id": "voaozdDZXQw-"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            0         1         2         3         4         5         6    \\\n",
              "12271  0.011548  0.008488  0.003629 -0.001850 -0.006585 -0.009308 -0.009111   \n",
              "13301 -0.104636 -0.105467 -0.105061 -0.104101 -0.103201 -0.102838 -0.103294   \n",
              "5332  -0.012883 -0.035286 -0.053552 -0.067500 -0.077205 -0.082961 -0.085256   \n",
              "1880  -0.076507 -0.079781 -0.081961 -0.083390 -0.084599 -0.086177 -0.088622   \n",
              "973   -0.094721 -0.093787 -0.091414 -0.087818 -0.083370 -0.078623 -0.074293   \n",
              "...         ...       ...       ...       ...       ...       ...       ...   \n",
              "7058  -0.084069 -0.052032 -0.027035 -0.011892 -0.006866 -0.010255 -0.019178   \n",
              "3680  -0.044854 -0.041024 -0.038506 -0.037514 -0.038143 -0.040339 -0.043899   \n",
              "16151 -0.037901 -0.043779 -0.049116 -0.053866 -0.058078 -0.061812 -0.065067   \n",
              "4129   0.000499  0.006756  0.012140  0.016098  0.018497  0.019539  0.019597   \n",
              "8631   0.101586  0.096040  0.089434  0.081187  0.071438  0.061105  0.051676   \n",
              "\n",
              "            7         8         9    ...       503       504       505  \\\n",
              "12271 -0.005618  0.000966  0.009941  ... -0.092055 -0.088837 -0.083735   \n",
              "13301 -0.104638 -0.106737 -0.109303  ... -0.434067 -0.397023 -0.336771   \n",
              "5332  -0.084736 -0.082161 -0.078343  ...  0.017269  0.112567  0.214263   \n",
              "1880  -0.092196 -0.096821 -0.102044  ... -0.130756 -0.132104 -0.134025   \n",
              "973   -0.071172 -0.069985 -0.071207  ...  0.006448  0.005379  0.003184   \n",
              "...         ...       ...       ...  ...       ...       ...       ...   \n",
              "7058  -0.030371 -0.040835 -0.048291  ...  0.080720  0.094054  0.106730   \n",
              "3680  -0.048501 -0.053753 -0.059266  ... -0.028767 -0.040588 -0.050093   \n",
              "16151 -0.067736 -0.069620 -0.070469  ... -0.048270 -0.052330 -0.054928   \n",
              "4129   0.019024  0.018025  0.016620  ...  0.106401  0.094250  0.080913   \n",
              "8631   0.044777  0.041656  0.042705  ... -0.090154 -0.091253 -0.093450   \n",
              "\n",
              "            506       507       508       509       510       511   512  \n",
              "12271 -0.079199 -0.078123 -0.083104 -0.095548 -0.114823 -0.137707  69.0  \n",
              "13301 -0.266172 -0.196964 -0.138022 -0.094467 -0.067628 -0.055680  74.0  \n",
              "5332   0.308861  0.382117  0.421823  0.420427  0.376841  0.296926  30.0  \n",
              "1880  -0.136428 -0.139013 -0.141338 -0.142913 -0.143320 -0.142310  11.0  \n",
              "973   -0.000138 -0.004271 -0.008627 -0.012468 -0.015068 -0.015879   6.0  \n",
              "...         ...       ...       ...       ...       ...       ...   ...  \n",
              "7058   0.118500  0.129080  0.138144  0.145345  0.150342  0.152823  40.0  \n",
              "3680  -0.057467 -0.063021 -0.067091 -0.069939 -0.071706 -0.072419  21.0  \n",
              "16151 -0.055776 -0.054865 -0.052461 -0.049072 -0.045366 -0.042066  90.0  \n",
              "4129   0.067193  0.053784  0.041198  0.029706  0.019338  0.009902  23.0  \n",
              "8631  -0.096227 -0.099108 -0.101726 -0.103858 -0.105403 -0.106348  48.0  \n",
              "\n",
              "[14400 rows x 513 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-811b05ca-3eb8-4790-9d03-e90288a8ac06\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>503</th>\n",
              "      <th>504</th>\n",
              "      <th>505</th>\n",
              "      <th>506</th>\n",
              "      <th>507</th>\n",
              "      <th>508</th>\n",
              "      <th>509</th>\n",
              "      <th>510</th>\n",
              "      <th>511</th>\n",
              "      <th>512</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>12271</th>\n",
              "      <td>0.011548</td>\n",
              "      <td>0.008488</td>\n",
              "      <td>0.003629</td>\n",
              "      <td>-0.001850</td>\n",
              "      <td>-0.006585</td>\n",
              "      <td>-0.009308</td>\n",
              "      <td>-0.009111</td>\n",
              "      <td>-0.005618</td>\n",
              "      <td>0.000966</td>\n",
              "      <td>0.009941</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.092055</td>\n",
              "      <td>-0.088837</td>\n",
              "      <td>-0.083735</td>\n",
              "      <td>-0.079199</td>\n",
              "      <td>-0.078123</td>\n",
              "      <td>-0.083104</td>\n",
              "      <td>-0.095548</td>\n",
              "      <td>-0.114823</td>\n",
              "      <td>-0.137707</td>\n",
              "      <td>69.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13301</th>\n",
              "      <td>-0.104636</td>\n",
              "      <td>-0.105467</td>\n",
              "      <td>-0.105061</td>\n",
              "      <td>-0.104101</td>\n",
              "      <td>-0.103201</td>\n",
              "      <td>-0.102838</td>\n",
              "      <td>-0.103294</td>\n",
              "      <td>-0.104638</td>\n",
              "      <td>-0.106737</td>\n",
              "      <td>-0.109303</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.434067</td>\n",
              "      <td>-0.397023</td>\n",
              "      <td>-0.336771</td>\n",
              "      <td>-0.266172</td>\n",
              "      <td>-0.196964</td>\n",
              "      <td>-0.138022</td>\n",
              "      <td>-0.094467</td>\n",
              "      <td>-0.067628</td>\n",
              "      <td>-0.055680</td>\n",
              "      <td>74.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5332</th>\n",
              "      <td>-0.012883</td>\n",
              "      <td>-0.035286</td>\n",
              "      <td>-0.053552</td>\n",
              "      <td>-0.067500</td>\n",
              "      <td>-0.077205</td>\n",
              "      <td>-0.082961</td>\n",
              "      <td>-0.085256</td>\n",
              "      <td>-0.084736</td>\n",
              "      <td>-0.082161</td>\n",
              "      <td>-0.078343</td>\n",
              "      <td>...</td>\n",
              "      <td>0.017269</td>\n",
              "      <td>0.112567</td>\n",
              "      <td>0.214263</td>\n",
              "      <td>0.308861</td>\n",
              "      <td>0.382117</td>\n",
              "      <td>0.421823</td>\n",
              "      <td>0.420427</td>\n",
              "      <td>0.376841</td>\n",
              "      <td>0.296926</td>\n",
              "      <td>30.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1880</th>\n",
              "      <td>-0.076507</td>\n",
              "      <td>-0.079781</td>\n",
              "      <td>-0.081961</td>\n",
              "      <td>-0.083390</td>\n",
              "      <td>-0.084599</td>\n",
              "      <td>-0.086177</td>\n",
              "      <td>-0.088622</td>\n",
              "      <td>-0.092196</td>\n",
              "      <td>-0.096821</td>\n",
              "      <td>-0.102044</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.130756</td>\n",
              "      <td>-0.132104</td>\n",
              "      <td>-0.134025</td>\n",
              "      <td>-0.136428</td>\n",
              "      <td>-0.139013</td>\n",
              "      <td>-0.141338</td>\n",
              "      <td>-0.142913</td>\n",
              "      <td>-0.143320</td>\n",
              "      <td>-0.142310</td>\n",
              "      <td>11.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>973</th>\n",
              "      <td>-0.094721</td>\n",
              "      <td>-0.093787</td>\n",
              "      <td>-0.091414</td>\n",
              "      <td>-0.087818</td>\n",
              "      <td>-0.083370</td>\n",
              "      <td>-0.078623</td>\n",
              "      <td>-0.074293</td>\n",
              "      <td>-0.071172</td>\n",
              "      <td>-0.069985</td>\n",
              "      <td>-0.071207</td>\n",
              "      <td>...</td>\n",
              "      <td>0.006448</td>\n",
              "      <td>0.005379</td>\n",
              "      <td>0.003184</td>\n",
              "      <td>-0.000138</td>\n",
              "      <td>-0.004271</td>\n",
              "      <td>-0.008627</td>\n",
              "      <td>-0.012468</td>\n",
              "      <td>-0.015068</td>\n",
              "      <td>-0.015879</td>\n",
              "      <td>6.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7058</th>\n",
              "      <td>-0.084069</td>\n",
              "      <td>-0.052032</td>\n",
              "      <td>-0.027035</td>\n",
              "      <td>-0.011892</td>\n",
              "      <td>-0.006866</td>\n",
              "      <td>-0.010255</td>\n",
              "      <td>-0.019178</td>\n",
              "      <td>-0.030371</td>\n",
              "      <td>-0.040835</td>\n",
              "      <td>-0.048291</td>\n",
              "      <td>...</td>\n",
              "      <td>0.080720</td>\n",
              "      <td>0.094054</td>\n",
              "      <td>0.106730</td>\n",
              "      <td>0.118500</td>\n",
              "      <td>0.129080</td>\n",
              "      <td>0.138144</td>\n",
              "      <td>0.145345</td>\n",
              "      <td>0.150342</td>\n",
              "      <td>0.152823</td>\n",
              "      <td>40.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3680</th>\n",
              "      <td>-0.044854</td>\n",
              "      <td>-0.041024</td>\n",
              "      <td>-0.038506</td>\n",
              "      <td>-0.037514</td>\n",
              "      <td>-0.038143</td>\n",
              "      <td>-0.040339</td>\n",
              "      <td>-0.043899</td>\n",
              "      <td>-0.048501</td>\n",
              "      <td>-0.053753</td>\n",
              "      <td>-0.059266</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.028767</td>\n",
              "      <td>-0.040588</td>\n",
              "      <td>-0.050093</td>\n",
              "      <td>-0.057467</td>\n",
              "      <td>-0.063021</td>\n",
              "      <td>-0.067091</td>\n",
              "      <td>-0.069939</td>\n",
              "      <td>-0.071706</td>\n",
              "      <td>-0.072419</td>\n",
              "      <td>21.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16151</th>\n",
              "      <td>-0.037901</td>\n",
              "      <td>-0.043779</td>\n",
              "      <td>-0.049116</td>\n",
              "      <td>-0.053866</td>\n",
              "      <td>-0.058078</td>\n",
              "      <td>-0.061812</td>\n",
              "      <td>-0.065067</td>\n",
              "      <td>-0.067736</td>\n",
              "      <td>-0.069620</td>\n",
              "      <td>-0.070469</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.048270</td>\n",
              "      <td>-0.052330</td>\n",
              "      <td>-0.054928</td>\n",
              "      <td>-0.055776</td>\n",
              "      <td>-0.054865</td>\n",
              "      <td>-0.052461</td>\n",
              "      <td>-0.049072</td>\n",
              "      <td>-0.045366</td>\n",
              "      <td>-0.042066</td>\n",
              "      <td>90.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4129</th>\n",
              "      <td>0.000499</td>\n",
              "      <td>0.006756</td>\n",
              "      <td>0.012140</td>\n",
              "      <td>0.016098</td>\n",
              "      <td>0.018497</td>\n",
              "      <td>0.019539</td>\n",
              "      <td>0.019597</td>\n",
              "      <td>0.019024</td>\n",
              "      <td>0.018025</td>\n",
              "      <td>0.016620</td>\n",
              "      <td>...</td>\n",
              "      <td>0.106401</td>\n",
              "      <td>0.094250</td>\n",
              "      <td>0.080913</td>\n",
              "      <td>0.067193</td>\n",
              "      <td>0.053784</td>\n",
              "      <td>0.041198</td>\n",
              "      <td>0.029706</td>\n",
              "      <td>0.019338</td>\n",
              "      <td>0.009902</td>\n",
              "      <td>23.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8631</th>\n",
              "      <td>0.101586</td>\n",
              "      <td>0.096040</td>\n",
              "      <td>0.089434</td>\n",
              "      <td>0.081187</td>\n",
              "      <td>0.071438</td>\n",
              "      <td>0.061105</td>\n",
              "      <td>0.051676</td>\n",
              "      <td>0.044777</td>\n",
              "      <td>0.041656</td>\n",
              "      <td>0.042705</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.090154</td>\n",
              "      <td>-0.091253</td>\n",
              "      <td>-0.093450</td>\n",
              "      <td>-0.096227</td>\n",
              "      <td>-0.099108</td>\n",
              "      <td>-0.101726</td>\n",
              "      <td>-0.103858</td>\n",
              "      <td>-0.105403</td>\n",
              "      <td>-0.106348</td>\n",
              "      <td>48.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>14400 rows × 513 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-811b05ca-3eb8-4790-9d03-e90288a8ac06')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-811b05ca-3eb8-4790-9d03-e90288a8ac06 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-811b05ca-3eb8-4790-9d03-e90288a8ac06');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 전처리"
      ],
      "metadata": {
        "id": "Qynh1gNZXQw-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = to_categorical(data_train[512])\n",
        "y_test = to_categorical(data_test[512])\n",
        "y_val = to_categorical(data_val[512])\n",
        "# y_train = to_categorical(arr)\n",
        "# y_test = to_categorical(arr_t)\n",
        "# y_val = to_categorical(arr_v)\n",
        "y_train[14399]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c45224c3-898a-43f6-d3be-281aa073aef1",
        "id": "XzR-X6g-XQw-"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_train.drop([512], axis=1, inplace=True)\n",
        "data_test.drop([512], axis=1, inplace=True)\n",
        "data_val.drop([512], axis=1, inplace=True)\n",
        "data_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "outputId": "1772220f-1f19-4952-a9ed-4fe2781e3711",
        "id": "PQ1i2-xkXQw-"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py:4906: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  return super().drop(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            0         1         2         3         4         5         6    \\\n",
              "12271  0.011548  0.008488  0.003629 -0.001850 -0.006585 -0.009308 -0.009111   \n",
              "13301 -0.104636 -0.105467 -0.105061 -0.104101 -0.103201 -0.102838 -0.103294   \n",
              "5332  -0.012883 -0.035286 -0.053552 -0.067500 -0.077205 -0.082961 -0.085256   \n",
              "1880  -0.076507 -0.079781 -0.081961 -0.083390 -0.084599 -0.086177 -0.088622   \n",
              "973   -0.094721 -0.093787 -0.091414 -0.087818 -0.083370 -0.078623 -0.074293   \n",
              "...         ...       ...       ...       ...       ...       ...       ...   \n",
              "7058  -0.084069 -0.052032 -0.027035 -0.011892 -0.006866 -0.010255 -0.019178   \n",
              "3680  -0.044854 -0.041024 -0.038506 -0.037514 -0.038143 -0.040339 -0.043899   \n",
              "16151 -0.037901 -0.043779 -0.049116 -0.053866 -0.058078 -0.061812 -0.065067   \n",
              "4129   0.000499  0.006756  0.012140  0.016098  0.018497  0.019539  0.019597   \n",
              "8631   0.101586  0.096040  0.089434  0.081187  0.071438  0.061105  0.051676   \n",
              "\n",
              "            7         8         9    ...       502       503       504  \\\n",
              "12271 -0.005618  0.000966  0.009941  ... -0.091861 -0.092055 -0.088837   \n",
              "13301 -0.104638 -0.106737 -0.109303  ... -0.436294 -0.434067 -0.397023   \n",
              "5332  -0.084736 -0.082161 -0.078343  ... -0.061198  0.017269  0.112567   \n",
              "1880  -0.092196 -0.096821 -0.102044  ... -0.129850 -0.130756 -0.132104   \n",
              "973   -0.071172 -0.069985 -0.071207  ...  0.006683  0.006448  0.005379   \n",
              "...         ...       ...       ...  ...       ...       ...       ...   \n",
              "7058  -0.030371 -0.040835 -0.048291  ...  0.066962  0.080720  0.094054   \n",
              "3680  -0.048501 -0.053753 -0.059266  ... -0.014650 -0.028767 -0.040588   \n",
              "16151 -0.067736 -0.069620 -0.070469  ... -0.043265 -0.048270 -0.052330   \n",
              "4129   0.019024  0.018025  0.016620  ...  0.116555  0.106401  0.094250   \n",
              "8631   0.044777  0.041656  0.042705  ... -0.090597 -0.090154 -0.091253   \n",
              "\n",
              "            505       506       507       508       509       510       511  \n",
              "12271 -0.083735 -0.079199 -0.078123 -0.083104 -0.095548 -0.114823 -0.137707  \n",
              "13301 -0.336771 -0.266172 -0.196964 -0.138022 -0.094467 -0.067628 -0.055680  \n",
              "5332   0.214263  0.308861  0.382117  0.421823  0.420427  0.376841  0.296926  \n",
              "1880  -0.134025 -0.136428 -0.139013 -0.141338 -0.142913 -0.143320 -0.142310  \n",
              "973    0.003184 -0.000138 -0.004271 -0.008627 -0.012468 -0.015068 -0.015879  \n",
              "...         ...       ...       ...       ...       ...       ...       ...  \n",
              "7058   0.106730  0.118500  0.129080  0.138144  0.145345  0.150342  0.152823  \n",
              "3680  -0.050093 -0.057467 -0.063021 -0.067091 -0.069939 -0.071706 -0.072419  \n",
              "16151 -0.054928 -0.055776 -0.054865 -0.052461 -0.049072 -0.045366 -0.042066  \n",
              "4129   0.080913  0.067193  0.053784  0.041198  0.029706  0.019338  0.009902  \n",
              "8631  -0.093450 -0.096227 -0.099108 -0.101726 -0.103858 -0.105403 -0.106348  \n",
              "\n",
              "[14400 rows x 512 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ee0b9168-993b-4438-8739-acf2a5eaa756\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>502</th>\n",
              "      <th>503</th>\n",
              "      <th>504</th>\n",
              "      <th>505</th>\n",
              "      <th>506</th>\n",
              "      <th>507</th>\n",
              "      <th>508</th>\n",
              "      <th>509</th>\n",
              "      <th>510</th>\n",
              "      <th>511</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>12271</th>\n",
              "      <td>0.011548</td>\n",
              "      <td>0.008488</td>\n",
              "      <td>0.003629</td>\n",
              "      <td>-0.001850</td>\n",
              "      <td>-0.006585</td>\n",
              "      <td>-0.009308</td>\n",
              "      <td>-0.009111</td>\n",
              "      <td>-0.005618</td>\n",
              "      <td>0.000966</td>\n",
              "      <td>0.009941</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.091861</td>\n",
              "      <td>-0.092055</td>\n",
              "      <td>-0.088837</td>\n",
              "      <td>-0.083735</td>\n",
              "      <td>-0.079199</td>\n",
              "      <td>-0.078123</td>\n",
              "      <td>-0.083104</td>\n",
              "      <td>-0.095548</td>\n",
              "      <td>-0.114823</td>\n",
              "      <td>-0.137707</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13301</th>\n",
              "      <td>-0.104636</td>\n",
              "      <td>-0.105467</td>\n",
              "      <td>-0.105061</td>\n",
              "      <td>-0.104101</td>\n",
              "      <td>-0.103201</td>\n",
              "      <td>-0.102838</td>\n",
              "      <td>-0.103294</td>\n",
              "      <td>-0.104638</td>\n",
              "      <td>-0.106737</td>\n",
              "      <td>-0.109303</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.436294</td>\n",
              "      <td>-0.434067</td>\n",
              "      <td>-0.397023</td>\n",
              "      <td>-0.336771</td>\n",
              "      <td>-0.266172</td>\n",
              "      <td>-0.196964</td>\n",
              "      <td>-0.138022</td>\n",
              "      <td>-0.094467</td>\n",
              "      <td>-0.067628</td>\n",
              "      <td>-0.055680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5332</th>\n",
              "      <td>-0.012883</td>\n",
              "      <td>-0.035286</td>\n",
              "      <td>-0.053552</td>\n",
              "      <td>-0.067500</td>\n",
              "      <td>-0.077205</td>\n",
              "      <td>-0.082961</td>\n",
              "      <td>-0.085256</td>\n",
              "      <td>-0.084736</td>\n",
              "      <td>-0.082161</td>\n",
              "      <td>-0.078343</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.061198</td>\n",
              "      <td>0.017269</td>\n",
              "      <td>0.112567</td>\n",
              "      <td>0.214263</td>\n",
              "      <td>0.308861</td>\n",
              "      <td>0.382117</td>\n",
              "      <td>0.421823</td>\n",
              "      <td>0.420427</td>\n",
              "      <td>0.376841</td>\n",
              "      <td>0.296926</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1880</th>\n",
              "      <td>-0.076507</td>\n",
              "      <td>-0.079781</td>\n",
              "      <td>-0.081961</td>\n",
              "      <td>-0.083390</td>\n",
              "      <td>-0.084599</td>\n",
              "      <td>-0.086177</td>\n",
              "      <td>-0.088622</td>\n",
              "      <td>-0.092196</td>\n",
              "      <td>-0.096821</td>\n",
              "      <td>-0.102044</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.129850</td>\n",
              "      <td>-0.130756</td>\n",
              "      <td>-0.132104</td>\n",
              "      <td>-0.134025</td>\n",
              "      <td>-0.136428</td>\n",
              "      <td>-0.139013</td>\n",
              "      <td>-0.141338</td>\n",
              "      <td>-0.142913</td>\n",
              "      <td>-0.143320</td>\n",
              "      <td>-0.142310</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>973</th>\n",
              "      <td>-0.094721</td>\n",
              "      <td>-0.093787</td>\n",
              "      <td>-0.091414</td>\n",
              "      <td>-0.087818</td>\n",
              "      <td>-0.083370</td>\n",
              "      <td>-0.078623</td>\n",
              "      <td>-0.074293</td>\n",
              "      <td>-0.071172</td>\n",
              "      <td>-0.069985</td>\n",
              "      <td>-0.071207</td>\n",
              "      <td>...</td>\n",
              "      <td>0.006683</td>\n",
              "      <td>0.006448</td>\n",
              "      <td>0.005379</td>\n",
              "      <td>0.003184</td>\n",
              "      <td>-0.000138</td>\n",
              "      <td>-0.004271</td>\n",
              "      <td>-0.008627</td>\n",
              "      <td>-0.012468</td>\n",
              "      <td>-0.015068</td>\n",
              "      <td>-0.015879</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7058</th>\n",
              "      <td>-0.084069</td>\n",
              "      <td>-0.052032</td>\n",
              "      <td>-0.027035</td>\n",
              "      <td>-0.011892</td>\n",
              "      <td>-0.006866</td>\n",
              "      <td>-0.010255</td>\n",
              "      <td>-0.019178</td>\n",
              "      <td>-0.030371</td>\n",
              "      <td>-0.040835</td>\n",
              "      <td>-0.048291</td>\n",
              "      <td>...</td>\n",
              "      <td>0.066962</td>\n",
              "      <td>0.080720</td>\n",
              "      <td>0.094054</td>\n",
              "      <td>0.106730</td>\n",
              "      <td>0.118500</td>\n",
              "      <td>0.129080</td>\n",
              "      <td>0.138144</td>\n",
              "      <td>0.145345</td>\n",
              "      <td>0.150342</td>\n",
              "      <td>0.152823</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3680</th>\n",
              "      <td>-0.044854</td>\n",
              "      <td>-0.041024</td>\n",
              "      <td>-0.038506</td>\n",
              "      <td>-0.037514</td>\n",
              "      <td>-0.038143</td>\n",
              "      <td>-0.040339</td>\n",
              "      <td>-0.043899</td>\n",
              "      <td>-0.048501</td>\n",
              "      <td>-0.053753</td>\n",
              "      <td>-0.059266</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.014650</td>\n",
              "      <td>-0.028767</td>\n",
              "      <td>-0.040588</td>\n",
              "      <td>-0.050093</td>\n",
              "      <td>-0.057467</td>\n",
              "      <td>-0.063021</td>\n",
              "      <td>-0.067091</td>\n",
              "      <td>-0.069939</td>\n",
              "      <td>-0.071706</td>\n",
              "      <td>-0.072419</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16151</th>\n",
              "      <td>-0.037901</td>\n",
              "      <td>-0.043779</td>\n",
              "      <td>-0.049116</td>\n",
              "      <td>-0.053866</td>\n",
              "      <td>-0.058078</td>\n",
              "      <td>-0.061812</td>\n",
              "      <td>-0.065067</td>\n",
              "      <td>-0.067736</td>\n",
              "      <td>-0.069620</td>\n",
              "      <td>-0.070469</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.043265</td>\n",
              "      <td>-0.048270</td>\n",
              "      <td>-0.052330</td>\n",
              "      <td>-0.054928</td>\n",
              "      <td>-0.055776</td>\n",
              "      <td>-0.054865</td>\n",
              "      <td>-0.052461</td>\n",
              "      <td>-0.049072</td>\n",
              "      <td>-0.045366</td>\n",
              "      <td>-0.042066</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4129</th>\n",
              "      <td>0.000499</td>\n",
              "      <td>0.006756</td>\n",
              "      <td>0.012140</td>\n",
              "      <td>0.016098</td>\n",
              "      <td>0.018497</td>\n",
              "      <td>0.019539</td>\n",
              "      <td>0.019597</td>\n",
              "      <td>0.019024</td>\n",
              "      <td>0.018025</td>\n",
              "      <td>0.016620</td>\n",
              "      <td>...</td>\n",
              "      <td>0.116555</td>\n",
              "      <td>0.106401</td>\n",
              "      <td>0.094250</td>\n",
              "      <td>0.080913</td>\n",
              "      <td>0.067193</td>\n",
              "      <td>0.053784</td>\n",
              "      <td>0.041198</td>\n",
              "      <td>0.029706</td>\n",
              "      <td>0.019338</td>\n",
              "      <td>0.009902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8631</th>\n",
              "      <td>0.101586</td>\n",
              "      <td>0.096040</td>\n",
              "      <td>0.089434</td>\n",
              "      <td>0.081187</td>\n",
              "      <td>0.071438</td>\n",
              "      <td>0.061105</td>\n",
              "      <td>0.051676</td>\n",
              "      <td>0.044777</td>\n",
              "      <td>0.041656</td>\n",
              "      <td>0.042705</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.090597</td>\n",
              "      <td>-0.090154</td>\n",
              "      <td>-0.091253</td>\n",
              "      <td>-0.093450</td>\n",
              "      <td>-0.096227</td>\n",
              "      <td>-0.099108</td>\n",
              "      <td>-0.101726</td>\n",
              "      <td>-0.103858</td>\n",
              "      <td>-0.105403</td>\n",
              "      <td>-0.106348</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>14400 rows × 512 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ee0b9168-993b-4438-8739-acf2a5eaa756')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ee0b9168-993b-4438-8739-acf2a5eaa756 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ee0b9168-993b-4438-8739-acf2a5eaa756');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train=data_train.values\n",
        "X_test=data_test.values\n",
        "X_val=data_val.values"
      ],
      "metadata": {
        "id": "I-857ihoXQw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(X_val.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)\n",
        "print(y_val.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8498ecd-05e7-43c3-9e69-93651a362b22",
        "id": "dkVpjofrXQw-"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(14400, 512)\n",
            "(1800, 512)\n",
            "(1800, 512)\n",
            "(14400, 101)\n",
            "(1800, 101)\n",
            "(1800, 101)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# X_train = X_train.reshape(1, 256, 24000)\n",
        "# X_test = X_test.reshape(1, 256, 6000)\n",
        "# X_val = X_val.reshape(1, 256, 6000)\n",
        "X_train = X_train.reshape(14400, 512, 1)\n",
        "X_test = X_test.reshape(1800, 512, 1)\n",
        "X_val = X_val.reshape(1800, 512, 1)\n",
        "X_train.shape, X_test.shape, X_val.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac581ace-45b0-4113-df84-ce4e6ebd95f4",
        "id": "DcourDAvXQw_"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((14400, 512, 1), (1800, 512, 1), (1800, 512, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## cnn-lstm 모델"
      ],
      "metadata": {
        "id": "_S-nvuluXQw_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model():\n",
        "    model = Sequential()\n",
        "    model.add(Conv1D(filters=64, kernel_size=6, activation='relu', \n",
        "                    padding='same', input_shape=(256, 1)))\n",
        "    model.add(BatchNormalization())\n",
        "    \n",
        "    # adding a pooling layer\n",
        "    model.add(MaxPooling1D(pool_size=(3), strides=2, padding='same'))\n",
        "    \n",
        "    model.add(Conv1D(filters=64, kernel_size=6, activation='relu', \n",
        "                    padding='same', input_shape=(256, 1)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling1D(pool_size=(3), strides=2, padding='same'))\n",
        "    \n",
        "    model.add(Conv1D(filters=64, kernel_size=6, activation='relu', \n",
        "                    padding='same', input_shape=(256, 1)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling1D(pool_size=(3), strides=2, padding='same'))\n",
        "    \n",
        "    model.add(Flatten())\n",
        "    #model.add(Dense(64, activation='relu'))\n",
        "    #model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(101, activation='softmax'))\n",
        "    \n",
        "    model.compile(optimizer='adam', loss='mse',metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "_V_p9dqHXQw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model()\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2397946-5a7a-4f1f-e09b-f35229db7ab7",
        "id": "cuCmpuaLXQw_"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_18\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_17 (Conv1D)          (None, 256, 64)           448       \n",
            "                                                                 \n",
            " batch_normalization_12 (Bat  (None, 256, 64)          256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling1d_17 (MaxPoolin  (None, 128, 64)          0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_18 (Conv1D)          (None, 128, 64)           24640     \n",
            "                                                                 \n",
            " batch_normalization_13 (Bat  (None, 128, 64)          256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling1d_18 (MaxPoolin  (None, 64, 64)           0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_19 (Conv1D)          (None, 64, 64)            24640     \n",
            "                                                                 \n",
            " batch_normalization_14 (Bat  (None, 64, 64)           256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling1d_19 (MaxPoolin  (None, 32, 64)           0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " flatten_13 (Flatten)        (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_24 (Dense)            (None, 100)               204900    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 255,396\n",
            "Trainable params: 255,012\n",
            "Non-trainable params: 384\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## cnn-gru 모델"
      ],
      "metadata": {
        "id": "ga30ttYyXQw_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Input, Embedding, GRU, LSTM, MaxPooling1D, GlobalMaxPool1D, CuDNNGRU\n",
        "from keras.layers import Dropout, Dense, Activation, Flatten, Conv1D, SpatialDropout1D\n",
        "from keras.models import Sequential\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv1D(filters=64, kernel_size=6, activation='relu', padding='same', input_shape=(256, 1)))\n",
        "model.add(SpatialDropout1D(0.2))\n",
        "model.add(MaxPooling1D(4))\n",
        "model.add(CuDNNGRU(64))\n",
        "\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Dense(101, activation='softmax'))\n",
        "model.compile(optimizer='adam', loss='mse',metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33f77877-e599-4ded-8400-373f5cec14bc",
        "id": "5dr75HDdXQw_"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_13 (Conv1D)          (None, 256, 64)           448       \n",
            "                                                                 \n",
            " spatial_dropout1d_4 (Spatia  (None, 256, 64)          0         \n",
            " lDropout1D)                                                     \n",
            "                                                                 \n",
            " max_pooling1d_13 (MaxPoolin  (None, 64, 64)           0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " cu_dnngru_4 (CuDNNGRU)      (None, 64)                24960     \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 101)               6565      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 31,973\n",
            "Trainable params: 31,973\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## gru모델"
      ],
      "metadata": {
        "id": "7-upX5eWXQw_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Input, Embedding, GRU, LSTM, SimpleRNN, MaxPooling1D, GlobalMaxPool1D, CuDNNGRU\n",
        "from keras.layers import Dropout, Dense, Activation, Flatten, Conv1D, SpatialDropout1D\n",
        "from keras.models import Sequential\n",
        "\n",
        "model = Sequential()\n",
        "model.add(GRU(units=150, return_sequences=True, input_shape=(512,1))) #shape[1]=열\n",
        "model.add(GRU(units=50, return_sequences=True))\n",
        "model.add(GRU(units=50, return_sequences=True))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(101, activation='softmax'))\n",
        "model.compile(optimizer='adam', loss='mse',metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "473a82ca-7807-4f32-b99b-23a57c8479dc",
        "id": "OTKe1HgUXQw_"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " gru_3 (GRU)                 (None, 512, 150)          68850     \n",
            "                                                                 \n",
            " gru_4 (GRU)                 (None, 512, 50)           30300     \n",
            "                                                                 \n",
            " gru_5 (GRU)                 (None, 512, 50)           15300     \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 25600)             0         \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 25600)             0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 101)               2585701   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,700,151\n",
            "Trainable params: 2,700,151\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSTM"
      ],
      "metadata": {
        "id": "H17aq1x5XQw_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(units=100, return_sequences=True, input_shape=(256,1))) #shape[1]=열\n",
        "model.add(LSTM(units=50, return_sequences=True))\n",
        "model.add(LSTM(units=50, return_sequences=True))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Dense(101, activation='softmax'))\n",
        "model.compile(optimizer='adam', loss='mse',metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8055881e-f18e-4941-e7d3-1d853b39d2e3",
        "id": "-QSzScZpXQw_"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_3 (LSTM)               (None, 256, 100)          40800     \n",
            "                                                                 \n",
            " lstm_4 (LSTM)               (None, 256, 50)           30200     \n",
            "                                                                 \n",
            " lstm_5 (LSTM)               (None, 256, 50)           20200     \n",
            "                                                                 \n",
            " flatten_6 (Flatten)         (None, 12800)             0         \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 12800)             0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 101)               1292901   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,384,101\n",
            "Trainable params: 1,384,101\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RNN모델"
      ],
      "metadata": {
        "id": "-m7ELKxLXQw_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Input, Embedding, GRU, LSTM, SimpleRNN, MaxPooling1D, GlobalMaxPool1D, CuDNNGRU\n",
        "from keras.layers import Dropout, Dense, Activation, Flatten, Conv1D, SpatialDropout1D\n",
        "from keras.models import Sequential\n",
        "\n",
        "model = Sequential()\n",
        "model.add(SimpleRNN(units=100, return_sequences=True, input_shape=(256,1))) #shape[1]=열\n",
        "model.add(SimpleRNN(units=50, return_sequences=True))\n",
        "model.add(SimpleRNN(units=50, return_sequences=True))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(101, activation='softmax'))\n",
        "model.compile(optimizer='adam', loss='mse',metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f56aa04-4c34-4860-fc44-299596d4f96f",
        "id": "NOY-7wrnXQxA"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " simple_rnn_12 (SimpleRNN)   (None, 256, 100)          10200     \n",
            "                                                                 \n",
            " simple_rnn_13 (SimpleRNN)   (None, 256, 50)           7550      \n",
            "                                                                 \n",
            " simple_rnn_14 (SimpleRNN)   (None, 256, 50)           5050      \n",
            "                                                                 \n",
            " flatten_4 (Flatten)         (None, 12800)             0         \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 12800)             0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 101)               1292901   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,315,701\n",
            "Trainable params: 1,315,701\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNN"
      ],
      "metadata": {
        "id": "HunwenumXQxA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model():\n",
        "  model = Sequential()\n",
        "  model.add(Conv1D(256, 8, padding='same', activation='relu', input_shape= (256,1)))\n",
        "  model.add(MaxPooling1D())\n",
        "  model.add(Conv1D(256, 8, padding='same', activation='relu'))\n",
        "  model.add(MaxPooling1D())\n",
        "  model.add(Conv1D(64, 8, padding='same',activation='relu'))\n",
        "  model.add(MaxPooling1D())\n",
        "  model.add(Conv1D(64, 8, padding='same', activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(101, activation='softmax')) # activation='softmax'\n",
        "\n",
        "  model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
        "  model.summary()\n",
        "  return model"
      ],
      "metadata": {
        "id": "5QOAkKeFXQxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델학습/평가"
      ],
      "metadata": {
        "id": "qeo7ft9TXQxA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train, y_train, batch_size =1024, epochs = 400, verbose = 1, validation_data=(X_val, y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bed8c25-51a2-4681-d512-d31ec3ecbf40",
        "id": "fMpys2YKXQxA"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/400\n",
            "15/15 [==============================] - 5s 345ms/step - loss: 3.6500e-04 - accuracy: 0.9764 - val_loss: 0.0059 - val_accuracy: 0.6256\n",
            "Epoch 2/400\n",
            "15/15 [==============================] - 5s 344ms/step - loss: 3.2755e-04 - accuracy: 0.9790 - val_loss: 0.0057 - val_accuracy: 0.6456\n",
            "Epoch 3/400\n",
            "15/15 [==============================] - 5s 346ms/step - loss: 3.2645e-04 - accuracy: 0.9788 - val_loss: 0.0056 - val_accuracy: 0.6489\n",
            "Epoch 4/400\n",
            "15/15 [==============================] - 5s 345ms/step - loss: 3.1191e-04 - accuracy: 0.9799 - val_loss: 0.0058 - val_accuracy: 0.6261\n",
            "Epoch 5/400\n",
            "15/15 [==============================] - 5s 345ms/step - loss: 3.0775e-04 - accuracy: 0.9803 - val_loss: 0.0055 - val_accuracy: 0.6494\n",
            "Epoch 6/400\n",
            "15/15 [==============================] - 5s 347ms/step - loss: 2.9940e-04 - accuracy: 0.9806 - val_loss: 0.0056 - val_accuracy: 0.6417\n",
            "Epoch 7/400\n",
            "15/15 [==============================] - 5s 347ms/step - loss: 3.0320e-04 - accuracy: 0.9806 - val_loss: 0.0055 - val_accuracy: 0.6478\n",
            "Epoch 8/400\n",
            "15/15 [==============================] - 5s 347ms/step - loss: 2.9496e-04 - accuracy: 0.9806 - val_loss: 0.0055 - val_accuracy: 0.6522\n",
            "Epoch 9/400\n",
            "15/15 [==============================] - 5s 347ms/step - loss: 2.9578e-04 - accuracy: 0.9806 - val_loss: 0.0055 - val_accuracy: 0.6450\n",
            "Epoch 10/400\n",
            "15/15 [==============================] - 5s 348ms/step - loss: 2.9104e-04 - accuracy: 0.9807 - val_loss: 0.0054 - val_accuracy: 0.6500\n",
            "Epoch 11/400\n",
            "15/15 [==============================] - 5s 348ms/step - loss: 2.8375e-04 - accuracy: 0.9808 - val_loss: 0.0055 - val_accuracy: 0.6439\n",
            "Epoch 12/400\n",
            "15/15 [==============================] - 5s 349ms/step - loss: 3.0592e-04 - accuracy: 0.9798 - val_loss: 0.0055 - val_accuracy: 0.6450\n",
            "Epoch 13/400\n",
            "15/15 [==============================] - 5s 348ms/step - loss: 2.9241e-04 - accuracy: 0.9805 - val_loss: 0.0055 - val_accuracy: 0.6439\n",
            "Epoch 14/400\n",
            "15/15 [==============================] - 5s 349ms/step - loss: 2.8765e-04 - accuracy: 0.9808 - val_loss: 0.0055 - val_accuracy: 0.6444\n",
            "Epoch 15/400\n",
            "15/15 [==============================] - 5s 349ms/step - loss: 2.8281e-04 - accuracy: 0.9808 - val_loss: 0.0055 - val_accuracy: 0.6461\n",
            "Epoch 16/400\n",
            "15/15 [==============================] - 5s 349ms/step - loss: 2.8161e-04 - accuracy: 0.9808 - val_loss: 0.0054 - val_accuracy: 0.6483\n",
            "Epoch 17/400\n",
            "15/15 [==============================] - 5s 349ms/step - loss: 2.9027e-04 - accuracy: 0.9804 - val_loss: 0.0054 - val_accuracy: 0.6533\n",
            "Epoch 18/400\n",
            "15/15 [==============================] - 5s 349ms/step - loss: 2.8186e-04 - accuracy: 0.9806 - val_loss: 0.0054 - val_accuracy: 0.6506\n",
            "Epoch 19/400\n",
            "15/15 [==============================] - 5s 350ms/step - loss: 2.7698e-04 - accuracy: 0.9808 - val_loss: 0.0054 - val_accuracy: 0.6456\n",
            "Epoch 20/400\n",
            "15/15 [==============================] - 5s 349ms/step - loss: 2.7871e-04 - accuracy: 0.9808 - val_loss: 0.0056 - val_accuracy: 0.6333\n",
            "Epoch 21/400\n",
            "15/15 [==============================] - 5s 350ms/step - loss: 2.8132e-04 - accuracy: 0.9808 - val_loss: 0.0055 - val_accuracy: 0.6422\n",
            "Epoch 22/400\n",
            "15/15 [==============================] - 5s 350ms/step - loss: 2.8750e-04 - accuracy: 0.9807 - val_loss: 0.0055 - val_accuracy: 0.6444\n",
            "Epoch 23/400\n",
            "15/15 [==============================] - 5s 349ms/step - loss: 2.8023e-04 - accuracy: 0.9807 - val_loss: 0.0055 - val_accuracy: 0.6411\n",
            "Epoch 24/400\n",
            "15/15 [==============================] - 5s 350ms/step - loss: 2.7979e-04 - accuracy: 0.9808 - val_loss: 0.0053 - val_accuracy: 0.6517\n",
            "Epoch 25/400\n",
            "15/15 [==============================] - 5s 349ms/step - loss: 2.7366e-04 - accuracy: 0.9808 - val_loss: 0.0055 - val_accuracy: 0.6456\n",
            "Epoch 26/400\n",
            "15/15 [==============================] - 5s 351ms/step - loss: 2.7513e-04 - accuracy: 0.9808 - val_loss: 0.0054 - val_accuracy: 0.6456\n",
            "Epoch 27/400\n",
            "15/15 [==============================] - 5s 350ms/step - loss: 2.8515e-04 - accuracy: 0.9807 - val_loss: 0.0054 - val_accuracy: 0.6428\n",
            "Epoch 28/400\n",
            "15/15 [==============================] - 5s 351ms/step - loss: 2.8054e-04 - accuracy: 0.9808 - val_loss: 0.0053 - val_accuracy: 0.6556\n",
            "Epoch 29/400\n",
            "15/15 [==============================] - 5s 350ms/step - loss: 2.7506e-04 - accuracy: 0.9808 - val_loss: 0.0053 - val_accuracy: 0.6533\n",
            "Epoch 30/400\n",
            "15/15 [==============================] - 5s 350ms/step - loss: 2.8443e-04 - accuracy: 0.9803 - val_loss: 0.0053 - val_accuracy: 0.6517\n",
            "Epoch 31/400\n",
            "15/15 [==============================] - 5s 350ms/step - loss: 2.9884e-04 - accuracy: 0.9801 - val_loss: 0.0054 - val_accuracy: 0.6472\n",
            "Epoch 32/400\n",
            "15/15 [==============================] - 5s 350ms/step - loss: 2.8549e-04 - accuracy: 0.9804 - val_loss: 0.0054 - val_accuracy: 0.6456\n",
            "Epoch 33/400\n",
            "15/15 [==============================] - 5s 350ms/step - loss: 2.8177e-04 - accuracy: 0.9807 - val_loss: 0.0053 - val_accuracy: 0.6506\n",
            "Epoch 34/400\n",
            "15/15 [==============================] - 5s 350ms/step - loss: 2.8120e-04 - accuracy: 0.9808 - val_loss: 0.0054 - val_accuracy: 0.6439\n",
            "Epoch 35/400\n",
            "15/15 [==============================] - 5s 350ms/step - loss: 2.7901e-04 - accuracy: 0.9808 - val_loss: 0.0055 - val_accuracy: 0.6367\n",
            "Epoch 36/400\n",
            "15/15 [==============================] - 5s 350ms/step - loss: 3.2143e-04 - accuracy: 0.9790 - val_loss: 0.0065 - val_accuracy: 0.5867\n",
            "Epoch 37/400\n",
            "15/15 [==============================] - 5s 350ms/step - loss: 3.0433e-04 - accuracy: 0.9800 - val_loss: 0.0054 - val_accuracy: 0.6461\n",
            "Epoch 38/400\n",
            "15/15 [==============================] - 5s 350ms/step - loss: 2.9511e-04 - accuracy: 0.9801 - val_loss: 0.0055 - val_accuracy: 0.6422\n",
            "Epoch 39/400\n",
            "15/15 [==============================] - 5s 350ms/step - loss: 2.9122e-04 - accuracy: 0.9808 - val_loss: 0.0056 - val_accuracy: 0.6422\n",
            "Epoch 40/400\n",
            "15/15 [==============================] - 5s 350ms/step - loss: 3.0864e-04 - accuracy: 0.9799 - val_loss: 0.0054 - val_accuracy: 0.6544\n",
            "Epoch 41/400\n",
            "15/15 [==============================] - 5s 351ms/step - loss: 2.9059e-04 - accuracy: 0.9806 - val_loss: 0.0054 - val_accuracy: 0.6544\n",
            "Epoch 42/400\n",
            "15/15 [==============================] - 5s 351ms/step - loss: 2.8788e-04 - accuracy: 0.9807 - val_loss: 0.0054 - val_accuracy: 0.6589\n",
            "Epoch 43/400\n",
            "15/15 [==============================] - 5s 351ms/step - loss: 2.8668e-04 - accuracy: 0.9806 - val_loss: 0.0054 - val_accuracy: 0.6528\n",
            "Epoch 44/400\n",
            "15/15 [==============================] - 5s 350ms/step - loss: 2.7837e-04 - accuracy: 0.9808 - val_loss: 0.0054 - val_accuracy: 0.6472\n",
            "Epoch 45/400\n",
            "15/15 [==============================] - 5s 350ms/step - loss: 2.8078e-04 - accuracy: 0.9808 - val_loss: 0.0055 - val_accuracy: 0.6433\n",
            "Epoch 46/400\n",
            "15/15 [==============================] - 5s 351ms/step - loss: 2.8094e-04 - accuracy: 0.9807 - val_loss: 0.0054 - val_accuracy: 0.6478\n",
            "Epoch 47/400\n",
            "15/15 [==============================] - 5s 350ms/step - loss: 2.8659e-04 - accuracy: 0.9804 - val_loss: 0.0055 - val_accuracy: 0.6483\n",
            "Epoch 48/400\n",
            "15/15 [==============================] - 5s 351ms/step - loss: 2.8030e-04 - accuracy: 0.9807 - val_loss: 0.0053 - val_accuracy: 0.6583\n",
            "Epoch 49/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.7966e-04 - accuracy: 0.9808 - val_loss: 0.0052 - val_accuracy: 0.6600\n",
            "Epoch 50/400\n",
            "15/15 [==============================] - 5s 351ms/step - loss: 2.7658e-04 - accuracy: 0.9808 - val_loss: 0.0055 - val_accuracy: 0.6467\n",
            "Epoch 51/400\n",
            "15/15 [==============================] - 5s 351ms/step - loss: 2.7678e-04 - accuracy: 0.9808 - val_loss: 0.0055 - val_accuracy: 0.6439\n",
            "Epoch 52/400\n",
            "15/15 [==============================] - 5s 351ms/step - loss: 2.9289e-04 - accuracy: 0.9806 - val_loss: 0.0056 - val_accuracy: 0.6311\n",
            "Epoch 53/400\n",
            "15/15 [==============================] - 5s 351ms/step - loss: 2.8887e-04 - accuracy: 0.9806 - val_loss: 0.0055 - val_accuracy: 0.6350\n",
            "Epoch 54/400\n",
            "15/15 [==============================] - 5s 351ms/step - loss: 2.8477e-04 - accuracy: 0.9806 - val_loss: 0.0054 - val_accuracy: 0.6433\n",
            "Epoch 55/400\n",
            "15/15 [==============================] - 5s 351ms/step - loss: 2.8021e-04 - accuracy: 0.9808 - val_loss: 0.0056 - val_accuracy: 0.6283\n",
            "Epoch 56/400\n",
            "15/15 [==============================] - 5s 351ms/step - loss: 2.8570e-04 - accuracy: 0.9806 - val_loss: 0.0055 - val_accuracy: 0.6333\n",
            "Epoch 57/400\n",
            "15/15 [==============================] - 5s 350ms/step - loss: 2.8007e-04 - accuracy: 0.9808 - val_loss: 0.0053 - val_accuracy: 0.6506\n",
            "Epoch 58/400\n",
            "15/15 [==============================] - 5s 351ms/step - loss: 2.7731e-04 - accuracy: 0.9809 - val_loss: 0.0054 - val_accuracy: 0.6444\n",
            "Epoch 59/400\n",
            "15/15 [==============================] - 5s 351ms/step - loss: 2.7479e-04 - accuracy: 0.9808 - val_loss: 0.0053 - val_accuracy: 0.6528\n",
            "Epoch 60/400\n",
            "15/15 [==============================] - 5s 351ms/step - loss: 2.7477e-04 - accuracy: 0.9809 - val_loss: 0.0053 - val_accuracy: 0.6478\n",
            "Epoch 61/400\n",
            "15/15 [==============================] - 5s 351ms/step - loss: 2.7476e-04 - accuracy: 0.9809 - val_loss: 0.0053 - val_accuracy: 0.6467\n",
            "Epoch 62/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.7060e-04 - accuracy: 0.9809 - val_loss: 0.0054 - val_accuracy: 0.6439\n",
            "Epoch 63/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.6939e-04 - accuracy: 0.9810 - val_loss: 0.0053 - val_accuracy: 0.6478\n",
            "Epoch 64/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.7183e-04 - accuracy: 0.9810 - val_loss: 0.0054 - val_accuracy: 0.6444\n",
            "Epoch 65/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.6699e-04 - accuracy: 0.9810 - val_loss: 0.0052 - val_accuracy: 0.6606\n",
            "Epoch 66/400\n",
            "15/15 [==============================] - 5s 354ms/step - loss: 2.6987e-04 - accuracy: 0.9810 - val_loss: 0.0053 - val_accuracy: 0.6539\n",
            "Epoch 67/400\n",
            "15/15 [==============================] - 5s 354ms/step - loss: 2.7422e-04 - accuracy: 0.9806 - val_loss: 0.0053 - val_accuracy: 0.6506\n",
            "Epoch 68/400\n",
            "15/15 [==============================] - 5s 354ms/step - loss: 2.7486e-04 - accuracy: 0.9809 - val_loss: 0.0053 - val_accuracy: 0.6456\n",
            "Epoch 69/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.7112e-04 - accuracy: 0.9810 - val_loss: 0.0052 - val_accuracy: 0.6594\n",
            "Epoch 70/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.6825e-04 - accuracy: 0.9810 - val_loss: 0.0053 - val_accuracy: 0.6506\n",
            "Epoch 71/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.6711e-04 - accuracy: 0.9810 - val_loss: 0.0052 - val_accuracy: 0.6478\n",
            "Epoch 72/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.6527e-04 - accuracy: 0.9810 - val_loss: 0.0054 - val_accuracy: 0.6389\n",
            "Epoch 73/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.6773e-04 - accuracy: 0.9810 - val_loss: 0.0054 - val_accuracy: 0.6422\n",
            "Epoch 74/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.7068e-04 - accuracy: 0.9806 - val_loss: 0.0055 - val_accuracy: 0.6383\n",
            "Epoch 75/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.6995e-04 - accuracy: 0.9810 - val_loss: 0.0056 - val_accuracy: 0.6328\n",
            "Epoch 76/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.7005e-04 - accuracy: 0.9810 - val_loss: 0.0053 - val_accuracy: 0.6589\n",
            "Epoch 77/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.7096e-04 - accuracy: 0.9812 - val_loss: 0.0052 - val_accuracy: 0.6594\n",
            "Epoch 78/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.6357e-04 - accuracy: 0.9813 - val_loss: 0.0054 - val_accuracy: 0.6472\n",
            "Epoch 79/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.7080e-04 - accuracy: 0.9812 - val_loss: 0.0053 - val_accuracy: 0.6550\n",
            "Epoch 80/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.6704e-04 - accuracy: 0.9813 - val_loss: 0.0052 - val_accuracy: 0.6683\n",
            "Epoch 81/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.6346e-04 - accuracy: 0.9814 - val_loss: 0.0054 - val_accuracy: 0.6561\n",
            "Epoch 82/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.6543e-04 - accuracy: 0.9814 - val_loss: 0.0053 - val_accuracy: 0.6506\n",
            "Epoch 83/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.6349e-04 - accuracy: 0.9814 - val_loss: 0.0053 - val_accuracy: 0.6472\n",
            "Epoch 84/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.6230e-04 - accuracy: 0.9814 - val_loss: 0.0054 - val_accuracy: 0.6506\n",
            "Epoch 85/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.6795e-04 - accuracy: 0.9812 - val_loss: 0.0054 - val_accuracy: 0.6428\n",
            "Epoch 86/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.6630e-04 - accuracy: 0.9812 - val_loss: 0.0056 - val_accuracy: 0.6278\n",
            "Epoch 87/400\n",
            "15/15 [==============================] - 5s 354ms/step - loss: 2.8824e-04 - accuracy: 0.9810 - val_loss: 0.0054 - val_accuracy: 0.6439\n",
            "Epoch 88/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.7362e-04 - accuracy: 0.9813 - val_loss: 0.0055 - val_accuracy: 0.6417\n",
            "Epoch 89/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.8008e-04 - accuracy: 0.9811 - val_loss: 0.0053 - val_accuracy: 0.6544\n",
            "Epoch 90/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.7847e-04 - accuracy: 0.9812 - val_loss: 0.0053 - val_accuracy: 0.6606\n",
            "Epoch 91/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.9249e-04 - accuracy: 0.9805 - val_loss: 0.0053 - val_accuracy: 0.6600\n",
            "Epoch 92/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.8991e-04 - accuracy: 0.9809 - val_loss: 0.0054 - val_accuracy: 0.6511\n",
            "Epoch 93/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.9592e-04 - accuracy: 0.9806 - val_loss: 0.0055 - val_accuracy: 0.6422\n",
            "Epoch 94/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.9485e-04 - accuracy: 0.9810 - val_loss: 0.0055 - val_accuracy: 0.6467\n",
            "Epoch 95/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.8384e-04 - accuracy: 0.9815 - val_loss: 0.0055 - val_accuracy: 0.6467\n",
            "Epoch 96/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.7512e-04 - accuracy: 0.9817 - val_loss: 0.0055 - val_accuracy: 0.6389\n",
            "Epoch 97/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.7650e-04 - accuracy: 0.9816 - val_loss: 0.0057 - val_accuracy: 0.6261\n",
            "Epoch 98/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.8467e-04 - accuracy: 0.9813 - val_loss: 0.0057 - val_accuracy: 0.6328\n",
            "Epoch 99/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.6872e-04 - accuracy: 0.9817 - val_loss: 0.0054 - val_accuracy: 0.6544\n",
            "Epoch 100/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.7875e-04 - accuracy: 0.9813 - val_loss: 0.0054 - val_accuracy: 0.6506\n",
            "Epoch 101/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.7786e-04 - accuracy: 0.9815 - val_loss: 0.0055 - val_accuracy: 0.6450\n",
            "Epoch 102/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.7274e-04 - accuracy: 0.9817 - val_loss: 0.0053 - val_accuracy: 0.6578\n",
            "Epoch 103/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.6225e-04 - accuracy: 0.9820 - val_loss: 0.0053 - val_accuracy: 0.6572\n",
            "Epoch 104/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.6161e-04 - accuracy: 0.9819 - val_loss: 0.0053 - val_accuracy: 0.6500\n",
            "Epoch 105/400\n",
            "15/15 [==============================] - 5s 351ms/step - loss: 2.5987e-04 - accuracy: 0.9821 - val_loss: 0.0054 - val_accuracy: 0.6544\n",
            "Epoch 106/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.6295e-04 - accuracy: 0.9820 - val_loss: 0.0053 - val_accuracy: 0.6539\n",
            "Epoch 107/400\n",
            "15/15 [==============================] - 5s 351ms/step - loss: 2.5686e-04 - accuracy: 0.9822 - val_loss: 0.0053 - val_accuracy: 0.6511\n",
            "Epoch 108/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.6151e-04 - accuracy: 0.9821 - val_loss: 0.0055 - val_accuracy: 0.6333\n",
            "Epoch 109/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.6060e-04 - accuracy: 0.9822 - val_loss: 0.0053 - val_accuracy: 0.6533\n",
            "Epoch 110/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.5479e-04 - accuracy: 0.9822 - val_loss: 0.0053 - val_accuracy: 0.6494\n",
            "Epoch 111/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.6252e-04 - accuracy: 0.9823 - val_loss: 0.0053 - val_accuracy: 0.6528\n",
            "Epoch 112/400\n",
            "15/15 [==============================] - 5s 351ms/step - loss: 2.5742e-04 - accuracy: 0.9824 - val_loss: 0.0052 - val_accuracy: 0.6556\n",
            "Epoch 113/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.5543e-04 - accuracy: 0.9825 - val_loss: 0.0054 - val_accuracy: 0.6472\n",
            "Epoch 114/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.5553e-04 - accuracy: 0.9827 - val_loss: 0.0051 - val_accuracy: 0.6683\n",
            "Epoch 115/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.5398e-04 - accuracy: 0.9826 - val_loss: 0.0053 - val_accuracy: 0.6567\n",
            "Epoch 116/400\n",
            "15/15 [==============================] - 5s 351ms/step - loss: 2.5609e-04 - accuracy: 0.9826 - val_loss: 0.0053 - val_accuracy: 0.6556\n",
            "Epoch 117/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.5114e-04 - accuracy: 0.9828 - val_loss: 0.0054 - val_accuracy: 0.6417\n",
            "Epoch 118/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.5188e-04 - accuracy: 0.9830 - val_loss: 0.0052 - val_accuracy: 0.6606\n",
            "Epoch 119/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.5064e-04 - accuracy: 0.9829 - val_loss: 0.0052 - val_accuracy: 0.6472\n",
            "Epoch 120/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.4761e-04 - accuracy: 0.9832 - val_loss: 0.0051 - val_accuracy: 0.6650\n",
            "Epoch 121/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.4535e-04 - accuracy: 0.9832 - val_loss: 0.0052 - val_accuracy: 0.6522\n",
            "Epoch 122/400\n",
            "15/15 [==============================] - 5s 354ms/step - loss: 2.4588e-04 - accuracy: 0.9831 - val_loss: 0.0051 - val_accuracy: 0.6622\n",
            "Epoch 123/400\n",
            "15/15 [==============================] - 5s 354ms/step - loss: 2.4479e-04 - accuracy: 0.9831 - val_loss: 0.0051 - val_accuracy: 0.6611\n",
            "Epoch 124/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.4189e-04 - accuracy: 0.9835 - val_loss: 0.0054 - val_accuracy: 0.6417\n",
            "Epoch 125/400\n",
            "15/15 [==============================] - 5s 354ms/step - loss: 2.4870e-04 - accuracy: 0.9835 - val_loss: 0.0053 - val_accuracy: 0.6494\n",
            "Epoch 126/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.5222e-04 - accuracy: 0.9830 - val_loss: 0.0054 - val_accuracy: 0.6522\n",
            "Epoch 127/400\n",
            "15/15 [==============================] - 5s 354ms/step - loss: 2.5680e-04 - accuracy: 0.9829 - val_loss: 0.0056 - val_accuracy: 0.6406\n",
            "Epoch 128/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.5667e-04 - accuracy: 0.9831 - val_loss: 0.0055 - val_accuracy: 0.6400\n",
            "Epoch 129/400\n",
            "15/15 [==============================] - 5s 354ms/step - loss: 3.1398e-04 - accuracy: 0.9800 - val_loss: 0.0056 - val_accuracy: 0.6311\n",
            "Epoch 130/400\n",
            "15/15 [==============================] - 5s 354ms/step - loss: 2.6484e-04 - accuracy: 0.9832 - val_loss: 0.0055 - val_accuracy: 0.6400\n",
            "Epoch 131/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.7764e-04 - accuracy: 0.9824 - val_loss: 0.0053 - val_accuracy: 0.6617\n",
            "Epoch 132/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.5742e-04 - accuracy: 0.9834 - val_loss: 0.0055 - val_accuracy: 0.6533\n",
            "Epoch 133/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.6257e-04 - accuracy: 0.9832 - val_loss: 0.0060 - val_accuracy: 0.6161\n",
            "Epoch 134/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 3.0639e-04 - accuracy: 0.9806 - val_loss: 0.0060 - val_accuracy: 0.6150\n",
            "Epoch 135/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.9741e-04 - accuracy: 0.9812 - val_loss: 0.0056 - val_accuracy: 0.6372\n",
            "Epoch 136/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.7909e-04 - accuracy: 0.9826 - val_loss: 0.0055 - val_accuracy: 0.6483\n",
            "Epoch 137/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.6343e-04 - accuracy: 0.9835 - val_loss: 0.0055 - val_accuracy: 0.6467\n",
            "Epoch 138/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.5466e-04 - accuracy: 0.9840 - val_loss: 0.0056 - val_accuracy: 0.6406\n",
            "Epoch 139/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.5517e-04 - accuracy: 0.9840 - val_loss: 0.0054 - val_accuracy: 0.6522\n",
            "Epoch 140/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.8767e-04 - accuracy: 0.9823 - val_loss: 0.0057 - val_accuracy: 0.6356\n",
            "Epoch 141/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.6534e-04 - accuracy: 0.9835 - val_loss: 0.0058 - val_accuracy: 0.6300\n",
            "Epoch 142/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 0.0021 - accuracy: 0.8621 - val_loss: 0.0073 - val_accuracy: 0.5533\n",
            "Epoch 143/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 7.5434e-04 - accuracy: 0.9516 - val_loss: 0.0061 - val_accuracy: 0.6250\n",
            "Epoch 144/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 3.8690e-04 - accuracy: 0.9763 - val_loss: 0.0056 - val_accuracy: 0.6522\n",
            "Epoch 145/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 3.6015e-04 - accuracy: 0.9776 - val_loss: 0.0060 - val_accuracy: 0.6311\n",
            "Epoch 146/400\n",
            "15/15 [==============================] - 5s 354ms/step - loss: 3.6270e-04 - accuracy: 0.9775 - val_loss: 0.0057 - val_accuracy: 0.6511\n",
            "Epoch 147/400\n",
            "15/15 [==============================] - 5s 354ms/step - loss: 4.0174e-04 - accuracy: 0.9749 - val_loss: 0.0061 - val_accuracy: 0.6278\n",
            "Epoch 148/400\n",
            "15/15 [==============================] - 5s 355ms/step - loss: 2.9882e-04 - accuracy: 0.9821 - val_loss: 0.0061 - val_accuracy: 0.6294\n",
            "Epoch 149/400\n",
            "15/15 [==============================] - 5s 354ms/step - loss: 2.8043e-04 - accuracy: 0.9829 - val_loss: 0.0055 - val_accuracy: 0.6661\n",
            "Epoch 150/400\n",
            "15/15 [==============================] - 5s 354ms/step - loss: 2.6750e-04 - accuracy: 0.9839 - val_loss: 0.0055 - val_accuracy: 0.6661\n",
            "Epoch 151/400\n",
            "15/15 [==============================] - 5s 354ms/step - loss: 2.5998e-04 - accuracy: 0.9844 - val_loss: 0.0054 - val_accuracy: 0.6656\n",
            "Epoch 152/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.5788e-04 - accuracy: 0.9844 - val_loss: 0.0054 - val_accuracy: 0.6683\n",
            "Epoch 153/400\n",
            "15/15 [==============================] - 5s 354ms/step - loss: 2.5052e-04 - accuracy: 0.9847 - val_loss: 0.0053 - val_accuracy: 0.6744\n",
            "Epoch 154/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.5058e-04 - accuracy: 0.9848 - val_loss: 0.0053 - val_accuracy: 0.6739\n",
            "Epoch 155/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.4935e-04 - accuracy: 0.9848 - val_loss: 0.0053 - val_accuracy: 0.6706\n",
            "Epoch 156/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.4672e-04 - accuracy: 0.9847 - val_loss: 0.0053 - val_accuracy: 0.6700\n",
            "Epoch 157/400\n",
            "15/15 [==============================] - 5s 351ms/step - loss: 2.4770e-04 - accuracy: 0.9849 - val_loss: 0.0053 - val_accuracy: 0.6644\n",
            "Epoch 158/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.6674e-04 - accuracy: 0.9841 - val_loss: 0.0054 - val_accuracy: 0.6611\n",
            "Epoch 159/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.5887e-04 - accuracy: 0.9844 - val_loss: 0.0054 - val_accuracy: 0.6667\n",
            "Epoch 160/400\n",
            "15/15 [==============================] - 5s 351ms/step - loss: 2.5006e-04 - accuracy: 0.9848 - val_loss: 0.0054 - val_accuracy: 0.6594\n",
            "Epoch 161/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.4677e-04 - accuracy: 0.9848 - val_loss: 0.0055 - val_accuracy: 0.6589\n",
            "Epoch 162/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.4322e-04 - accuracy: 0.9849 - val_loss: 0.0055 - val_accuracy: 0.6583\n",
            "Epoch 163/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.4342e-04 - accuracy: 0.9849 - val_loss: 0.0053 - val_accuracy: 0.6644\n",
            "Epoch 164/400\n",
            "15/15 [==============================] - 5s 351ms/step - loss: 2.6022e-04 - accuracy: 0.9843 - val_loss: 0.0056 - val_accuracy: 0.6567\n",
            "Epoch 165/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.5386e-04 - accuracy: 0.9846 - val_loss: 0.0054 - val_accuracy: 0.6633\n",
            "Epoch 166/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.4956e-04 - accuracy: 0.9849 - val_loss: 0.0053 - val_accuracy: 0.6656\n",
            "Epoch 167/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.4553e-04 - accuracy: 0.9849 - val_loss: 0.0053 - val_accuracy: 0.6689\n",
            "Epoch 168/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.4075e-04 - accuracy: 0.9849 - val_loss: 0.0054 - val_accuracy: 0.6639\n",
            "Epoch 169/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.4746e-04 - accuracy: 0.9848 - val_loss: 0.0053 - val_accuracy: 0.6711\n",
            "Epoch 170/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.4321e-04 - accuracy: 0.9849 - val_loss: 0.0053 - val_accuracy: 0.6678\n",
            "Epoch 171/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.4663e-04 - accuracy: 0.9849 - val_loss: 0.0054 - val_accuracy: 0.6644\n",
            "Epoch 172/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.3997e-04 - accuracy: 0.9849 - val_loss: 0.0054 - val_accuracy: 0.6572\n",
            "Epoch 173/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.4343e-04 - accuracy: 0.9849 - val_loss: 0.0054 - val_accuracy: 0.6606\n",
            "Epoch 174/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.4889e-04 - accuracy: 0.9847 - val_loss: 0.0055 - val_accuracy: 0.6594\n",
            "Epoch 175/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.4380e-04 - accuracy: 0.9849 - val_loss: 0.0054 - val_accuracy: 0.6678\n",
            "Epoch 176/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.4487e-04 - accuracy: 0.9848 - val_loss: 0.0053 - val_accuracy: 0.6667\n",
            "Epoch 177/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.4865e-04 - accuracy: 0.9849 - val_loss: 0.0054 - val_accuracy: 0.6672\n",
            "Epoch 178/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.4660e-04 - accuracy: 0.9847 - val_loss: 0.0053 - val_accuracy: 0.6683\n",
            "Epoch 179/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.4698e-04 - accuracy: 0.9850 - val_loss: 0.0053 - val_accuracy: 0.6650\n",
            "Epoch 180/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.5098e-04 - accuracy: 0.9849 - val_loss: 0.0056 - val_accuracy: 0.6461\n",
            "Epoch 181/400\n",
            "15/15 [==============================] - 5s 354ms/step - loss: 2.4998e-04 - accuracy: 0.9849 - val_loss: 0.0054 - val_accuracy: 0.6667\n",
            "Epoch 182/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.4455e-04 - accuracy: 0.9850 - val_loss: 0.0054 - val_accuracy: 0.6639\n",
            "Epoch 183/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.4270e-04 - accuracy: 0.9851 - val_loss: 0.0054 - val_accuracy: 0.6572\n",
            "Epoch 184/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.5619e-04 - accuracy: 0.9841 - val_loss: 0.0055 - val_accuracy: 0.6606\n",
            "Epoch 185/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.4956e-04 - accuracy: 0.9846 - val_loss: 0.0054 - val_accuracy: 0.6517\n",
            "Epoch 186/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.4524e-04 - accuracy: 0.9850 - val_loss: 0.0054 - val_accuracy: 0.6533\n",
            "Epoch 187/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.3764e-04 - accuracy: 0.9851 - val_loss: 0.0055 - val_accuracy: 0.6594\n",
            "Epoch 188/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.3863e-04 - accuracy: 0.9851 - val_loss: 0.0055 - val_accuracy: 0.6483\n",
            "Epoch 189/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.3734e-04 - accuracy: 0.9850 - val_loss: 0.0055 - val_accuracy: 0.6550\n",
            "Epoch 190/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.3667e-04 - accuracy: 0.9851 - val_loss: 0.0056 - val_accuracy: 0.6522\n",
            "Epoch 191/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.3656e-04 - accuracy: 0.9851 - val_loss: 0.0054 - val_accuracy: 0.6583\n",
            "Epoch 192/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.3981e-04 - accuracy: 0.9850 - val_loss: 0.0055 - val_accuracy: 0.6561\n",
            "Epoch 193/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.3510e-04 - accuracy: 0.9851 - val_loss: 0.0055 - val_accuracy: 0.6539\n",
            "Epoch 194/400\n",
            "15/15 [==============================] - 5s 354ms/step - loss: 2.3696e-04 - accuracy: 0.9850 - val_loss: 0.0055 - val_accuracy: 0.6456\n",
            "Epoch 195/400\n",
            "15/15 [==============================] - 5s 354ms/step - loss: 2.3294e-04 - accuracy: 0.9851 - val_loss: 0.0056 - val_accuracy: 0.6450\n",
            "Epoch 196/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.3492e-04 - accuracy: 0.9850 - val_loss: 0.0055 - val_accuracy: 0.6456\n",
            "Epoch 197/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.3683e-04 - accuracy: 0.9850 - val_loss: 0.0055 - val_accuracy: 0.6522\n",
            "Epoch 198/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.3970e-04 - accuracy: 0.9849 - val_loss: 0.0054 - val_accuracy: 0.6539\n",
            "Epoch 199/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.3737e-04 - accuracy: 0.9851 - val_loss: 0.0054 - val_accuracy: 0.6567\n",
            "Epoch 200/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.3743e-04 - accuracy: 0.9851 - val_loss: 0.0054 - val_accuracy: 0.6539\n",
            "Epoch 201/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.3664e-04 - accuracy: 0.9851 - val_loss: 0.0056 - val_accuracy: 0.6478\n",
            "Epoch 202/400\n",
            "15/15 [==============================] - 5s 354ms/step - loss: 2.3645e-04 - accuracy: 0.9850 - val_loss: 0.0055 - val_accuracy: 0.6472\n",
            "Epoch 203/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.3746e-04 - accuracy: 0.9851 - val_loss: 0.0055 - val_accuracy: 0.6461\n",
            "Epoch 204/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.3361e-04 - accuracy: 0.9851 - val_loss: 0.0055 - val_accuracy: 0.6472\n",
            "Epoch 205/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.3869e-04 - accuracy: 0.9849 - val_loss: 0.0055 - val_accuracy: 0.6528\n",
            "Epoch 206/400\n",
            "15/15 [==============================] - 5s 354ms/step - loss: 2.3353e-04 - accuracy: 0.9851 - val_loss: 0.0055 - val_accuracy: 0.6467\n",
            "Epoch 207/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.3219e-04 - accuracy: 0.9851 - val_loss: 0.0055 - val_accuracy: 0.6439\n",
            "Epoch 208/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.3497e-04 - accuracy: 0.9851 - val_loss: 0.0055 - val_accuracy: 0.6400\n",
            "Epoch 209/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.3301e-04 - accuracy: 0.9851 - val_loss: 0.0055 - val_accuracy: 0.6456\n",
            "Epoch 210/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.3384e-04 - accuracy: 0.9851 - val_loss: 0.0056 - val_accuracy: 0.6422\n",
            "Epoch 211/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.4175e-04 - accuracy: 0.9844 - val_loss: 0.0056 - val_accuracy: 0.6439\n",
            "Epoch 212/400\n",
            "15/15 [==============================] - 5s 354ms/step - loss: 2.5017e-04 - accuracy: 0.9842 - val_loss: 0.0060 - val_accuracy: 0.6189\n",
            "Epoch 213/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.4424e-04 - accuracy: 0.9846 - val_loss: 0.0057 - val_accuracy: 0.6406\n",
            "Epoch 214/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.3957e-04 - accuracy: 0.9849 - val_loss: 0.0055 - val_accuracy: 0.6500\n",
            "Epoch 215/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.4389e-04 - accuracy: 0.9847 - val_loss: 0.0057 - val_accuracy: 0.6372\n",
            "Epoch 216/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.3604e-04 - accuracy: 0.9849 - val_loss: 0.0054 - val_accuracy: 0.6539\n",
            "Epoch 217/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.3373e-04 - accuracy: 0.9850 - val_loss: 0.0054 - val_accuracy: 0.6567\n",
            "Epoch 218/400\n",
            "15/15 [==============================] - 5s 354ms/step - loss: 2.3350e-04 - accuracy: 0.9850 - val_loss: 0.0055 - val_accuracy: 0.6500\n",
            "Epoch 219/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.3098e-04 - accuracy: 0.9851 - val_loss: 0.0054 - val_accuracy: 0.6511\n",
            "Epoch 220/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.3191e-04 - accuracy: 0.9851 - val_loss: 0.0054 - val_accuracy: 0.6522\n",
            "Epoch 221/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.3293e-04 - accuracy: 0.9851 - val_loss: 0.0054 - val_accuracy: 0.6567\n",
            "Epoch 222/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.2816e-04 - accuracy: 0.9851 - val_loss: 0.0054 - val_accuracy: 0.6517\n",
            "Epoch 223/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.2915e-04 - accuracy: 0.9851 - val_loss: 0.0054 - val_accuracy: 0.6556\n",
            "Epoch 224/400\n",
            "15/15 [==============================] - 5s 354ms/step - loss: 2.2796e-04 - accuracy: 0.9852 - val_loss: 0.0054 - val_accuracy: 0.6500\n",
            "Epoch 225/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.2534e-04 - accuracy: 0.9852 - val_loss: 0.0055 - val_accuracy: 0.6422\n",
            "Epoch 226/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.2724e-04 - accuracy: 0.9852 - val_loss: 0.0055 - val_accuracy: 0.6456\n",
            "Epoch 227/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.2405e-04 - accuracy: 0.9853 - val_loss: 0.0055 - val_accuracy: 0.6478\n",
            "Epoch 228/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.2901e-04 - accuracy: 0.9852 - val_loss: 0.0055 - val_accuracy: 0.6439\n",
            "Epoch 229/400\n",
            "15/15 [==============================] - 5s 354ms/step - loss: 2.2963e-04 - accuracy: 0.9853 - val_loss: 0.0056 - val_accuracy: 0.6428\n",
            "Epoch 230/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.2644e-04 - accuracy: 0.9853 - val_loss: 0.0055 - val_accuracy: 0.6467\n",
            "Epoch 231/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.2491e-04 - accuracy: 0.9853 - val_loss: 0.0056 - val_accuracy: 0.6389\n",
            "Epoch 232/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.2987e-04 - accuracy: 0.9853 - val_loss: 0.0055 - val_accuracy: 0.6433\n",
            "Epoch 233/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.2778e-04 - accuracy: 0.9853 - val_loss: 0.0055 - val_accuracy: 0.6456\n",
            "Epoch 234/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.2439e-04 - accuracy: 0.9852 - val_loss: 0.0054 - val_accuracy: 0.6500\n",
            "Epoch 235/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.2402e-04 - accuracy: 0.9853 - val_loss: 0.0055 - val_accuracy: 0.6472\n",
            "Epoch 236/400\n",
            "15/15 [==============================] - 5s 354ms/step - loss: 2.2976e-04 - accuracy: 0.9851 - val_loss: 0.0054 - val_accuracy: 0.6494\n",
            "Epoch 237/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.2368e-04 - accuracy: 0.9853 - val_loss: 0.0055 - val_accuracy: 0.6433\n",
            "Epoch 238/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.3072e-04 - accuracy: 0.9851 - val_loss: 0.0055 - val_accuracy: 0.6411\n",
            "Epoch 239/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.2937e-04 - accuracy: 0.9853 - val_loss: 0.0054 - val_accuracy: 0.6450\n",
            "Epoch 240/400\n",
            "15/15 [==============================] - 5s 354ms/step - loss: 2.2739e-04 - accuracy: 0.9853 - val_loss: 0.0054 - val_accuracy: 0.6450\n",
            "Epoch 241/400\n",
            "15/15 [==============================] - 5s 354ms/step - loss: 2.2598e-04 - accuracy: 0.9853 - val_loss: 0.0055 - val_accuracy: 0.6394\n",
            "Epoch 242/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.2662e-04 - accuracy: 0.9853 - val_loss: 0.0055 - val_accuracy: 0.6472\n",
            "Epoch 243/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.2461e-04 - accuracy: 0.9853 - val_loss: 0.0055 - val_accuracy: 0.6461\n",
            "Epoch 244/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.2301e-04 - accuracy: 0.9853 - val_loss: 0.0055 - val_accuracy: 0.6411\n",
            "Epoch 245/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.2510e-04 - accuracy: 0.9853 - val_loss: 0.0056 - val_accuracy: 0.6356\n",
            "Epoch 246/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.3090e-04 - accuracy: 0.9853 - val_loss: 0.0056 - val_accuracy: 0.6328\n",
            "Epoch 247/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.2950e-04 - accuracy: 0.9853 - val_loss: 0.0056 - val_accuracy: 0.6439\n",
            "Epoch 248/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.2758e-04 - accuracy: 0.9851 - val_loss: 0.0054 - val_accuracy: 0.6478\n",
            "Epoch 249/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.2565e-04 - accuracy: 0.9853 - val_loss: 0.0054 - val_accuracy: 0.6506\n",
            "Epoch 250/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.2436e-04 - accuracy: 0.9853 - val_loss: 0.0054 - val_accuracy: 0.6478\n",
            "Epoch 251/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.2343e-04 - accuracy: 0.9853 - val_loss: 0.0054 - val_accuracy: 0.6483\n",
            "Epoch 252/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.2154e-04 - accuracy: 0.9853 - val_loss: 0.0054 - val_accuracy: 0.6483\n",
            "Epoch 253/400\n",
            "15/15 [==============================] - 5s 354ms/step - loss: 2.1946e-04 - accuracy: 0.9853 - val_loss: 0.0055 - val_accuracy: 0.6439\n",
            "Epoch 254/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.2015e-04 - accuracy: 0.9853 - val_loss: 0.0055 - val_accuracy: 0.6389\n",
            "Epoch 255/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.3021e-04 - accuracy: 0.9852 - val_loss: 0.0056 - val_accuracy: 0.6406\n",
            "Epoch 256/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.2494e-04 - accuracy: 0.9853 - val_loss: 0.0054 - val_accuracy: 0.6489\n",
            "Epoch 257/400\n",
            "15/15 [==============================] - 5s 354ms/step - loss: 2.2470e-04 - accuracy: 0.9853 - val_loss: 0.0054 - val_accuracy: 0.6467\n",
            "Epoch 258/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.2552e-04 - accuracy: 0.9853 - val_loss: 0.0053 - val_accuracy: 0.6528\n",
            "Epoch 259/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.2132e-04 - accuracy: 0.9853 - val_loss: 0.0055 - val_accuracy: 0.6483\n",
            "Epoch 260/400\n",
            "15/15 [==============================] - 5s 354ms/step - loss: 2.2590e-04 - accuracy: 0.9852 - val_loss: 0.0054 - val_accuracy: 0.6467\n",
            "Epoch 261/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.2537e-04 - accuracy: 0.9853 - val_loss: 0.0054 - val_accuracy: 0.6517\n",
            "Epoch 262/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.2336e-04 - accuracy: 0.9853 - val_loss: 0.0055 - val_accuracy: 0.6428\n",
            "Epoch 263/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.2538e-04 - accuracy: 0.9854 - val_loss: 0.0053 - val_accuracy: 0.6494\n",
            "Epoch 264/400\n",
            "15/15 [==============================] - 5s 354ms/step - loss: 2.2121e-04 - accuracy: 0.9854 - val_loss: 0.0054 - val_accuracy: 0.6422\n",
            "Epoch 265/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.1810e-04 - accuracy: 0.9854 - val_loss: 0.0055 - val_accuracy: 0.6411\n",
            "Epoch 266/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.2214e-04 - accuracy: 0.9853 - val_loss: 0.0055 - val_accuracy: 0.6383\n",
            "Epoch 267/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.2394e-04 - accuracy: 0.9854 - val_loss: 0.0054 - val_accuracy: 0.6472\n",
            "Epoch 268/400\n",
            "15/15 [==============================] - 5s 354ms/step - loss: 2.2124e-04 - accuracy: 0.9854 - val_loss: 0.0053 - val_accuracy: 0.6506\n",
            "Epoch 269/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.1881e-04 - accuracy: 0.9854 - val_loss: 0.0054 - val_accuracy: 0.6472\n",
            "Epoch 270/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.2609e-04 - accuracy: 0.9849 - val_loss: 0.0056 - val_accuracy: 0.6322\n",
            "Epoch 271/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.1806e-04 - accuracy: 0.9853 - val_loss: 0.0055 - val_accuracy: 0.6433\n",
            "Epoch 272/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.1815e-04 - accuracy: 0.9853 - val_loss: 0.0054 - val_accuracy: 0.6439\n",
            "Epoch 273/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.1720e-04 - accuracy: 0.9854 - val_loss: 0.0053 - val_accuracy: 0.6511\n",
            "Epoch 274/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.1911e-04 - accuracy: 0.9854 - val_loss: 0.0054 - val_accuracy: 0.6417\n",
            "Epoch 275/400\n",
            "15/15 [==============================] - 5s 354ms/step - loss: 2.1739e-04 - accuracy: 0.9854 - val_loss: 0.0054 - val_accuracy: 0.6428\n",
            "Epoch 276/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.1700e-04 - accuracy: 0.9854 - val_loss: 0.0055 - val_accuracy: 0.6361\n",
            "Epoch 277/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.1687e-04 - accuracy: 0.9853 - val_loss: 0.0054 - val_accuracy: 0.6478\n",
            "Epoch 278/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.2383e-04 - accuracy: 0.9853 - val_loss: 0.0056 - val_accuracy: 0.6328\n",
            "Epoch 279/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.2037e-04 - accuracy: 0.9854 - val_loss: 0.0054 - val_accuracy: 0.6411\n",
            "Epoch 280/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.2487e-04 - accuracy: 0.9853 - val_loss: 0.0054 - val_accuracy: 0.6422\n",
            "Epoch 281/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.2140e-04 - accuracy: 0.9854 - val_loss: 0.0053 - val_accuracy: 0.6500\n",
            "Epoch 282/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.2172e-04 - accuracy: 0.9855 - val_loss: 0.0055 - val_accuracy: 0.6444\n",
            "Epoch 283/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.2239e-04 - accuracy: 0.9853 - val_loss: 0.0054 - val_accuracy: 0.6461\n",
            "Epoch 284/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.2107e-04 - accuracy: 0.9856 - val_loss: 0.0054 - val_accuracy: 0.6483\n",
            "Epoch 285/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.2051e-04 - accuracy: 0.9856 - val_loss: 0.0054 - val_accuracy: 0.6444\n",
            "Epoch 286/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.1827e-04 - accuracy: 0.9855 - val_loss: 0.0054 - val_accuracy: 0.6406\n",
            "Epoch 287/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.1894e-04 - accuracy: 0.9856 - val_loss: 0.0053 - val_accuracy: 0.6522\n",
            "Epoch 288/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.1290e-04 - accuracy: 0.9856 - val_loss: 0.0054 - val_accuracy: 0.6433\n",
            "Epoch 289/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.1430e-04 - accuracy: 0.9856 - val_loss: 0.0054 - val_accuracy: 0.6456\n",
            "Epoch 290/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.1616e-04 - accuracy: 0.9856 - val_loss: 0.0054 - val_accuracy: 0.6411\n",
            "Epoch 291/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.1808e-04 - accuracy: 0.9856 - val_loss: 0.0053 - val_accuracy: 0.6511\n",
            "Epoch 292/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.1173e-04 - accuracy: 0.9856 - val_loss: 0.0054 - val_accuracy: 0.6411\n",
            "Epoch 293/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.1347e-04 - accuracy: 0.9856 - val_loss: 0.0054 - val_accuracy: 0.6417\n",
            "Epoch 294/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.1175e-04 - accuracy: 0.9856 - val_loss: 0.0054 - val_accuracy: 0.6472\n",
            "Epoch 295/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.1285e-04 - accuracy: 0.9856 - val_loss: 0.0054 - val_accuracy: 0.6444\n",
            "Epoch 296/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.1412e-04 - accuracy: 0.9856 - val_loss: 0.0054 - val_accuracy: 0.6378\n",
            "Epoch 297/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.2189e-04 - accuracy: 0.9855 - val_loss: 0.0055 - val_accuracy: 0.6406\n",
            "Epoch 298/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.2104e-04 - accuracy: 0.9855 - val_loss: 0.0053 - val_accuracy: 0.6517\n",
            "Epoch 299/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.2026e-04 - accuracy: 0.9855 - val_loss: 0.0053 - val_accuracy: 0.6533\n",
            "Epoch 300/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.1696e-04 - accuracy: 0.9856 - val_loss: 0.0054 - val_accuracy: 0.6467\n",
            "Epoch 301/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.1631e-04 - accuracy: 0.9856 - val_loss: 0.0054 - val_accuracy: 0.6467\n",
            "Epoch 302/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.1398e-04 - accuracy: 0.9856 - val_loss: 0.0054 - val_accuracy: 0.6444\n",
            "Epoch 303/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.1627e-04 - accuracy: 0.9855 - val_loss: 0.0056 - val_accuracy: 0.6222\n",
            "Epoch 304/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.1988e-04 - accuracy: 0.9853 - val_loss: 0.0055 - val_accuracy: 0.6372\n",
            "Epoch 305/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.1843e-04 - accuracy: 0.9854 - val_loss: 0.0053 - val_accuracy: 0.6489\n",
            "Epoch 306/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.1795e-04 - accuracy: 0.9854 - val_loss: 0.0055 - val_accuracy: 0.6439\n",
            "Epoch 307/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.2074e-04 - accuracy: 0.9853 - val_loss: 0.0054 - val_accuracy: 0.6456\n",
            "Epoch 308/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.1968e-04 - accuracy: 0.9855 - val_loss: 0.0054 - val_accuracy: 0.6378\n",
            "Epoch 309/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.1717e-04 - accuracy: 0.9856 - val_loss: 0.0054 - val_accuracy: 0.6367\n",
            "Epoch 310/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.1990e-04 - accuracy: 0.9855 - val_loss: 0.0055 - val_accuracy: 0.6339\n",
            "Epoch 311/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.1725e-04 - accuracy: 0.9856 - val_loss: 0.0057 - val_accuracy: 0.6267\n",
            "Epoch 312/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.1814e-04 - accuracy: 0.9855 - val_loss: 0.0055 - val_accuracy: 0.6361\n",
            "Epoch 313/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.1527e-04 - accuracy: 0.9856 - val_loss: 0.0054 - val_accuracy: 0.6400\n",
            "Epoch 314/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.1998e-04 - accuracy: 0.9855 - val_loss: 0.0055 - val_accuracy: 0.6378\n",
            "Epoch 315/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.1321e-04 - accuracy: 0.9856 - val_loss: 0.0056 - val_accuracy: 0.6272\n",
            "Epoch 316/400\n",
            "15/15 [==============================] - 5s 354ms/step - loss: 2.1243e-04 - accuracy: 0.9856 - val_loss: 0.0054 - val_accuracy: 0.6317\n",
            "Epoch 317/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.1949e-04 - accuracy: 0.9855 - val_loss: 0.0056 - val_accuracy: 0.6261\n",
            "Epoch 318/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.1800e-04 - accuracy: 0.9854 - val_loss: 0.0053 - val_accuracy: 0.6461\n",
            "Epoch 319/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.1604e-04 - accuracy: 0.9856 - val_loss: 0.0053 - val_accuracy: 0.6456\n",
            "Epoch 320/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.1452e-04 - accuracy: 0.9856 - val_loss: 0.0054 - val_accuracy: 0.6439\n",
            "Epoch 321/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.1513e-04 - accuracy: 0.9856 - val_loss: 0.0053 - val_accuracy: 0.6511\n",
            "Epoch 322/400\n",
            "15/15 [==============================] - 5s 354ms/step - loss: 2.1578e-04 - accuracy: 0.9855 - val_loss: 0.0055 - val_accuracy: 0.6406\n",
            "Epoch 323/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.1263e-04 - accuracy: 0.9856 - val_loss: 0.0054 - val_accuracy: 0.6472\n",
            "Epoch 324/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.2885e-04 - accuracy: 0.9848 - val_loss: 0.0057 - val_accuracy: 0.6206\n",
            "Epoch 325/400\n",
            "15/15 [==============================] - 5s 354ms/step - loss: 2.2616e-04 - accuracy: 0.9853 - val_loss: 0.0054 - val_accuracy: 0.6450\n",
            "Epoch 326/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.1827e-04 - accuracy: 0.9856 - val_loss: 0.0054 - val_accuracy: 0.6450\n",
            "Epoch 327/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.1388e-04 - accuracy: 0.9857 - val_loss: 0.0055 - val_accuracy: 0.6456\n",
            "Epoch 328/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.1381e-04 - accuracy: 0.9856 - val_loss: 0.0055 - val_accuracy: 0.6372\n",
            "Epoch 329/400\n",
            "15/15 [==============================] - 5s 351ms/step - loss: 2.1409e-04 - accuracy: 0.9858 - val_loss: 0.0053 - val_accuracy: 0.6389\n",
            "Epoch 330/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.1062e-04 - accuracy: 0.9859 - val_loss: 0.0054 - val_accuracy: 0.6439\n",
            "Epoch 331/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.1370e-04 - accuracy: 0.9858 - val_loss: 0.0053 - val_accuracy: 0.6422\n",
            "Epoch 332/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.0908e-04 - accuracy: 0.9859 - val_loss: 0.0054 - val_accuracy: 0.6417\n",
            "Epoch 333/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.0856e-04 - accuracy: 0.9860 - val_loss: 0.0055 - val_accuracy: 0.6317\n",
            "Epoch 334/400\n",
            "15/15 [==============================] - 5s 351ms/step - loss: 2.1121e-04 - accuracy: 0.9858 - val_loss: 0.0054 - val_accuracy: 0.6422\n",
            "Epoch 335/400\n",
            "15/15 [==============================] - 5s 351ms/step - loss: 2.0943e-04 - accuracy: 0.9861 - val_loss: 0.0054 - val_accuracy: 0.6433\n",
            "Epoch 336/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.0581e-04 - accuracy: 0.9862 - val_loss: 0.0054 - val_accuracy: 0.6422\n",
            "Epoch 337/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.0288e-04 - accuracy: 0.9862 - val_loss: 0.0053 - val_accuracy: 0.6450\n",
            "Epoch 338/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.0144e-04 - accuracy: 0.9863 - val_loss: 0.0053 - val_accuracy: 0.6489\n",
            "Epoch 339/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.0208e-04 - accuracy: 0.9863 - val_loss: 0.0054 - val_accuracy: 0.6450\n",
            "Epoch 340/400\n",
            "15/15 [==============================] - 5s 354ms/step - loss: 2.0306e-04 - accuracy: 0.9862 - val_loss: 0.0053 - val_accuracy: 0.6450\n",
            "Epoch 341/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.0313e-04 - accuracy: 0.9862 - val_loss: 0.0053 - val_accuracy: 0.6456\n",
            "Epoch 342/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.0073e-04 - accuracy: 0.9863 - val_loss: 0.0052 - val_accuracy: 0.6494\n",
            "Epoch 343/400\n",
            "15/15 [==============================] - 5s 354ms/step - loss: 2.0620e-04 - accuracy: 0.9862 - val_loss: 0.0055 - val_accuracy: 0.6322\n",
            "Epoch 344/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.0535e-04 - accuracy: 0.9863 - val_loss: 0.0055 - val_accuracy: 0.6439\n",
            "Epoch 345/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.1088e-04 - accuracy: 0.9859 - val_loss: 0.0053 - val_accuracy: 0.6556\n",
            "Epoch 346/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.0345e-04 - accuracy: 0.9863 - val_loss: 0.0055 - val_accuracy: 0.6406\n",
            "Epoch 347/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.7737e-04 - accuracy: 0.9824 - val_loss: 0.0057 - val_accuracy: 0.6367\n",
            "Epoch 348/400\n",
            "15/15 [==============================] - 5s 351ms/step - loss: 2.3804e-04 - accuracy: 0.9846 - val_loss: 0.0055 - val_accuracy: 0.6483\n",
            "Epoch 349/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.2133e-04 - accuracy: 0.9860 - val_loss: 0.0055 - val_accuracy: 0.6483\n",
            "Epoch 350/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 1.9872e-04 - accuracy: 0.9872 - val_loss: 0.0054 - val_accuracy: 0.6594\n",
            "Epoch 351/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 1.9529e-04 - accuracy: 0.9871 - val_loss: 0.0054 - val_accuracy: 0.6583\n",
            "Epoch 352/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 1.8932e-04 - accuracy: 0.9875 - val_loss: 0.0052 - val_accuracy: 0.6661\n",
            "Epoch 353/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 1.7994e-04 - accuracy: 0.9879 - val_loss: 0.0053 - val_accuracy: 0.6578\n",
            "Epoch 354/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 1.8206e-04 - accuracy: 0.9879 - val_loss: 0.0055 - val_accuracy: 0.6472\n",
            "Epoch 355/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 1.8177e-04 - accuracy: 0.9880 - val_loss: 0.0053 - val_accuracy: 0.6572\n",
            "Epoch 356/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 1.8599e-04 - accuracy: 0.9879 - val_loss: 0.0054 - val_accuracy: 0.6544\n",
            "Epoch 357/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 1.8576e-04 - accuracy: 0.9879 - val_loss: 0.0054 - val_accuracy: 0.6522\n",
            "Epoch 358/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 1.8874e-04 - accuracy: 0.9880 - val_loss: 0.0052 - val_accuracy: 0.6578\n",
            "Epoch 359/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 1.8300e-04 - accuracy: 0.9881 - val_loss: 0.0052 - val_accuracy: 0.6639\n",
            "Epoch 360/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 1.9525e-04 - accuracy: 0.9876 - val_loss: 0.0053 - val_accuracy: 0.6694\n",
            "Epoch 361/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 1.8635e-04 - accuracy: 0.9878 - val_loss: 0.0052 - val_accuracy: 0.6667\n",
            "Epoch 362/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 1.8277e-04 - accuracy: 0.9880 - val_loss: 0.0055 - val_accuracy: 0.6472\n",
            "Epoch 363/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 1.7939e-04 - accuracy: 0.9881 - val_loss: 0.0054 - val_accuracy: 0.6517\n",
            "Epoch 364/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 1.7876e-04 - accuracy: 0.9881 - val_loss: 0.0054 - val_accuracy: 0.6500\n",
            "Epoch 365/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 1.7896e-04 - accuracy: 0.9881 - val_loss: 0.0054 - val_accuracy: 0.6506\n",
            "Epoch 366/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 1.7976e-04 - accuracy: 0.9881 - val_loss: 0.0054 - val_accuracy: 0.6483\n",
            "Epoch 367/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 1.8066e-04 - accuracy: 0.9882 - val_loss: 0.0054 - val_accuracy: 0.6478\n",
            "Epoch 368/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 1.8135e-04 - accuracy: 0.9881 - val_loss: 0.0059 - val_accuracy: 0.6294\n",
            "Epoch 369/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 1.8358e-04 - accuracy: 0.9881 - val_loss: 0.0052 - val_accuracy: 0.6617\n",
            "Epoch 370/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 1.7911e-04 - accuracy: 0.9881 - val_loss: 0.0052 - val_accuracy: 0.6600\n",
            "Epoch 371/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 1.7978e-04 - accuracy: 0.9881 - val_loss: 0.0052 - val_accuracy: 0.6589\n",
            "Epoch 372/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 1.8021e-04 - accuracy: 0.9881 - val_loss: 0.0054 - val_accuracy: 0.6567\n",
            "Epoch 373/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 1.8180e-04 - accuracy: 0.9882 - val_loss: 0.0053 - val_accuracy: 0.6589\n",
            "Epoch 374/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 1.8188e-04 - accuracy: 0.9882 - val_loss: 0.0053 - val_accuracy: 0.6567\n",
            "Epoch 375/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 1.8194e-04 - accuracy: 0.9881 - val_loss: 0.0053 - val_accuracy: 0.6578\n",
            "Epoch 376/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 1.8423e-04 - accuracy: 0.9883 - val_loss: 0.0053 - val_accuracy: 0.6522\n",
            "Epoch 377/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 1.8113e-04 - accuracy: 0.9883 - val_loss: 0.0053 - val_accuracy: 0.6506\n",
            "Epoch 378/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 1.8676e-04 - accuracy: 0.9882 - val_loss: 0.0053 - val_accuracy: 0.6494\n",
            "Epoch 379/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 1.8043e-04 - accuracy: 0.9882 - val_loss: 0.0053 - val_accuracy: 0.6544\n",
            "Epoch 380/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 1.8432e-04 - accuracy: 0.9881 - val_loss: 0.0052 - val_accuracy: 0.6528\n",
            "Epoch 381/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 1.7935e-04 - accuracy: 0.9883 - val_loss: 0.0054 - val_accuracy: 0.6433\n",
            "Epoch 382/400\n",
            "15/15 [==============================] - 5s 354ms/step - loss: 1.7631e-04 - accuracy: 0.9883 - val_loss: 0.0053 - val_accuracy: 0.6528\n",
            "Epoch 383/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 1.8141e-04 - accuracy: 0.9880 - val_loss: 0.0056 - val_accuracy: 0.6322\n",
            "Epoch 384/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 1.8465e-04 - accuracy: 0.9880 - val_loss: 0.0053 - val_accuracy: 0.6539\n",
            "Epoch 385/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 1.7938e-04 - accuracy: 0.9881 - val_loss: 0.0054 - val_accuracy: 0.6494\n",
            "Epoch 386/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 1.8010e-04 - accuracy: 0.9882 - val_loss: 0.0055 - val_accuracy: 0.6356\n",
            "Epoch 387/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 1.9358e-04 - accuracy: 0.9874 - val_loss: 0.0055 - val_accuracy: 0.6428\n",
            "Epoch 388/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 1.8649e-04 - accuracy: 0.9881 - val_loss: 0.0056 - val_accuracy: 0.6417\n",
            "Epoch 389/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 1.8362e-04 - accuracy: 0.9881 - val_loss: 0.0054 - val_accuracy: 0.6550\n",
            "Epoch 390/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 1.8138e-04 - accuracy: 0.9881 - val_loss: 0.0053 - val_accuracy: 0.6617\n",
            "Epoch 391/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 1.7741e-04 - accuracy: 0.9882 - val_loss: 0.0055 - val_accuracy: 0.6411\n",
            "Epoch 392/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 1.7521e-04 - accuracy: 0.9882 - val_loss: 0.0055 - val_accuracy: 0.6433\n",
            "Epoch 393/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 1.7691e-04 - accuracy: 0.9882 - val_loss: 0.0054 - val_accuracy: 0.6461\n",
            "Epoch 394/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 1.7659e-04 - accuracy: 0.9882 - val_loss: 0.0054 - val_accuracy: 0.6433\n",
            "Epoch 395/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 1.7571e-04 - accuracy: 0.9883 - val_loss: 0.0055 - val_accuracy: 0.6461\n",
            "Epoch 396/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 1.7594e-04 - accuracy: 0.9883 - val_loss: 0.0056 - val_accuracy: 0.6367\n",
            "Epoch 397/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 1.7663e-04 - accuracy: 0.9883 - val_loss: 0.0053 - val_accuracy: 0.6539\n",
            "Epoch 398/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 1.7482e-04 - accuracy: 0.9882 - val_loss: 0.0054 - val_accuracy: 0.6461\n",
            "Epoch 399/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 1.7136e-04 - accuracy: 0.9883 - val_loss: 0.0054 - val_accuracy: 0.6428\n",
            "Epoch 400/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 1.7370e-04 - accuracy: 0.9882 - val_loss: 0.0054 - val_accuracy: 0.6450\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b4f0694-ea77-404b-c830-c09fb231f2ed",
        "id": "_pBhnZ6sXQxB"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "57/57 [==============================] - 1s 17ms/step - loss: 0.0057 - accuracy: 0.6294\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.0056536439806222916, 0.629444420337677]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 그래프 분석"
      ],
      "metadata": {
        "id": "2X93IcrvXQxB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history=history.history\n",
        "epochs = range(1, len(history['loss']) + 1)\n",
        "acc = history['accuracy']\n",
        "loss = history['loss']\n",
        "val_acc = history['val_accuracy']\n",
        "val_loss = history['val_loss']\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.plot(epochs, acc, label='accuracy')\n",
        "plt.plot(epochs, val_acc, label='val_acc')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.plot(epochs, loss, label='loss', color='g')\n",
        "plt.plot(epochs, val_loss, label='val_loss', color='r')\n",
        "plt.legend()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 701
        },
        "outputId": "e26ade9a-295a-409d-f83d-cfed635b150b",
        "id": "k1NEZobCXQxB"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fd57c0320d0>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 예측해보기(predict)"
      ],
      "metadata": {
        "id": "jcQ6Ov1VXQxB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e06685b-4c91-4d7b-804a-f36f83b0daf8",
        "id": "hm9zHp-NXQxB"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "188/188 [==============================] - 2s 9ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[44,  0,  0, ...,  0,  0,  0],\n",
              "       [ 0, 57,  0, ...,  0,  0,  0],\n",
              "       [ 0,  0, 65, ...,  0,  0,  0],\n",
              "       ...,\n",
              "       [ 0,  0,  0, ..., 68,  0,  0],\n",
              "       [ 0,  0,  0, ...,  0, 67,  0],\n",
              "       [ 0,  0,  0, ...,  0,  0, 57]])"
            ]
          },
          "metadata": {},
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_p=pd.DataFrame(y_test)\n",
        "\n",
        "for i in range(1,101):\n",
        "  y_test_p.loc[y_test_p[i] == 1.0 , 'y_test'] = i  \n",
        "y_test_p.y_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "348ddc8c-b4b0-4b45-9b9b-a83b87cbce28",
        "id": "vulS0JfoXQxB"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       59.0\n",
              "1       68.0\n",
              "2       72.0\n",
              "3        2.0\n",
              "4       57.0\n",
              "        ... \n",
              "5995     3.0\n",
              "5996    87.0\n",
              "5997    40.0\n",
              "5998    68.0\n",
              "5999    37.0\n",
              "Name: y_test, Length: 6000, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ch1 = pd.DataFrame(np.rint(y_pred))\n",
        "# for i in range(1,101):\n",
        "#   ch1.loc[ch1[i] == 1.0 , 'pred'] = i \n",
        "\n",
        " \n",
        "#predict = pd.concat([data_test,ch1['pred'], y_test_p['y_test']],axis = 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "35e79746-e41a-4713-b942-733f497e3b6c",
        "id": "v2tH7BGsXQxB"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        0    1    2    3    4    5    6    7    8    9  ...   92   93   94  \\\n",
              "0     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
              "1     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
              "2     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
              "3     0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
              "4     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
              "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
              "5995  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
              "5996  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
              "5997  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
              "5998  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
              "5999  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
              "\n",
              "       95   96   97   98   99  100  pred  \n",
              "0     0.0  0.0  0.0  0.0  0.0  0.0  59.0  \n",
              "1     0.0  0.0  0.0  0.0  0.0  0.0  68.0  \n",
              "2     0.0  0.0  0.0  0.0  0.0  0.0  72.0  \n",
              "3     0.0  0.0  0.0  0.0  0.0  0.0   2.0  \n",
              "4     0.0  0.0  0.0  0.0  0.0  0.0  57.0  \n",
              "...   ...  ...  ...  ...  ...  ...   ...  \n",
              "5995  0.0  0.0  0.0  0.0  0.0  0.0   3.0  \n",
              "5996  0.0  0.0  0.0  0.0  0.0  0.0  87.0  \n",
              "5997  0.0  0.0  0.0  0.0  0.0  0.0  40.0  \n",
              "5998  0.0  0.0  0.0  0.0  0.0  0.0  68.0  \n",
              "5999  0.0  0.0  0.0  0.0  0.0  0.0  45.0  \n",
              "\n",
              "[6000 rows x 102 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d66cc86c-ae23-4a9f-8fb4-dbad40e0c5ea\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "      <th>100</th>\n",
              "      <th>pred</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>59.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>68.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>72.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>57.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5995</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5996</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>87.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5997</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5998</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>68.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5999</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>45.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6000 rows × 102 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d66cc86c-ae23-4a9f-8fb4-dbad40e0c5ea')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d66cc86c-ae23-4a9f-8fb4-dbad40e0c5ea button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d66cc86c-ae23-4a9f-8fb4-dbad40e0c5ea');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ex=np.argmax(y_pred, axis=1)\n",
        "ex=ex.astype('float64')\n",
        "ex=pd.DataFrame(ex)\n",
        "ex"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "22f2ac13-d7ce-409c-db36-bf92c9ca0cad",
        "id": "fHoV4EKnXQxB"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         0\n",
              "0     59.0\n",
              "1     68.0\n",
              "2     72.0\n",
              "3      2.0\n",
              "4     57.0\n",
              "...    ...\n",
              "5995   3.0\n",
              "5996  87.0\n",
              "5997  40.0\n",
              "5998  68.0\n",
              "5999  45.0\n",
              "\n",
              "[6000 rows x 1 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-33a2dfcf-6ed7-497f-9b45-70f7cc7db973\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>59.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>68.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>72.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>57.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5995</th>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5996</th>\n",
              "      <td>87.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5997</th>\n",
              "      <td>40.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5998</th>\n",
              "      <td>68.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5999</th>\n",
              "      <td>45.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6000 rows × 1 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-33a2dfcf-6ed7-497f-9b45-70f7cc7db973')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-33a2dfcf-6ed7-497f-9b45-70f7cc7db973 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-33a2dfcf-6ed7-497f-9b45-70f7cc7db973');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ch1 = pd.DataFrame(np.rint(y_pred))\n",
        "ch1['pred'] = ex\n",
        "data_test.reset_index(inplace=True, drop=True)\n",
        "predict = pd.concat([data_test,ch1['pred'], y_test_p['y_test']],axis = 1)"
      ],
      "metadata": {
        "id": "CzzM4S3tXQxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict['compare'] = np.where(predict['pred']==predict['y_test'],'yes', 'no')"
      ],
      "metadata": {
        "id": "8fm4TevpXQxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_1 = predict[predict[\"compare\"]=='no']\n",
        "predict_1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "outputId": "833df983-0f21-460a-de46-668f491d18e1",
        "id": "p4E5sLogXQxC"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             0         1         2         3         4         5         6  \\\n",
              "30    0.001909  0.008317  0.014489  0.020412  0.025995  0.031137  0.035801   \n",
              "46    0.096114  0.095362  0.095047  0.095447  0.096676  0.098677  0.101247   \n",
              "54   -0.041972 -0.048778 -0.055266 -0.061636 -0.067728 -0.073065 -0.076978   \n",
              "55    0.050686  0.056807  0.061922  0.066330  0.070463  0.074831  0.079964   \n",
              "71   -0.511413 -0.507140 -0.469849 -0.408462 -0.333769 -0.256404 -0.185178   \n",
              "...        ...       ...       ...       ...       ...       ...       ...   \n",
              "5971  0.005315 -0.043084 -0.083200 -0.110473 -0.122973 -0.121490 -0.109050   \n",
              "5981 -0.005281 -0.018280 -0.031669 -0.042939 -0.050464 -0.053648 -0.052817   \n",
              "5992 -0.084734 -0.091664 -0.098196 -0.101411 -0.098148 -0.085744 -0.062829   \n",
              "5994  0.008461  0.011468  0.012707  0.012096  0.009875  0.006558  0.002841   \n",
              "5999  0.052145  0.038833  0.024907  0.010402 -0.004429 -0.019087 -0.032867   \n",
              "\n",
              "             7         8         9  ...       249       250       251  \\\n",
              "30    0.040075  0.044195  0.048538  ...  0.044631  0.050399  0.057432   \n",
              "46    0.104083  0.106844  0.109202  ... -0.004398 -0.008214 -0.013081   \n",
              "54   -0.078796 -0.078050 -0.074630  ... -0.102289 -0.108650 -0.114708   \n",
              "55    0.086349  0.094365  0.104239  ...  0.125449  0.136371  0.147691   \n",
              "71   -0.126038 -0.081724 -0.052058  ...  0.031533  0.034874  0.038518   \n",
              "...        ...       ...       ...  ...       ...       ...       ...   \n",
              "5971 -0.090022 -0.069069 -0.050204  ... -0.027012 -0.029614 -0.031356   \n",
              "5981 -0.048958 -0.043371 -0.037307  ... -0.016826 -0.013496 -0.011275   \n",
              "5992 -0.030004  0.009808  0.051498  ...  0.072150  0.081214  0.090895   \n",
              "5994 -0.000513 -0.002782 -0.003375  ... -0.048723 -0.051678 -0.052151   \n",
              "5999 -0.044991 -0.054774 -0.061789  ... -0.056943 -0.054300 -0.054324   \n",
              "\n",
              "           252       253       254       255   pred  y_test  compare  \n",
              "30    0.066046  0.076371  0.088327  0.101648   85.0    36.0       no  \n",
              "46   -0.019006 -0.025674 -0.032320 -0.037693   83.0    80.0       no  \n",
              "54   -0.119718 -0.122885 -0.123496 -0.121095   20.0    71.0       no  \n",
              "55    0.159697  0.172642  0.186692  0.201860   41.0    50.0       no  \n",
              "71    0.042180  0.045745  0.049241  0.052764    4.0    93.0       no  \n",
              "...        ...       ...       ...       ...    ...     ...      ...  \n",
              "5971 -0.033020 -0.035244 -0.038329 -0.042129   95.0    23.0       no  \n",
              "5981 -0.010594 -0.011830 -0.015208 -0.020697   52.0    40.0       no  \n",
              "5992  0.101388  0.112932  0.125713  0.139775  100.0    83.0       no  \n",
              "5994 -0.050205 -0.046318 -0.041326 -0.036264   95.0    49.0       no  \n",
              "5999 -0.058068 -0.065599 -0.075635 -0.085448   45.0    37.0       no  \n",
              "\n",
              "[545 rows x 259 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-22dffb56-3200-49eb-89c1-97f537806f07\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>249</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "      <th>pred</th>\n",
              "      <th>y_test</th>\n",
              "      <th>compare</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.001909</td>\n",
              "      <td>0.008317</td>\n",
              "      <td>0.014489</td>\n",
              "      <td>0.020412</td>\n",
              "      <td>0.025995</td>\n",
              "      <td>0.031137</td>\n",
              "      <td>0.035801</td>\n",
              "      <td>0.040075</td>\n",
              "      <td>0.044195</td>\n",
              "      <td>0.048538</td>\n",
              "      <td>...</td>\n",
              "      <td>0.044631</td>\n",
              "      <td>0.050399</td>\n",
              "      <td>0.057432</td>\n",
              "      <td>0.066046</td>\n",
              "      <td>0.076371</td>\n",
              "      <td>0.088327</td>\n",
              "      <td>0.101648</td>\n",
              "      <td>85.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>0.096114</td>\n",
              "      <td>0.095362</td>\n",
              "      <td>0.095047</td>\n",
              "      <td>0.095447</td>\n",
              "      <td>0.096676</td>\n",
              "      <td>0.098677</td>\n",
              "      <td>0.101247</td>\n",
              "      <td>0.104083</td>\n",
              "      <td>0.106844</td>\n",
              "      <td>0.109202</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.004398</td>\n",
              "      <td>-0.008214</td>\n",
              "      <td>-0.013081</td>\n",
              "      <td>-0.019006</td>\n",
              "      <td>-0.025674</td>\n",
              "      <td>-0.032320</td>\n",
              "      <td>-0.037693</td>\n",
              "      <td>83.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>-0.041972</td>\n",
              "      <td>-0.048778</td>\n",
              "      <td>-0.055266</td>\n",
              "      <td>-0.061636</td>\n",
              "      <td>-0.067728</td>\n",
              "      <td>-0.073065</td>\n",
              "      <td>-0.076978</td>\n",
              "      <td>-0.078796</td>\n",
              "      <td>-0.078050</td>\n",
              "      <td>-0.074630</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.102289</td>\n",
              "      <td>-0.108650</td>\n",
              "      <td>-0.114708</td>\n",
              "      <td>-0.119718</td>\n",
              "      <td>-0.122885</td>\n",
              "      <td>-0.123496</td>\n",
              "      <td>-0.121095</td>\n",
              "      <td>20.0</td>\n",
              "      <td>71.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>0.050686</td>\n",
              "      <td>0.056807</td>\n",
              "      <td>0.061922</td>\n",
              "      <td>0.066330</td>\n",
              "      <td>0.070463</td>\n",
              "      <td>0.074831</td>\n",
              "      <td>0.079964</td>\n",
              "      <td>0.086349</td>\n",
              "      <td>0.094365</td>\n",
              "      <td>0.104239</td>\n",
              "      <td>...</td>\n",
              "      <td>0.125449</td>\n",
              "      <td>0.136371</td>\n",
              "      <td>0.147691</td>\n",
              "      <td>0.159697</td>\n",
              "      <td>0.172642</td>\n",
              "      <td>0.186692</td>\n",
              "      <td>0.201860</td>\n",
              "      <td>41.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>-0.511413</td>\n",
              "      <td>-0.507140</td>\n",
              "      <td>-0.469849</td>\n",
              "      <td>-0.408462</td>\n",
              "      <td>-0.333769</td>\n",
              "      <td>-0.256404</td>\n",
              "      <td>-0.185178</td>\n",
              "      <td>-0.126038</td>\n",
              "      <td>-0.081724</td>\n",
              "      <td>-0.052058</td>\n",
              "      <td>...</td>\n",
              "      <td>0.031533</td>\n",
              "      <td>0.034874</td>\n",
              "      <td>0.038518</td>\n",
              "      <td>0.042180</td>\n",
              "      <td>0.045745</td>\n",
              "      <td>0.049241</td>\n",
              "      <td>0.052764</td>\n",
              "      <td>4.0</td>\n",
              "      <td>93.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5971</th>\n",
              "      <td>0.005315</td>\n",
              "      <td>-0.043084</td>\n",
              "      <td>-0.083200</td>\n",
              "      <td>-0.110473</td>\n",
              "      <td>-0.122973</td>\n",
              "      <td>-0.121490</td>\n",
              "      <td>-0.109050</td>\n",
              "      <td>-0.090022</td>\n",
              "      <td>-0.069069</td>\n",
              "      <td>-0.050204</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.027012</td>\n",
              "      <td>-0.029614</td>\n",
              "      <td>-0.031356</td>\n",
              "      <td>-0.033020</td>\n",
              "      <td>-0.035244</td>\n",
              "      <td>-0.038329</td>\n",
              "      <td>-0.042129</td>\n",
              "      <td>95.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5981</th>\n",
              "      <td>-0.005281</td>\n",
              "      <td>-0.018280</td>\n",
              "      <td>-0.031669</td>\n",
              "      <td>-0.042939</td>\n",
              "      <td>-0.050464</td>\n",
              "      <td>-0.053648</td>\n",
              "      <td>-0.052817</td>\n",
              "      <td>-0.048958</td>\n",
              "      <td>-0.043371</td>\n",
              "      <td>-0.037307</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.016826</td>\n",
              "      <td>-0.013496</td>\n",
              "      <td>-0.011275</td>\n",
              "      <td>-0.010594</td>\n",
              "      <td>-0.011830</td>\n",
              "      <td>-0.015208</td>\n",
              "      <td>-0.020697</td>\n",
              "      <td>52.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5992</th>\n",
              "      <td>-0.084734</td>\n",
              "      <td>-0.091664</td>\n",
              "      <td>-0.098196</td>\n",
              "      <td>-0.101411</td>\n",
              "      <td>-0.098148</td>\n",
              "      <td>-0.085744</td>\n",
              "      <td>-0.062829</td>\n",
              "      <td>-0.030004</td>\n",
              "      <td>0.009808</td>\n",
              "      <td>0.051498</td>\n",
              "      <td>...</td>\n",
              "      <td>0.072150</td>\n",
              "      <td>0.081214</td>\n",
              "      <td>0.090895</td>\n",
              "      <td>0.101388</td>\n",
              "      <td>0.112932</td>\n",
              "      <td>0.125713</td>\n",
              "      <td>0.139775</td>\n",
              "      <td>100.0</td>\n",
              "      <td>83.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5994</th>\n",
              "      <td>0.008461</td>\n",
              "      <td>0.011468</td>\n",
              "      <td>0.012707</td>\n",
              "      <td>0.012096</td>\n",
              "      <td>0.009875</td>\n",
              "      <td>0.006558</td>\n",
              "      <td>0.002841</td>\n",
              "      <td>-0.000513</td>\n",
              "      <td>-0.002782</td>\n",
              "      <td>-0.003375</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.048723</td>\n",
              "      <td>-0.051678</td>\n",
              "      <td>-0.052151</td>\n",
              "      <td>-0.050205</td>\n",
              "      <td>-0.046318</td>\n",
              "      <td>-0.041326</td>\n",
              "      <td>-0.036264</td>\n",
              "      <td>95.0</td>\n",
              "      <td>49.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5999</th>\n",
              "      <td>0.052145</td>\n",
              "      <td>0.038833</td>\n",
              "      <td>0.024907</td>\n",
              "      <td>0.010402</td>\n",
              "      <td>-0.004429</td>\n",
              "      <td>-0.019087</td>\n",
              "      <td>-0.032867</td>\n",
              "      <td>-0.044991</td>\n",
              "      <td>-0.054774</td>\n",
              "      <td>-0.061789</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.056943</td>\n",
              "      <td>-0.054300</td>\n",
              "      <td>-0.054324</td>\n",
              "      <td>-0.058068</td>\n",
              "      <td>-0.065599</td>\n",
              "      <td>-0.075635</td>\n",
              "      <td>-0.085448</td>\n",
              "      <td>45.0</td>\n",
              "      <td>37.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>545 rows × 259 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-22dffb56-3200-49eb-89c1-97f537806f07')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-22dffb56-3200-49eb-89c1-97f537806f07 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-22dffb56-3200-49eb-89c1-97f537806f07');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 163
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.lowpass"
      ],
      "metadata": {
        "id": "TxQgyw2cXV0x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8dHwUuZXV0x"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import itertools\n",
        "import time\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import (CosineAnnealingLR,\n",
        "                                      CosineAnnealingWarmRestarts,\n",
        "                                      StepLR,\n",
        "                                      ExponentialLR)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import accuracy_score, auc, f1_score, precision_score, recall_score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0r_aewQMXV0y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm, re, sys\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "from keras.layers import Conv2D, Conv1D, MaxPooling2D, MaxPooling1D, Flatten, BatchNormalization, Dense\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.callbacks import CSVLogger, ModelCheckpoint"
      ],
      "metadata": {
        "id": "BM5kCFwlXV0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_total = pd.read_csv('/content/drive/MyDrive/model_2s/total_band.csv', header=None)"
      ],
      "metadata": {
        "id": "xKiM2m7xXV0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_total"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "outputId": "663239a1-1b07-4904-c8e1-952a25f4da50",
        "id": "FMdrrLQoXV0y"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            0         1         2         3         4         5         6    \\\n",
              "0     -0.031820 -0.015874 -0.001172  0.011249  0.020713  0.026974  0.030190   \n",
              "1     -0.057454 -0.056822 -0.055984 -0.054985 -0.053945 -0.052999 -0.052250   \n",
              "2     -0.054004 -0.050825 -0.048322 -0.046824 -0.046532 -0.047445 -0.049330   \n",
              "3      0.053295  0.053439  0.055066  0.059409  0.067278  0.078961  0.094216   \n",
              "4      0.134007  0.088232  0.042803 -0.000932 -0.041686 -0.078336 -0.110032   \n",
              "...         ...       ...       ...       ...       ...       ...       ...   \n",
              "17995 -0.087605 -0.069234 -0.058444 -0.053914 -0.053427 -0.054525 -0.055053   \n",
              "17996 -0.033347 -0.035287 -0.036169 -0.036014 -0.035070 -0.033744 -0.032503   \n",
              "17997 -0.076867 -0.083002 -0.088128 -0.091454 -0.092582 -0.091623 -0.089189   \n",
              "17998 -0.079288 -0.083358 -0.086464 -0.087995 -0.087513 -0.084880 -0.080336   \n",
              "17999 -0.041533 -0.041870 -0.042836 -0.044782 -0.047871 -0.052004 -0.056828   \n",
              "\n",
              "            7         8         9    ...       503       504       505  \\\n",
              "0      0.030831  0.029559  0.027102  ... -0.072854 -0.070401 -0.067539   \n",
              "1     -0.051733 -0.051414 -0.051213  ... -0.071806 -0.071741 -0.071302   \n",
              "2     -0.051735 -0.054058 -0.055645  ...  0.009824  0.013030  0.016239   \n",
              "3      0.112361  0.132422  0.153321  ...  0.388900  0.382609  0.370062   \n",
              "4     -0.136271 -0.156943 -0.172313  ... -0.113220 -0.141579 -0.164040   \n",
              "...         ...       ...       ...  ...       ...       ...       ...   \n",
              "17995 -0.053525 -0.049262 -0.042344  ... -0.020100 -0.018824 -0.018132   \n",
              "17996 -0.031760 -0.031802 -0.032746  ... -0.060069 -0.061489 -0.061986   \n",
              "17997 -0.086281 -0.084051 -0.083490  ... -0.053679 -0.057318 -0.059515   \n",
              "17998 -0.074488 -0.068211 -0.062467  ... -0.048461 -0.044455 -0.041670   \n",
              "17999 -0.061806 -0.066332 -0.069845  ...  0.136895  0.147996  0.158825   \n",
              "\n",
              "            506       507       508       509       510       511    512  \n",
              "0     -0.064716 -0.062285 -0.060443 -0.059208 -0.058448 -0.057940    1.0  \n",
              "1     -0.070367 -0.068861 -0.066758 -0.064085 -0.060936 -0.057487    1.0  \n",
              "2      0.019380  0.022418  0.025327  0.028062  0.030516  0.032514    1.0  \n",
              "3      0.351265  0.326472  0.296135  0.260876  0.221465  0.178820    1.0  \n",
              "4     -0.180986 -0.193086 -0.201181 -0.206146 -0.208783 -0.209743    1.0  \n",
              "...         ...       ...       ...       ...       ...       ...    ...  \n",
              "17995 -0.018247 -0.019303 -0.021303 -0.024082 -0.027314 -0.030559  100.0  \n",
              "17996 -0.061385 -0.059834 -0.057803 -0.055972 -0.055062 -0.055631  100.0  \n",
              "17997 -0.061043 -0.062554 -0.064528 -0.067243 -0.070758 -0.074902  100.0  \n",
              "17998 -0.040185 -0.039799 -0.040120 -0.040694 -0.041169 -0.041409  100.0  \n",
              "17999  0.169190  0.179084  0.188748  0.198658  0.209439  0.221705  100.0  \n",
              "\n",
              "[18000 rows x 513 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b940805d-3827-4d7f-9e69-c5013a7aab61\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>503</th>\n",
              "      <th>504</th>\n",
              "      <th>505</th>\n",
              "      <th>506</th>\n",
              "      <th>507</th>\n",
              "      <th>508</th>\n",
              "      <th>509</th>\n",
              "      <th>510</th>\n",
              "      <th>511</th>\n",
              "      <th>512</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.031820</td>\n",
              "      <td>-0.015874</td>\n",
              "      <td>-0.001172</td>\n",
              "      <td>0.011249</td>\n",
              "      <td>0.020713</td>\n",
              "      <td>0.026974</td>\n",
              "      <td>0.030190</td>\n",
              "      <td>0.030831</td>\n",
              "      <td>0.029559</td>\n",
              "      <td>0.027102</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.072854</td>\n",
              "      <td>-0.070401</td>\n",
              "      <td>-0.067539</td>\n",
              "      <td>-0.064716</td>\n",
              "      <td>-0.062285</td>\n",
              "      <td>-0.060443</td>\n",
              "      <td>-0.059208</td>\n",
              "      <td>-0.058448</td>\n",
              "      <td>-0.057940</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.057454</td>\n",
              "      <td>-0.056822</td>\n",
              "      <td>-0.055984</td>\n",
              "      <td>-0.054985</td>\n",
              "      <td>-0.053945</td>\n",
              "      <td>-0.052999</td>\n",
              "      <td>-0.052250</td>\n",
              "      <td>-0.051733</td>\n",
              "      <td>-0.051414</td>\n",
              "      <td>-0.051213</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.071806</td>\n",
              "      <td>-0.071741</td>\n",
              "      <td>-0.071302</td>\n",
              "      <td>-0.070367</td>\n",
              "      <td>-0.068861</td>\n",
              "      <td>-0.066758</td>\n",
              "      <td>-0.064085</td>\n",
              "      <td>-0.060936</td>\n",
              "      <td>-0.057487</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.054004</td>\n",
              "      <td>-0.050825</td>\n",
              "      <td>-0.048322</td>\n",
              "      <td>-0.046824</td>\n",
              "      <td>-0.046532</td>\n",
              "      <td>-0.047445</td>\n",
              "      <td>-0.049330</td>\n",
              "      <td>-0.051735</td>\n",
              "      <td>-0.054058</td>\n",
              "      <td>-0.055645</td>\n",
              "      <td>...</td>\n",
              "      <td>0.009824</td>\n",
              "      <td>0.013030</td>\n",
              "      <td>0.016239</td>\n",
              "      <td>0.019380</td>\n",
              "      <td>0.022418</td>\n",
              "      <td>0.025327</td>\n",
              "      <td>0.028062</td>\n",
              "      <td>0.030516</td>\n",
              "      <td>0.032514</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.053295</td>\n",
              "      <td>0.053439</td>\n",
              "      <td>0.055066</td>\n",
              "      <td>0.059409</td>\n",
              "      <td>0.067278</td>\n",
              "      <td>0.078961</td>\n",
              "      <td>0.094216</td>\n",
              "      <td>0.112361</td>\n",
              "      <td>0.132422</td>\n",
              "      <td>0.153321</td>\n",
              "      <td>...</td>\n",
              "      <td>0.388900</td>\n",
              "      <td>0.382609</td>\n",
              "      <td>0.370062</td>\n",
              "      <td>0.351265</td>\n",
              "      <td>0.326472</td>\n",
              "      <td>0.296135</td>\n",
              "      <td>0.260876</td>\n",
              "      <td>0.221465</td>\n",
              "      <td>0.178820</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.134007</td>\n",
              "      <td>0.088232</td>\n",
              "      <td>0.042803</td>\n",
              "      <td>-0.000932</td>\n",
              "      <td>-0.041686</td>\n",
              "      <td>-0.078336</td>\n",
              "      <td>-0.110032</td>\n",
              "      <td>-0.136271</td>\n",
              "      <td>-0.156943</td>\n",
              "      <td>-0.172313</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.113220</td>\n",
              "      <td>-0.141579</td>\n",
              "      <td>-0.164040</td>\n",
              "      <td>-0.180986</td>\n",
              "      <td>-0.193086</td>\n",
              "      <td>-0.201181</td>\n",
              "      <td>-0.206146</td>\n",
              "      <td>-0.208783</td>\n",
              "      <td>-0.209743</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17995</th>\n",
              "      <td>-0.087605</td>\n",
              "      <td>-0.069234</td>\n",
              "      <td>-0.058444</td>\n",
              "      <td>-0.053914</td>\n",
              "      <td>-0.053427</td>\n",
              "      <td>-0.054525</td>\n",
              "      <td>-0.055053</td>\n",
              "      <td>-0.053525</td>\n",
              "      <td>-0.049262</td>\n",
              "      <td>-0.042344</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.020100</td>\n",
              "      <td>-0.018824</td>\n",
              "      <td>-0.018132</td>\n",
              "      <td>-0.018247</td>\n",
              "      <td>-0.019303</td>\n",
              "      <td>-0.021303</td>\n",
              "      <td>-0.024082</td>\n",
              "      <td>-0.027314</td>\n",
              "      <td>-0.030559</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17996</th>\n",
              "      <td>-0.033347</td>\n",
              "      <td>-0.035287</td>\n",
              "      <td>-0.036169</td>\n",
              "      <td>-0.036014</td>\n",
              "      <td>-0.035070</td>\n",
              "      <td>-0.033744</td>\n",
              "      <td>-0.032503</td>\n",
              "      <td>-0.031760</td>\n",
              "      <td>-0.031802</td>\n",
              "      <td>-0.032746</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.060069</td>\n",
              "      <td>-0.061489</td>\n",
              "      <td>-0.061986</td>\n",
              "      <td>-0.061385</td>\n",
              "      <td>-0.059834</td>\n",
              "      <td>-0.057803</td>\n",
              "      <td>-0.055972</td>\n",
              "      <td>-0.055062</td>\n",
              "      <td>-0.055631</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17997</th>\n",
              "      <td>-0.076867</td>\n",
              "      <td>-0.083002</td>\n",
              "      <td>-0.088128</td>\n",
              "      <td>-0.091454</td>\n",
              "      <td>-0.092582</td>\n",
              "      <td>-0.091623</td>\n",
              "      <td>-0.089189</td>\n",
              "      <td>-0.086281</td>\n",
              "      <td>-0.084051</td>\n",
              "      <td>-0.083490</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.053679</td>\n",
              "      <td>-0.057318</td>\n",
              "      <td>-0.059515</td>\n",
              "      <td>-0.061043</td>\n",
              "      <td>-0.062554</td>\n",
              "      <td>-0.064528</td>\n",
              "      <td>-0.067243</td>\n",
              "      <td>-0.070758</td>\n",
              "      <td>-0.074902</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17998</th>\n",
              "      <td>-0.079288</td>\n",
              "      <td>-0.083358</td>\n",
              "      <td>-0.086464</td>\n",
              "      <td>-0.087995</td>\n",
              "      <td>-0.087513</td>\n",
              "      <td>-0.084880</td>\n",
              "      <td>-0.080336</td>\n",
              "      <td>-0.074488</td>\n",
              "      <td>-0.068211</td>\n",
              "      <td>-0.062467</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.048461</td>\n",
              "      <td>-0.044455</td>\n",
              "      <td>-0.041670</td>\n",
              "      <td>-0.040185</td>\n",
              "      <td>-0.039799</td>\n",
              "      <td>-0.040120</td>\n",
              "      <td>-0.040694</td>\n",
              "      <td>-0.041169</td>\n",
              "      <td>-0.041409</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17999</th>\n",
              "      <td>-0.041533</td>\n",
              "      <td>-0.041870</td>\n",
              "      <td>-0.042836</td>\n",
              "      <td>-0.044782</td>\n",
              "      <td>-0.047871</td>\n",
              "      <td>-0.052004</td>\n",
              "      <td>-0.056828</td>\n",
              "      <td>-0.061806</td>\n",
              "      <td>-0.066332</td>\n",
              "      <td>-0.069845</td>\n",
              "      <td>...</td>\n",
              "      <td>0.136895</td>\n",
              "      <td>0.147996</td>\n",
              "      <td>0.158825</td>\n",
              "      <td>0.169190</td>\n",
              "      <td>0.179084</td>\n",
              "      <td>0.188748</td>\n",
              "      <td>0.198658</td>\n",
              "      <td>0.209439</td>\n",
              "      <td>0.221705</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>18000 rows × 513 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b940805d-3827-4d7f-9e69-c5013a7aab61')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b940805d-3827-4d7f-9e69-c5013a7aab61 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b940805d-3827-4d7f-9e69-c5013a7aab61');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터 전체 섞기"
      ],
      "metadata": {
        "id": "emk-yrL-XV0z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_total=data_total.sample(frac=1)"
      ],
      "metadata": {
        "id": "540KXJZLXV0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train=data_total.iloc[0:14400, :]\n",
        "data_val=data_total.iloc[14400:16200, :]\n",
        "data_test=data_total.iloc[16200:18000,:]"
      ],
      "metadata": {
        "id": "48WQbinjXV0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "outputId": "c2fd17df-b966-4239-b14c-c0ba66c4cdb6",
        "id": "25A0urlgXV0z"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            0         1         2         3         4         5         6    \\\n",
              "12271  0.011548  0.008488  0.003629 -0.001850 -0.006585 -0.009308 -0.009111   \n",
              "13301 -0.104636 -0.105467 -0.105061 -0.104101 -0.103201 -0.102838 -0.103294   \n",
              "5332  -0.012883 -0.035286 -0.053552 -0.067500 -0.077205 -0.082961 -0.085256   \n",
              "1880  -0.076507 -0.079781 -0.081961 -0.083390 -0.084599 -0.086177 -0.088622   \n",
              "973   -0.094721 -0.093787 -0.091414 -0.087818 -0.083370 -0.078623 -0.074293   \n",
              "...         ...       ...       ...       ...       ...       ...       ...   \n",
              "7058  -0.084069 -0.052032 -0.027035 -0.011892 -0.006866 -0.010255 -0.019178   \n",
              "3680  -0.044854 -0.041024 -0.038506 -0.037514 -0.038143 -0.040339 -0.043899   \n",
              "16151 -0.037901 -0.043779 -0.049116 -0.053866 -0.058078 -0.061812 -0.065067   \n",
              "4129   0.000499  0.006756  0.012140  0.016098  0.018497  0.019539  0.019597   \n",
              "8631   0.101586  0.096040  0.089434  0.081187  0.071438  0.061105  0.051676   \n",
              "\n",
              "            7         8         9    ...       503       504       505  \\\n",
              "12271 -0.005618  0.000966  0.009941  ... -0.092055 -0.088837 -0.083735   \n",
              "13301 -0.104638 -0.106737 -0.109303  ... -0.434067 -0.397023 -0.336771   \n",
              "5332  -0.084736 -0.082161 -0.078343  ...  0.017269  0.112567  0.214263   \n",
              "1880  -0.092196 -0.096821 -0.102044  ... -0.130756 -0.132104 -0.134025   \n",
              "973   -0.071172 -0.069985 -0.071207  ...  0.006448  0.005379  0.003184   \n",
              "...         ...       ...       ...  ...       ...       ...       ...   \n",
              "7058  -0.030371 -0.040835 -0.048291  ...  0.080720  0.094054  0.106730   \n",
              "3680  -0.048501 -0.053753 -0.059266  ... -0.028767 -0.040588 -0.050093   \n",
              "16151 -0.067736 -0.069620 -0.070469  ... -0.048270 -0.052330 -0.054928   \n",
              "4129   0.019024  0.018025  0.016620  ...  0.106401  0.094250  0.080913   \n",
              "8631   0.044777  0.041656  0.042705  ... -0.090154 -0.091253 -0.093450   \n",
              "\n",
              "            506       507       508       509       510       511   512  \n",
              "12271 -0.079199 -0.078123 -0.083104 -0.095548 -0.114823 -0.137707  69.0  \n",
              "13301 -0.266172 -0.196964 -0.138022 -0.094467 -0.067628 -0.055680  74.0  \n",
              "5332   0.308861  0.382117  0.421823  0.420427  0.376841  0.296926  30.0  \n",
              "1880  -0.136428 -0.139013 -0.141338 -0.142913 -0.143320 -0.142310  11.0  \n",
              "973   -0.000138 -0.004271 -0.008627 -0.012468 -0.015068 -0.015879   6.0  \n",
              "...         ...       ...       ...       ...       ...       ...   ...  \n",
              "7058   0.118500  0.129080  0.138144  0.145345  0.150342  0.152823  40.0  \n",
              "3680  -0.057467 -0.063021 -0.067091 -0.069939 -0.071706 -0.072419  21.0  \n",
              "16151 -0.055776 -0.054865 -0.052461 -0.049072 -0.045366 -0.042066  90.0  \n",
              "4129   0.067193  0.053784  0.041198  0.029706  0.019338  0.009902  23.0  \n",
              "8631  -0.096227 -0.099108 -0.101726 -0.103858 -0.105403 -0.106348  48.0  \n",
              "\n",
              "[14400 rows x 513 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-811b05ca-3eb8-4790-9d03-e90288a8ac06\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>503</th>\n",
              "      <th>504</th>\n",
              "      <th>505</th>\n",
              "      <th>506</th>\n",
              "      <th>507</th>\n",
              "      <th>508</th>\n",
              "      <th>509</th>\n",
              "      <th>510</th>\n",
              "      <th>511</th>\n",
              "      <th>512</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>12271</th>\n",
              "      <td>0.011548</td>\n",
              "      <td>0.008488</td>\n",
              "      <td>0.003629</td>\n",
              "      <td>-0.001850</td>\n",
              "      <td>-0.006585</td>\n",
              "      <td>-0.009308</td>\n",
              "      <td>-0.009111</td>\n",
              "      <td>-0.005618</td>\n",
              "      <td>0.000966</td>\n",
              "      <td>0.009941</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.092055</td>\n",
              "      <td>-0.088837</td>\n",
              "      <td>-0.083735</td>\n",
              "      <td>-0.079199</td>\n",
              "      <td>-0.078123</td>\n",
              "      <td>-0.083104</td>\n",
              "      <td>-0.095548</td>\n",
              "      <td>-0.114823</td>\n",
              "      <td>-0.137707</td>\n",
              "      <td>69.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13301</th>\n",
              "      <td>-0.104636</td>\n",
              "      <td>-0.105467</td>\n",
              "      <td>-0.105061</td>\n",
              "      <td>-0.104101</td>\n",
              "      <td>-0.103201</td>\n",
              "      <td>-0.102838</td>\n",
              "      <td>-0.103294</td>\n",
              "      <td>-0.104638</td>\n",
              "      <td>-0.106737</td>\n",
              "      <td>-0.109303</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.434067</td>\n",
              "      <td>-0.397023</td>\n",
              "      <td>-0.336771</td>\n",
              "      <td>-0.266172</td>\n",
              "      <td>-0.196964</td>\n",
              "      <td>-0.138022</td>\n",
              "      <td>-0.094467</td>\n",
              "      <td>-0.067628</td>\n",
              "      <td>-0.055680</td>\n",
              "      <td>74.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5332</th>\n",
              "      <td>-0.012883</td>\n",
              "      <td>-0.035286</td>\n",
              "      <td>-0.053552</td>\n",
              "      <td>-0.067500</td>\n",
              "      <td>-0.077205</td>\n",
              "      <td>-0.082961</td>\n",
              "      <td>-0.085256</td>\n",
              "      <td>-0.084736</td>\n",
              "      <td>-0.082161</td>\n",
              "      <td>-0.078343</td>\n",
              "      <td>...</td>\n",
              "      <td>0.017269</td>\n",
              "      <td>0.112567</td>\n",
              "      <td>0.214263</td>\n",
              "      <td>0.308861</td>\n",
              "      <td>0.382117</td>\n",
              "      <td>0.421823</td>\n",
              "      <td>0.420427</td>\n",
              "      <td>0.376841</td>\n",
              "      <td>0.296926</td>\n",
              "      <td>30.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1880</th>\n",
              "      <td>-0.076507</td>\n",
              "      <td>-0.079781</td>\n",
              "      <td>-0.081961</td>\n",
              "      <td>-0.083390</td>\n",
              "      <td>-0.084599</td>\n",
              "      <td>-0.086177</td>\n",
              "      <td>-0.088622</td>\n",
              "      <td>-0.092196</td>\n",
              "      <td>-0.096821</td>\n",
              "      <td>-0.102044</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.130756</td>\n",
              "      <td>-0.132104</td>\n",
              "      <td>-0.134025</td>\n",
              "      <td>-0.136428</td>\n",
              "      <td>-0.139013</td>\n",
              "      <td>-0.141338</td>\n",
              "      <td>-0.142913</td>\n",
              "      <td>-0.143320</td>\n",
              "      <td>-0.142310</td>\n",
              "      <td>11.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>973</th>\n",
              "      <td>-0.094721</td>\n",
              "      <td>-0.093787</td>\n",
              "      <td>-0.091414</td>\n",
              "      <td>-0.087818</td>\n",
              "      <td>-0.083370</td>\n",
              "      <td>-0.078623</td>\n",
              "      <td>-0.074293</td>\n",
              "      <td>-0.071172</td>\n",
              "      <td>-0.069985</td>\n",
              "      <td>-0.071207</td>\n",
              "      <td>...</td>\n",
              "      <td>0.006448</td>\n",
              "      <td>0.005379</td>\n",
              "      <td>0.003184</td>\n",
              "      <td>-0.000138</td>\n",
              "      <td>-0.004271</td>\n",
              "      <td>-0.008627</td>\n",
              "      <td>-0.012468</td>\n",
              "      <td>-0.015068</td>\n",
              "      <td>-0.015879</td>\n",
              "      <td>6.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7058</th>\n",
              "      <td>-0.084069</td>\n",
              "      <td>-0.052032</td>\n",
              "      <td>-0.027035</td>\n",
              "      <td>-0.011892</td>\n",
              "      <td>-0.006866</td>\n",
              "      <td>-0.010255</td>\n",
              "      <td>-0.019178</td>\n",
              "      <td>-0.030371</td>\n",
              "      <td>-0.040835</td>\n",
              "      <td>-0.048291</td>\n",
              "      <td>...</td>\n",
              "      <td>0.080720</td>\n",
              "      <td>0.094054</td>\n",
              "      <td>0.106730</td>\n",
              "      <td>0.118500</td>\n",
              "      <td>0.129080</td>\n",
              "      <td>0.138144</td>\n",
              "      <td>0.145345</td>\n",
              "      <td>0.150342</td>\n",
              "      <td>0.152823</td>\n",
              "      <td>40.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3680</th>\n",
              "      <td>-0.044854</td>\n",
              "      <td>-0.041024</td>\n",
              "      <td>-0.038506</td>\n",
              "      <td>-0.037514</td>\n",
              "      <td>-0.038143</td>\n",
              "      <td>-0.040339</td>\n",
              "      <td>-0.043899</td>\n",
              "      <td>-0.048501</td>\n",
              "      <td>-0.053753</td>\n",
              "      <td>-0.059266</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.028767</td>\n",
              "      <td>-0.040588</td>\n",
              "      <td>-0.050093</td>\n",
              "      <td>-0.057467</td>\n",
              "      <td>-0.063021</td>\n",
              "      <td>-0.067091</td>\n",
              "      <td>-0.069939</td>\n",
              "      <td>-0.071706</td>\n",
              "      <td>-0.072419</td>\n",
              "      <td>21.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16151</th>\n",
              "      <td>-0.037901</td>\n",
              "      <td>-0.043779</td>\n",
              "      <td>-0.049116</td>\n",
              "      <td>-0.053866</td>\n",
              "      <td>-0.058078</td>\n",
              "      <td>-0.061812</td>\n",
              "      <td>-0.065067</td>\n",
              "      <td>-0.067736</td>\n",
              "      <td>-0.069620</td>\n",
              "      <td>-0.070469</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.048270</td>\n",
              "      <td>-0.052330</td>\n",
              "      <td>-0.054928</td>\n",
              "      <td>-0.055776</td>\n",
              "      <td>-0.054865</td>\n",
              "      <td>-0.052461</td>\n",
              "      <td>-0.049072</td>\n",
              "      <td>-0.045366</td>\n",
              "      <td>-0.042066</td>\n",
              "      <td>90.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4129</th>\n",
              "      <td>0.000499</td>\n",
              "      <td>0.006756</td>\n",
              "      <td>0.012140</td>\n",
              "      <td>0.016098</td>\n",
              "      <td>0.018497</td>\n",
              "      <td>0.019539</td>\n",
              "      <td>0.019597</td>\n",
              "      <td>0.019024</td>\n",
              "      <td>0.018025</td>\n",
              "      <td>0.016620</td>\n",
              "      <td>...</td>\n",
              "      <td>0.106401</td>\n",
              "      <td>0.094250</td>\n",
              "      <td>0.080913</td>\n",
              "      <td>0.067193</td>\n",
              "      <td>0.053784</td>\n",
              "      <td>0.041198</td>\n",
              "      <td>0.029706</td>\n",
              "      <td>0.019338</td>\n",
              "      <td>0.009902</td>\n",
              "      <td>23.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8631</th>\n",
              "      <td>0.101586</td>\n",
              "      <td>0.096040</td>\n",
              "      <td>0.089434</td>\n",
              "      <td>0.081187</td>\n",
              "      <td>0.071438</td>\n",
              "      <td>0.061105</td>\n",
              "      <td>0.051676</td>\n",
              "      <td>0.044777</td>\n",
              "      <td>0.041656</td>\n",
              "      <td>0.042705</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.090154</td>\n",
              "      <td>-0.091253</td>\n",
              "      <td>-0.093450</td>\n",
              "      <td>-0.096227</td>\n",
              "      <td>-0.099108</td>\n",
              "      <td>-0.101726</td>\n",
              "      <td>-0.103858</td>\n",
              "      <td>-0.105403</td>\n",
              "      <td>-0.106348</td>\n",
              "      <td>48.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>14400 rows × 513 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-811b05ca-3eb8-4790-9d03-e90288a8ac06')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-811b05ca-3eb8-4790-9d03-e90288a8ac06 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-811b05ca-3eb8-4790-9d03-e90288a8ac06');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 전처리"
      ],
      "metadata": {
        "id": "RzBTB5AbXV0z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = to_categorical(data_train[512])\n",
        "y_test = to_categorical(data_test[512])\n",
        "y_val = to_categorical(data_val[512])\n",
        "# y_train = to_categorical(arr)\n",
        "# y_test = to_categorical(arr_t)\n",
        "# y_val = to_categorical(arr_v)\n",
        "y_train[14399]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c45224c3-898a-43f6-d3be-281aa073aef1",
        "id": "KRuRjvVMXV0z"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_train.drop([512], axis=1, inplace=True)\n",
        "data_test.drop([512], axis=1, inplace=True)\n",
        "data_val.drop([512], axis=1, inplace=True)\n",
        "data_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "outputId": "1772220f-1f19-4952-a9ed-4fe2781e3711",
        "id": "x3T9Cl49XV0z"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py:4906: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  return super().drop(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            0         1         2         3         4         5         6    \\\n",
              "12271  0.011548  0.008488  0.003629 -0.001850 -0.006585 -0.009308 -0.009111   \n",
              "13301 -0.104636 -0.105467 -0.105061 -0.104101 -0.103201 -0.102838 -0.103294   \n",
              "5332  -0.012883 -0.035286 -0.053552 -0.067500 -0.077205 -0.082961 -0.085256   \n",
              "1880  -0.076507 -0.079781 -0.081961 -0.083390 -0.084599 -0.086177 -0.088622   \n",
              "973   -0.094721 -0.093787 -0.091414 -0.087818 -0.083370 -0.078623 -0.074293   \n",
              "...         ...       ...       ...       ...       ...       ...       ...   \n",
              "7058  -0.084069 -0.052032 -0.027035 -0.011892 -0.006866 -0.010255 -0.019178   \n",
              "3680  -0.044854 -0.041024 -0.038506 -0.037514 -0.038143 -0.040339 -0.043899   \n",
              "16151 -0.037901 -0.043779 -0.049116 -0.053866 -0.058078 -0.061812 -0.065067   \n",
              "4129   0.000499  0.006756  0.012140  0.016098  0.018497  0.019539  0.019597   \n",
              "8631   0.101586  0.096040  0.089434  0.081187  0.071438  0.061105  0.051676   \n",
              "\n",
              "            7         8         9    ...       502       503       504  \\\n",
              "12271 -0.005618  0.000966  0.009941  ... -0.091861 -0.092055 -0.088837   \n",
              "13301 -0.104638 -0.106737 -0.109303  ... -0.436294 -0.434067 -0.397023   \n",
              "5332  -0.084736 -0.082161 -0.078343  ... -0.061198  0.017269  0.112567   \n",
              "1880  -0.092196 -0.096821 -0.102044  ... -0.129850 -0.130756 -0.132104   \n",
              "973   -0.071172 -0.069985 -0.071207  ...  0.006683  0.006448  0.005379   \n",
              "...         ...       ...       ...  ...       ...       ...       ...   \n",
              "7058  -0.030371 -0.040835 -0.048291  ...  0.066962  0.080720  0.094054   \n",
              "3680  -0.048501 -0.053753 -0.059266  ... -0.014650 -0.028767 -0.040588   \n",
              "16151 -0.067736 -0.069620 -0.070469  ... -0.043265 -0.048270 -0.052330   \n",
              "4129   0.019024  0.018025  0.016620  ...  0.116555  0.106401  0.094250   \n",
              "8631   0.044777  0.041656  0.042705  ... -0.090597 -0.090154 -0.091253   \n",
              "\n",
              "            505       506       507       508       509       510       511  \n",
              "12271 -0.083735 -0.079199 -0.078123 -0.083104 -0.095548 -0.114823 -0.137707  \n",
              "13301 -0.336771 -0.266172 -0.196964 -0.138022 -0.094467 -0.067628 -0.055680  \n",
              "5332   0.214263  0.308861  0.382117  0.421823  0.420427  0.376841  0.296926  \n",
              "1880  -0.134025 -0.136428 -0.139013 -0.141338 -0.142913 -0.143320 -0.142310  \n",
              "973    0.003184 -0.000138 -0.004271 -0.008627 -0.012468 -0.015068 -0.015879  \n",
              "...         ...       ...       ...       ...       ...       ...       ...  \n",
              "7058   0.106730  0.118500  0.129080  0.138144  0.145345  0.150342  0.152823  \n",
              "3680  -0.050093 -0.057467 -0.063021 -0.067091 -0.069939 -0.071706 -0.072419  \n",
              "16151 -0.054928 -0.055776 -0.054865 -0.052461 -0.049072 -0.045366 -0.042066  \n",
              "4129   0.080913  0.067193  0.053784  0.041198  0.029706  0.019338  0.009902  \n",
              "8631  -0.093450 -0.096227 -0.099108 -0.101726 -0.103858 -0.105403 -0.106348  \n",
              "\n",
              "[14400 rows x 512 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ee0b9168-993b-4438-8739-acf2a5eaa756\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>502</th>\n",
              "      <th>503</th>\n",
              "      <th>504</th>\n",
              "      <th>505</th>\n",
              "      <th>506</th>\n",
              "      <th>507</th>\n",
              "      <th>508</th>\n",
              "      <th>509</th>\n",
              "      <th>510</th>\n",
              "      <th>511</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>12271</th>\n",
              "      <td>0.011548</td>\n",
              "      <td>0.008488</td>\n",
              "      <td>0.003629</td>\n",
              "      <td>-0.001850</td>\n",
              "      <td>-0.006585</td>\n",
              "      <td>-0.009308</td>\n",
              "      <td>-0.009111</td>\n",
              "      <td>-0.005618</td>\n",
              "      <td>0.000966</td>\n",
              "      <td>0.009941</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.091861</td>\n",
              "      <td>-0.092055</td>\n",
              "      <td>-0.088837</td>\n",
              "      <td>-0.083735</td>\n",
              "      <td>-0.079199</td>\n",
              "      <td>-0.078123</td>\n",
              "      <td>-0.083104</td>\n",
              "      <td>-0.095548</td>\n",
              "      <td>-0.114823</td>\n",
              "      <td>-0.137707</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13301</th>\n",
              "      <td>-0.104636</td>\n",
              "      <td>-0.105467</td>\n",
              "      <td>-0.105061</td>\n",
              "      <td>-0.104101</td>\n",
              "      <td>-0.103201</td>\n",
              "      <td>-0.102838</td>\n",
              "      <td>-0.103294</td>\n",
              "      <td>-0.104638</td>\n",
              "      <td>-0.106737</td>\n",
              "      <td>-0.109303</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.436294</td>\n",
              "      <td>-0.434067</td>\n",
              "      <td>-0.397023</td>\n",
              "      <td>-0.336771</td>\n",
              "      <td>-0.266172</td>\n",
              "      <td>-0.196964</td>\n",
              "      <td>-0.138022</td>\n",
              "      <td>-0.094467</td>\n",
              "      <td>-0.067628</td>\n",
              "      <td>-0.055680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5332</th>\n",
              "      <td>-0.012883</td>\n",
              "      <td>-0.035286</td>\n",
              "      <td>-0.053552</td>\n",
              "      <td>-0.067500</td>\n",
              "      <td>-0.077205</td>\n",
              "      <td>-0.082961</td>\n",
              "      <td>-0.085256</td>\n",
              "      <td>-0.084736</td>\n",
              "      <td>-0.082161</td>\n",
              "      <td>-0.078343</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.061198</td>\n",
              "      <td>0.017269</td>\n",
              "      <td>0.112567</td>\n",
              "      <td>0.214263</td>\n",
              "      <td>0.308861</td>\n",
              "      <td>0.382117</td>\n",
              "      <td>0.421823</td>\n",
              "      <td>0.420427</td>\n",
              "      <td>0.376841</td>\n",
              "      <td>0.296926</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1880</th>\n",
              "      <td>-0.076507</td>\n",
              "      <td>-0.079781</td>\n",
              "      <td>-0.081961</td>\n",
              "      <td>-0.083390</td>\n",
              "      <td>-0.084599</td>\n",
              "      <td>-0.086177</td>\n",
              "      <td>-0.088622</td>\n",
              "      <td>-0.092196</td>\n",
              "      <td>-0.096821</td>\n",
              "      <td>-0.102044</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.129850</td>\n",
              "      <td>-0.130756</td>\n",
              "      <td>-0.132104</td>\n",
              "      <td>-0.134025</td>\n",
              "      <td>-0.136428</td>\n",
              "      <td>-0.139013</td>\n",
              "      <td>-0.141338</td>\n",
              "      <td>-0.142913</td>\n",
              "      <td>-0.143320</td>\n",
              "      <td>-0.142310</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>973</th>\n",
              "      <td>-0.094721</td>\n",
              "      <td>-0.093787</td>\n",
              "      <td>-0.091414</td>\n",
              "      <td>-0.087818</td>\n",
              "      <td>-0.083370</td>\n",
              "      <td>-0.078623</td>\n",
              "      <td>-0.074293</td>\n",
              "      <td>-0.071172</td>\n",
              "      <td>-0.069985</td>\n",
              "      <td>-0.071207</td>\n",
              "      <td>...</td>\n",
              "      <td>0.006683</td>\n",
              "      <td>0.006448</td>\n",
              "      <td>0.005379</td>\n",
              "      <td>0.003184</td>\n",
              "      <td>-0.000138</td>\n",
              "      <td>-0.004271</td>\n",
              "      <td>-0.008627</td>\n",
              "      <td>-0.012468</td>\n",
              "      <td>-0.015068</td>\n",
              "      <td>-0.015879</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7058</th>\n",
              "      <td>-0.084069</td>\n",
              "      <td>-0.052032</td>\n",
              "      <td>-0.027035</td>\n",
              "      <td>-0.011892</td>\n",
              "      <td>-0.006866</td>\n",
              "      <td>-0.010255</td>\n",
              "      <td>-0.019178</td>\n",
              "      <td>-0.030371</td>\n",
              "      <td>-0.040835</td>\n",
              "      <td>-0.048291</td>\n",
              "      <td>...</td>\n",
              "      <td>0.066962</td>\n",
              "      <td>0.080720</td>\n",
              "      <td>0.094054</td>\n",
              "      <td>0.106730</td>\n",
              "      <td>0.118500</td>\n",
              "      <td>0.129080</td>\n",
              "      <td>0.138144</td>\n",
              "      <td>0.145345</td>\n",
              "      <td>0.150342</td>\n",
              "      <td>0.152823</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3680</th>\n",
              "      <td>-0.044854</td>\n",
              "      <td>-0.041024</td>\n",
              "      <td>-0.038506</td>\n",
              "      <td>-0.037514</td>\n",
              "      <td>-0.038143</td>\n",
              "      <td>-0.040339</td>\n",
              "      <td>-0.043899</td>\n",
              "      <td>-0.048501</td>\n",
              "      <td>-0.053753</td>\n",
              "      <td>-0.059266</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.014650</td>\n",
              "      <td>-0.028767</td>\n",
              "      <td>-0.040588</td>\n",
              "      <td>-0.050093</td>\n",
              "      <td>-0.057467</td>\n",
              "      <td>-0.063021</td>\n",
              "      <td>-0.067091</td>\n",
              "      <td>-0.069939</td>\n",
              "      <td>-0.071706</td>\n",
              "      <td>-0.072419</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16151</th>\n",
              "      <td>-0.037901</td>\n",
              "      <td>-0.043779</td>\n",
              "      <td>-0.049116</td>\n",
              "      <td>-0.053866</td>\n",
              "      <td>-0.058078</td>\n",
              "      <td>-0.061812</td>\n",
              "      <td>-0.065067</td>\n",
              "      <td>-0.067736</td>\n",
              "      <td>-0.069620</td>\n",
              "      <td>-0.070469</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.043265</td>\n",
              "      <td>-0.048270</td>\n",
              "      <td>-0.052330</td>\n",
              "      <td>-0.054928</td>\n",
              "      <td>-0.055776</td>\n",
              "      <td>-0.054865</td>\n",
              "      <td>-0.052461</td>\n",
              "      <td>-0.049072</td>\n",
              "      <td>-0.045366</td>\n",
              "      <td>-0.042066</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4129</th>\n",
              "      <td>0.000499</td>\n",
              "      <td>0.006756</td>\n",
              "      <td>0.012140</td>\n",
              "      <td>0.016098</td>\n",
              "      <td>0.018497</td>\n",
              "      <td>0.019539</td>\n",
              "      <td>0.019597</td>\n",
              "      <td>0.019024</td>\n",
              "      <td>0.018025</td>\n",
              "      <td>0.016620</td>\n",
              "      <td>...</td>\n",
              "      <td>0.116555</td>\n",
              "      <td>0.106401</td>\n",
              "      <td>0.094250</td>\n",
              "      <td>0.080913</td>\n",
              "      <td>0.067193</td>\n",
              "      <td>0.053784</td>\n",
              "      <td>0.041198</td>\n",
              "      <td>0.029706</td>\n",
              "      <td>0.019338</td>\n",
              "      <td>0.009902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8631</th>\n",
              "      <td>0.101586</td>\n",
              "      <td>0.096040</td>\n",
              "      <td>0.089434</td>\n",
              "      <td>0.081187</td>\n",
              "      <td>0.071438</td>\n",
              "      <td>0.061105</td>\n",
              "      <td>0.051676</td>\n",
              "      <td>0.044777</td>\n",
              "      <td>0.041656</td>\n",
              "      <td>0.042705</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.090597</td>\n",
              "      <td>-0.090154</td>\n",
              "      <td>-0.091253</td>\n",
              "      <td>-0.093450</td>\n",
              "      <td>-0.096227</td>\n",
              "      <td>-0.099108</td>\n",
              "      <td>-0.101726</td>\n",
              "      <td>-0.103858</td>\n",
              "      <td>-0.105403</td>\n",
              "      <td>-0.106348</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>14400 rows × 512 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ee0b9168-993b-4438-8739-acf2a5eaa756')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ee0b9168-993b-4438-8739-acf2a5eaa756 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ee0b9168-993b-4438-8739-acf2a5eaa756');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train=data_train.values\n",
        "X_test=data_test.values\n",
        "X_val=data_val.values"
      ],
      "metadata": {
        "id": "yfPWlsBMXV0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(X_val.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)\n",
        "print(y_val.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8498ecd-05e7-43c3-9e69-93651a362b22",
        "id": "UYaKEQ1DXV00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(14400, 512)\n",
            "(1800, 512)\n",
            "(1800, 512)\n",
            "(14400, 101)\n",
            "(1800, 101)\n",
            "(1800, 101)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# X_train = X_train.reshape(1, 256, 24000)\n",
        "# X_test = X_test.reshape(1, 256, 6000)\n",
        "# X_val = X_val.reshape(1, 256, 6000)\n",
        "X_train = X_train.reshape(14400, 512, 1)\n",
        "X_test = X_test.reshape(1800, 512, 1)\n",
        "X_val = X_val.reshape(1800, 512, 1)\n",
        "X_train.shape, X_test.shape, X_val.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac581ace-45b0-4113-df84-ce4e6ebd95f4",
        "id": "o5FvqBVsXV00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((14400, 512, 1), (1800, 512, 1), (1800, 512, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## cnn-lstm 모델"
      ],
      "metadata": {
        "id": "RCFhIPEEXV00"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model():\n",
        "    model = Sequential()\n",
        "    model.add(Conv1D(filters=64, kernel_size=6, activation='relu', \n",
        "                    padding='same', input_shape=(256, 1)))\n",
        "    model.add(BatchNormalization())\n",
        "    \n",
        "    # adding a pooling layer\n",
        "    model.add(MaxPooling1D(pool_size=(3), strides=2, padding='same'))\n",
        "    \n",
        "    model.add(Conv1D(filters=64, kernel_size=6, activation='relu', \n",
        "                    padding='same', input_shape=(256, 1)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling1D(pool_size=(3), strides=2, padding='same'))\n",
        "    \n",
        "    model.add(Conv1D(filters=64, kernel_size=6, activation='relu', \n",
        "                    padding='same', input_shape=(256, 1)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling1D(pool_size=(3), strides=2, padding='same'))\n",
        "    \n",
        "    model.add(Flatten())\n",
        "    #model.add(Dense(64, activation='relu'))\n",
        "    #model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(101, activation='softmax'))\n",
        "    \n",
        "    model.compile(optimizer='adam', loss='mse',metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "UUFsAgvYXV00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model()\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2397946-5a7a-4f1f-e09b-f35229db7ab7",
        "id": "r6dSz0IZXV00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_18\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_17 (Conv1D)          (None, 256, 64)           448       \n",
            "                                                                 \n",
            " batch_normalization_12 (Bat  (None, 256, 64)          256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling1d_17 (MaxPoolin  (None, 128, 64)          0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_18 (Conv1D)          (None, 128, 64)           24640     \n",
            "                                                                 \n",
            " batch_normalization_13 (Bat  (None, 128, 64)          256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling1d_18 (MaxPoolin  (None, 64, 64)           0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_19 (Conv1D)          (None, 64, 64)            24640     \n",
            "                                                                 \n",
            " batch_normalization_14 (Bat  (None, 64, 64)           256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling1d_19 (MaxPoolin  (None, 32, 64)           0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " flatten_13 (Flatten)        (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_24 (Dense)            (None, 100)               204900    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 255,396\n",
            "Trainable params: 255,012\n",
            "Non-trainable params: 384\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## cnn-gru 모델"
      ],
      "metadata": {
        "id": "VutMhYFYXV01"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Input, Embedding, GRU, LSTM, MaxPooling1D, GlobalMaxPool1D, CuDNNGRU\n",
        "from keras.layers import Dropout, Dense, Activation, Flatten, Conv1D, SpatialDropout1D\n",
        "from keras.models import Sequential\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv1D(filters=64, kernel_size=6, activation='relu', padding='same', input_shape=(256, 1)))\n",
        "model.add(SpatialDropout1D(0.2))\n",
        "model.add(MaxPooling1D(4))\n",
        "model.add(CuDNNGRU(64))\n",
        "\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Dense(101, activation='softmax'))\n",
        "model.compile(optimizer='adam', loss='mse',metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33f77877-e599-4ded-8400-373f5cec14bc",
        "id": "rsf18-cjXV01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_13 (Conv1D)          (None, 256, 64)           448       \n",
            "                                                                 \n",
            " spatial_dropout1d_4 (Spatia  (None, 256, 64)          0         \n",
            " lDropout1D)                                                     \n",
            "                                                                 \n",
            " max_pooling1d_13 (MaxPoolin  (None, 64, 64)           0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " cu_dnngru_4 (CuDNNGRU)      (None, 64)                24960     \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 101)               6565      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 31,973\n",
            "Trainable params: 31,973\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## gru모델"
      ],
      "metadata": {
        "id": "_sKS-TQjXV01"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Input, Embedding, GRU, LSTM, SimpleRNN, MaxPooling1D, GlobalMaxPool1D, CuDNNGRU\n",
        "from keras.layers import Dropout, Dense, Activation, Flatten, Conv1D, SpatialDropout1D\n",
        "from keras.models import Sequential\n",
        "\n",
        "model = Sequential()\n",
        "model.add(GRU(units=150, return_sequences=True, input_shape=(512,1))) #shape[1]=열\n",
        "model.add(GRU(units=50, return_sequences=True))\n",
        "model.add(GRU(units=50, return_sequences=True))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(101, activation='softmax'))\n",
        "model.compile(optimizer='adam', loss='mse',metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "473a82ca-7807-4f32-b99b-23a57c8479dc",
        "id": "sy9hFXTmXV01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " gru_3 (GRU)                 (None, 512, 150)          68850     \n",
            "                                                                 \n",
            " gru_4 (GRU)                 (None, 512, 50)           30300     \n",
            "                                                                 \n",
            " gru_5 (GRU)                 (None, 512, 50)           15300     \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 25600)             0         \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 25600)             0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 101)               2585701   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,700,151\n",
            "Trainable params: 2,700,151\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSTM"
      ],
      "metadata": {
        "id": "mWFnRtN3XV01"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(units=100, return_sequences=True, input_shape=(256,1))) #shape[1]=열\n",
        "model.add(LSTM(units=50, return_sequences=True))\n",
        "model.add(LSTM(units=50, return_sequences=True))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Dense(101, activation='softmax'))\n",
        "model.compile(optimizer='adam', loss='mse',metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8055881e-f18e-4941-e7d3-1d853b39d2e3",
        "id": "WoaPIWALXV01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_3 (LSTM)               (None, 256, 100)          40800     \n",
            "                                                                 \n",
            " lstm_4 (LSTM)               (None, 256, 50)           30200     \n",
            "                                                                 \n",
            " lstm_5 (LSTM)               (None, 256, 50)           20200     \n",
            "                                                                 \n",
            " flatten_6 (Flatten)         (None, 12800)             0         \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 12800)             0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 101)               1292901   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,384,101\n",
            "Trainable params: 1,384,101\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RNN모델"
      ],
      "metadata": {
        "id": "PtLUoqNCXV02"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Input, Embedding, GRU, LSTM, SimpleRNN, MaxPooling1D, GlobalMaxPool1D, CuDNNGRU\n",
        "from keras.layers import Dropout, Dense, Activation, Flatten, Conv1D, SpatialDropout1D\n",
        "from keras.models import Sequential\n",
        "\n",
        "model = Sequential()\n",
        "model.add(SimpleRNN(units=100, return_sequences=True, input_shape=(256,1))) #shape[1]=열\n",
        "model.add(SimpleRNN(units=50, return_sequences=True))\n",
        "model.add(SimpleRNN(units=50, return_sequences=True))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(101, activation='softmax'))\n",
        "model.compile(optimizer='adam', loss='mse',metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f56aa04-4c34-4860-fc44-299596d4f96f",
        "id": "QyStIJAcXV02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " simple_rnn_12 (SimpleRNN)   (None, 256, 100)          10200     \n",
            "                                                                 \n",
            " simple_rnn_13 (SimpleRNN)   (None, 256, 50)           7550      \n",
            "                                                                 \n",
            " simple_rnn_14 (SimpleRNN)   (None, 256, 50)           5050      \n",
            "                                                                 \n",
            " flatten_4 (Flatten)         (None, 12800)             0         \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 12800)             0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 101)               1292901   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,315,701\n",
            "Trainable params: 1,315,701\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNN"
      ],
      "metadata": {
        "id": "Px4SXQkkXV02"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model():\n",
        "  model = Sequential()\n",
        "  model.add(Conv1D(256, 8, padding='same', activation='relu', input_shape= (256,1)))\n",
        "  model.add(MaxPooling1D())\n",
        "  model.add(Conv1D(256, 8, padding='same', activation='relu'))\n",
        "  model.add(MaxPooling1D())\n",
        "  model.add(Conv1D(64, 8, padding='same',activation='relu'))\n",
        "  model.add(MaxPooling1D())\n",
        "  model.add(Conv1D(64, 8, padding='same', activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(101, activation='softmax')) # activation='softmax'\n",
        "\n",
        "  model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
        "  model.summary()\n",
        "  return model"
      ],
      "metadata": {
        "id": "jWa5rXSUXV02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델학습/평가"
      ],
      "metadata": {
        "id": "HrTYz_IWXV02"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train, y_train, batch_size =1024, epochs = 400, verbose = 1, validation_data=(X_val, y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bed8c25-51a2-4681-d512-d31ec3ecbf40",
        "id": "Ir5X42DPXV02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/400\n",
            "15/15 [==============================] - 5s 345ms/step - loss: 3.6500e-04 - accuracy: 0.9764 - val_loss: 0.0059 - val_accuracy: 0.6256\n",
            "Epoch 2/400\n",
            "15/15 [==============================] - 5s 344ms/step - loss: 3.2755e-04 - accuracy: 0.9790 - val_loss: 0.0057 - val_accuracy: 0.6456\n",
            "Epoch 3/400\n",
            "15/15 [==============================] - 5s 346ms/step - loss: 3.2645e-04 - accuracy: 0.9788 - val_loss: 0.0056 - val_accuracy: 0.6489\n",
            "Epoch 4/400\n",
            "15/15 [==============================] - 5s 345ms/step - loss: 3.1191e-04 - accuracy: 0.9799 - val_loss: 0.0058 - val_accuracy: 0.6261\n",
            "Epoch 5/400\n",
            "15/15 [==============================] - 5s 345ms/step - loss: 3.0775e-04 - accuracy: 0.9803 - val_loss: 0.0055 - val_accuracy: 0.6494\n",
            "Epoch 6/400\n",
            "15/15 [==============================] - 5s 347ms/step - loss: 2.9940e-04 - accuracy: 0.9806 - val_loss: 0.0056 - val_accuracy: 0.6417\n",
            "Epoch 7/400\n",
            "15/15 [==============================] - 5s 347ms/step - loss: 3.0320e-04 - accuracy: 0.9806 - val_loss: 0.0055 - val_accuracy: 0.6478\n",
            "Epoch 8/400\n",
            "15/15 [==============================] - 5s 347ms/step - loss: 2.9496e-04 - accuracy: 0.9806 - val_loss: 0.0055 - val_accuracy: 0.6522\n",
            "Epoch 9/400\n",
            "15/15 [==============================] - 5s 347ms/step - loss: 2.9578e-04 - accuracy: 0.9806 - val_loss: 0.0055 - val_accuracy: 0.6450\n",
            "Epoch 10/400\n",
            "15/15 [==============================] - 5s 348ms/step - loss: 2.9104e-04 - accuracy: 0.9807 - val_loss: 0.0054 - val_accuracy: 0.6500\n",
            "Epoch 11/400\n",
            "15/15 [==============================] - 5s 348ms/step - loss: 2.8375e-04 - accuracy: 0.9808 - val_loss: 0.0055 - val_accuracy: 0.6439\n",
            "Epoch 12/400\n",
            "15/15 [==============================] - 5s 349ms/step - loss: 3.0592e-04 - accuracy: 0.9798 - val_loss: 0.0055 - val_accuracy: 0.6450\n",
            "Epoch 13/400\n",
            "15/15 [==============================] - 5s 348ms/step - loss: 2.9241e-04 - accuracy: 0.9805 - val_loss: 0.0055 - val_accuracy: 0.6439\n",
            "Epoch 14/400\n",
            "15/15 [==============================] - 5s 349ms/step - loss: 2.8765e-04 - accuracy: 0.9808 - val_loss: 0.0055 - val_accuracy: 0.6444\n",
            "Epoch 15/400\n",
            "15/15 [==============================] - 5s 349ms/step - loss: 2.8281e-04 - accuracy: 0.9808 - val_loss: 0.0055 - val_accuracy: 0.6461\n",
            "Epoch 16/400\n",
            "15/15 [==============================] - 5s 349ms/step - loss: 2.8161e-04 - accuracy: 0.9808 - val_loss: 0.0054 - val_accuracy: 0.6483\n",
            "Epoch 17/400\n",
            "15/15 [==============================] - 5s 349ms/step - loss: 2.9027e-04 - accuracy: 0.9804 - val_loss: 0.0054 - val_accuracy: 0.6533\n",
            "Epoch 18/400\n",
            "15/15 [==============================] - 5s 349ms/step - loss: 2.8186e-04 - accuracy: 0.9806 - val_loss: 0.0054 - val_accuracy: 0.6506\n",
            "Epoch 19/400\n",
            "15/15 [==============================] - 5s 350ms/step - loss: 2.7698e-04 - accuracy: 0.9808 - val_loss: 0.0054 - val_accuracy: 0.6456\n",
            "Epoch 20/400\n",
            "15/15 [==============================] - 5s 349ms/step - loss: 2.7871e-04 - accuracy: 0.9808 - val_loss: 0.0056 - val_accuracy: 0.6333\n",
            "Epoch 21/400\n",
            "15/15 [==============================] - 5s 350ms/step - loss: 2.8132e-04 - accuracy: 0.9808 - val_loss: 0.0055 - val_accuracy: 0.6422\n",
            "Epoch 22/400\n",
            "15/15 [==============================] - 5s 350ms/step - loss: 2.8750e-04 - accuracy: 0.9807 - val_loss: 0.0055 - val_accuracy: 0.6444\n",
            "Epoch 23/400\n",
            "15/15 [==============================] - 5s 349ms/step - loss: 2.8023e-04 - accuracy: 0.9807 - val_loss: 0.0055 - val_accuracy: 0.6411\n",
            "Epoch 24/400\n",
            "15/15 [==============================] - 5s 350ms/step - loss: 2.7979e-04 - accuracy: 0.9808 - val_loss: 0.0053 - val_accuracy: 0.6517\n",
            "Epoch 25/400\n",
            "15/15 [==============================] - 5s 349ms/step - loss: 2.7366e-04 - accuracy: 0.9808 - val_loss: 0.0055 - val_accuracy: 0.6456\n",
            "Epoch 26/400\n",
            "15/15 [==============================] - 5s 351ms/step - loss: 2.7513e-04 - accuracy: 0.9808 - val_loss: 0.0054 - val_accuracy: 0.6456\n",
            "Epoch 27/400\n",
            "15/15 [==============================] - 5s 350ms/step - loss: 2.8515e-04 - accuracy: 0.9807 - val_loss: 0.0054 - val_accuracy: 0.6428\n",
            "Epoch 28/400\n",
            "15/15 [==============================] - 5s 351ms/step - loss: 2.8054e-04 - accuracy: 0.9808 - val_loss: 0.0053 - val_accuracy: 0.6556\n",
            "Epoch 29/400\n",
            "15/15 [==============================] - 5s 350ms/step - loss: 2.7506e-04 - accuracy: 0.9808 - val_loss: 0.0053 - val_accuracy: 0.6533\n",
            "Epoch 30/400\n",
            "15/15 [==============================] - 5s 350ms/step - loss: 2.8443e-04 - accuracy: 0.9803 - val_loss: 0.0053 - val_accuracy: 0.6517\n",
            "Epoch 31/400\n",
            "15/15 [==============================] - 5s 350ms/step - loss: 2.9884e-04 - accuracy: 0.9801 - val_loss: 0.0054 - val_accuracy: 0.6472\n",
            "Epoch 32/400\n",
            "15/15 [==============================] - 5s 350ms/step - loss: 2.8549e-04 - accuracy: 0.9804 - val_loss: 0.0054 - val_accuracy: 0.6456\n",
            "Epoch 33/400\n",
            "15/15 [==============================] - 5s 350ms/step - loss: 2.8177e-04 - accuracy: 0.9807 - val_loss: 0.0053 - val_accuracy: 0.6506\n",
            "Epoch 34/400\n",
            "15/15 [==============================] - 5s 350ms/step - loss: 2.8120e-04 - accuracy: 0.9808 - val_loss: 0.0054 - val_accuracy: 0.6439\n",
            "Epoch 35/400\n",
            "15/15 [==============================] - 5s 350ms/step - loss: 2.7901e-04 - accuracy: 0.9808 - val_loss: 0.0055 - val_accuracy: 0.6367\n",
            "Epoch 36/400\n",
            "15/15 [==============================] - 5s 350ms/step - loss: 3.2143e-04 - accuracy: 0.9790 - val_loss: 0.0065 - val_accuracy: 0.5867\n",
            "Epoch 37/400\n",
            "15/15 [==============================] - 5s 350ms/step - loss: 3.0433e-04 - accuracy: 0.9800 - val_loss: 0.0054 - val_accuracy: 0.6461\n",
            "Epoch 38/400\n",
            "15/15 [==============================] - 5s 350ms/step - loss: 2.9511e-04 - accuracy: 0.9801 - val_loss: 0.0055 - val_accuracy: 0.6422\n",
            "Epoch 39/400\n",
            "15/15 [==============================] - 5s 350ms/step - loss: 2.9122e-04 - accuracy: 0.9808 - val_loss: 0.0056 - val_accuracy: 0.6422\n",
            "Epoch 40/400\n",
            "15/15 [==============================] - 5s 350ms/step - loss: 3.0864e-04 - accuracy: 0.9799 - val_loss: 0.0054 - val_accuracy: 0.6544\n",
            "Epoch 41/400\n",
            "15/15 [==============================] - 5s 351ms/step - loss: 2.9059e-04 - accuracy: 0.9806 - val_loss: 0.0054 - val_accuracy: 0.6544\n",
            "Epoch 42/400\n",
            "15/15 [==============================] - 5s 351ms/step - loss: 2.8788e-04 - accuracy: 0.9807 - val_loss: 0.0054 - val_accuracy: 0.6589\n",
            "Epoch 43/400\n",
            "15/15 [==============================] - 5s 351ms/step - loss: 2.8668e-04 - accuracy: 0.9806 - val_loss: 0.0054 - val_accuracy: 0.6528\n",
            "Epoch 44/400\n",
            "15/15 [==============================] - 5s 350ms/step - loss: 2.7837e-04 - accuracy: 0.9808 - val_loss: 0.0054 - val_accuracy: 0.6472\n",
            "Epoch 45/400\n",
            "15/15 [==============================] - 5s 350ms/step - loss: 2.8078e-04 - accuracy: 0.9808 - val_loss: 0.0055 - val_accuracy: 0.6433\n",
            "Epoch 46/400\n",
            "15/15 [==============================] - 5s 351ms/step - loss: 2.8094e-04 - accuracy: 0.9807 - val_loss: 0.0054 - val_accuracy: 0.6478\n",
            "Epoch 47/400\n",
            "15/15 [==============================] - 5s 350ms/step - loss: 2.8659e-04 - accuracy: 0.9804 - val_loss: 0.0055 - val_accuracy: 0.6483\n",
            "Epoch 48/400\n",
            "15/15 [==============================] - 5s 351ms/step - loss: 2.8030e-04 - accuracy: 0.9807 - val_loss: 0.0053 - val_accuracy: 0.6583\n",
            "Epoch 49/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.7966e-04 - accuracy: 0.9808 - val_loss: 0.0052 - val_accuracy: 0.6600\n",
            "Epoch 50/400\n",
            "15/15 [==============================] - 5s 351ms/step - loss: 2.7658e-04 - accuracy: 0.9808 - val_loss: 0.0055 - val_accuracy: 0.6467\n",
            "Epoch 51/400\n",
            "15/15 [==============================] - 5s 351ms/step - loss: 2.7678e-04 - accuracy: 0.9808 - val_loss: 0.0055 - val_accuracy: 0.6439\n",
            "Epoch 52/400\n",
            "15/15 [==============================] - 5s 351ms/step - loss: 2.9289e-04 - accuracy: 0.9806 - val_loss: 0.0056 - val_accuracy: 0.6311\n",
            "Epoch 53/400\n",
            "15/15 [==============================] - 5s 351ms/step - loss: 2.8887e-04 - accuracy: 0.9806 - val_loss: 0.0055 - val_accuracy: 0.6350\n",
            "Epoch 54/400\n",
            "15/15 [==============================] - 5s 351ms/step - loss: 2.8477e-04 - accuracy: 0.9806 - val_loss: 0.0054 - val_accuracy: 0.6433\n",
            "Epoch 55/400\n",
            "15/15 [==============================] - 5s 351ms/step - loss: 2.8021e-04 - accuracy: 0.9808 - val_loss: 0.0056 - val_accuracy: 0.6283\n",
            "Epoch 56/400\n",
            "15/15 [==============================] - 5s 351ms/step - loss: 2.8570e-04 - accuracy: 0.9806 - val_loss: 0.0055 - val_accuracy: 0.6333\n",
            "Epoch 57/400\n",
            "15/15 [==============================] - 5s 350ms/step - loss: 2.8007e-04 - accuracy: 0.9808 - val_loss: 0.0053 - val_accuracy: 0.6506\n",
            "Epoch 58/400\n",
            "15/15 [==============================] - 5s 351ms/step - loss: 2.7731e-04 - accuracy: 0.9809 - val_loss: 0.0054 - val_accuracy: 0.6444\n",
            "Epoch 59/400\n",
            "15/15 [==============================] - 5s 351ms/step - loss: 2.7479e-04 - accuracy: 0.9808 - val_loss: 0.0053 - val_accuracy: 0.6528\n",
            "Epoch 60/400\n",
            "15/15 [==============================] - 5s 351ms/step - loss: 2.7477e-04 - accuracy: 0.9809 - val_loss: 0.0053 - val_accuracy: 0.6478\n",
            "Epoch 61/400\n",
            "15/15 [==============================] - 5s 351ms/step - loss: 2.7476e-04 - accuracy: 0.9809 - val_loss: 0.0053 - val_accuracy: 0.6467\n",
            "Epoch 62/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.7060e-04 - accuracy: 0.9809 - val_loss: 0.0054 - val_accuracy: 0.6439\n",
            "Epoch 63/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.6939e-04 - accuracy: 0.9810 - val_loss: 0.0053 - val_accuracy: 0.6478\n",
            "Epoch 64/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.7183e-04 - accuracy: 0.9810 - val_loss: 0.0054 - val_accuracy: 0.6444\n",
            "Epoch 65/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.6699e-04 - accuracy: 0.9810 - val_loss: 0.0052 - val_accuracy: 0.6606\n",
            "Epoch 66/400\n",
            "15/15 [==============================] - 5s 354ms/step - loss: 2.6987e-04 - accuracy: 0.9810 - val_loss: 0.0053 - val_accuracy: 0.6539\n",
            "Epoch 67/400\n",
            "15/15 [==============================] - 5s 354ms/step - loss: 2.7422e-04 - accuracy: 0.9806 - val_loss: 0.0053 - val_accuracy: 0.6506\n",
            "Epoch 68/400\n",
            "15/15 [==============================] - 5s 354ms/step - loss: 2.7486e-04 - accuracy: 0.9809 - val_loss: 0.0053 - val_accuracy: 0.6456\n",
            "Epoch 69/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.7112e-04 - accuracy: 0.9810 - val_loss: 0.0052 - val_accuracy: 0.6594\n",
            "Epoch 70/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.6825e-04 - accuracy: 0.9810 - val_loss: 0.0053 - val_accuracy: 0.6506\n",
            "Epoch 71/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.6711e-04 - accuracy: 0.9810 - val_loss: 0.0052 - val_accuracy: 0.6478\n",
            "Epoch 72/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.6527e-04 - accuracy: 0.9810 - val_loss: 0.0054 - val_accuracy: 0.6389\n",
            "Epoch 73/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.6773e-04 - accuracy: 0.9810 - val_loss: 0.0054 - val_accuracy: 0.6422\n",
            "Epoch 74/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.7068e-04 - accuracy: 0.9806 - val_loss: 0.0055 - val_accuracy: 0.6383\n",
            "Epoch 75/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.6995e-04 - accuracy: 0.9810 - val_loss: 0.0056 - val_accuracy: 0.6328\n",
            "Epoch 76/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.7005e-04 - accuracy: 0.9810 - val_loss: 0.0053 - val_accuracy: 0.6589\n",
            "Epoch 77/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.7096e-04 - accuracy: 0.9812 - val_loss: 0.0052 - val_accuracy: 0.6594\n",
            "Epoch 78/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.6357e-04 - accuracy: 0.9813 - val_loss: 0.0054 - val_accuracy: 0.6472\n",
            "Epoch 79/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.7080e-04 - accuracy: 0.9812 - val_loss: 0.0053 - val_accuracy: 0.6550\n",
            "Epoch 80/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.6704e-04 - accuracy: 0.9813 - val_loss: 0.0052 - val_accuracy: 0.6683\n",
            "Epoch 81/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.6346e-04 - accuracy: 0.9814 - val_loss: 0.0054 - val_accuracy: 0.6561\n",
            "Epoch 82/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.6543e-04 - accuracy: 0.9814 - val_loss: 0.0053 - val_accuracy: 0.6506\n",
            "Epoch 83/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.6349e-04 - accuracy: 0.9814 - val_loss: 0.0053 - val_accuracy: 0.6472\n",
            "Epoch 84/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.6230e-04 - accuracy: 0.9814 - val_loss: 0.0054 - val_accuracy: 0.6506\n",
            "Epoch 85/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.6795e-04 - accuracy: 0.9812 - val_loss: 0.0054 - val_accuracy: 0.6428\n",
            "Epoch 86/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.6630e-04 - accuracy: 0.9812 - val_loss: 0.0056 - val_accuracy: 0.6278\n",
            "Epoch 87/400\n",
            "15/15 [==============================] - 5s 354ms/step - loss: 2.8824e-04 - accuracy: 0.9810 - val_loss: 0.0054 - val_accuracy: 0.6439\n",
            "Epoch 88/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.7362e-04 - accuracy: 0.9813 - val_loss: 0.0055 - val_accuracy: 0.6417\n",
            "Epoch 89/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.8008e-04 - accuracy: 0.9811 - val_loss: 0.0053 - val_accuracy: 0.6544\n",
            "Epoch 90/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.7847e-04 - accuracy: 0.9812 - val_loss: 0.0053 - val_accuracy: 0.6606\n",
            "Epoch 91/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.9249e-04 - accuracy: 0.9805 - val_loss: 0.0053 - val_accuracy: 0.6600\n",
            "Epoch 92/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.8991e-04 - accuracy: 0.9809 - val_loss: 0.0054 - val_accuracy: 0.6511\n",
            "Epoch 93/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.9592e-04 - accuracy: 0.9806 - val_loss: 0.0055 - val_accuracy: 0.6422\n",
            "Epoch 94/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.9485e-04 - accuracy: 0.9810 - val_loss: 0.0055 - val_accuracy: 0.6467\n",
            "Epoch 95/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.8384e-04 - accuracy: 0.9815 - val_loss: 0.0055 - val_accuracy: 0.6467\n",
            "Epoch 96/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.7512e-04 - accuracy: 0.9817 - val_loss: 0.0055 - val_accuracy: 0.6389\n",
            "Epoch 97/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.7650e-04 - accuracy: 0.9816 - val_loss: 0.0057 - val_accuracy: 0.6261\n",
            "Epoch 98/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.8467e-04 - accuracy: 0.9813 - val_loss: 0.0057 - val_accuracy: 0.6328\n",
            "Epoch 99/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.6872e-04 - accuracy: 0.9817 - val_loss: 0.0054 - val_accuracy: 0.6544\n",
            "Epoch 100/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.7875e-04 - accuracy: 0.9813 - val_loss: 0.0054 - val_accuracy: 0.6506\n",
            "Epoch 101/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.7786e-04 - accuracy: 0.9815 - val_loss: 0.0055 - val_accuracy: 0.6450\n",
            "Epoch 102/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.7274e-04 - accuracy: 0.9817 - val_loss: 0.0053 - val_accuracy: 0.6578\n",
            "Epoch 103/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.6225e-04 - accuracy: 0.9820 - val_loss: 0.0053 - val_accuracy: 0.6572\n",
            "Epoch 104/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.6161e-04 - accuracy: 0.9819 - val_loss: 0.0053 - val_accuracy: 0.6500\n",
            "Epoch 105/400\n",
            "15/15 [==============================] - 5s 351ms/step - loss: 2.5987e-04 - accuracy: 0.9821 - val_loss: 0.0054 - val_accuracy: 0.6544\n",
            "Epoch 106/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.6295e-04 - accuracy: 0.9820 - val_loss: 0.0053 - val_accuracy: 0.6539\n",
            "Epoch 107/400\n",
            "15/15 [==============================] - 5s 351ms/step - loss: 2.5686e-04 - accuracy: 0.9822 - val_loss: 0.0053 - val_accuracy: 0.6511\n",
            "Epoch 108/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.6151e-04 - accuracy: 0.9821 - val_loss: 0.0055 - val_accuracy: 0.6333\n",
            "Epoch 109/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.6060e-04 - accuracy: 0.9822 - val_loss: 0.0053 - val_accuracy: 0.6533\n",
            "Epoch 110/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.5479e-04 - accuracy: 0.9822 - val_loss: 0.0053 - val_accuracy: 0.6494\n",
            "Epoch 111/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.6252e-04 - accuracy: 0.9823 - val_loss: 0.0053 - val_accuracy: 0.6528\n",
            "Epoch 112/400\n",
            "15/15 [==============================] - 5s 351ms/step - loss: 2.5742e-04 - accuracy: 0.9824 - val_loss: 0.0052 - val_accuracy: 0.6556\n",
            "Epoch 113/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.5543e-04 - accuracy: 0.9825 - val_loss: 0.0054 - val_accuracy: 0.6472\n",
            "Epoch 114/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.5553e-04 - accuracy: 0.9827 - val_loss: 0.0051 - val_accuracy: 0.6683\n",
            "Epoch 115/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.5398e-04 - accuracy: 0.9826 - val_loss: 0.0053 - val_accuracy: 0.6567\n",
            "Epoch 116/400\n",
            "15/15 [==============================] - 5s 351ms/step - loss: 2.5609e-04 - accuracy: 0.9826 - val_loss: 0.0053 - val_accuracy: 0.6556\n",
            "Epoch 117/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.5114e-04 - accuracy: 0.9828 - val_loss: 0.0054 - val_accuracy: 0.6417\n",
            "Epoch 118/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.5188e-04 - accuracy: 0.9830 - val_loss: 0.0052 - val_accuracy: 0.6606\n",
            "Epoch 119/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.5064e-04 - accuracy: 0.9829 - val_loss: 0.0052 - val_accuracy: 0.6472\n",
            "Epoch 120/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.4761e-04 - accuracy: 0.9832 - val_loss: 0.0051 - val_accuracy: 0.6650\n",
            "Epoch 121/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.4535e-04 - accuracy: 0.9832 - val_loss: 0.0052 - val_accuracy: 0.6522\n",
            "Epoch 122/400\n",
            "15/15 [==============================] - 5s 354ms/step - loss: 2.4588e-04 - accuracy: 0.9831 - val_loss: 0.0051 - val_accuracy: 0.6622\n",
            "Epoch 123/400\n",
            "15/15 [==============================] - 5s 354ms/step - loss: 2.4479e-04 - accuracy: 0.9831 - val_loss: 0.0051 - val_accuracy: 0.6611\n",
            "Epoch 124/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.4189e-04 - accuracy: 0.9835 - val_loss: 0.0054 - val_accuracy: 0.6417\n",
            "Epoch 125/400\n",
            "15/15 [==============================] - 5s 354ms/step - loss: 2.4870e-04 - accuracy: 0.9835 - val_loss: 0.0053 - val_accuracy: 0.6494\n",
            "Epoch 126/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.5222e-04 - accuracy: 0.9830 - val_loss: 0.0054 - val_accuracy: 0.6522\n",
            "Epoch 127/400\n",
            "15/15 [==============================] - 5s 354ms/step - loss: 2.5680e-04 - accuracy: 0.9829 - val_loss: 0.0056 - val_accuracy: 0.6406\n",
            "Epoch 128/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.5667e-04 - accuracy: 0.9831 - val_loss: 0.0055 - val_accuracy: 0.6400\n",
            "Epoch 129/400\n",
            "15/15 [==============================] - 5s 354ms/step - loss: 3.1398e-04 - accuracy: 0.9800 - val_loss: 0.0056 - val_accuracy: 0.6311\n",
            "Epoch 130/400\n",
            "15/15 [==============================] - 5s 354ms/step - loss: 2.6484e-04 - accuracy: 0.9832 - val_loss: 0.0055 - val_accuracy: 0.6400\n",
            "Epoch 131/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.7764e-04 - accuracy: 0.9824 - val_loss: 0.0053 - val_accuracy: 0.6617\n",
            "Epoch 132/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.5742e-04 - accuracy: 0.9834 - val_loss: 0.0055 - val_accuracy: 0.6533\n",
            "Epoch 133/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.6257e-04 - accuracy: 0.9832 - val_loss: 0.0060 - val_accuracy: 0.6161\n",
            "Epoch 134/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 3.0639e-04 - accuracy: 0.9806 - val_loss: 0.0060 - val_accuracy: 0.6150\n",
            "Epoch 135/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.9741e-04 - accuracy: 0.9812 - val_loss: 0.0056 - val_accuracy: 0.6372\n",
            "Epoch 136/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.7909e-04 - accuracy: 0.9826 - val_loss: 0.0055 - val_accuracy: 0.6483\n",
            "Epoch 137/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.6343e-04 - accuracy: 0.9835 - val_loss: 0.0055 - val_accuracy: 0.6467\n",
            "Epoch 138/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.5466e-04 - accuracy: 0.9840 - val_loss: 0.0056 - val_accuracy: 0.6406\n",
            "Epoch 139/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.5517e-04 - accuracy: 0.9840 - val_loss: 0.0054 - val_accuracy: 0.6522\n",
            "Epoch 140/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.8767e-04 - accuracy: 0.9823 - val_loss: 0.0057 - val_accuracy: 0.6356\n",
            "Epoch 141/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.6534e-04 - accuracy: 0.9835 - val_loss: 0.0058 - val_accuracy: 0.6300\n",
            "Epoch 142/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 0.0021 - accuracy: 0.8621 - val_loss: 0.0073 - val_accuracy: 0.5533\n",
            "Epoch 143/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 7.5434e-04 - accuracy: 0.9516 - val_loss: 0.0061 - val_accuracy: 0.6250\n",
            "Epoch 144/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 3.8690e-04 - accuracy: 0.9763 - val_loss: 0.0056 - val_accuracy: 0.6522\n",
            "Epoch 145/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 3.6015e-04 - accuracy: 0.9776 - val_loss: 0.0060 - val_accuracy: 0.6311\n",
            "Epoch 146/400\n",
            "15/15 [==============================] - 5s 354ms/step - loss: 3.6270e-04 - accuracy: 0.9775 - val_loss: 0.0057 - val_accuracy: 0.6511\n",
            "Epoch 147/400\n",
            "15/15 [==============================] - 5s 354ms/step - loss: 4.0174e-04 - accuracy: 0.9749 - val_loss: 0.0061 - val_accuracy: 0.6278\n",
            "Epoch 148/400\n",
            "15/15 [==============================] - 5s 355ms/step - loss: 2.9882e-04 - accuracy: 0.9821 - val_loss: 0.0061 - val_accuracy: 0.6294\n",
            "Epoch 149/400\n",
            "15/15 [==============================] - 5s 354ms/step - loss: 2.8043e-04 - accuracy: 0.9829 - val_loss: 0.0055 - val_accuracy: 0.6661\n",
            "Epoch 150/400\n",
            "15/15 [==============================] - 5s 354ms/step - loss: 2.6750e-04 - accuracy: 0.9839 - val_loss: 0.0055 - val_accuracy: 0.6661\n",
            "Epoch 151/400\n",
            "15/15 [==============================] - 5s 354ms/step - loss: 2.5998e-04 - accuracy: 0.9844 - val_loss: 0.0054 - val_accuracy: 0.6656\n",
            "Epoch 152/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.5788e-04 - accuracy: 0.9844 - val_loss: 0.0054 - val_accuracy: 0.6683\n",
            "Epoch 153/400\n",
            "15/15 [==============================] - 5s 354ms/step - loss: 2.5052e-04 - accuracy: 0.9847 - val_loss: 0.0053 - val_accuracy: 0.6744\n",
            "Epoch 154/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.5058e-04 - accuracy: 0.9848 - val_loss: 0.0053 - val_accuracy: 0.6739\n",
            "Epoch 155/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.4935e-04 - accuracy: 0.9848 - val_loss: 0.0053 - val_accuracy: 0.6706\n",
            "Epoch 156/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.4672e-04 - accuracy: 0.9847 - val_loss: 0.0053 - val_accuracy: 0.6700\n",
            "Epoch 157/400\n",
            "15/15 [==============================] - 5s 351ms/step - loss: 2.4770e-04 - accuracy: 0.9849 - val_loss: 0.0053 - val_accuracy: 0.6644\n",
            "Epoch 158/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.6674e-04 - accuracy: 0.9841 - val_loss: 0.0054 - val_accuracy: 0.6611\n",
            "Epoch 159/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.5887e-04 - accuracy: 0.9844 - val_loss: 0.0054 - val_accuracy: 0.6667\n",
            "Epoch 160/400\n",
            "15/15 [==============================] - 5s 351ms/step - loss: 2.5006e-04 - accuracy: 0.9848 - val_loss: 0.0054 - val_accuracy: 0.6594\n",
            "Epoch 161/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.4677e-04 - accuracy: 0.9848 - val_loss: 0.0055 - val_accuracy: 0.6589\n",
            "Epoch 162/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.4322e-04 - accuracy: 0.9849 - val_loss: 0.0055 - val_accuracy: 0.6583\n",
            "Epoch 163/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.4342e-04 - accuracy: 0.9849 - val_loss: 0.0053 - val_accuracy: 0.6644\n",
            "Epoch 164/400\n",
            "15/15 [==============================] - 5s 351ms/step - loss: 2.6022e-04 - accuracy: 0.9843 - val_loss: 0.0056 - val_accuracy: 0.6567\n",
            "Epoch 165/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.5386e-04 - accuracy: 0.9846 - val_loss: 0.0054 - val_accuracy: 0.6633\n",
            "Epoch 166/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.4956e-04 - accuracy: 0.9849 - val_loss: 0.0053 - val_accuracy: 0.6656\n",
            "Epoch 167/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.4553e-04 - accuracy: 0.9849 - val_loss: 0.0053 - val_accuracy: 0.6689\n",
            "Epoch 168/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.4075e-04 - accuracy: 0.9849 - val_loss: 0.0054 - val_accuracy: 0.6639\n",
            "Epoch 169/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.4746e-04 - accuracy: 0.9848 - val_loss: 0.0053 - val_accuracy: 0.6711\n",
            "Epoch 170/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.4321e-04 - accuracy: 0.9849 - val_loss: 0.0053 - val_accuracy: 0.6678\n",
            "Epoch 171/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.4663e-04 - accuracy: 0.9849 - val_loss: 0.0054 - val_accuracy: 0.6644\n",
            "Epoch 172/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.3997e-04 - accuracy: 0.9849 - val_loss: 0.0054 - val_accuracy: 0.6572\n",
            "Epoch 173/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.4343e-04 - accuracy: 0.9849 - val_loss: 0.0054 - val_accuracy: 0.6606\n",
            "Epoch 174/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.4889e-04 - accuracy: 0.9847 - val_loss: 0.0055 - val_accuracy: 0.6594\n",
            "Epoch 175/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.4380e-04 - accuracy: 0.9849 - val_loss: 0.0054 - val_accuracy: 0.6678\n",
            "Epoch 176/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.4487e-04 - accuracy: 0.9848 - val_loss: 0.0053 - val_accuracy: 0.6667\n",
            "Epoch 177/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.4865e-04 - accuracy: 0.9849 - val_loss: 0.0054 - val_accuracy: 0.6672\n",
            "Epoch 178/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.4660e-04 - accuracy: 0.9847 - val_loss: 0.0053 - val_accuracy: 0.6683\n",
            "Epoch 179/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.4698e-04 - accuracy: 0.9850 - val_loss: 0.0053 - val_accuracy: 0.6650\n",
            "Epoch 180/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.5098e-04 - accuracy: 0.9849 - val_loss: 0.0056 - val_accuracy: 0.6461\n",
            "Epoch 181/400\n",
            "15/15 [==============================] - 5s 354ms/step - loss: 2.4998e-04 - accuracy: 0.9849 - val_loss: 0.0054 - val_accuracy: 0.6667\n",
            "Epoch 182/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.4455e-04 - accuracy: 0.9850 - val_loss: 0.0054 - val_accuracy: 0.6639\n",
            "Epoch 183/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.4270e-04 - accuracy: 0.9851 - val_loss: 0.0054 - val_accuracy: 0.6572\n",
            "Epoch 184/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.5619e-04 - accuracy: 0.9841 - val_loss: 0.0055 - val_accuracy: 0.6606\n",
            "Epoch 185/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.4956e-04 - accuracy: 0.9846 - val_loss: 0.0054 - val_accuracy: 0.6517\n",
            "Epoch 186/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.4524e-04 - accuracy: 0.9850 - val_loss: 0.0054 - val_accuracy: 0.6533\n",
            "Epoch 187/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.3764e-04 - accuracy: 0.9851 - val_loss: 0.0055 - val_accuracy: 0.6594\n",
            "Epoch 188/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.3863e-04 - accuracy: 0.9851 - val_loss: 0.0055 - val_accuracy: 0.6483\n",
            "Epoch 189/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.3734e-04 - accuracy: 0.9850 - val_loss: 0.0055 - val_accuracy: 0.6550\n",
            "Epoch 190/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.3667e-04 - accuracy: 0.9851 - val_loss: 0.0056 - val_accuracy: 0.6522\n",
            "Epoch 191/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.3656e-04 - accuracy: 0.9851 - val_loss: 0.0054 - val_accuracy: 0.6583\n",
            "Epoch 192/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.3981e-04 - accuracy: 0.9850 - val_loss: 0.0055 - val_accuracy: 0.6561\n",
            "Epoch 193/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.3510e-04 - accuracy: 0.9851 - val_loss: 0.0055 - val_accuracy: 0.6539\n",
            "Epoch 194/400\n",
            "15/15 [==============================] - 5s 354ms/step - loss: 2.3696e-04 - accuracy: 0.9850 - val_loss: 0.0055 - val_accuracy: 0.6456\n",
            "Epoch 195/400\n",
            "15/15 [==============================] - 5s 354ms/step - loss: 2.3294e-04 - accuracy: 0.9851 - val_loss: 0.0056 - val_accuracy: 0.6450\n",
            "Epoch 196/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.3492e-04 - accuracy: 0.9850 - val_loss: 0.0055 - val_accuracy: 0.6456\n",
            "Epoch 197/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.3683e-04 - accuracy: 0.9850 - val_loss: 0.0055 - val_accuracy: 0.6522\n",
            "Epoch 198/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.3970e-04 - accuracy: 0.9849 - val_loss: 0.0054 - val_accuracy: 0.6539\n",
            "Epoch 199/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.3737e-04 - accuracy: 0.9851 - val_loss: 0.0054 - val_accuracy: 0.6567\n",
            "Epoch 200/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.3743e-04 - accuracy: 0.9851 - val_loss: 0.0054 - val_accuracy: 0.6539\n",
            "Epoch 201/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.3664e-04 - accuracy: 0.9851 - val_loss: 0.0056 - val_accuracy: 0.6478\n",
            "Epoch 202/400\n",
            "15/15 [==============================] - 5s 354ms/step - loss: 2.3645e-04 - accuracy: 0.9850 - val_loss: 0.0055 - val_accuracy: 0.6472\n",
            "Epoch 203/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.3746e-04 - accuracy: 0.9851 - val_loss: 0.0055 - val_accuracy: 0.6461\n",
            "Epoch 204/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.3361e-04 - accuracy: 0.9851 - val_loss: 0.0055 - val_accuracy: 0.6472\n",
            "Epoch 205/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.3869e-04 - accuracy: 0.9849 - val_loss: 0.0055 - val_accuracy: 0.6528\n",
            "Epoch 206/400\n",
            "15/15 [==============================] - 5s 354ms/step - loss: 2.3353e-04 - accuracy: 0.9851 - val_loss: 0.0055 - val_accuracy: 0.6467\n",
            "Epoch 207/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.3219e-04 - accuracy: 0.9851 - val_loss: 0.0055 - val_accuracy: 0.6439\n",
            "Epoch 208/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.3497e-04 - accuracy: 0.9851 - val_loss: 0.0055 - val_accuracy: 0.6400\n",
            "Epoch 209/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.3301e-04 - accuracy: 0.9851 - val_loss: 0.0055 - val_accuracy: 0.6456\n",
            "Epoch 210/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.3384e-04 - accuracy: 0.9851 - val_loss: 0.0056 - val_accuracy: 0.6422\n",
            "Epoch 211/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.4175e-04 - accuracy: 0.9844 - val_loss: 0.0056 - val_accuracy: 0.6439\n",
            "Epoch 212/400\n",
            "15/15 [==============================] - 5s 354ms/step - loss: 2.5017e-04 - accuracy: 0.9842 - val_loss: 0.0060 - val_accuracy: 0.6189\n",
            "Epoch 213/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.4424e-04 - accuracy: 0.9846 - val_loss: 0.0057 - val_accuracy: 0.6406\n",
            "Epoch 214/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.3957e-04 - accuracy: 0.9849 - val_loss: 0.0055 - val_accuracy: 0.6500\n",
            "Epoch 215/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.4389e-04 - accuracy: 0.9847 - val_loss: 0.0057 - val_accuracy: 0.6372\n",
            "Epoch 216/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.3604e-04 - accuracy: 0.9849 - val_loss: 0.0054 - val_accuracy: 0.6539\n",
            "Epoch 217/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.3373e-04 - accuracy: 0.9850 - val_loss: 0.0054 - val_accuracy: 0.6567\n",
            "Epoch 218/400\n",
            "15/15 [==============================] - 5s 354ms/step - loss: 2.3350e-04 - accuracy: 0.9850 - val_loss: 0.0055 - val_accuracy: 0.6500\n",
            "Epoch 219/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.3098e-04 - accuracy: 0.9851 - val_loss: 0.0054 - val_accuracy: 0.6511\n",
            "Epoch 220/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.3191e-04 - accuracy: 0.9851 - val_loss: 0.0054 - val_accuracy: 0.6522\n",
            "Epoch 221/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.3293e-04 - accuracy: 0.9851 - val_loss: 0.0054 - val_accuracy: 0.6567\n",
            "Epoch 222/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.2816e-04 - accuracy: 0.9851 - val_loss: 0.0054 - val_accuracy: 0.6517\n",
            "Epoch 223/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.2915e-04 - accuracy: 0.9851 - val_loss: 0.0054 - val_accuracy: 0.6556\n",
            "Epoch 224/400\n",
            "15/15 [==============================] - 5s 354ms/step - loss: 2.2796e-04 - accuracy: 0.9852 - val_loss: 0.0054 - val_accuracy: 0.6500\n",
            "Epoch 225/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.2534e-04 - accuracy: 0.9852 - val_loss: 0.0055 - val_accuracy: 0.6422\n",
            "Epoch 226/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.2724e-04 - accuracy: 0.9852 - val_loss: 0.0055 - val_accuracy: 0.6456\n",
            "Epoch 227/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.2405e-04 - accuracy: 0.9853 - val_loss: 0.0055 - val_accuracy: 0.6478\n",
            "Epoch 228/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.2901e-04 - accuracy: 0.9852 - val_loss: 0.0055 - val_accuracy: 0.6439\n",
            "Epoch 229/400\n",
            "15/15 [==============================] - 5s 354ms/step - loss: 2.2963e-04 - accuracy: 0.9853 - val_loss: 0.0056 - val_accuracy: 0.6428\n",
            "Epoch 230/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.2644e-04 - accuracy: 0.9853 - val_loss: 0.0055 - val_accuracy: 0.6467\n",
            "Epoch 231/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.2491e-04 - accuracy: 0.9853 - val_loss: 0.0056 - val_accuracy: 0.6389\n",
            "Epoch 232/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.2987e-04 - accuracy: 0.9853 - val_loss: 0.0055 - val_accuracy: 0.6433\n",
            "Epoch 233/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.2778e-04 - accuracy: 0.9853 - val_loss: 0.0055 - val_accuracy: 0.6456\n",
            "Epoch 234/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.2439e-04 - accuracy: 0.9852 - val_loss: 0.0054 - val_accuracy: 0.6500\n",
            "Epoch 235/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.2402e-04 - accuracy: 0.9853 - val_loss: 0.0055 - val_accuracy: 0.6472\n",
            "Epoch 236/400\n",
            "15/15 [==============================] - 5s 354ms/step - loss: 2.2976e-04 - accuracy: 0.9851 - val_loss: 0.0054 - val_accuracy: 0.6494\n",
            "Epoch 237/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.2368e-04 - accuracy: 0.9853 - val_loss: 0.0055 - val_accuracy: 0.6433\n",
            "Epoch 238/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.3072e-04 - accuracy: 0.9851 - val_loss: 0.0055 - val_accuracy: 0.6411\n",
            "Epoch 239/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.2937e-04 - accuracy: 0.9853 - val_loss: 0.0054 - val_accuracy: 0.6450\n",
            "Epoch 240/400\n",
            "15/15 [==============================] - 5s 354ms/step - loss: 2.2739e-04 - accuracy: 0.9853 - val_loss: 0.0054 - val_accuracy: 0.6450\n",
            "Epoch 241/400\n",
            "15/15 [==============================] - 5s 354ms/step - loss: 2.2598e-04 - accuracy: 0.9853 - val_loss: 0.0055 - val_accuracy: 0.6394\n",
            "Epoch 242/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.2662e-04 - accuracy: 0.9853 - val_loss: 0.0055 - val_accuracy: 0.6472\n",
            "Epoch 243/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.2461e-04 - accuracy: 0.9853 - val_loss: 0.0055 - val_accuracy: 0.6461\n",
            "Epoch 244/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.2301e-04 - accuracy: 0.9853 - val_loss: 0.0055 - val_accuracy: 0.6411\n",
            "Epoch 245/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.2510e-04 - accuracy: 0.9853 - val_loss: 0.0056 - val_accuracy: 0.6356\n",
            "Epoch 246/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.3090e-04 - accuracy: 0.9853 - val_loss: 0.0056 - val_accuracy: 0.6328\n",
            "Epoch 247/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.2950e-04 - accuracy: 0.9853 - val_loss: 0.0056 - val_accuracy: 0.6439\n",
            "Epoch 248/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.2758e-04 - accuracy: 0.9851 - val_loss: 0.0054 - val_accuracy: 0.6478\n",
            "Epoch 249/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.2565e-04 - accuracy: 0.9853 - val_loss: 0.0054 - val_accuracy: 0.6506\n",
            "Epoch 250/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.2436e-04 - accuracy: 0.9853 - val_loss: 0.0054 - val_accuracy: 0.6478\n",
            "Epoch 251/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.2343e-04 - accuracy: 0.9853 - val_loss: 0.0054 - val_accuracy: 0.6483\n",
            "Epoch 252/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.2154e-04 - accuracy: 0.9853 - val_loss: 0.0054 - val_accuracy: 0.6483\n",
            "Epoch 253/400\n",
            "15/15 [==============================] - 5s 354ms/step - loss: 2.1946e-04 - accuracy: 0.9853 - val_loss: 0.0055 - val_accuracy: 0.6439\n",
            "Epoch 254/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.2015e-04 - accuracy: 0.9853 - val_loss: 0.0055 - val_accuracy: 0.6389\n",
            "Epoch 255/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.3021e-04 - accuracy: 0.9852 - val_loss: 0.0056 - val_accuracy: 0.6406\n",
            "Epoch 256/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.2494e-04 - accuracy: 0.9853 - val_loss: 0.0054 - val_accuracy: 0.6489\n",
            "Epoch 257/400\n",
            "15/15 [==============================] - 5s 354ms/step - loss: 2.2470e-04 - accuracy: 0.9853 - val_loss: 0.0054 - val_accuracy: 0.6467\n",
            "Epoch 258/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.2552e-04 - accuracy: 0.9853 - val_loss: 0.0053 - val_accuracy: 0.6528\n",
            "Epoch 259/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.2132e-04 - accuracy: 0.9853 - val_loss: 0.0055 - val_accuracy: 0.6483\n",
            "Epoch 260/400\n",
            "15/15 [==============================] - 5s 354ms/step - loss: 2.2590e-04 - accuracy: 0.9852 - val_loss: 0.0054 - val_accuracy: 0.6467\n",
            "Epoch 261/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.2537e-04 - accuracy: 0.9853 - val_loss: 0.0054 - val_accuracy: 0.6517\n",
            "Epoch 262/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.2336e-04 - accuracy: 0.9853 - val_loss: 0.0055 - val_accuracy: 0.6428\n",
            "Epoch 263/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.2538e-04 - accuracy: 0.9854 - val_loss: 0.0053 - val_accuracy: 0.6494\n",
            "Epoch 264/400\n",
            "15/15 [==============================] - 5s 354ms/step - loss: 2.2121e-04 - accuracy: 0.9854 - val_loss: 0.0054 - val_accuracy: 0.6422\n",
            "Epoch 265/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.1810e-04 - accuracy: 0.9854 - val_loss: 0.0055 - val_accuracy: 0.6411\n",
            "Epoch 266/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.2214e-04 - accuracy: 0.9853 - val_loss: 0.0055 - val_accuracy: 0.6383\n",
            "Epoch 267/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.2394e-04 - accuracy: 0.9854 - val_loss: 0.0054 - val_accuracy: 0.6472\n",
            "Epoch 268/400\n",
            "15/15 [==============================] - 5s 354ms/step - loss: 2.2124e-04 - accuracy: 0.9854 - val_loss: 0.0053 - val_accuracy: 0.6506\n",
            "Epoch 269/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.1881e-04 - accuracy: 0.9854 - val_loss: 0.0054 - val_accuracy: 0.6472\n",
            "Epoch 270/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.2609e-04 - accuracy: 0.9849 - val_loss: 0.0056 - val_accuracy: 0.6322\n",
            "Epoch 271/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.1806e-04 - accuracy: 0.9853 - val_loss: 0.0055 - val_accuracy: 0.6433\n",
            "Epoch 272/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.1815e-04 - accuracy: 0.9853 - val_loss: 0.0054 - val_accuracy: 0.6439\n",
            "Epoch 273/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.1720e-04 - accuracy: 0.9854 - val_loss: 0.0053 - val_accuracy: 0.6511\n",
            "Epoch 274/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.1911e-04 - accuracy: 0.9854 - val_loss: 0.0054 - val_accuracy: 0.6417\n",
            "Epoch 275/400\n",
            "15/15 [==============================] - 5s 354ms/step - loss: 2.1739e-04 - accuracy: 0.9854 - val_loss: 0.0054 - val_accuracy: 0.6428\n",
            "Epoch 276/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.1700e-04 - accuracy: 0.9854 - val_loss: 0.0055 - val_accuracy: 0.6361\n",
            "Epoch 277/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.1687e-04 - accuracy: 0.9853 - val_loss: 0.0054 - val_accuracy: 0.6478\n",
            "Epoch 278/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.2383e-04 - accuracy: 0.9853 - val_loss: 0.0056 - val_accuracy: 0.6328\n",
            "Epoch 279/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.2037e-04 - accuracy: 0.9854 - val_loss: 0.0054 - val_accuracy: 0.6411\n",
            "Epoch 280/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.2487e-04 - accuracy: 0.9853 - val_loss: 0.0054 - val_accuracy: 0.6422\n",
            "Epoch 281/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.2140e-04 - accuracy: 0.9854 - val_loss: 0.0053 - val_accuracy: 0.6500\n",
            "Epoch 282/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.2172e-04 - accuracy: 0.9855 - val_loss: 0.0055 - val_accuracy: 0.6444\n",
            "Epoch 283/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.2239e-04 - accuracy: 0.9853 - val_loss: 0.0054 - val_accuracy: 0.6461\n",
            "Epoch 284/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.2107e-04 - accuracy: 0.9856 - val_loss: 0.0054 - val_accuracy: 0.6483\n",
            "Epoch 285/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.2051e-04 - accuracy: 0.9856 - val_loss: 0.0054 - val_accuracy: 0.6444\n",
            "Epoch 286/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.1827e-04 - accuracy: 0.9855 - val_loss: 0.0054 - val_accuracy: 0.6406\n",
            "Epoch 287/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.1894e-04 - accuracy: 0.9856 - val_loss: 0.0053 - val_accuracy: 0.6522\n",
            "Epoch 288/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.1290e-04 - accuracy: 0.9856 - val_loss: 0.0054 - val_accuracy: 0.6433\n",
            "Epoch 289/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.1430e-04 - accuracy: 0.9856 - val_loss: 0.0054 - val_accuracy: 0.6456\n",
            "Epoch 290/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.1616e-04 - accuracy: 0.9856 - val_loss: 0.0054 - val_accuracy: 0.6411\n",
            "Epoch 291/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.1808e-04 - accuracy: 0.9856 - val_loss: 0.0053 - val_accuracy: 0.6511\n",
            "Epoch 292/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.1173e-04 - accuracy: 0.9856 - val_loss: 0.0054 - val_accuracy: 0.6411\n",
            "Epoch 293/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.1347e-04 - accuracy: 0.9856 - val_loss: 0.0054 - val_accuracy: 0.6417\n",
            "Epoch 294/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.1175e-04 - accuracy: 0.9856 - val_loss: 0.0054 - val_accuracy: 0.6472\n",
            "Epoch 295/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.1285e-04 - accuracy: 0.9856 - val_loss: 0.0054 - val_accuracy: 0.6444\n",
            "Epoch 296/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.1412e-04 - accuracy: 0.9856 - val_loss: 0.0054 - val_accuracy: 0.6378\n",
            "Epoch 297/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.2189e-04 - accuracy: 0.9855 - val_loss: 0.0055 - val_accuracy: 0.6406\n",
            "Epoch 298/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.2104e-04 - accuracy: 0.9855 - val_loss: 0.0053 - val_accuracy: 0.6517\n",
            "Epoch 299/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.2026e-04 - accuracy: 0.9855 - val_loss: 0.0053 - val_accuracy: 0.6533\n",
            "Epoch 300/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.1696e-04 - accuracy: 0.9856 - val_loss: 0.0054 - val_accuracy: 0.6467\n",
            "Epoch 301/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.1631e-04 - accuracy: 0.9856 - val_loss: 0.0054 - val_accuracy: 0.6467\n",
            "Epoch 302/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.1398e-04 - accuracy: 0.9856 - val_loss: 0.0054 - val_accuracy: 0.6444\n",
            "Epoch 303/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.1627e-04 - accuracy: 0.9855 - val_loss: 0.0056 - val_accuracy: 0.6222\n",
            "Epoch 304/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.1988e-04 - accuracy: 0.9853 - val_loss: 0.0055 - val_accuracy: 0.6372\n",
            "Epoch 305/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.1843e-04 - accuracy: 0.9854 - val_loss: 0.0053 - val_accuracy: 0.6489\n",
            "Epoch 306/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.1795e-04 - accuracy: 0.9854 - val_loss: 0.0055 - val_accuracy: 0.6439\n",
            "Epoch 307/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.2074e-04 - accuracy: 0.9853 - val_loss: 0.0054 - val_accuracy: 0.6456\n",
            "Epoch 308/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.1968e-04 - accuracy: 0.9855 - val_loss: 0.0054 - val_accuracy: 0.6378\n",
            "Epoch 309/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.1717e-04 - accuracy: 0.9856 - val_loss: 0.0054 - val_accuracy: 0.6367\n",
            "Epoch 310/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.1990e-04 - accuracy: 0.9855 - val_loss: 0.0055 - val_accuracy: 0.6339\n",
            "Epoch 311/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.1725e-04 - accuracy: 0.9856 - val_loss: 0.0057 - val_accuracy: 0.6267\n",
            "Epoch 312/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.1814e-04 - accuracy: 0.9855 - val_loss: 0.0055 - val_accuracy: 0.6361\n",
            "Epoch 313/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.1527e-04 - accuracy: 0.9856 - val_loss: 0.0054 - val_accuracy: 0.6400\n",
            "Epoch 314/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.1998e-04 - accuracy: 0.9855 - val_loss: 0.0055 - val_accuracy: 0.6378\n",
            "Epoch 315/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.1321e-04 - accuracy: 0.9856 - val_loss: 0.0056 - val_accuracy: 0.6272\n",
            "Epoch 316/400\n",
            "15/15 [==============================] - 5s 354ms/step - loss: 2.1243e-04 - accuracy: 0.9856 - val_loss: 0.0054 - val_accuracy: 0.6317\n",
            "Epoch 317/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.1949e-04 - accuracy: 0.9855 - val_loss: 0.0056 - val_accuracy: 0.6261\n",
            "Epoch 318/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.1800e-04 - accuracy: 0.9854 - val_loss: 0.0053 - val_accuracy: 0.6461\n",
            "Epoch 319/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.1604e-04 - accuracy: 0.9856 - val_loss: 0.0053 - val_accuracy: 0.6456\n",
            "Epoch 320/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.1452e-04 - accuracy: 0.9856 - val_loss: 0.0054 - val_accuracy: 0.6439\n",
            "Epoch 321/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.1513e-04 - accuracy: 0.9856 - val_loss: 0.0053 - val_accuracy: 0.6511\n",
            "Epoch 322/400\n",
            "15/15 [==============================] - 5s 354ms/step - loss: 2.1578e-04 - accuracy: 0.9855 - val_loss: 0.0055 - val_accuracy: 0.6406\n",
            "Epoch 323/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.1263e-04 - accuracy: 0.9856 - val_loss: 0.0054 - val_accuracy: 0.6472\n",
            "Epoch 324/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.2885e-04 - accuracy: 0.9848 - val_loss: 0.0057 - val_accuracy: 0.6206\n",
            "Epoch 325/400\n",
            "15/15 [==============================] - 5s 354ms/step - loss: 2.2616e-04 - accuracy: 0.9853 - val_loss: 0.0054 - val_accuracy: 0.6450\n",
            "Epoch 326/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.1827e-04 - accuracy: 0.9856 - val_loss: 0.0054 - val_accuracy: 0.6450\n",
            "Epoch 327/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.1388e-04 - accuracy: 0.9857 - val_loss: 0.0055 - val_accuracy: 0.6456\n",
            "Epoch 328/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.1381e-04 - accuracy: 0.9856 - val_loss: 0.0055 - val_accuracy: 0.6372\n",
            "Epoch 329/400\n",
            "15/15 [==============================] - 5s 351ms/step - loss: 2.1409e-04 - accuracy: 0.9858 - val_loss: 0.0053 - val_accuracy: 0.6389\n",
            "Epoch 330/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.1062e-04 - accuracy: 0.9859 - val_loss: 0.0054 - val_accuracy: 0.6439\n",
            "Epoch 331/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.1370e-04 - accuracy: 0.9858 - val_loss: 0.0053 - val_accuracy: 0.6422\n",
            "Epoch 332/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.0908e-04 - accuracy: 0.9859 - val_loss: 0.0054 - val_accuracy: 0.6417\n",
            "Epoch 333/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.0856e-04 - accuracy: 0.9860 - val_loss: 0.0055 - val_accuracy: 0.6317\n",
            "Epoch 334/400\n",
            "15/15 [==============================] - 5s 351ms/step - loss: 2.1121e-04 - accuracy: 0.9858 - val_loss: 0.0054 - val_accuracy: 0.6422\n",
            "Epoch 335/400\n",
            "15/15 [==============================] - 5s 351ms/step - loss: 2.0943e-04 - accuracy: 0.9861 - val_loss: 0.0054 - val_accuracy: 0.6433\n",
            "Epoch 336/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.0581e-04 - accuracy: 0.9862 - val_loss: 0.0054 - val_accuracy: 0.6422\n",
            "Epoch 337/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.0288e-04 - accuracy: 0.9862 - val_loss: 0.0053 - val_accuracy: 0.6450\n",
            "Epoch 338/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.0144e-04 - accuracy: 0.9863 - val_loss: 0.0053 - val_accuracy: 0.6489\n",
            "Epoch 339/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.0208e-04 - accuracy: 0.9863 - val_loss: 0.0054 - val_accuracy: 0.6450\n",
            "Epoch 340/400\n",
            "15/15 [==============================] - 5s 354ms/step - loss: 2.0306e-04 - accuracy: 0.9862 - val_loss: 0.0053 - val_accuracy: 0.6450\n",
            "Epoch 341/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.0313e-04 - accuracy: 0.9862 - val_loss: 0.0053 - val_accuracy: 0.6456\n",
            "Epoch 342/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.0073e-04 - accuracy: 0.9863 - val_loss: 0.0052 - val_accuracy: 0.6494\n",
            "Epoch 343/400\n",
            "15/15 [==============================] - 5s 354ms/step - loss: 2.0620e-04 - accuracy: 0.9862 - val_loss: 0.0055 - val_accuracy: 0.6322\n",
            "Epoch 344/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.0535e-04 - accuracy: 0.9863 - val_loss: 0.0055 - val_accuracy: 0.6439\n",
            "Epoch 345/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.1088e-04 - accuracy: 0.9859 - val_loss: 0.0053 - val_accuracy: 0.6556\n",
            "Epoch 346/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.0345e-04 - accuracy: 0.9863 - val_loss: 0.0055 - val_accuracy: 0.6406\n",
            "Epoch 347/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 2.7737e-04 - accuracy: 0.9824 - val_loss: 0.0057 - val_accuracy: 0.6367\n",
            "Epoch 348/400\n",
            "15/15 [==============================] - 5s 351ms/step - loss: 2.3804e-04 - accuracy: 0.9846 - val_loss: 0.0055 - val_accuracy: 0.6483\n",
            "Epoch 349/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 2.2133e-04 - accuracy: 0.9860 - val_loss: 0.0055 - val_accuracy: 0.6483\n",
            "Epoch 350/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 1.9872e-04 - accuracy: 0.9872 - val_loss: 0.0054 - val_accuracy: 0.6594\n",
            "Epoch 351/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 1.9529e-04 - accuracy: 0.9871 - val_loss: 0.0054 - val_accuracy: 0.6583\n",
            "Epoch 352/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 1.8932e-04 - accuracy: 0.9875 - val_loss: 0.0052 - val_accuracy: 0.6661\n",
            "Epoch 353/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 1.7994e-04 - accuracy: 0.9879 - val_loss: 0.0053 - val_accuracy: 0.6578\n",
            "Epoch 354/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 1.8206e-04 - accuracy: 0.9879 - val_loss: 0.0055 - val_accuracy: 0.6472\n",
            "Epoch 355/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 1.8177e-04 - accuracy: 0.9880 - val_loss: 0.0053 - val_accuracy: 0.6572\n",
            "Epoch 356/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 1.8599e-04 - accuracy: 0.9879 - val_loss: 0.0054 - val_accuracy: 0.6544\n",
            "Epoch 357/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 1.8576e-04 - accuracy: 0.9879 - val_loss: 0.0054 - val_accuracy: 0.6522\n",
            "Epoch 358/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 1.8874e-04 - accuracy: 0.9880 - val_loss: 0.0052 - val_accuracy: 0.6578\n",
            "Epoch 359/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 1.8300e-04 - accuracy: 0.9881 - val_loss: 0.0052 - val_accuracy: 0.6639\n",
            "Epoch 360/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 1.9525e-04 - accuracy: 0.9876 - val_loss: 0.0053 - val_accuracy: 0.6694\n",
            "Epoch 361/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 1.8635e-04 - accuracy: 0.9878 - val_loss: 0.0052 - val_accuracy: 0.6667\n",
            "Epoch 362/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 1.8277e-04 - accuracy: 0.9880 - val_loss: 0.0055 - val_accuracy: 0.6472\n",
            "Epoch 363/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 1.7939e-04 - accuracy: 0.9881 - val_loss: 0.0054 - val_accuracy: 0.6517\n",
            "Epoch 364/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 1.7876e-04 - accuracy: 0.9881 - val_loss: 0.0054 - val_accuracy: 0.6500\n",
            "Epoch 365/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 1.7896e-04 - accuracy: 0.9881 - val_loss: 0.0054 - val_accuracy: 0.6506\n",
            "Epoch 366/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 1.7976e-04 - accuracy: 0.9881 - val_loss: 0.0054 - val_accuracy: 0.6483\n",
            "Epoch 367/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 1.8066e-04 - accuracy: 0.9882 - val_loss: 0.0054 - val_accuracy: 0.6478\n",
            "Epoch 368/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 1.8135e-04 - accuracy: 0.9881 - val_loss: 0.0059 - val_accuracy: 0.6294\n",
            "Epoch 369/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 1.8358e-04 - accuracy: 0.9881 - val_loss: 0.0052 - val_accuracy: 0.6617\n",
            "Epoch 370/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 1.7911e-04 - accuracy: 0.9881 - val_loss: 0.0052 - val_accuracy: 0.6600\n",
            "Epoch 371/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 1.7978e-04 - accuracy: 0.9881 - val_loss: 0.0052 - val_accuracy: 0.6589\n",
            "Epoch 372/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 1.8021e-04 - accuracy: 0.9881 - val_loss: 0.0054 - val_accuracy: 0.6567\n",
            "Epoch 373/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 1.8180e-04 - accuracy: 0.9882 - val_loss: 0.0053 - val_accuracy: 0.6589\n",
            "Epoch 374/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 1.8188e-04 - accuracy: 0.9882 - val_loss: 0.0053 - val_accuracy: 0.6567\n",
            "Epoch 375/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 1.8194e-04 - accuracy: 0.9881 - val_loss: 0.0053 - val_accuracy: 0.6578\n",
            "Epoch 376/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 1.8423e-04 - accuracy: 0.9883 - val_loss: 0.0053 - val_accuracy: 0.6522\n",
            "Epoch 377/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 1.8113e-04 - accuracy: 0.9883 - val_loss: 0.0053 - val_accuracy: 0.6506\n",
            "Epoch 378/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 1.8676e-04 - accuracy: 0.9882 - val_loss: 0.0053 - val_accuracy: 0.6494\n",
            "Epoch 379/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 1.8043e-04 - accuracy: 0.9882 - val_loss: 0.0053 - val_accuracy: 0.6544\n",
            "Epoch 380/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 1.8432e-04 - accuracy: 0.9881 - val_loss: 0.0052 - val_accuracy: 0.6528\n",
            "Epoch 381/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 1.7935e-04 - accuracy: 0.9883 - val_loss: 0.0054 - val_accuracy: 0.6433\n",
            "Epoch 382/400\n",
            "15/15 [==============================] - 5s 354ms/step - loss: 1.7631e-04 - accuracy: 0.9883 - val_loss: 0.0053 - val_accuracy: 0.6528\n",
            "Epoch 383/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 1.8141e-04 - accuracy: 0.9880 - val_loss: 0.0056 - val_accuracy: 0.6322\n",
            "Epoch 384/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 1.8465e-04 - accuracy: 0.9880 - val_loss: 0.0053 - val_accuracy: 0.6539\n",
            "Epoch 385/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 1.7938e-04 - accuracy: 0.9881 - val_loss: 0.0054 - val_accuracy: 0.6494\n",
            "Epoch 386/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 1.8010e-04 - accuracy: 0.9882 - val_loss: 0.0055 - val_accuracy: 0.6356\n",
            "Epoch 387/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 1.9358e-04 - accuracy: 0.9874 - val_loss: 0.0055 - val_accuracy: 0.6428\n",
            "Epoch 388/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 1.8649e-04 - accuracy: 0.9881 - val_loss: 0.0056 - val_accuracy: 0.6417\n",
            "Epoch 389/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 1.8362e-04 - accuracy: 0.9881 - val_loss: 0.0054 - val_accuracy: 0.6550\n",
            "Epoch 390/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 1.8138e-04 - accuracy: 0.9881 - val_loss: 0.0053 - val_accuracy: 0.6617\n",
            "Epoch 391/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 1.7741e-04 - accuracy: 0.9882 - val_loss: 0.0055 - val_accuracy: 0.6411\n",
            "Epoch 392/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 1.7521e-04 - accuracy: 0.9882 - val_loss: 0.0055 - val_accuracy: 0.6433\n",
            "Epoch 393/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 1.7691e-04 - accuracy: 0.9882 - val_loss: 0.0054 - val_accuracy: 0.6461\n",
            "Epoch 394/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 1.7659e-04 - accuracy: 0.9882 - val_loss: 0.0054 - val_accuracy: 0.6433\n",
            "Epoch 395/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 1.7571e-04 - accuracy: 0.9883 - val_loss: 0.0055 - val_accuracy: 0.6461\n",
            "Epoch 396/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 1.7594e-04 - accuracy: 0.9883 - val_loss: 0.0056 - val_accuracy: 0.6367\n",
            "Epoch 397/400\n",
            "15/15 [==============================] - 5s 353ms/step - loss: 1.7663e-04 - accuracy: 0.9883 - val_loss: 0.0053 - val_accuracy: 0.6539\n",
            "Epoch 398/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 1.7482e-04 - accuracy: 0.9882 - val_loss: 0.0054 - val_accuracy: 0.6461\n",
            "Epoch 399/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 1.7136e-04 - accuracy: 0.9883 - val_loss: 0.0054 - val_accuracy: 0.6428\n",
            "Epoch 400/400\n",
            "15/15 [==============================] - 5s 352ms/step - loss: 1.7370e-04 - accuracy: 0.9882 - val_loss: 0.0054 - val_accuracy: 0.6450\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b4f0694-ea77-404b-c830-c09fb231f2ed",
        "id": "ePHatJ5cXV02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "57/57 [==============================] - 1s 17ms/step - loss: 0.0057 - accuracy: 0.6294\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.0056536439806222916, 0.629444420337677]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 그래프 분석"
      ],
      "metadata": {
        "id": "MaqJTsTkXV02"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history=history.history\n",
        "epochs = range(1, len(history['loss']) + 1)\n",
        "acc = history['accuracy']\n",
        "loss = history['loss']\n",
        "val_acc = history['val_accuracy']\n",
        "val_loss = history['val_loss']\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.plot(epochs, acc, label='accuracy')\n",
        "plt.plot(epochs, val_acc, label='val_acc')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.plot(epochs, loss, label='loss', color='g')\n",
        "plt.plot(epochs, val_loss, label='val_loss', color='r')\n",
        "plt.legend()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 701
        },
        "outputId": "e26ade9a-295a-409d-f83d-cfed635b150b",
        "id": "CaZDtVGuXV02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fd57c0320d0>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 예측해보기(predict)"
      ],
      "metadata": {
        "id": "34NWHmQpXV02"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e06685b-4c91-4d7b-804a-f36f83b0daf8",
        "id": "c6EDJOkgXV02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "188/188 [==============================] - 2s 9ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[44,  0,  0, ...,  0,  0,  0],\n",
              "       [ 0, 57,  0, ...,  0,  0,  0],\n",
              "       [ 0,  0, 65, ...,  0,  0,  0],\n",
              "       ...,\n",
              "       [ 0,  0,  0, ..., 68,  0,  0],\n",
              "       [ 0,  0,  0, ...,  0, 67,  0],\n",
              "       [ 0,  0,  0, ...,  0,  0, 57]])"
            ]
          },
          "metadata": {},
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_p=pd.DataFrame(y_test)\n",
        "\n",
        "for i in range(1,101):\n",
        "  y_test_p.loc[y_test_p[i] == 1.0 , 'y_test'] = i  \n",
        "y_test_p.y_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "348ddc8c-b4b0-4b45-9b9b-a83b87cbce28",
        "id": "qG_6w2PdXV02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       59.0\n",
              "1       68.0\n",
              "2       72.0\n",
              "3        2.0\n",
              "4       57.0\n",
              "        ... \n",
              "5995     3.0\n",
              "5996    87.0\n",
              "5997    40.0\n",
              "5998    68.0\n",
              "5999    37.0\n",
              "Name: y_test, Length: 6000, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ch1 = pd.DataFrame(np.rint(y_pred))\n",
        "# for i in range(1,101):\n",
        "#   ch1.loc[ch1[i] == 1.0 , 'pred'] = i \n",
        "\n",
        " \n",
        "#predict = pd.concat([data_test,ch1['pred'], y_test_p['y_test']],axis = 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "35e79746-e41a-4713-b942-733f497e3b6c",
        "id": "dffPCw3yXV03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        0    1    2    3    4    5    6    7    8    9  ...   92   93   94  \\\n",
              "0     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
              "1     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
              "2     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
              "3     0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
              "4     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
              "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
              "5995  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
              "5996  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
              "5997  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
              "5998  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
              "5999  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
              "\n",
              "       95   96   97   98   99  100  pred  \n",
              "0     0.0  0.0  0.0  0.0  0.0  0.0  59.0  \n",
              "1     0.0  0.0  0.0  0.0  0.0  0.0  68.0  \n",
              "2     0.0  0.0  0.0  0.0  0.0  0.0  72.0  \n",
              "3     0.0  0.0  0.0  0.0  0.0  0.0   2.0  \n",
              "4     0.0  0.0  0.0  0.0  0.0  0.0  57.0  \n",
              "...   ...  ...  ...  ...  ...  ...   ...  \n",
              "5995  0.0  0.0  0.0  0.0  0.0  0.0   3.0  \n",
              "5996  0.0  0.0  0.0  0.0  0.0  0.0  87.0  \n",
              "5997  0.0  0.0  0.0  0.0  0.0  0.0  40.0  \n",
              "5998  0.0  0.0  0.0  0.0  0.0  0.0  68.0  \n",
              "5999  0.0  0.0  0.0  0.0  0.0  0.0  45.0  \n",
              "\n",
              "[6000 rows x 102 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d66cc86c-ae23-4a9f-8fb4-dbad40e0c5ea\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "      <th>100</th>\n",
              "      <th>pred</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>59.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>68.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>72.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>57.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5995</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5996</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>87.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5997</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5998</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>68.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5999</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>45.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6000 rows × 102 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d66cc86c-ae23-4a9f-8fb4-dbad40e0c5ea')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d66cc86c-ae23-4a9f-8fb4-dbad40e0c5ea button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d66cc86c-ae23-4a9f-8fb4-dbad40e0c5ea');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ex=np.argmax(y_pred, axis=1)\n",
        "ex=ex.astype('float64')\n",
        "ex=pd.DataFrame(ex)\n",
        "ex"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "22f2ac13-d7ce-409c-db36-bf92c9ca0cad",
        "id": "BENReSJBXV03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         0\n",
              "0     59.0\n",
              "1     68.0\n",
              "2     72.0\n",
              "3      2.0\n",
              "4     57.0\n",
              "...    ...\n",
              "5995   3.0\n",
              "5996  87.0\n",
              "5997  40.0\n",
              "5998  68.0\n",
              "5999  45.0\n",
              "\n",
              "[6000 rows x 1 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-33a2dfcf-6ed7-497f-9b45-70f7cc7db973\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>59.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>68.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>72.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>57.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5995</th>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5996</th>\n",
              "      <td>87.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5997</th>\n",
              "      <td>40.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5998</th>\n",
              "      <td>68.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5999</th>\n",
              "      <td>45.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6000 rows × 1 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-33a2dfcf-6ed7-497f-9b45-70f7cc7db973')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-33a2dfcf-6ed7-497f-9b45-70f7cc7db973 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-33a2dfcf-6ed7-497f-9b45-70f7cc7db973');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ch1 = pd.DataFrame(np.rint(y_pred))\n",
        "ch1['pred'] = ex\n",
        "data_test.reset_index(inplace=True, drop=True)\n",
        "predict = pd.concat([data_test,ch1['pred'], y_test_p['y_test']],axis = 1)"
      ],
      "metadata": {
        "id": "ugL7NU5bXV03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict['compare'] = np.where(predict['pred']==predict['y_test'],'yes', 'no')"
      ],
      "metadata": {
        "id": "0EEBQqpWXV04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_1 = predict[predict[\"compare\"]=='no']\n",
        "predict_1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "outputId": "833df983-0f21-460a-de46-668f491d18e1",
        "id": "tNSp1VXMXV04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             0         1         2         3         4         5         6  \\\n",
              "30    0.001909  0.008317  0.014489  0.020412  0.025995  0.031137  0.035801   \n",
              "46    0.096114  0.095362  0.095047  0.095447  0.096676  0.098677  0.101247   \n",
              "54   -0.041972 -0.048778 -0.055266 -0.061636 -0.067728 -0.073065 -0.076978   \n",
              "55    0.050686  0.056807  0.061922  0.066330  0.070463  0.074831  0.079964   \n",
              "71   -0.511413 -0.507140 -0.469849 -0.408462 -0.333769 -0.256404 -0.185178   \n",
              "...        ...       ...       ...       ...       ...       ...       ...   \n",
              "5971  0.005315 -0.043084 -0.083200 -0.110473 -0.122973 -0.121490 -0.109050   \n",
              "5981 -0.005281 -0.018280 -0.031669 -0.042939 -0.050464 -0.053648 -0.052817   \n",
              "5992 -0.084734 -0.091664 -0.098196 -0.101411 -0.098148 -0.085744 -0.062829   \n",
              "5994  0.008461  0.011468  0.012707  0.012096  0.009875  0.006558  0.002841   \n",
              "5999  0.052145  0.038833  0.024907  0.010402 -0.004429 -0.019087 -0.032867   \n",
              "\n",
              "             7         8         9  ...       249       250       251  \\\n",
              "30    0.040075  0.044195  0.048538  ...  0.044631  0.050399  0.057432   \n",
              "46    0.104083  0.106844  0.109202  ... -0.004398 -0.008214 -0.013081   \n",
              "54   -0.078796 -0.078050 -0.074630  ... -0.102289 -0.108650 -0.114708   \n",
              "55    0.086349  0.094365  0.104239  ...  0.125449  0.136371  0.147691   \n",
              "71   -0.126038 -0.081724 -0.052058  ...  0.031533  0.034874  0.038518   \n",
              "...        ...       ...       ...  ...       ...       ...       ...   \n",
              "5971 -0.090022 -0.069069 -0.050204  ... -0.027012 -0.029614 -0.031356   \n",
              "5981 -0.048958 -0.043371 -0.037307  ... -0.016826 -0.013496 -0.011275   \n",
              "5992 -0.030004  0.009808  0.051498  ...  0.072150  0.081214  0.090895   \n",
              "5994 -0.000513 -0.002782 -0.003375  ... -0.048723 -0.051678 -0.052151   \n",
              "5999 -0.044991 -0.054774 -0.061789  ... -0.056943 -0.054300 -0.054324   \n",
              "\n",
              "           252       253       254       255   pred  y_test  compare  \n",
              "30    0.066046  0.076371  0.088327  0.101648   85.0    36.0       no  \n",
              "46   -0.019006 -0.025674 -0.032320 -0.037693   83.0    80.0       no  \n",
              "54   -0.119718 -0.122885 -0.123496 -0.121095   20.0    71.0       no  \n",
              "55    0.159697  0.172642  0.186692  0.201860   41.0    50.0       no  \n",
              "71    0.042180  0.045745  0.049241  0.052764    4.0    93.0       no  \n",
              "...        ...       ...       ...       ...    ...     ...      ...  \n",
              "5971 -0.033020 -0.035244 -0.038329 -0.042129   95.0    23.0       no  \n",
              "5981 -0.010594 -0.011830 -0.015208 -0.020697   52.0    40.0       no  \n",
              "5992  0.101388  0.112932  0.125713  0.139775  100.0    83.0       no  \n",
              "5994 -0.050205 -0.046318 -0.041326 -0.036264   95.0    49.0       no  \n",
              "5999 -0.058068 -0.065599 -0.075635 -0.085448   45.0    37.0       no  \n",
              "\n",
              "[545 rows x 259 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-22dffb56-3200-49eb-89c1-97f537806f07\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>249</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "      <th>pred</th>\n",
              "      <th>y_test</th>\n",
              "      <th>compare</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.001909</td>\n",
              "      <td>0.008317</td>\n",
              "      <td>0.014489</td>\n",
              "      <td>0.020412</td>\n",
              "      <td>0.025995</td>\n",
              "      <td>0.031137</td>\n",
              "      <td>0.035801</td>\n",
              "      <td>0.040075</td>\n",
              "      <td>0.044195</td>\n",
              "      <td>0.048538</td>\n",
              "      <td>...</td>\n",
              "      <td>0.044631</td>\n",
              "      <td>0.050399</td>\n",
              "      <td>0.057432</td>\n",
              "      <td>0.066046</td>\n",
              "      <td>0.076371</td>\n",
              "      <td>0.088327</td>\n",
              "      <td>0.101648</td>\n",
              "      <td>85.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>0.096114</td>\n",
              "      <td>0.095362</td>\n",
              "      <td>0.095047</td>\n",
              "      <td>0.095447</td>\n",
              "      <td>0.096676</td>\n",
              "      <td>0.098677</td>\n",
              "      <td>0.101247</td>\n",
              "      <td>0.104083</td>\n",
              "      <td>0.106844</td>\n",
              "      <td>0.109202</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.004398</td>\n",
              "      <td>-0.008214</td>\n",
              "      <td>-0.013081</td>\n",
              "      <td>-0.019006</td>\n",
              "      <td>-0.025674</td>\n",
              "      <td>-0.032320</td>\n",
              "      <td>-0.037693</td>\n",
              "      <td>83.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>-0.041972</td>\n",
              "      <td>-0.048778</td>\n",
              "      <td>-0.055266</td>\n",
              "      <td>-0.061636</td>\n",
              "      <td>-0.067728</td>\n",
              "      <td>-0.073065</td>\n",
              "      <td>-0.076978</td>\n",
              "      <td>-0.078796</td>\n",
              "      <td>-0.078050</td>\n",
              "      <td>-0.074630</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.102289</td>\n",
              "      <td>-0.108650</td>\n",
              "      <td>-0.114708</td>\n",
              "      <td>-0.119718</td>\n",
              "      <td>-0.122885</td>\n",
              "      <td>-0.123496</td>\n",
              "      <td>-0.121095</td>\n",
              "      <td>20.0</td>\n",
              "      <td>71.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>0.050686</td>\n",
              "      <td>0.056807</td>\n",
              "      <td>0.061922</td>\n",
              "      <td>0.066330</td>\n",
              "      <td>0.070463</td>\n",
              "      <td>0.074831</td>\n",
              "      <td>0.079964</td>\n",
              "      <td>0.086349</td>\n",
              "      <td>0.094365</td>\n",
              "      <td>0.104239</td>\n",
              "      <td>...</td>\n",
              "      <td>0.125449</td>\n",
              "      <td>0.136371</td>\n",
              "      <td>0.147691</td>\n",
              "      <td>0.159697</td>\n",
              "      <td>0.172642</td>\n",
              "      <td>0.186692</td>\n",
              "      <td>0.201860</td>\n",
              "      <td>41.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>-0.511413</td>\n",
              "      <td>-0.507140</td>\n",
              "      <td>-0.469849</td>\n",
              "      <td>-0.408462</td>\n",
              "      <td>-0.333769</td>\n",
              "      <td>-0.256404</td>\n",
              "      <td>-0.185178</td>\n",
              "      <td>-0.126038</td>\n",
              "      <td>-0.081724</td>\n",
              "      <td>-0.052058</td>\n",
              "      <td>...</td>\n",
              "      <td>0.031533</td>\n",
              "      <td>0.034874</td>\n",
              "      <td>0.038518</td>\n",
              "      <td>0.042180</td>\n",
              "      <td>0.045745</td>\n",
              "      <td>0.049241</td>\n",
              "      <td>0.052764</td>\n",
              "      <td>4.0</td>\n",
              "      <td>93.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5971</th>\n",
              "      <td>0.005315</td>\n",
              "      <td>-0.043084</td>\n",
              "      <td>-0.083200</td>\n",
              "      <td>-0.110473</td>\n",
              "      <td>-0.122973</td>\n",
              "      <td>-0.121490</td>\n",
              "      <td>-0.109050</td>\n",
              "      <td>-0.090022</td>\n",
              "      <td>-0.069069</td>\n",
              "      <td>-0.050204</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.027012</td>\n",
              "      <td>-0.029614</td>\n",
              "      <td>-0.031356</td>\n",
              "      <td>-0.033020</td>\n",
              "      <td>-0.035244</td>\n",
              "      <td>-0.038329</td>\n",
              "      <td>-0.042129</td>\n",
              "      <td>95.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5981</th>\n",
              "      <td>-0.005281</td>\n",
              "      <td>-0.018280</td>\n",
              "      <td>-0.031669</td>\n",
              "      <td>-0.042939</td>\n",
              "      <td>-0.050464</td>\n",
              "      <td>-0.053648</td>\n",
              "      <td>-0.052817</td>\n",
              "      <td>-0.048958</td>\n",
              "      <td>-0.043371</td>\n",
              "      <td>-0.037307</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.016826</td>\n",
              "      <td>-0.013496</td>\n",
              "      <td>-0.011275</td>\n",
              "      <td>-0.010594</td>\n",
              "      <td>-0.011830</td>\n",
              "      <td>-0.015208</td>\n",
              "      <td>-0.020697</td>\n",
              "      <td>52.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5992</th>\n",
              "      <td>-0.084734</td>\n",
              "      <td>-0.091664</td>\n",
              "      <td>-0.098196</td>\n",
              "      <td>-0.101411</td>\n",
              "      <td>-0.098148</td>\n",
              "      <td>-0.085744</td>\n",
              "      <td>-0.062829</td>\n",
              "      <td>-0.030004</td>\n",
              "      <td>0.009808</td>\n",
              "      <td>0.051498</td>\n",
              "      <td>...</td>\n",
              "      <td>0.072150</td>\n",
              "      <td>0.081214</td>\n",
              "      <td>0.090895</td>\n",
              "      <td>0.101388</td>\n",
              "      <td>0.112932</td>\n",
              "      <td>0.125713</td>\n",
              "      <td>0.139775</td>\n",
              "      <td>100.0</td>\n",
              "      <td>83.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5994</th>\n",
              "      <td>0.008461</td>\n",
              "      <td>0.011468</td>\n",
              "      <td>0.012707</td>\n",
              "      <td>0.012096</td>\n",
              "      <td>0.009875</td>\n",
              "      <td>0.006558</td>\n",
              "      <td>0.002841</td>\n",
              "      <td>-0.000513</td>\n",
              "      <td>-0.002782</td>\n",
              "      <td>-0.003375</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.048723</td>\n",
              "      <td>-0.051678</td>\n",
              "      <td>-0.052151</td>\n",
              "      <td>-0.050205</td>\n",
              "      <td>-0.046318</td>\n",
              "      <td>-0.041326</td>\n",
              "      <td>-0.036264</td>\n",
              "      <td>95.0</td>\n",
              "      <td>49.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5999</th>\n",
              "      <td>0.052145</td>\n",
              "      <td>0.038833</td>\n",
              "      <td>0.024907</td>\n",
              "      <td>0.010402</td>\n",
              "      <td>-0.004429</td>\n",
              "      <td>-0.019087</td>\n",
              "      <td>-0.032867</td>\n",
              "      <td>-0.044991</td>\n",
              "      <td>-0.054774</td>\n",
              "      <td>-0.061789</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.056943</td>\n",
              "      <td>-0.054300</td>\n",
              "      <td>-0.054324</td>\n",
              "      <td>-0.058068</td>\n",
              "      <td>-0.065599</td>\n",
              "      <td>-0.075635</td>\n",
              "      <td>-0.085448</td>\n",
              "      <td>45.0</td>\n",
              "      <td>37.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>545 rows × 259 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-22dffb56-3200-49eb-89c1-97f537806f07')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-22dffb56-3200-49eb-89c1-97f537806f07 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-22dffb56-3200-49eb-89c1-97f537806f07');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 163
        }
      ]
    }
  ]
}