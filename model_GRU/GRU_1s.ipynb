{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [
        "Lzt1SMEoSshL",
        "EgfaelmnXe04",
        "m-Hm3lsWXkbE",
        "Na73oRb9Xq69",
        "DBnTIBCDGX94",
        "mLO6GGJ5Nn9w",
        "2aZgEs15Kln5"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1.bandpass"
      ],
      "metadata": {
        "id": "WuakxNwvvczU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "cXWIFrrheAli",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00afaeaa-41cc-407e-fc91-8633ef7f4c7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip '/content/drive/MyDrive/model_line.zip' -d '/content/drive/MyDrive/model_line'"
      ],
      "metadata": {
        "id": "IMsCkbbpeDvJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c6ba509-4e6d-42b3-8562-1567bcc35d70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/model_line.zip\n",
            "   creating: /content/drive/MyDrive/model_line/model_line/\n",
            "   creating: /content/drive/MyDrive/model_line/model_line/band/\n",
            "  inflating: /content/drive/MyDrive/model_line/model_line/band/test_band_line.csv  \n",
            "  inflating: /content/drive/MyDrive/model_line/model_line/band/train_band_line.csv  \n",
            "  inflating: /content/drive/MyDrive/model_line/model_line/band/valid_band_line.csv  \n",
            "   creating: /content/drive/MyDrive/model_line/model_line/high/\n",
            "  inflating: /content/drive/MyDrive/model_line/model_line/high/test_high_line.csv  \n",
            "  inflating: /content/drive/MyDrive/model_line/model_line/high/train_high_line.csv  \n",
            "  inflating: /content/drive/MyDrive/model_line/model_line/high/valid_high_line.csv  \n",
            "   creating: /content/drive/MyDrive/model_line/model_line/low/\n",
            "  inflating: /content/drive/MyDrive/model_line/model_line/low/test_low_line.csv  \n",
            "  inflating: /content/drive/MyDrive/model_line/model_line/low/train_low_line.csv  \n",
            "  inflating: /content/drive/MyDrive/model_line/model_line/low/valid_low_line.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nhn3soxJQYzl"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import itertools\n",
        "import time\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import (CosineAnnealingLR,\n",
        "                                      CosineAnnealingWarmRestarts,\n",
        "                                      StepLR,\n",
        "                                      ExponentialLR)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import accuracy_score, auc, f1_score, precision_score, recall_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm, re, sys\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "from keras.layers import Conv2D, Conv1D, MaxPooling2D, MaxPooling1D, Flatten, BatchNormalization, Dense\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.callbacks import CSVLogger, ModelCheckpoint"
      ],
      "metadata": {
        "id": "88FIZSPSbA_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train = pd.read_csv('/content/drive/MyDrive/model_line/model_line/band/train_band_line.csv', header=None)\n",
        "data_test = pd.read_csv('/content/drive/MyDrive/model_line/model_line/band/test_band_line.csv', header=None)\n",
        "data_val= pd.read_csv('/content/drive/MyDrive/model_line/model_line/band/valid_band_line.csv', header=None)"
      ],
      "metadata": {
        "id": "Fo2pj1pSfN4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "YXRC4G52hrnx",
        "outputId": "2088679f-4e11-4a22-f395-97f75f7f0aad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           0         1         2         3         4         5         6    \\\n",
              "0    -0.013974 -0.011168 -0.009118 -0.008067 -0.008125 -0.009210 -0.011005   \n",
              "1    -0.168748 -0.165213 -0.162149 -0.159616 -0.157617 -0.156088 -0.154895   \n",
              "2    -0.065523 -0.063387 -0.062618 -0.063184 -0.064857 -0.067233 -0.069777   \n",
              "3     0.517435  0.633848  0.709706  0.733996  0.703265  0.622346  0.503432   \n",
              "4    -0.069052 -0.066942 -0.064951 -0.062974 -0.060851 -0.058435 -0.055658   \n",
              "...        ...       ...       ...       ...       ...       ...       ...   \n",
              "5995  0.004594  0.009590  0.017135  0.025880  0.034416  0.041620  0.046867   \n",
              "5996  0.017135  0.017694  0.019432  0.022356  0.026302  0.030971  0.035976   \n",
              "5997 -0.061678 -0.060521 -0.059365 -0.058206 -0.057080 -0.056067 -0.055283   \n",
              "5998 -0.039287 -0.039374 -0.039054 -0.038510 -0.037933 -0.037494 -0.037327   \n",
              "5999  0.030423 -0.043405 -0.110988 -0.163599 -0.195505 -0.204771 -0.193219   \n",
              "\n",
              "           7         8         9    ...       246       247       248  \\\n",
              "0    -0.012983 -0.014471 -0.014771  ...  0.006733  0.009824  0.013030   \n",
              "1    -0.153851 -0.152741 -0.151345  ... -0.112212 -0.109266 -0.106187   \n",
              "2    -0.071909 -0.073104 -0.072994  ... -0.108012 -0.104863 -0.102909   \n",
              "3     0.363695  0.221988  0.095392  ... -0.079957 -0.067902 -0.046360   \n",
              "4    -0.052576 -0.049378 -0.046363  ... -0.134398 -0.135559 -0.138471   \n",
              "...        ...       ...       ...  ...       ...       ...       ...   \n",
              "5995  0.050103  0.051777  0.052674  ...  0.045016  0.016083 -0.009705   \n",
              "5996  0.040891  0.045291  0.048799  ...  0.005152  0.044268  0.089211   \n",
              "5997 -0.054843 -0.054816 -0.055175  ... -0.046773 -0.050592 -0.052317   \n",
              "5998 -0.037532 -0.038153 -0.039167  ... -0.058137 -0.060069 -0.061489   \n",
              "5999 -0.165648 -0.128563 -0.088765  ...  0.125802  0.136895  0.147996   \n",
              "\n",
              "           249       250       251       252       253       254       255  \n",
              "0     0.016239  0.019380  0.022418  0.025327  0.028062  0.030516  0.032514  \n",
              "1    -0.103433 -0.101427 -0.100452 -0.100579 -0.101653 -0.103329 -0.105151  \n",
              "2    -0.101926 -0.101481 -0.101056 -0.100197 -0.098638 -0.096375 -0.093669  \n",
              "3    -0.023911 -0.006614  0.002560  0.003524 -0.001689 -0.009950 -0.018044  \n",
              "4    -0.142558 -0.147015 -0.150950 -0.153535 -0.154163 -0.152557 -0.148830  \n",
              "...        ...       ...       ...       ...       ...       ...       ...  \n",
              "5995 -0.032355 -0.052129 -0.069386 -0.084460 -0.097612 -0.109031 -0.118868  \n",
              "5996  0.134152  0.171846  0.194955  0.197629  0.176948  0.133840  0.073184  \n",
              "5997 -0.052018 -0.049965 -0.046587 -0.042433 -0.038139 -0.034393 -0.031913  \n",
              "5998 -0.061986 -0.061385 -0.059834 -0.057803 -0.055972 -0.055062 -0.055631  \n",
              "5999  0.158825  0.169190  0.179084  0.188748  0.198658  0.209439  0.221705  \n",
              "\n",
              "[6000 rows x 256 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9cdde9a3-d8b4-496c-be6b-6d326dc003a3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>246</th>\n",
              "      <th>247</th>\n",
              "      <th>248</th>\n",
              "      <th>249</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.013974</td>\n",
              "      <td>-0.011168</td>\n",
              "      <td>-0.009118</td>\n",
              "      <td>-0.008067</td>\n",
              "      <td>-0.008125</td>\n",
              "      <td>-0.009210</td>\n",
              "      <td>-0.011005</td>\n",
              "      <td>-0.012983</td>\n",
              "      <td>-0.014471</td>\n",
              "      <td>-0.014771</td>\n",
              "      <td>...</td>\n",
              "      <td>0.006733</td>\n",
              "      <td>0.009824</td>\n",
              "      <td>0.013030</td>\n",
              "      <td>0.016239</td>\n",
              "      <td>0.019380</td>\n",
              "      <td>0.022418</td>\n",
              "      <td>0.025327</td>\n",
              "      <td>0.028062</td>\n",
              "      <td>0.030516</td>\n",
              "      <td>0.032514</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.168748</td>\n",
              "      <td>-0.165213</td>\n",
              "      <td>-0.162149</td>\n",
              "      <td>-0.159616</td>\n",
              "      <td>-0.157617</td>\n",
              "      <td>-0.156088</td>\n",
              "      <td>-0.154895</td>\n",
              "      <td>-0.153851</td>\n",
              "      <td>-0.152741</td>\n",
              "      <td>-0.151345</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.112212</td>\n",
              "      <td>-0.109266</td>\n",
              "      <td>-0.106187</td>\n",
              "      <td>-0.103433</td>\n",
              "      <td>-0.101427</td>\n",
              "      <td>-0.100452</td>\n",
              "      <td>-0.100579</td>\n",
              "      <td>-0.101653</td>\n",
              "      <td>-0.103329</td>\n",
              "      <td>-0.105151</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.065523</td>\n",
              "      <td>-0.063387</td>\n",
              "      <td>-0.062618</td>\n",
              "      <td>-0.063184</td>\n",
              "      <td>-0.064857</td>\n",
              "      <td>-0.067233</td>\n",
              "      <td>-0.069777</td>\n",
              "      <td>-0.071909</td>\n",
              "      <td>-0.073104</td>\n",
              "      <td>-0.072994</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.108012</td>\n",
              "      <td>-0.104863</td>\n",
              "      <td>-0.102909</td>\n",
              "      <td>-0.101926</td>\n",
              "      <td>-0.101481</td>\n",
              "      <td>-0.101056</td>\n",
              "      <td>-0.100197</td>\n",
              "      <td>-0.098638</td>\n",
              "      <td>-0.096375</td>\n",
              "      <td>-0.093669</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.517435</td>\n",
              "      <td>0.633848</td>\n",
              "      <td>0.709706</td>\n",
              "      <td>0.733996</td>\n",
              "      <td>0.703265</td>\n",
              "      <td>0.622346</td>\n",
              "      <td>0.503432</td>\n",
              "      <td>0.363695</td>\n",
              "      <td>0.221988</td>\n",
              "      <td>0.095392</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.079957</td>\n",
              "      <td>-0.067902</td>\n",
              "      <td>-0.046360</td>\n",
              "      <td>-0.023911</td>\n",
              "      <td>-0.006614</td>\n",
              "      <td>0.002560</td>\n",
              "      <td>0.003524</td>\n",
              "      <td>-0.001689</td>\n",
              "      <td>-0.009950</td>\n",
              "      <td>-0.018044</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.069052</td>\n",
              "      <td>-0.066942</td>\n",
              "      <td>-0.064951</td>\n",
              "      <td>-0.062974</td>\n",
              "      <td>-0.060851</td>\n",
              "      <td>-0.058435</td>\n",
              "      <td>-0.055658</td>\n",
              "      <td>-0.052576</td>\n",
              "      <td>-0.049378</td>\n",
              "      <td>-0.046363</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.134398</td>\n",
              "      <td>-0.135559</td>\n",
              "      <td>-0.138471</td>\n",
              "      <td>-0.142558</td>\n",
              "      <td>-0.147015</td>\n",
              "      <td>-0.150950</td>\n",
              "      <td>-0.153535</td>\n",
              "      <td>-0.154163</td>\n",
              "      <td>-0.152557</td>\n",
              "      <td>-0.148830</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5995</th>\n",
              "      <td>0.004594</td>\n",
              "      <td>0.009590</td>\n",
              "      <td>0.017135</td>\n",
              "      <td>0.025880</td>\n",
              "      <td>0.034416</td>\n",
              "      <td>0.041620</td>\n",
              "      <td>0.046867</td>\n",
              "      <td>0.050103</td>\n",
              "      <td>0.051777</td>\n",
              "      <td>0.052674</td>\n",
              "      <td>...</td>\n",
              "      <td>0.045016</td>\n",
              "      <td>0.016083</td>\n",
              "      <td>-0.009705</td>\n",
              "      <td>-0.032355</td>\n",
              "      <td>-0.052129</td>\n",
              "      <td>-0.069386</td>\n",
              "      <td>-0.084460</td>\n",
              "      <td>-0.097612</td>\n",
              "      <td>-0.109031</td>\n",
              "      <td>-0.118868</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5996</th>\n",
              "      <td>0.017135</td>\n",
              "      <td>0.017694</td>\n",
              "      <td>0.019432</td>\n",
              "      <td>0.022356</td>\n",
              "      <td>0.026302</td>\n",
              "      <td>0.030971</td>\n",
              "      <td>0.035976</td>\n",
              "      <td>0.040891</td>\n",
              "      <td>0.045291</td>\n",
              "      <td>0.048799</td>\n",
              "      <td>...</td>\n",
              "      <td>0.005152</td>\n",
              "      <td>0.044268</td>\n",
              "      <td>0.089211</td>\n",
              "      <td>0.134152</td>\n",
              "      <td>0.171846</td>\n",
              "      <td>0.194955</td>\n",
              "      <td>0.197629</td>\n",
              "      <td>0.176948</td>\n",
              "      <td>0.133840</td>\n",
              "      <td>0.073184</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5997</th>\n",
              "      <td>-0.061678</td>\n",
              "      <td>-0.060521</td>\n",
              "      <td>-0.059365</td>\n",
              "      <td>-0.058206</td>\n",
              "      <td>-0.057080</td>\n",
              "      <td>-0.056067</td>\n",
              "      <td>-0.055283</td>\n",
              "      <td>-0.054843</td>\n",
              "      <td>-0.054816</td>\n",
              "      <td>-0.055175</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.046773</td>\n",
              "      <td>-0.050592</td>\n",
              "      <td>-0.052317</td>\n",
              "      <td>-0.052018</td>\n",
              "      <td>-0.049965</td>\n",
              "      <td>-0.046587</td>\n",
              "      <td>-0.042433</td>\n",
              "      <td>-0.038139</td>\n",
              "      <td>-0.034393</td>\n",
              "      <td>-0.031913</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5998</th>\n",
              "      <td>-0.039287</td>\n",
              "      <td>-0.039374</td>\n",
              "      <td>-0.039054</td>\n",
              "      <td>-0.038510</td>\n",
              "      <td>-0.037933</td>\n",
              "      <td>-0.037494</td>\n",
              "      <td>-0.037327</td>\n",
              "      <td>-0.037532</td>\n",
              "      <td>-0.038153</td>\n",
              "      <td>-0.039167</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.058137</td>\n",
              "      <td>-0.060069</td>\n",
              "      <td>-0.061489</td>\n",
              "      <td>-0.061986</td>\n",
              "      <td>-0.061385</td>\n",
              "      <td>-0.059834</td>\n",
              "      <td>-0.057803</td>\n",
              "      <td>-0.055972</td>\n",
              "      <td>-0.055062</td>\n",
              "      <td>-0.055631</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5999</th>\n",
              "      <td>0.030423</td>\n",
              "      <td>-0.043405</td>\n",
              "      <td>-0.110988</td>\n",
              "      <td>-0.163599</td>\n",
              "      <td>-0.195505</td>\n",
              "      <td>-0.204771</td>\n",
              "      <td>-0.193219</td>\n",
              "      <td>-0.165648</td>\n",
              "      <td>-0.128563</td>\n",
              "      <td>-0.088765</td>\n",
              "      <td>...</td>\n",
              "      <td>0.125802</td>\n",
              "      <td>0.136895</td>\n",
              "      <td>0.147996</td>\n",
              "      <td>0.158825</td>\n",
              "      <td>0.169190</td>\n",
              "      <td>0.179084</td>\n",
              "      <td>0.188748</td>\n",
              "      <td>0.198658</td>\n",
              "      <td>0.209439</td>\n",
              "      <td>0.221705</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6000 rows × 256 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9cdde9a3-d8b4-496c-be6b-6d326dc003a3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9cdde9a3-d8b4-496c-be6b-6d326dc003a3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9cdde9a3-d8b4-496c-be6b-6d326dc003a3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CiFAK_crtJzg",
        "outputId": "8818564a-8dd2-41dc-de71-1e58ea435afa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(24000, 256)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 라벨생성"
      ],
      "metadata": {
        "id": "jaOKRwI5j8AT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "arr = np.array([])\n",
        "\n",
        "for i in range(1,101):\n",
        "  for j in range(240):\n",
        "    arr = np.append(arr, np.array([i]))"
      ],
      "metadata": {
        "id": "fGX-38SD_5Ec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train[256]=arr"
      ],
      "metadata": {
        "id": "j9sXeCZSMUpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "a2aUe3Cpex1M",
        "outputId": "570f589e-4197-417f-c872-b98565692d48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            0         1         2         3         4         5         6    \\\n",
              "0     -0.031820 -0.015874 -0.001172  0.011249  0.020713  0.026974  0.030190   \n",
              "1     -0.019872 -0.020594 -0.021637 -0.022515 -0.022798 -0.022202 -0.020644   \n",
              "2     -0.057454 -0.056822 -0.055984 -0.054985 -0.053945 -0.052999 -0.052250   \n",
              "3     -0.056099 -0.057035 -0.058395 -0.060043 -0.061694 -0.062967 -0.063465   \n",
              "4      0.053295  0.053439  0.055066  0.059409  0.067278  0.078961  0.094216   \n",
              "...         ...       ...       ...       ...       ...       ...       ...   \n",
              "23995  0.043512  0.020932  0.001382 -0.014947 -0.027957 -0.037670 -0.044270   \n",
              "23996 -0.076867 -0.083002 -0.088128 -0.091454 -0.092582 -0.091623 -0.089189   \n",
              "23997  0.022090  0.024929  0.027786  0.031100  0.035075  0.039622  0.044402   \n",
              "23998 -0.079288 -0.083358 -0.086464 -0.087995 -0.087513 -0.084880 -0.080336   \n",
              "23999 -0.076782 -0.072269 -0.066348 -0.059776 -0.053461 -0.048271 -0.044867   \n",
              "\n",
              "            7         8         9    ...       247       248       249  \\\n",
              "0      0.030831  0.029559  0.027102  ... -0.035003 -0.034647 -0.033418   \n",
              "1     -0.018236 -0.015233 -0.011971  ... -0.072854 -0.070401 -0.067539   \n",
              "2     -0.051733 -0.051414 -0.051213  ... -0.059447 -0.057278 -0.056011   \n",
              "3     -0.062887 -0.061116 -0.058272  ... -0.071806 -0.071741 -0.071302   \n",
              "4      0.112361  0.132422  0.153321  ...  0.150072  0.164082  0.180535   \n",
              "...         ...       ...       ...  ...       ...       ...       ...   \n",
              "23995 -0.048123 -0.049775 -0.049906  ... -0.020100 -0.018824 -0.018132   \n",
              "23996 -0.086281 -0.084051 -0.083490  ... -0.019352 -0.019054 -0.016308   \n",
              "23997  0.048933  0.052740  0.055510  ... -0.053679 -0.057318 -0.059515   \n",
              "23998 -0.074488 -0.068211 -0.062467  ... -0.066882 -0.067113 -0.068424   \n",
              "23999 -0.043568 -0.044302 -0.046625  ... -0.048461 -0.044455 -0.041670   \n",
              "\n",
              "            250       251       252       253       254       255    256  \n",
              "0     -0.031305 -0.028545 -0.025565 -0.022868 -0.020891 -0.019891    1.0  \n",
              "1     -0.064716 -0.062285 -0.060443 -0.059208 -0.058448 -0.057940    1.0  \n",
              "2     -0.055424 -0.055246 -0.055239 -0.055275 -0.055352 -0.055577    1.0  \n",
              "3     -0.070367 -0.068861 -0.066758 -0.064085 -0.060936 -0.057487    1.0  \n",
              "4      0.199168  0.219491  0.240843  0.262473  0.283635  0.303673    1.0  \n",
              "...         ...       ...       ...       ...       ...       ...    ...  \n",
              "23995 -0.018247 -0.019303 -0.021303 -0.024082 -0.027314 -0.030559  100.0  \n",
              "23996 -0.011295 -0.004783  0.002224  0.008807  0.014368  0.018726  100.0  \n",
              "23997 -0.061043 -0.062554 -0.064528 -0.067243 -0.070758 -0.074902  100.0  \n",
              "23998 -0.070704 -0.073600 -0.076563 -0.078930 -0.080050 -0.079413  100.0  \n",
              "23999 -0.040185 -0.039799 -0.040120 -0.040694 -0.041169 -0.041409  100.0  \n",
              "\n",
              "[24000 rows x 257 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7f7b4fb1-0ae1-46d8-b80b-af72371b1d25\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>247</th>\n",
              "      <th>248</th>\n",
              "      <th>249</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "      <th>256</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.031820</td>\n",
              "      <td>-0.015874</td>\n",
              "      <td>-0.001172</td>\n",
              "      <td>0.011249</td>\n",
              "      <td>0.020713</td>\n",
              "      <td>0.026974</td>\n",
              "      <td>0.030190</td>\n",
              "      <td>0.030831</td>\n",
              "      <td>0.029559</td>\n",
              "      <td>0.027102</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.035003</td>\n",
              "      <td>-0.034647</td>\n",
              "      <td>-0.033418</td>\n",
              "      <td>-0.031305</td>\n",
              "      <td>-0.028545</td>\n",
              "      <td>-0.025565</td>\n",
              "      <td>-0.022868</td>\n",
              "      <td>-0.020891</td>\n",
              "      <td>-0.019891</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.019872</td>\n",
              "      <td>-0.020594</td>\n",
              "      <td>-0.021637</td>\n",
              "      <td>-0.022515</td>\n",
              "      <td>-0.022798</td>\n",
              "      <td>-0.022202</td>\n",
              "      <td>-0.020644</td>\n",
              "      <td>-0.018236</td>\n",
              "      <td>-0.015233</td>\n",
              "      <td>-0.011971</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.072854</td>\n",
              "      <td>-0.070401</td>\n",
              "      <td>-0.067539</td>\n",
              "      <td>-0.064716</td>\n",
              "      <td>-0.062285</td>\n",
              "      <td>-0.060443</td>\n",
              "      <td>-0.059208</td>\n",
              "      <td>-0.058448</td>\n",
              "      <td>-0.057940</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.057454</td>\n",
              "      <td>-0.056822</td>\n",
              "      <td>-0.055984</td>\n",
              "      <td>-0.054985</td>\n",
              "      <td>-0.053945</td>\n",
              "      <td>-0.052999</td>\n",
              "      <td>-0.052250</td>\n",
              "      <td>-0.051733</td>\n",
              "      <td>-0.051414</td>\n",
              "      <td>-0.051213</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.059447</td>\n",
              "      <td>-0.057278</td>\n",
              "      <td>-0.056011</td>\n",
              "      <td>-0.055424</td>\n",
              "      <td>-0.055246</td>\n",
              "      <td>-0.055239</td>\n",
              "      <td>-0.055275</td>\n",
              "      <td>-0.055352</td>\n",
              "      <td>-0.055577</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.056099</td>\n",
              "      <td>-0.057035</td>\n",
              "      <td>-0.058395</td>\n",
              "      <td>-0.060043</td>\n",
              "      <td>-0.061694</td>\n",
              "      <td>-0.062967</td>\n",
              "      <td>-0.063465</td>\n",
              "      <td>-0.062887</td>\n",
              "      <td>-0.061116</td>\n",
              "      <td>-0.058272</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.071806</td>\n",
              "      <td>-0.071741</td>\n",
              "      <td>-0.071302</td>\n",
              "      <td>-0.070367</td>\n",
              "      <td>-0.068861</td>\n",
              "      <td>-0.066758</td>\n",
              "      <td>-0.064085</td>\n",
              "      <td>-0.060936</td>\n",
              "      <td>-0.057487</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.053295</td>\n",
              "      <td>0.053439</td>\n",
              "      <td>0.055066</td>\n",
              "      <td>0.059409</td>\n",
              "      <td>0.067278</td>\n",
              "      <td>0.078961</td>\n",
              "      <td>0.094216</td>\n",
              "      <td>0.112361</td>\n",
              "      <td>0.132422</td>\n",
              "      <td>0.153321</td>\n",
              "      <td>...</td>\n",
              "      <td>0.150072</td>\n",
              "      <td>0.164082</td>\n",
              "      <td>0.180535</td>\n",
              "      <td>0.199168</td>\n",
              "      <td>0.219491</td>\n",
              "      <td>0.240843</td>\n",
              "      <td>0.262473</td>\n",
              "      <td>0.283635</td>\n",
              "      <td>0.303673</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23995</th>\n",
              "      <td>0.043512</td>\n",
              "      <td>0.020932</td>\n",
              "      <td>0.001382</td>\n",
              "      <td>-0.014947</td>\n",
              "      <td>-0.027957</td>\n",
              "      <td>-0.037670</td>\n",
              "      <td>-0.044270</td>\n",
              "      <td>-0.048123</td>\n",
              "      <td>-0.049775</td>\n",
              "      <td>-0.049906</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.020100</td>\n",
              "      <td>-0.018824</td>\n",
              "      <td>-0.018132</td>\n",
              "      <td>-0.018247</td>\n",
              "      <td>-0.019303</td>\n",
              "      <td>-0.021303</td>\n",
              "      <td>-0.024082</td>\n",
              "      <td>-0.027314</td>\n",
              "      <td>-0.030559</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23996</th>\n",
              "      <td>-0.076867</td>\n",
              "      <td>-0.083002</td>\n",
              "      <td>-0.088128</td>\n",
              "      <td>-0.091454</td>\n",
              "      <td>-0.092582</td>\n",
              "      <td>-0.091623</td>\n",
              "      <td>-0.089189</td>\n",
              "      <td>-0.086281</td>\n",
              "      <td>-0.084051</td>\n",
              "      <td>-0.083490</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.019352</td>\n",
              "      <td>-0.019054</td>\n",
              "      <td>-0.016308</td>\n",
              "      <td>-0.011295</td>\n",
              "      <td>-0.004783</td>\n",
              "      <td>0.002224</td>\n",
              "      <td>0.008807</td>\n",
              "      <td>0.014368</td>\n",
              "      <td>0.018726</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23997</th>\n",
              "      <td>0.022090</td>\n",
              "      <td>0.024929</td>\n",
              "      <td>0.027786</td>\n",
              "      <td>0.031100</td>\n",
              "      <td>0.035075</td>\n",
              "      <td>0.039622</td>\n",
              "      <td>0.044402</td>\n",
              "      <td>0.048933</td>\n",
              "      <td>0.052740</td>\n",
              "      <td>0.055510</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.053679</td>\n",
              "      <td>-0.057318</td>\n",
              "      <td>-0.059515</td>\n",
              "      <td>-0.061043</td>\n",
              "      <td>-0.062554</td>\n",
              "      <td>-0.064528</td>\n",
              "      <td>-0.067243</td>\n",
              "      <td>-0.070758</td>\n",
              "      <td>-0.074902</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23998</th>\n",
              "      <td>-0.079288</td>\n",
              "      <td>-0.083358</td>\n",
              "      <td>-0.086464</td>\n",
              "      <td>-0.087995</td>\n",
              "      <td>-0.087513</td>\n",
              "      <td>-0.084880</td>\n",
              "      <td>-0.080336</td>\n",
              "      <td>-0.074488</td>\n",
              "      <td>-0.068211</td>\n",
              "      <td>-0.062467</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.066882</td>\n",
              "      <td>-0.067113</td>\n",
              "      <td>-0.068424</td>\n",
              "      <td>-0.070704</td>\n",
              "      <td>-0.073600</td>\n",
              "      <td>-0.076563</td>\n",
              "      <td>-0.078930</td>\n",
              "      <td>-0.080050</td>\n",
              "      <td>-0.079413</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23999</th>\n",
              "      <td>-0.076782</td>\n",
              "      <td>-0.072269</td>\n",
              "      <td>-0.066348</td>\n",
              "      <td>-0.059776</td>\n",
              "      <td>-0.053461</td>\n",
              "      <td>-0.048271</td>\n",
              "      <td>-0.044867</td>\n",
              "      <td>-0.043568</td>\n",
              "      <td>-0.044302</td>\n",
              "      <td>-0.046625</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.048461</td>\n",
              "      <td>-0.044455</td>\n",
              "      <td>-0.041670</td>\n",
              "      <td>-0.040185</td>\n",
              "      <td>-0.039799</td>\n",
              "      <td>-0.040120</td>\n",
              "      <td>-0.040694</td>\n",
              "      <td>-0.041169</td>\n",
              "      <td>-0.041409</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>24000 rows × 257 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7f7b4fb1-0ae1-46d8-b80b-af72371b1d25')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7f7b4fb1-0ae1-46d8-b80b-af72371b1d25 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7f7b4fb1-0ae1-46d8-b80b-af72371b1d25');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "arr_t = np.array([])\n",
        "\n",
        "for i in range(1,101):\n",
        "  for j in range(60):\n",
        "    arr_t = np.append(arr_t, np.array([i]))"
      ],
      "metadata": {
        "id": "flq_Q9uOe5fd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arr_v = np.array([])\n",
        "\n",
        "for i in range(1,101):\n",
        "  for j in range(60):\n",
        "    arr_v = np.append(arr_v, np.array([i]))"
      ],
      "metadata": {
        "id": "ddgT-ntvfAbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_test[256]=arr_t\n",
        "data_val[256]=arr_v\n",
        "data_val"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "daFrtTuHfD96",
        "outputId": "fb13f398-3ae2-4a7e-f66c-c8ab7e10cf87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           0         1         2         3         4         5         6    \\\n",
              "0    -0.054004 -0.050825 -0.048322 -0.046824 -0.046532 -0.047445 -0.049330   \n",
              "1    -0.209505 -0.208403 -0.206688 -0.204586 -0.202338 -0.200196 -0.198379   \n",
              "2    -0.067735 -0.065806 -0.063513 -0.060990 -0.058453 -0.056174 -0.054439   \n",
              "3    -0.002916 -0.007004 -0.012949 -0.020332 -0.028344 -0.035829 -0.041422   \n",
              "4    -0.028844 -0.028027 -0.028309 -0.029364 -0.030767 -0.032072 -0.032878   \n",
              "...        ...       ...       ...       ...       ...       ...       ...   \n",
              "5995  0.015685  0.016545  0.014765  0.010665  0.004884 -0.001771 -0.008494   \n",
              "5996 -0.171624 -0.175496 -0.177258 -0.176626 -0.173694 -0.168915 -0.163017   \n",
              "5997  0.000168 -0.001110 -0.003542 -0.006028 -0.007548 -0.007396 -0.005302   \n",
              "5998 -0.033347 -0.035287 -0.036169 -0.036014 -0.035070 -0.033744 -0.032503   \n",
              "5999 -0.041533 -0.041870 -0.042836 -0.044782 -0.047871 -0.052004 -0.056828   \n",
              "\n",
              "           7         8         9    ...       247       248       249  \\\n",
              "0    -0.051735 -0.054058 -0.055645  ... -0.028530 -0.029492 -0.029945   \n",
              "1    -0.197017 -0.196097 -0.195447  ... -0.205948 -0.202740 -0.198907   \n",
              "2    -0.053477 -0.053402 -0.054158  ... -0.093394 -0.093535 -0.092581   \n",
              "3    -0.043775 -0.041865 -0.035337  ... -0.097242 -0.124400 -0.137850   \n",
              "4    -0.032899 -0.032005 -0.030247  ... -0.074225 -0.075851 -0.077197   \n",
              "...        ...       ...       ...  ...       ...       ...       ...   \n",
              "5995 -0.014652 -0.019878 -0.024092  ... -0.052522 -0.020671  0.001335   \n",
              "5996 -0.156862 -0.151287 -0.146943  ...  0.013368  0.016800  0.019352   \n",
              "5997 -0.001433  0.003712  0.009473  ... -0.072272 -0.071268 -0.070220   \n",
              "5998 -0.031760 -0.031802 -0.032746  ... -0.036757 -0.033683 -0.032034   \n",
              "5999 -0.061806 -0.066332 -0.069845  ...  0.007247  0.056263  0.107776   \n",
              "\n",
              "           250       251       252       253       254       255    256  \n",
              "0    -0.029648 -0.028487 -0.026485 -0.023782 -0.020603 -0.017228    1.0  \n",
              "1    -0.194673 -0.190228 -0.185713 -0.181227 -0.176850 -0.172660    1.0  \n",
              "2    -0.090373 -0.086983 -0.082689 -0.077914 -0.073148 -0.068878    1.0  \n",
              "3    -0.128147 -0.087559 -0.012153  0.096530  0.230711  0.376848    1.0  \n",
              "4    -0.077950 -0.077920 -0.077074 -0.075531 -0.073514 -0.071283    1.0  \n",
              "...        ...       ...       ...       ...       ...       ...    ...  \n",
              "5995  0.013226  0.016606  0.014270  0.009428  0.004988  0.003041  100.0  \n",
              "5996  0.020889  0.021392  0.020985  0.019940  0.018650  0.017569  100.0  \n",
              "5997 -0.069094 -0.067885 -0.066618 -0.065335 -0.064074 -0.062856  100.0  \n",
              "5998 -0.031757 -0.032584 -0.034107 -0.035874 -0.037486 -0.038665  100.0  \n",
              "5999  0.154395  0.188125  0.201986  0.191623  0.156499  0.100301  100.0  \n",
              "\n",
              "[6000 rows x 257 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a665cee1-89ce-466f-b8ef-cefdb43b2b79\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>247</th>\n",
              "      <th>248</th>\n",
              "      <th>249</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "      <th>256</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.054004</td>\n",
              "      <td>-0.050825</td>\n",
              "      <td>-0.048322</td>\n",
              "      <td>-0.046824</td>\n",
              "      <td>-0.046532</td>\n",
              "      <td>-0.047445</td>\n",
              "      <td>-0.049330</td>\n",
              "      <td>-0.051735</td>\n",
              "      <td>-0.054058</td>\n",
              "      <td>-0.055645</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.028530</td>\n",
              "      <td>-0.029492</td>\n",
              "      <td>-0.029945</td>\n",
              "      <td>-0.029648</td>\n",
              "      <td>-0.028487</td>\n",
              "      <td>-0.026485</td>\n",
              "      <td>-0.023782</td>\n",
              "      <td>-0.020603</td>\n",
              "      <td>-0.017228</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.209505</td>\n",
              "      <td>-0.208403</td>\n",
              "      <td>-0.206688</td>\n",
              "      <td>-0.204586</td>\n",
              "      <td>-0.202338</td>\n",
              "      <td>-0.200196</td>\n",
              "      <td>-0.198379</td>\n",
              "      <td>-0.197017</td>\n",
              "      <td>-0.196097</td>\n",
              "      <td>-0.195447</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.205948</td>\n",
              "      <td>-0.202740</td>\n",
              "      <td>-0.198907</td>\n",
              "      <td>-0.194673</td>\n",
              "      <td>-0.190228</td>\n",
              "      <td>-0.185713</td>\n",
              "      <td>-0.181227</td>\n",
              "      <td>-0.176850</td>\n",
              "      <td>-0.172660</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.067735</td>\n",
              "      <td>-0.065806</td>\n",
              "      <td>-0.063513</td>\n",
              "      <td>-0.060990</td>\n",
              "      <td>-0.058453</td>\n",
              "      <td>-0.056174</td>\n",
              "      <td>-0.054439</td>\n",
              "      <td>-0.053477</td>\n",
              "      <td>-0.053402</td>\n",
              "      <td>-0.054158</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.093394</td>\n",
              "      <td>-0.093535</td>\n",
              "      <td>-0.092581</td>\n",
              "      <td>-0.090373</td>\n",
              "      <td>-0.086983</td>\n",
              "      <td>-0.082689</td>\n",
              "      <td>-0.077914</td>\n",
              "      <td>-0.073148</td>\n",
              "      <td>-0.068878</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.002916</td>\n",
              "      <td>-0.007004</td>\n",
              "      <td>-0.012949</td>\n",
              "      <td>-0.020332</td>\n",
              "      <td>-0.028344</td>\n",
              "      <td>-0.035829</td>\n",
              "      <td>-0.041422</td>\n",
              "      <td>-0.043775</td>\n",
              "      <td>-0.041865</td>\n",
              "      <td>-0.035337</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.097242</td>\n",
              "      <td>-0.124400</td>\n",
              "      <td>-0.137850</td>\n",
              "      <td>-0.128147</td>\n",
              "      <td>-0.087559</td>\n",
              "      <td>-0.012153</td>\n",
              "      <td>0.096530</td>\n",
              "      <td>0.230711</td>\n",
              "      <td>0.376848</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.028844</td>\n",
              "      <td>-0.028027</td>\n",
              "      <td>-0.028309</td>\n",
              "      <td>-0.029364</td>\n",
              "      <td>-0.030767</td>\n",
              "      <td>-0.032072</td>\n",
              "      <td>-0.032878</td>\n",
              "      <td>-0.032899</td>\n",
              "      <td>-0.032005</td>\n",
              "      <td>-0.030247</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.074225</td>\n",
              "      <td>-0.075851</td>\n",
              "      <td>-0.077197</td>\n",
              "      <td>-0.077950</td>\n",
              "      <td>-0.077920</td>\n",
              "      <td>-0.077074</td>\n",
              "      <td>-0.075531</td>\n",
              "      <td>-0.073514</td>\n",
              "      <td>-0.071283</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5995</th>\n",
              "      <td>0.015685</td>\n",
              "      <td>0.016545</td>\n",
              "      <td>0.014765</td>\n",
              "      <td>0.010665</td>\n",
              "      <td>0.004884</td>\n",
              "      <td>-0.001771</td>\n",
              "      <td>-0.008494</td>\n",
              "      <td>-0.014652</td>\n",
              "      <td>-0.019878</td>\n",
              "      <td>-0.024092</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.052522</td>\n",
              "      <td>-0.020671</td>\n",
              "      <td>0.001335</td>\n",
              "      <td>0.013226</td>\n",
              "      <td>0.016606</td>\n",
              "      <td>0.014270</td>\n",
              "      <td>0.009428</td>\n",
              "      <td>0.004988</td>\n",
              "      <td>0.003041</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5996</th>\n",
              "      <td>-0.171624</td>\n",
              "      <td>-0.175496</td>\n",
              "      <td>-0.177258</td>\n",
              "      <td>-0.176626</td>\n",
              "      <td>-0.173694</td>\n",
              "      <td>-0.168915</td>\n",
              "      <td>-0.163017</td>\n",
              "      <td>-0.156862</td>\n",
              "      <td>-0.151287</td>\n",
              "      <td>-0.146943</td>\n",
              "      <td>...</td>\n",
              "      <td>0.013368</td>\n",
              "      <td>0.016800</td>\n",
              "      <td>0.019352</td>\n",
              "      <td>0.020889</td>\n",
              "      <td>0.021392</td>\n",
              "      <td>0.020985</td>\n",
              "      <td>0.019940</td>\n",
              "      <td>0.018650</td>\n",
              "      <td>0.017569</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5997</th>\n",
              "      <td>0.000168</td>\n",
              "      <td>-0.001110</td>\n",
              "      <td>-0.003542</td>\n",
              "      <td>-0.006028</td>\n",
              "      <td>-0.007548</td>\n",
              "      <td>-0.007396</td>\n",
              "      <td>-0.005302</td>\n",
              "      <td>-0.001433</td>\n",
              "      <td>0.003712</td>\n",
              "      <td>0.009473</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.072272</td>\n",
              "      <td>-0.071268</td>\n",
              "      <td>-0.070220</td>\n",
              "      <td>-0.069094</td>\n",
              "      <td>-0.067885</td>\n",
              "      <td>-0.066618</td>\n",
              "      <td>-0.065335</td>\n",
              "      <td>-0.064074</td>\n",
              "      <td>-0.062856</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5998</th>\n",
              "      <td>-0.033347</td>\n",
              "      <td>-0.035287</td>\n",
              "      <td>-0.036169</td>\n",
              "      <td>-0.036014</td>\n",
              "      <td>-0.035070</td>\n",
              "      <td>-0.033744</td>\n",
              "      <td>-0.032503</td>\n",
              "      <td>-0.031760</td>\n",
              "      <td>-0.031802</td>\n",
              "      <td>-0.032746</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.036757</td>\n",
              "      <td>-0.033683</td>\n",
              "      <td>-0.032034</td>\n",
              "      <td>-0.031757</td>\n",
              "      <td>-0.032584</td>\n",
              "      <td>-0.034107</td>\n",
              "      <td>-0.035874</td>\n",
              "      <td>-0.037486</td>\n",
              "      <td>-0.038665</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5999</th>\n",
              "      <td>-0.041533</td>\n",
              "      <td>-0.041870</td>\n",
              "      <td>-0.042836</td>\n",
              "      <td>-0.044782</td>\n",
              "      <td>-0.047871</td>\n",
              "      <td>-0.052004</td>\n",
              "      <td>-0.056828</td>\n",
              "      <td>-0.061806</td>\n",
              "      <td>-0.066332</td>\n",
              "      <td>-0.069845</td>\n",
              "      <td>...</td>\n",
              "      <td>0.007247</td>\n",
              "      <td>0.056263</td>\n",
              "      <td>0.107776</td>\n",
              "      <td>0.154395</td>\n",
              "      <td>0.188125</td>\n",
              "      <td>0.201986</td>\n",
              "      <td>0.191623</td>\n",
              "      <td>0.156499</td>\n",
              "      <td>0.100301</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6000 rows × 257 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a665cee1-89ce-466f-b8ef-cefdb43b2b79')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a665cee1-89ce-466f-b8ef-cefdb43b2b79 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a665cee1-89ce-466f-b8ef-cefdb43b2b79');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터 전체 섞기"
      ],
      "metadata": {
        "id": "I7BNW6vLSn40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_total=pd.concat([data_train, data_test, data_val])\n",
        "data_total"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "Tb9_tMy0SUkK",
        "outputId": "e5606acd-ca95-40e3-ac6c-a81efb00c5d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           0         1         2         3         4         5         6    \\\n",
              "0    -0.031820 -0.015874 -0.001172  0.011249  0.020713  0.026974  0.030190   \n",
              "1    -0.019872 -0.020594 -0.021637 -0.022515 -0.022798 -0.022202 -0.020644   \n",
              "2    -0.057454 -0.056822 -0.055984 -0.054985 -0.053945 -0.052999 -0.052250   \n",
              "3    -0.056099 -0.057035 -0.058395 -0.060043 -0.061694 -0.062967 -0.063465   \n",
              "4     0.053295  0.053439  0.055066  0.059409  0.067278  0.078961  0.094216   \n",
              "...        ...       ...       ...       ...       ...       ...       ...   \n",
              "5995  0.015685  0.016545  0.014765  0.010665  0.004884 -0.001771 -0.008494   \n",
              "5996 -0.171624 -0.175496 -0.177258 -0.176626 -0.173694 -0.168915 -0.163017   \n",
              "5997  0.000168 -0.001110 -0.003542 -0.006028 -0.007548 -0.007396 -0.005302   \n",
              "5998 -0.033347 -0.035287 -0.036169 -0.036014 -0.035070 -0.033744 -0.032503   \n",
              "5999 -0.041533 -0.041870 -0.042836 -0.044782 -0.047871 -0.052004 -0.056828   \n",
              "\n",
              "           7         8         9    ...       247       248       249  \\\n",
              "0     0.030831  0.029559  0.027102  ... -0.035003 -0.034647 -0.033418   \n",
              "1    -0.018236 -0.015233 -0.011971  ... -0.072854 -0.070401 -0.067539   \n",
              "2    -0.051733 -0.051414 -0.051213  ... -0.059447 -0.057278 -0.056011   \n",
              "3    -0.062887 -0.061116 -0.058272  ... -0.071806 -0.071741 -0.071302   \n",
              "4     0.112361  0.132422  0.153321  ...  0.150072  0.164082  0.180535   \n",
              "...        ...       ...       ...  ...       ...       ...       ...   \n",
              "5995 -0.014652 -0.019878 -0.024092  ... -0.052522 -0.020671  0.001335   \n",
              "5996 -0.156862 -0.151287 -0.146943  ...  0.013368  0.016800  0.019352   \n",
              "5997 -0.001433  0.003712  0.009473  ... -0.072272 -0.071268 -0.070220   \n",
              "5998 -0.031760 -0.031802 -0.032746  ... -0.036757 -0.033683 -0.032034   \n",
              "5999 -0.061806 -0.066332 -0.069845  ...  0.007247  0.056263  0.107776   \n",
              "\n",
              "           250       251       252       253       254       255    256  \n",
              "0    -0.031305 -0.028545 -0.025565 -0.022868 -0.020891 -0.019891    1.0  \n",
              "1    -0.064716 -0.062285 -0.060443 -0.059208 -0.058448 -0.057940    1.0  \n",
              "2    -0.055424 -0.055246 -0.055239 -0.055275 -0.055352 -0.055577    1.0  \n",
              "3    -0.070367 -0.068861 -0.066758 -0.064085 -0.060936 -0.057487    1.0  \n",
              "4     0.199168  0.219491  0.240843  0.262473  0.283635  0.303673    1.0  \n",
              "...        ...       ...       ...       ...       ...       ...    ...  \n",
              "5995  0.013226  0.016606  0.014270  0.009428  0.004988  0.003041  100.0  \n",
              "5996  0.020889  0.021392  0.020985  0.019940  0.018650  0.017569  100.0  \n",
              "5997 -0.069094 -0.067885 -0.066618 -0.065335 -0.064074 -0.062856  100.0  \n",
              "5998 -0.031757 -0.032584 -0.034107 -0.035874 -0.037486 -0.038665  100.0  \n",
              "5999  0.154395  0.188125  0.201986  0.191623  0.156499  0.100301  100.0  \n",
              "\n",
              "[36000 rows x 257 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-60c154a6-6379-42d5-9be0-256ea965f91b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>247</th>\n",
              "      <th>248</th>\n",
              "      <th>249</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "      <th>256</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.031820</td>\n",
              "      <td>-0.015874</td>\n",
              "      <td>-0.001172</td>\n",
              "      <td>0.011249</td>\n",
              "      <td>0.020713</td>\n",
              "      <td>0.026974</td>\n",
              "      <td>0.030190</td>\n",
              "      <td>0.030831</td>\n",
              "      <td>0.029559</td>\n",
              "      <td>0.027102</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.035003</td>\n",
              "      <td>-0.034647</td>\n",
              "      <td>-0.033418</td>\n",
              "      <td>-0.031305</td>\n",
              "      <td>-0.028545</td>\n",
              "      <td>-0.025565</td>\n",
              "      <td>-0.022868</td>\n",
              "      <td>-0.020891</td>\n",
              "      <td>-0.019891</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.019872</td>\n",
              "      <td>-0.020594</td>\n",
              "      <td>-0.021637</td>\n",
              "      <td>-0.022515</td>\n",
              "      <td>-0.022798</td>\n",
              "      <td>-0.022202</td>\n",
              "      <td>-0.020644</td>\n",
              "      <td>-0.018236</td>\n",
              "      <td>-0.015233</td>\n",
              "      <td>-0.011971</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.072854</td>\n",
              "      <td>-0.070401</td>\n",
              "      <td>-0.067539</td>\n",
              "      <td>-0.064716</td>\n",
              "      <td>-0.062285</td>\n",
              "      <td>-0.060443</td>\n",
              "      <td>-0.059208</td>\n",
              "      <td>-0.058448</td>\n",
              "      <td>-0.057940</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.057454</td>\n",
              "      <td>-0.056822</td>\n",
              "      <td>-0.055984</td>\n",
              "      <td>-0.054985</td>\n",
              "      <td>-0.053945</td>\n",
              "      <td>-0.052999</td>\n",
              "      <td>-0.052250</td>\n",
              "      <td>-0.051733</td>\n",
              "      <td>-0.051414</td>\n",
              "      <td>-0.051213</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.059447</td>\n",
              "      <td>-0.057278</td>\n",
              "      <td>-0.056011</td>\n",
              "      <td>-0.055424</td>\n",
              "      <td>-0.055246</td>\n",
              "      <td>-0.055239</td>\n",
              "      <td>-0.055275</td>\n",
              "      <td>-0.055352</td>\n",
              "      <td>-0.055577</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.056099</td>\n",
              "      <td>-0.057035</td>\n",
              "      <td>-0.058395</td>\n",
              "      <td>-0.060043</td>\n",
              "      <td>-0.061694</td>\n",
              "      <td>-0.062967</td>\n",
              "      <td>-0.063465</td>\n",
              "      <td>-0.062887</td>\n",
              "      <td>-0.061116</td>\n",
              "      <td>-0.058272</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.071806</td>\n",
              "      <td>-0.071741</td>\n",
              "      <td>-0.071302</td>\n",
              "      <td>-0.070367</td>\n",
              "      <td>-0.068861</td>\n",
              "      <td>-0.066758</td>\n",
              "      <td>-0.064085</td>\n",
              "      <td>-0.060936</td>\n",
              "      <td>-0.057487</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.053295</td>\n",
              "      <td>0.053439</td>\n",
              "      <td>0.055066</td>\n",
              "      <td>0.059409</td>\n",
              "      <td>0.067278</td>\n",
              "      <td>0.078961</td>\n",
              "      <td>0.094216</td>\n",
              "      <td>0.112361</td>\n",
              "      <td>0.132422</td>\n",
              "      <td>0.153321</td>\n",
              "      <td>...</td>\n",
              "      <td>0.150072</td>\n",
              "      <td>0.164082</td>\n",
              "      <td>0.180535</td>\n",
              "      <td>0.199168</td>\n",
              "      <td>0.219491</td>\n",
              "      <td>0.240843</td>\n",
              "      <td>0.262473</td>\n",
              "      <td>0.283635</td>\n",
              "      <td>0.303673</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5995</th>\n",
              "      <td>0.015685</td>\n",
              "      <td>0.016545</td>\n",
              "      <td>0.014765</td>\n",
              "      <td>0.010665</td>\n",
              "      <td>0.004884</td>\n",
              "      <td>-0.001771</td>\n",
              "      <td>-0.008494</td>\n",
              "      <td>-0.014652</td>\n",
              "      <td>-0.019878</td>\n",
              "      <td>-0.024092</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.052522</td>\n",
              "      <td>-0.020671</td>\n",
              "      <td>0.001335</td>\n",
              "      <td>0.013226</td>\n",
              "      <td>0.016606</td>\n",
              "      <td>0.014270</td>\n",
              "      <td>0.009428</td>\n",
              "      <td>0.004988</td>\n",
              "      <td>0.003041</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5996</th>\n",
              "      <td>-0.171624</td>\n",
              "      <td>-0.175496</td>\n",
              "      <td>-0.177258</td>\n",
              "      <td>-0.176626</td>\n",
              "      <td>-0.173694</td>\n",
              "      <td>-0.168915</td>\n",
              "      <td>-0.163017</td>\n",
              "      <td>-0.156862</td>\n",
              "      <td>-0.151287</td>\n",
              "      <td>-0.146943</td>\n",
              "      <td>...</td>\n",
              "      <td>0.013368</td>\n",
              "      <td>0.016800</td>\n",
              "      <td>0.019352</td>\n",
              "      <td>0.020889</td>\n",
              "      <td>0.021392</td>\n",
              "      <td>0.020985</td>\n",
              "      <td>0.019940</td>\n",
              "      <td>0.018650</td>\n",
              "      <td>0.017569</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5997</th>\n",
              "      <td>0.000168</td>\n",
              "      <td>-0.001110</td>\n",
              "      <td>-0.003542</td>\n",
              "      <td>-0.006028</td>\n",
              "      <td>-0.007548</td>\n",
              "      <td>-0.007396</td>\n",
              "      <td>-0.005302</td>\n",
              "      <td>-0.001433</td>\n",
              "      <td>0.003712</td>\n",
              "      <td>0.009473</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.072272</td>\n",
              "      <td>-0.071268</td>\n",
              "      <td>-0.070220</td>\n",
              "      <td>-0.069094</td>\n",
              "      <td>-0.067885</td>\n",
              "      <td>-0.066618</td>\n",
              "      <td>-0.065335</td>\n",
              "      <td>-0.064074</td>\n",
              "      <td>-0.062856</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5998</th>\n",
              "      <td>-0.033347</td>\n",
              "      <td>-0.035287</td>\n",
              "      <td>-0.036169</td>\n",
              "      <td>-0.036014</td>\n",
              "      <td>-0.035070</td>\n",
              "      <td>-0.033744</td>\n",
              "      <td>-0.032503</td>\n",
              "      <td>-0.031760</td>\n",
              "      <td>-0.031802</td>\n",
              "      <td>-0.032746</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.036757</td>\n",
              "      <td>-0.033683</td>\n",
              "      <td>-0.032034</td>\n",
              "      <td>-0.031757</td>\n",
              "      <td>-0.032584</td>\n",
              "      <td>-0.034107</td>\n",
              "      <td>-0.035874</td>\n",
              "      <td>-0.037486</td>\n",
              "      <td>-0.038665</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5999</th>\n",
              "      <td>-0.041533</td>\n",
              "      <td>-0.041870</td>\n",
              "      <td>-0.042836</td>\n",
              "      <td>-0.044782</td>\n",
              "      <td>-0.047871</td>\n",
              "      <td>-0.052004</td>\n",
              "      <td>-0.056828</td>\n",
              "      <td>-0.061806</td>\n",
              "      <td>-0.066332</td>\n",
              "      <td>-0.069845</td>\n",
              "      <td>...</td>\n",
              "      <td>0.007247</td>\n",
              "      <td>0.056263</td>\n",
              "      <td>0.107776</td>\n",
              "      <td>0.154395</td>\n",
              "      <td>0.188125</td>\n",
              "      <td>0.201986</td>\n",
              "      <td>0.191623</td>\n",
              "      <td>0.156499</td>\n",
              "      <td>0.100301</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>36000 rows × 257 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-60c154a6-6379-42d5-9be0-256ea965f91b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-60c154a6-6379-42d5-9be0-256ea965f91b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-60c154a6-6379-42d5-9be0-256ea965f91b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_total=data_total.sample(frac=1)"
      ],
      "metadata": {
        "id": "zuAljzhlSxO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train=data_total.iloc[0:24000, :]\n",
        "data_val=data_total.iloc[24000:30000, :]\n",
        "data_test=data_total.iloc[30000:36000,:]"
      ],
      "metadata": {
        "id": "dAO2XTRKS938"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "R03ForgAUEFw",
        "outputId": "f21d8149-07a9-4fd8-e12e-f9e6b23ebd03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           0         1         2         3         4         5         6    \\\n",
              "3079  0.091812  0.067493  0.045227  0.025677  0.009212 -0.004093 -0.014421   \n",
              "888   0.013494  0.006283 -0.000459 -0.006313 -0.010975 -0.014295 -0.016285   \n",
              "4549  0.173115  0.182854  0.191626  0.199422  0.206289  0.212299  0.217509   \n",
              "4092 -0.017760 -0.016474 -0.015390 -0.014666 -0.014388 -0.014557 -0.015102   \n",
              "5796  0.195117  0.201263  0.205637  0.207985  0.208163  0.206108  0.201811   \n",
              "...        ...       ...       ...       ...       ...       ...       ...   \n",
              "6203 -0.089165 -0.086464 -0.082844 -0.078363 -0.073256 -0.067948 -0.063018   \n",
              "4582  0.175126  0.180752  0.185047  0.188015  0.189637  0.189832  0.188450   \n",
              "3397 -0.045351 -0.049060 -0.052955 -0.056106 -0.057702 -0.057274 -0.054829   \n",
              "242   0.579771  0.695609  0.767799  0.786421  0.749400  0.662886  0.539988   \n",
              "8167 -0.051595 -0.061401 -0.075480 -0.091133 -0.104381 -0.110547 -0.105099   \n",
              "\n",
              "           7         8         9    ...       247       248       249  \\\n",
              "3079 -0.022142 -0.027747 -0.031768  ...  0.025624  0.026901  0.026220   \n",
              "888  -0.017112 -0.017064 -0.016505  ...  0.221332  0.231960  0.212712   \n",
              "4549  0.221928  0.225482  0.228003  ... -0.078112 -0.077942 -0.078028   \n",
              "4092 -0.015894 -0.016773 -0.017577  ... -0.042300 -0.040974 -0.038731   \n",
              "5796  0.195307  0.186682  0.176090  ... -0.040166 -0.039460 -0.039233   \n",
              "...        ...       ...       ...  ...       ...       ...       ...   \n",
              "6203 -0.059105 -0.056762 -0.056321  ... -0.363814 -0.368956 -0.340357   \n",
              "4582  0.185305  0.180208  0.173015  ...  0.122948  0.106362  0.088842   \n",
              "3397 -0.050884 -0.046355 -0.042346  ... -0.060359 -0.069059 -0.073274   \n",
              "242   0.398152  0.255832  0.129211  ...  0.180357  0.197160  0.214537   \n",
              "8167 -0.084613 -0.047708  0.004268  ...  0.012040  0.015572  0.018201   \n",
              "\n",
              "           250       251       252       253       254       255   256  \n",
              "3079  0.023670  0.019396  0.013589  0.006494 -0.001559 -0.010138  52.0  \n",
              "888   0.163127  0.087583 -0.005267 -0.103945 -0.196151 -0.270917   4.0  \n",
              "4549 -0.078270 -0.078531 -0.078654 -0.078493 -0.077941 -0.076955  19.0  \n",
              "4092 -0.035980 -0.033265 -0.031148 -0.030090 -0.030355 -0.031967  18.0  \n",
              "5796 -0.039064 -0.038449 -0.036960 -0.034394 -0.030848 -0.026697  97.0  \n",
              "...        ...       ...       ...       ...       ...       ...   ...  \n",
              "6203 -0.287416 -0.221866 -0.155281 -0.097001 -0.052862 -0.024853  26.0  \n",
              "4582  0.070717  0.052356  0.034165  0.016575  0.000022 -0.015090  77.0  \n",
              "3397 -0.071696 -0.064275 -0.052043 -0.036758 -0.020448 -0.004983  15.0  \n",
              "242   0.232097  0.249325  0.265591  0.280164  0.292234  0.300952   5.0  \n",
              "8167  0.020323  0.022372  0.024790  0.028000  0.032365  0.038159  35.0  \n",
              "\n",
              "[24000 rows x 257 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-eef4161a-61d8-464b-9726-803739f78de1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>247</th>\n",
              "      <th>248</th>\n",
              "      <th>249</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "      <th>256</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3079</th>\n",
              "      <td>0.091812</td>\n",
              "      <td>0.067493</td>\n",
              "      <td>0.045227</td>\n",
              "      <td>0.025677</td>\n",
              "      <td>0.009212</td>\n",
              "      <td>-0.004093</td>\n",
              "      <td>-0.014421</td>\n",
              "      <td>-0.022142</td>\n",
              "      <td>-0.027747</td>\n",
              "      <td>-0.031768</td>\n",
              "      <td>...</td>\n",
              "      <td>0.025624</td>\n",
              "      <td>0.026901</td>\n",
              "      <td>0.026220</td>\n",
              "      <td>0.023670</td>\n",
              "      <td>0.019396</td>\n",
              "      <td>0.013589</td>\n",
              "      <td>0.006494</td>\n",
              "      <td>-0.001559</td>\n",
              "      <td>-0.010138</td>\n",
              "      <td>52.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>888</th>\n",
              "      <td>0.013494</td>\n",
              "      <td>0.006283</td>\n",
              "      <td>-0.000459</td>\n",
              "      <td>-0.006313</td>\n",
              "      <td>-0.010975</td>\n",
              "      <td>-0.014295</td>\n",
              "      <td>-0.016285</td>\n",
              "      <td>-0.017112</td>\n",
              "      <td>-0.017064</td>\n",
              "      <td>-0.016505</td>\n",
              "      <td>...</td>\n",
              "      <td>0.221332</td>\n",
              "      <td>0.231960</td>\n",
              "      <td>0.212712</td>\n",
              "      <td>0.163127</td>\n",
              "      <td>0.087583</td>\n",
              "      <td>-0.005267</td>\n",
              "      <td>-0.103945</td>\n",
              "      <td>-0.196151</td>\n",
              "      <td>-0.270917</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4549</th>\n",
              "      <td>0.173115</td>\n",
              "      <td>0.182854</td>\n",
              "      <td>0.191626</td>\n",
              "      <td>0.199422</td>\n",
              "      <td>0.206289</td>\n",
              "      <td>0.212299</td>\n",
              "      <td>0.217509</td>\n",
              "      <td>0.221928</td>\n",
              "      <td>0.225482</td>\n",
              "      <td>0.228003</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.078112</td>\n",
              "      <td>-0.077942</td>\n",
              "      <td>-0.078028</td>\n",
              "      <td>-0.078270</td>\n",
              "      <td>-0.078531</td>\n",
              "      <td>-0.078654</td>\n",
              "      <td>-0.078493</td>\n",
              "      <td>-0.077941</td>\n",
              "      <td>-0.076955</td>\n",
              "      <td>19.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4092</th>\n",
              "      <td>-0.017760</td>\n",
              "      <td>-0.016474</td>\n",
              "      <td>-0.015390</td>\n",
              "      <td>-0.014666</td>\n",
              "      <td>-0.014388</td>\n",
              "      <td>-0.014557</td>\n",
              "      <td>-0.015102</td>\n",
              "      <td>-0.015894</td>\n",
              "      <td>-0.016773</td>\n",
              "      <td>-0.017577</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.042300</td>\n",
              "      <td>-0.040974</td>\n",
              "      <td>-0.038731</td>\n",
              "      <td>-0.035980</td>\n",
              "      <td>-0.033265</td>\n",
              "      <td>-0.031148</td>\n",
              "      <td>-0.030090</td>\n",
              "      <td>-0.030355</td>\n",
              "      <td>-0.031967</td>\n",
              "      <td>18.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5796</th>\n",
              "      <td>0.195117</td>\n",
              "      <td>0.201263</td>\n",
              "      <td>0.205637</td>\n",
              "      <td>0.207985</td>\n",
              "      <td>0.208163</td>\n",
              "      <td>0.206108</td>\n",
              "      <td>0.201811</td>\n",
              "      <td>0.195307</td>\n",
              "      <td>0.186682</td>\n",
              "      <td>0.176090</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.040166</td>\n",
              "      <td>-0.039460</td>\n",
              "      <td>-0.039233</td>\n",
              "      <td>-0.039064</td>\n",
              "      <td>-0.038449</td>\n",
              "      <td>-0.036960</td>\n",
              "      <td>-0.034394</td>\n",
              "      <td>-0.030848</td>\n",
              "      <td>-0.026697</td>\n",
              "      <td>97.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6203</th>\n",
              "      <td>-0.089165</td>\n",
              "      <td>-0.086464</td>\n",
              "      <td>-0.082844</td>\n",
              "      <td>-0.078363</td>\n",
              "      <td>-0.073256</td>\n",
              "      <td>-0.067948</td>\n",
              "      <td>-0.063018</td>\n",
              "      <td>-0.059105</td>\n",
              "      <td>-0.056762</td>\n",
              "      <td>-0.056321</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.363814</td>\n",
              "      <td>-0.368956</td>\n",
              "      <td>-0.340357</td>\n",
              "      <td>-0.287416</td>\n",
              "      <td>-0.221866</td>\n",
              "      <td>-0.155281</td>\n",
              "      <td>-0.097001</td>\n",
              "      <td>-0.052862</td>\n",
              "      <td>-0.024853</td>\n",
              "      <td>26.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4582</th>\n",
              "      <td>0.175126</td>\n",
              "      <td>0.180752</td>\n",
              "      <td>0.185047</td>\n",
              "      <td>0.188015</td>\n",
              "      <td>0.189637</td>\n",
              "      <td>0.189832</td>\n",
              "      <td>0.188450</td>\n",
              "      <td>0.185305</td>\n",
              "      <td>0.180208</td>\n",
              "      <td>0.173015</td>\n",
              "      <td>...</td>\n",
              "      <td>0.122948</td>\n",
              "      <td>0.106362</td>\n",
              "      <td>0.088842</td>\n",
              "      <td>0.070717</td>\n",
              "      <td>0.052356</td>\n",
              "      <td>0.034165</td>\n",
              "      <td>0.016575</td>\n",
              "      <td>0.000022</td>\n",
              "      <td>-0.015090</td>\n",
              "      <td>77.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3397</th>\n",
              "      <td>-0.045351</td>\n",
              "      <td>-0.049060</td>\n",
              "      <td>-0.052955</td>\n",
              "      <td>-0.056106</td>\n",
              "      <td>-0.057702</td>\n",
              "      <td>-0.057274</td>\n",
              "      <td>-0.054829</td>\n",
              "      <td>-0.050884</td>\n",
              "      <td>-0.046355</td>\n",
              "      <td>-0.042346</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.060359</td>\n",
              "      <td>-0.069059</td>\n",
              "      <td>-0.073274</td>\n",
              "      <td>-0.071696</td>\n",
              "      <td>-0.064275</td>\n",
              "      <td>-0.052043</td>\n",
              "      <td>-0.036758</td>\n",
              "      <td>-0.020448</td>\n",
              "      <td>-0.004983</td>\n",
              "      <td>15.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>242</th>\n",
              "      <td>0.579771</td>\n",
              "      <td>0.695609</td>\n",
              "      <td>0.767799</td>\n",
              "      <td>0.786421</td>\n",
              "      <td>0.749400</td>\n",
              "      <td>0.662886</td>\n",
              "      <td>0.539988</td>\n",
              "      <td>0.398152</td>\n",
              "      <td>0.255832</td>\n",
              "      <td>0.129211</td>\n",
              "      <td>...</td>\n",
              "      <td>0.180357</td>\n",
              "      <td>0.197160</td>\n",
              "      <td>0.214537</td>\n",
              "      <td>0.232097</td>\n",
              "      <td>0.249325</td>\n",
              "      <td>0.265591</td>\n",
              "      <td>0.280164</td>\n",
              "      <td>0.292234</td>\n",
              "      <td>0.300952</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8167</th>\n",
              "      <td>-0.051595</td>\n",
              "      <td>-0.061401</td>\n",
              "      <td>-0.075480</td>\n",
              "      <td>-0.091133</td>\n",
              "      <td>-0.104381</td>\n",
              "      <td>-0.110547</td>\n",
              "      <td>-0.105099</td>\n",
              "      <td>-0.084613</td>\n",
              "      <td>-0.047708</td>\n",
              "      <td>0.004268</td>\n",
              "      <td>...</td>\n",
              "      <td>0.012040</td>\n",
              "      <td>0.015572</td>\n",
              "      <td>0.018201</td>\n",
              "      <td>0.020323</td>\n",
              "      <td>0.022372</td>\n",
              "      <td>0.024790</td>\n",
              "      <td>0.028000</td>\n",
              "      <td>0.032365</td>\n",
              "      <td>0.038159</td>\n",
              "      <td>35.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>24000 rows × 257 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-eef4161a-61d8-464b-9726-803739f78de1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-eef4161a-61d8-464b-9726-803739f78de1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-eef4161a-61d8-464b-9726-803739f78de1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 전처리"
      ],
      "metadata": {
        "id": "JQBBj0ypT8T-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = to_categorical(data_train[256])\n",
        "y_test = to_categorical(data_test[256])\n",
        "y_val = to_categorical(data_val[256])\n",
        "y_train[23999]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "koQxh6-XfrG1",
        "outputId": "86c09241-736c-4cf6-f4b3-01cfd24b089c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_train.drop([256], axis=1, inplace=True)\n",
        "data_test.drop([256], axis=1, inplace=True)\n",
        "data_val.drop([256], axis=1, inplace=True)\n",
        "data_train"
      ],
      "metadata": {
        "id": "QJUFXwCRkGhC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "outputId": "178b2bd1-65ed-492f-de33-1ffce47d0121"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py:4906: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  return super().drop(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           0         1         2         3         4         5         6    \\\n",
              "3079  0.091812  0.067493  0.045227  0.025677  0.009212 -0.004093 -0.014421   \n",
              "888   0.013494  0.006283 -0.000459 -0.006313 -0.010975 -0.014295 -0.016285   \n",
              "4549  0.173115  0.182854  0.191626  0.199422  0.206289  0.212299  0.217509   \n",
              "4092 -0.017760 -0.016474 -0.015390 -0.014666 -0.014388 -0.014557 -0.015102   \n",
              "5796  0.195117  0.201263  0.205637  0.207985  0.208163  0.206108  0.201811   \n",
              "...        ...       ...       ...       ...       ...       ...       ...   \n",
              "6203 -0.089165 -0.086464 -0.082844 -0.078363 -0.073256 -0.067948 -0.063018   \n",
              "4582  0.175126  0.180752  0.185047  0.188015  0.189637  0.189832  0.188450   \n",
              "3397 -0.045351 -0.049060 -0.052955 -0.056106 -0.057702 -0.057274 -0.054829   \n",
              "242   0.579771  0.695609  0.767799  0.786421  0.749400  0.662886  0.539988   \n",
              "8167 -0.051595 -0.061401 -0.075480 -0.091133 -0.104381 -0.110547 -0.105099   \n",
              "\n",
              "           7         8         9    ...       246       247       248  \\\n",
              "3079 -0.022142 -0.027747 -0.031768  ...  0.022385  0.025624  0.026901   \n",
              "888  -0.017112 -0.017064 -0.016505  ...  0.185734  0.221332  0.231960   \n",
              "4549  0.221928  0.225482  0.228003  ... -0.078589 -0.078112 -0.077942   \n",
              "4092 -0.015894 -0.016773 -0.017577  ... -0.042519 -0.042300 -0.040974   \n",
              "5796  0.195307  0.186682  0.176090  ... -0.041576 -0.040166 -0.039460   \n",
              "...        ...       ...       ...  ...       ...       ...       ...   \n",
              "6203 -0.059105 -0.056762 -0.056321  ... -0.320205 -0.363814 -0.368956   \n",
              "4582  0.185305  0.180208  0.173015  ...  0.138298  0.122948  0.106362   \n",
              "3397 -0.050884 -0.046355 -0.042346  ... -0.049649 -0.060359 -0.069059   \n",
              "242   0.398152  0.255832  0.129211  ...  0.164391  0.180357  0.197160   \n",
              "8167 -0.084613 -0.047708  0.004268  ...  0.007282  0.012040  0.015572   \n",
              "\n",
              "           249       250       251       252       253       254       255  \n",
              "3079  0.026220  0.023670  0.019396  0.013589  0.006494 -0.001559 -0.010138  \n",
              "888   0.212712  0.163127  0.087583 -0.005267 -0.103945 -0.196151 -0.270917  \n",
              "4549 -0.078028 -0.078270 -0.078531 -0.078654 -0.078493 -0.077941 -0.076955  \n",
              "4092 -0.038731 -0.035980 -0.033265 -0.031148 -0.030090 -0.030355 -0.031967  \n",
              "5796 -0.039233 -0.039064 -0.038449 -0.036960 -0.034394 -0.030848 -0.026697  \n",
              "...        ...       ...       ...       ...       ...       ...       ...  \n",
              "6203 -0.340357 -0.287416 -0.221866 -0.155281 -0.097001 -0.052862 -0.024853  \n",
              "4582  0.088842  0.070717  0.052356  0.034165  0.016575  0.000022 -0.015090  \n",
              "3397 -0.073274 -0.071696 -0.064275 -0.052043 -0.036758 -0.020448 -0.004983  \n",
              "242   0.214537  0.232097  0.249325  0.265591  0.280164  0.292234  0.300952  \n",
              "8167  0.018201  0.020323  0.022372  0.024790  0.028000  0.032365  0.038159  \n",
              "\n",
              "[24000 rows x 256 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f0b34099-d87f-4b68-9443-494c1142a7ec\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>246</th>\n",
              "      <th>247</th>\n",
              "      <th>248</th>\n",
              "      <th>249</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3079</th>\n",
              "      <td>0.091812</td>\n",
              "      <td>0.067493</td>\n",
              "      <td>0.045227</td>\n",
              "      <td>0.025677</td>\n",
              "      <td>0.009212</td>\n",
              "      <td>-0.004093</td>\n",
              "      <td>-0.014421</td>\n",
              "      <td>-0.022142</td>\n",
              "      <td>-0.027747</td>\n",
              "      <td>-0.031768</td>\n",
              "      <td>...</td>\n",
              "      <td>0.022385</td>\n",
              "      <td>0.025624</td>\n",
              "      <td>0.026901</td>\n",
              "      <td>0.026220</td>\n",
              "      <td>0.023670</td>\n",
              "      <td>0.019396</td>\n",
              "      <td>0.013589</td>\n",
              "      <td>0.006494</td>\n",
              "      <td>-0.001559</td>\n",
              "      <td>-0.010138</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>888</th>\n",
              "      <td>0.013494</td>\n",
              "      <td>0.006283</td>\n",
              "      <td>-0.000459</td>\n",
              "      <td>-0.006313</td>\n",
              "      <td>-0.010975</td>\n",
              "      <td>-0.014295</td>\n",
              "      <td>-0.016285</td>\n",
              "      <td>-0.017112</td>\n",
              "      <td>-0.017064</td>\n",
              "      <td>-0.016505</td>\n",
              "      <td>...</td>\n",
              "      <td>0.185734</td>\n",
              "      <td>0.221332</td>\n",
              "      <td>0.231960</td>\n",
              "      <td>0.212712</td>\n",
              "      <td>0.163127</td>\n",
              "      <td>0.087583</td>\n",
              "      <td>-0.005267</td>\n",
              "      <td>-0.103945</td>\n",
              "      <td>-0.196151</td>\n",
              "      <td>-0.270917</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4549</th>\n",
              "      <td>0.173115</td>\n",
              "      <td>0.182854</td>\n",
              "      <td>0.191626</td>\n",
              "      <td>0.199422</td>\n",
              "      <td>0.206289</td>\n",
              "      <td>0.212299</td>\n",
              "      <td>0.217509</td>\n",
              "      <td>0.221928</td>\n",
              "      <td>0.225482</td>\n",
              "      <td>0.228003</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.078589</td>\n",
              "      <td>-0.078112</td>\n",
              "      <td>-0.077942</td>\n",
              "      <td>-0.078028</td>\n",
              "      <td>-0.078270</td>\n",
              "      <td>-0.078531</td>\n",
              "      <td>-0.078654</td>\n",
              "      <td>-0.078493</td>\n",
              "      <td>-0.077941</td>\n",
              "      <td>-0.076955</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4092</th>\n",
              "      <td>-0.017760</td>\n",
              "      <td>-0.016474</td>\n",
              "      <td>-0.015390</td>\n",
              "      <td>-0.014666</td>\n",
              "      <td>-0.014388</td>\n",
              "      <td>-0.014557</td>\n",
              "      <td>-0.015102</td>\n",
              "      <td>-0.015894</td>\n",
              "      <td>-0.016773</td>\n",
              "      <td>-0.017577</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.042519</td>\n",
              "      <td>-0.042300</td>\n",
              "      <td>-0.040974</td>\n",
              "      <td>-0.038731</td>\n",
              "      <td>-0.035980</td>\n",
              "      <td>-0.033265</td>\n",
              "      <td>-0.031148</td>\n",
              "      <td>-0.030090</td>\n",
              "      <td>-0.030355</td>\n",
              "      <td>-0.031967</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5796</th>\n",
              "      <td>0.195117</td>\n",
              "      <td>0.201263</td>\n",
              "      <td>0.205637</td>\n",
              "      <td>0.207985</td>\n",
              "      <td>0.208163</td>\n",
              "      <td>0.206108</td>\n",
              "      <td>0.201811</td>\n",
              "      <td>0.195307</td>\n",
              "      <td>0.186682</td>\n",
              "      <td>0.176090</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.041576</td>\n",
              "      <td>-0.040166</td>\n",
              "      <td>-0.039460</td>\n",
              "      <td>-0.039233</td>\n",
              "      <td>-0.039064</td>\n",
              "      <td>-0.038449</td>\n",
              "      <td>-0.036960</td>\n",
              "      <td>-0.034394</td>\n",
              "      <td>-0.030848</td>\n",
              "      <td>-0.026697</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6203</th>\n",
              "      <td>-0.089165</td>\n",
              "      <td>-0.086464</td>\n",
              "      <td>-0.082844</td>\n",
              "      <td>-0.078363</td>\n",
              "      <td>-0.073256</td>\n",
              "      <td>-0.067948</td>\n",
              "      <td>-0.063018</td>\n",
              "      <td>-0.059105</td>\n",
              "      <td>-0.056762</td>\n",
              "      <td>-0.056321</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.320205</td>\n",
              "      <td>-0.363814</td>\n",
              "      <td>-0.368956</td>\n",
              "      <td>-0.340357</td>\n",
              "      <td>-0.287416</td>\n",
              "      <td>-0.221866</td>\n",
              "      <td>-0.155281</td>\n",
              "      <td>-0.097001</td>\n",
              "      <td>-0.052862</td>\n",
              "      <td>-0.024853</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4582</th>\n",
              "      <td>0.175126</td>\n",
              "      <td>0.180752</td>\n",
              "      <td>0.185047</td>\n",
              "      <td>0.188015</td>\n",
              "      <td>0.189637</td>\n",
              "      <td>0.189832</td>\n",
              "      <td>0.188450</td>\n",
              "      <td>0.185305</td>\n",
              "      <td>0.180208</td>\n",
              "      <td>0.173015</td>\n",
              "      <td>...</td>\n",
              "      <td>0.138298</td>\n",
              "      <td>0.122948</td>\n",
              "      <td>0.106362</td>\n",
              "      <td>0.088842</td>\n",
              "      <td>0.070717</td>\n",
              "      <td>0.052356</td>\n",
              "      <td>0.034165</td>\n",
              "      <td>0.016575</td>\n",
              "      <td>0.000022</td>\n",
              "      <td>-0.015090</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3397</th>\n",
              "      <td>-0.045351</td>\n",
              "      <td>-0.049060</td>\n",
              "      <td>-0.052955</td>\n",
              "      <td>-0.056106</td>\n",
              "      <td>-0.057702</td>\n",
              "      <td>-0.057274</td>\n",
              "      <td>-0.054829</td>\n",
              "      <td>-0.050884</td>\n",
              "      <td>-0.046355</td>\n",
              "      <td>-0.042346</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.049649</td>\n",
              "      <td>-0.060359</td>\n",
              "      <td>-0.069059</td>\n",
              "      <td>-0.073274</td>\n",
              "      <td>-0.071696</td>\n",
              "      <td>-0.064275</td>\n",
              "      <td>-0.052043</td>\n",
              "      <td>-0.036758</td>\n",
              "      <td>-0.020448</td>\n",
              "      <td>-0.004983</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>242</th>\n",
              "      <td>0.579771</td>\n",
              "      <td>0.695609</td>\n",
              "      <td>0.767799</td>\n",
              "      <td>0.786421</td>\n",
              "      <td>0.749400</td>\n",
              "      <td>0.662886</td>\n",
              "      <td>0.539988</td>\n",
              "      <td>0.398152</td>\n",
              "      <td>0.255832</td>\n",
              "      <td>0.129211</td>\n",
              "      <td>...</td>\n",
              "      <td>0.164391</td>\n",
              "      <td>0.180357</td>\n",
              "      <td>0.197160</td>\n",
              "      <td>0.214537</td>\n",
              "      <td>0.232097</td>\n",
              "      <td>0.249325</td>\n",
              "      <td>0.265591</td>\n",
              "      <td>0.280164</td>\n",
              "      <td>0.292234</td>\n",
              "      <td>0.300952</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8167</th>\n",
              "      <td>-0.051595</td>\n",
              "      <td>-0.061401</td>\n",
              "      <td>-0.075480</td>\n",
              "      <td>-0.091133</td>\n",
              "      <td>-0.104381</td>\n",
              "      <td>-0.110547</td>\n",
              "      <td>-0.105099</td>\n",
              "      <td>-0.084613</td>\n",
              "      <td>-0.047708</td>\n",
              "      <td>0.004268</td>\n",
              "      <td>...</td>\n",
              "      <td>0.007282</td>\n",
              "      <td>0.012040</td>\n",
              "      <td>0.015572</td>\n",
              "      <td>0.018201</td>\n",
              "      <td>0.020323</td>\n",
              "      <td>0.022372</td>\n",
              "      <td>0.024790</td>\n",
              "      <td>0.028000</td>\n",
              "      <td>0.032365</td>\n",
              "      <td>0.038159</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>24000 rows × 256 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f0b34099-d87f-4b68-9443-494c1142a7ec')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f0b34099-d87f-4b68-9443-494c1142a7ec button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f0b34099-d87f-4b68-9443-494c1142a7ec');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train=data_train.values\n",
        "X_test=data_test.values\n",
        "X_val=data_val.values"
      ],
      "metadata": {
        "id": "EK7Q96RZtwZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(X_val.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)\n",
        "print(y_val.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwfh9AwhjkMM",
        "outputId": "82d93422-9983-4047-9d0a-2e5864b2b79b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(24000, 256)\n",
            "(6000, 256)\n",
            "(6000, 256)\n",
            "(24000, 101)\n",
            "(6000, 101)\n",
            "(6000, 101)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X_train = X_train.reshape(24000, 256, 1)\n",
        "X_test = X_test.reshape(6000, 256, 1)\n",
        "X_val = X_val.reshape(6000, 256, 1)\n",
        "X_train.shape, X_test.shape, X_val.shape"
      ],
      "metadata": {
        "id": "50YyZlL9htW3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e18e26a0-0da9-4f23-c0d0-c5f19eb9552c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((24000, 256, 1), (6000, 256, 1), (6000, 256, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***gru모델(이 모델을 실행했습니다)***"
      ],
      "metadata": {
        "id": "Na73oRb9Xq69"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Input, Embedding, GRU, LSTM, SimpleRNN, MaxPooling1D, GlobalMaxPool1D, CuDNNGRU\n",
        "from keras.layers import Dropout, Dense, Activation, Flatten, Conv1D, SpatialDropout1D\n",
        "from keras.models import Sequential\n",
        "\n",
        "model = Sequential()\n",
        "model.add(GRU(units=100, return_sequences=True, input_shape=(256,1))) #shape[1]=열\n",
        "model.add(GRU(units=50, return_sequences=True))\n",
        "model.add(GRU(units=50, return_sequences=True))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Dense(101, activation='softmax'))\n",
        "model.compile(optimizer='adam', loss='mse',metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWyVmfGGXar5",
        "outputId": "772cd282-d709-4783-9b22-8f180ba2cad0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " gru (GRU)                   (None, 256, 100)          30900     \n",
            "                                                                 \n",
            " gru_1 (GRU)                 (None, 256, 50)           22800     \n",
            "                                                                 \n",
            " gru_2 (GRU)                 (None, 256, 50)           15300     \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 12800)             0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 12800)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 101)               1292901   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,361,901\n",
            "Trainable params: 1,361,901\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## cnn"
      ],
      "metadata": {
        "id": "EgfaelmnXe04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model():\n",
        "    model = Sequential()\n",
        "    model.add(Conv1D(filters=64, kernel_size=6, activation='relu', \n",
        "                    padding='same', input_shape=(256, 1)))\n",
        "    model.add(BatchNormalization())\n",
        "    \n",
        "    # adding a pooling layer\n",
        "    model.add(MaxPooling1D(pool_size=(3), strides=2, padding='same'))\n",
        "    \n",
        "    model.add(Conv1D(filters=64, kernel_size=6, activation='relu', \n",
        "                    padding='same', input_shape=(256, 1)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling1D(pool_size=(3), strides=2, padding='same'))\n",
        "    \n",
        "    model.add(Conv1D(filters=64, kernel_size=6, activation='relu', \n",
        "                    padding='same', input_shape=(256, 1)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling1D(pool_size=(3), strides=2, padding='same'))\n",
        "    \n",
        "    model.add(Flatten())\n",
        "    #model.add(Dense(64, activation='relu'))\n",
        "    #model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(101, activation='softmax'))\n",
        "    \n",
        "    model.compile(optimizer='adam', loss='mse',metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "DqFKm7PSQS3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model()\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "wum-FyK7QVUe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "031d14c5-2a85-4ff5-858d-e68ad3ecaede"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d (Conv1D)             (None, 256, 64)           448       \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 256, 64)          256       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1D  (None, 128, 64)          0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, 128, 64)           24640     \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 128, 64)          256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling1d_1 (MaxPooling  (None, 64, 64)           0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " conv1d_2 (Conv1D)           (None, 64, 64)            24640     \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 64, 64)           256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling1d_2 (MaxPooling  (None, 32, 64)           0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 2048)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 101)               206949    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 257,445\n",
            "Trainable params: 257,061\n",
            "Non-trainable params: 384\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## cnn-gru"
      ],
      "metadata": {
        "id": "m-Hm3lsWXkbE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Input, Embedding, GRU, LSTM, MaxPooling1D, GlobalMaxPool1D, CuDNNGRU\n",
        "from keras.layers import Dropout, Dense, Activation, Flatten, Conv1D, SpatialDropout1D\n",
        "from keras.models import Sequential\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv1D(filters=64, kernel_size=6, activation='relu', padding='same', input_shape=(256, 1)))\n",
        "model.add(SpatialDropout1D(0.2))\n",
        "model.add(MaxPooling1D(4))\n",
        "model.add(CuDNNGRU(64))\n",
        "\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Dense(101, activation='softmax'))\n",
        "model.compile(optimizer='adam', loss='mse',metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXzfgBF75uk-",
        "outputId": "33f77877-e599-4ded-8400-373f5cec14bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_13 (Conv1D)          (None, 256, 64)           448       \n",
            "                                                                 \n",
            " spatial_dropout1d_4 (Spatia  (None, 256, 64)          0         \n",
            " lDropout1D)                                                     \n",
            "                                                                 \n",
            " max_pooling1d_13 (MaxPoolin  (None, 64, 64)           0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " cu_dnngru_4 (CuDNNGRU)      (None, 64)                24960     \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 101)               6565      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 31,973\n",
            "Trainable params: 31,973\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSTM"
      ],
      "metadata": {
        "id": "DBnTIBCDGX94"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(units=100, return_sequences=True, input_shape=(256,1))) #shape[1]=열\n",
        "model.add(LSTM(units=50, return_sequences=True))\n",
        "model.add(LSTM(units=50, return_sequences=True))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Dense(101, activation='softmax'))\n",
        "model.compile(optimizer='adam', loss='mse',metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jbh5mP0GWh5",
        "outputId": "8055881e-f18e-4941-e7d3-1d853b39d2e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_3 (LSTM)               (None, 256, 100)          40800     \n",
            "                                                                 \n",
            " lstm_4 (LSTM)               (None, 256, 50)           30200     \n",
            "                                                                 \n",
            " lstm_5 (LSTM)               (None, 256, 50)           20200     \n",
            "                                                                 \n",
            " flatten_6 (Flatten)         (None, 12800)             0         \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 12800)             0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 101)               1292901   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,384,101\n",
            "Trainable params: 1,384,101\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RNN"
      ],
      "metadata": {
        "id": "mLO6GGJ5Nn9w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Input, Embedding, GRU, LSTM, SimpleRNN, MaxPooling1D, GlobalMaxPool1D, CuDNNGRU\n",
        "from keras.layers import Dropout, Dense, Activation, Flatten, Conv1D, SpatialDropout1D\n",
        "from keras.models import Sequential\n",
        "\n",
        "model = Sequential()\n",
        "model.add(SimpleRNN(units=100, return_sequences=True, input_shape=(256,1))) #shape[1]=열\n",
        "model.add(SimpleRNN(units=50, return_sequences=True))\n",
        "model.add(SimpleRNN(units=50, return_sequences=True))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(101, activation='softmax'))\n",
        "model.compile(optimizer='adam', loss='mse',metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHOArXJVNnd0",
        "outputId": "4f56aa04-4c34-4860-fc44-299596d4f96f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " simple_rnn_12 (SimpleRNN)   (None, 256, 100)          10200     \n",
            "                                                                 \n",
            " simple_rnn_13 (SimpleRNN)   (None, 256, 50)           7550      \n",
            "                                                                 \n",
            " simple_rnn_14 (SimpleRNN)   (None, 256, 50)           5050      \n",
            "                                                                 \n",
            " flatten_4 (Flatten)         (None, 12800)             0         \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 12800)             0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 101)               1292901   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,315,701\n",
            "Trainable params: 1,315,701\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델학습/평가"
      ],
      "metadata": {
        "id": "bWixFuhvKbcz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train, y_train, batch_size = 1024, epochs = 1000, verbose = 1, validation_data=(X_val, y_val))"
      ],
      "metadata": {
        "id": "t-UtiKqRQYVv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55d110ab-6e90-4b1d-dda8-9eee3934d86b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "24/24 [==============================] - 13s 164ms/step - loss: 0.0098 - accuracy: 0.0095 - val_loss: 0.0098 - val_accuracy: 0.0085\n",
            "Epoch 2/1000\n",
            "24/24 [==============================] - 3s 124ms/step - loss: 0.0098 - accuracy: 0.0096 - val_loss: 0.0098 - val_accuracy: 0.0078\n",
            "Epoch 3/1000\n",
            "24/24 [==============================] - 3s 125ms/step - loss: 0.0098 - accuracy: 0.0097 - val_loss: 0.0098 - val_accuracy: 0.0065\n",
            "Epoch 4/1000\n",
            "24/24 [==============================] - 3s 125ms/step - loss: 0.0098 - accuracy: 0.0109 - val_loss: 0.0098 - val_accuracy: 0.0088\n",
            "Epoch 5/1000\n",
            "24/24 [==============================] - 3s 125ms/step - loss: 0.0098 - accuracy: 0.0095 - val_loss: 0.0098 - val_accuracy: 0.0073\n",
            "Epoch 6/1000\n",
            "24/24 [==============================] - 3s 125ms/step - loss: 0.0098 - accuracy: 0.0111 - val_loss: 0.0098 - val_accuracy: 0.0105\n",
            "Epoch 7/1000\n",
            "24/24 [==============================] - 3s 126ms/step - loss: 0.0098 - accuracy: 0.0150 - val_loss: 0.0098 - val_accuracy: 0.0103\n",
            "Epoch 8/1000\n",
            "24/24 [==============================] - 3s 126ms/step - loss: 0.0098 - accuracy: 0.0160 - val_loss: 0.0098 - val_accuracy: 0.0130\n",
            "Epoch 9/1000\n",
            "24/24 [==============================] - 3s 126ms/step - loss: 0.0098 - accuracy: 0.0172 - val_loss: 0.0098 - val_accuracy: 0.0073\n",
            "Epoch 10/1000\n",
            "24/24 [==============================] - 3s 126ms/step - loss: 0.0098 - accuracy: 0.0158 - val_loss: 0.0098 - val_accuracy: 0.0128\n",
            "Epoch 11/1000\n",
            "24/24 [==============================] - 3s 127ms/step - loss: 0.0098 - accuracy: 0.0244 - val_loss: 0.0098 - val_accuracy: 0.0232\n",
            "Epoch 12/1000\n",
            "24/24 [==============================] - 3s 127ms/step - loss: 0.0097 - accuracy: 0.0432 - val_loss: 0.0098 - val_accuracy: 0.0327\n",
            "Epoch 13/1000\n",
            "24/24 [==============================] - 3s 127ms/step - loss: 0.0097 - accuracy: 0.0507 - val_loss: 0.0098 - val_accuracy: 0.0433\n",
            "Epoch 14/1000\n",
            "24/24 [==============================] - 3s 128ms/step - loss: 0.0097 - accuracy: 0.0630 - val_loss: 0.0098 - val_accuracy: 0.0460\n",
            "Epoch 15/1000\n",
            "24/24 [==============================] - 3s 128ms/step - loss: 0.0097 - accuracy: 0.0717 - val_loss: 0.0098 - val_accuracy: 0.0520\n",
            "Epoch 16/1000\n",
            "24/24 [==============================] - 3s 128ms/step - loss: 0.0096 - accuracy: 0.0809 - val_loss: 0.0097 - val_accuracy: 0.0675\n",
            "Epoch 17/1000\n",
            "24/24 [==============================] - 3s 129ms/step - loss: 0.0096 - accuracy: 0.0916 - val_loss: 0.0097 - val_accuracy: 0.0802\n",
            "Epoch 18/1000\n",
            "24/24 [==============================] - 3s 129ms/step - loss: 0.0096 - accuracy: 0.1035 - val_loss: 0.0097 - val_accuracy: 0.0778\n",
            "Epoch 19/1000\n",
            "24/24 [==============================] - 3s 129ms/step - loss: 0.0095 - accuracy: 0.1195 - val_loss: 0.0096 - val_accuracy: 0.0865\n",
            "Epoch 20/1000\n",
            "24/24 [==============================] - 3s 129ms/step - loss: 0.0094 - accuracy: 0.1335 - val_loss: 0.0096 - val_accuracy: 0.1097\n",
            "Epoch 21/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 0.0093 - accuracy: 0.1525 - val_loss: 0.0095 - val_accuracy: 0.1287\n",
            "Epoch 22/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 0.0092 - accuracy: 0.1720 - val_loss: 0.0095 - val_accuracy: 0.1312\n",
            "Epoch 23/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 0.0091 - accuracy: 0.1869 - val_loss: 0.0094 - val_accuracy: 0.1405\n",
            "Epoch 24/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 0.0090 - accuracy: 0.2044 - val_loss: 0.0094 - val_accuracy: 0.1520\n",
            "Epoch 25/1000\n",
            "24/24 [==============================] - 3s 130ms/step - loss: 0.0089 - accuracy: 0.2225 - val_loss: 0.0093 - val_accuracy: 0.1647\n",
            "Epoch 26/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 0.0087 - accuracy: 0.2395 - val_loss: 0.0092 - val_accuracy: 0.1862\n",
            "Epoch 27/1000\n",
            "24/24 [==============================] - 3s 131ms/step - loss: 0.0086 - accuracy: 0.2617 - val_loss: 0.0091 - val_accuracy: 0.2020\n",
            "Epoch 28/1000\n",
            "24/24 [==============================] - 3s 132ms/step - loss: 0.0084 - accuracy: 0.2840 - val_loss: 0.0090 - val_accuracy: 0.2253\n",
            "Epoch 29/1000\n",
            "24/24 [==============================] - 3s 132ms/step - loss: 0.0082 - accuracy: 0.3069 - val_loss: 0.0088 - val_accuracy: 0.2462\n",
            "Epoch 30/1000\n",
            "24/24 [==============================] - 3s 133ms/step - loss: 0.0080 - accuracy: 0.3353 - val_loss: 0.0086 - val_accuracy: 0.2743\n",
            "Epoch 31/1000\n",
            "24/24 [==============================] - 3s 133ms/step - loss: 0.0077 - accuracy: 0.3658 - val_loss: 0.0085 - val_accuracy: 0.2923\n",
            "Epoch 32/1000\n",
            "24/24 [==============================] - 3s 133ms/step - loss: 0.0075 - accuracy: 0.3901 - val_loss: 0.0083 - val_accuracy: 0.3210\n",
            "Epoch 33/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 0.0073 - accuracy: 0.4127 - val_loss: 0.0082 - val_accuracy: 0.3212\n",
            "Epoch 34/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 0.0071 - accuracy: 0.4360 - val_loss: 0.0079 - val_accuracy: 0.3590\n",
            "Epoch 35/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 0.0068 - accuracy: 0.4667 - val_loss: 0.0077 - val_accuracy: 0.3870\n",
            "Epoch 36/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 0.0065 - accuracy: 0.4933 - val_loss: 0.0075 - val_accuracy: 0.4093\n",
            "Epoch 37/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 0.0063 - accuracy: 0.5166 - val_loss: 0.0072 - val_accuracy: 0.4357\n",
            "Epoch 38/1000\n",
            "24/24 [==============================] - 3s 136ms/step - loss: 0.0059 - accuracy: 0.5458 - val_loss: 0.0071 - val_accuracy: 0.4497\n",
            "Epoch 39/1000\n",
            "24/24 [==============================] - 3s 136ms/step - loss: 0.0057 - accuracy: 0.5702 - val_loss: 0.0068 - val_accuracy: 0.4785\n",
            "Epoch 40/1000\n",
            "24/24 [==============================] - 3s 136ms/step - loss: 0.0054 - accuracy: 0.5926 - val_loss: 0.0067 - val_accuracy: 0.4907\n",
            "Epoch 41/1000\n",
            "24/24 [==============================] - 3s 136ms/step - loss: 0.0052 - accuracy: 0.6093 - val_loss: 0.0065 - val_accuracy: 0.5108\n",
            "Epoch 42/1000\n",
            "24/24 [==============================] - 3s 136ms/step - loss: 0.0050 - accuracy: 0.6295 - val_loss: 0.0063 - val_accuracy: 0.5295\n",
            "Epoch 43/1000\n",
            "24/24 [==============================] - 3s 137ms/step - loss: 0.0048 - accuracy: 0.6480 - val_loss: 0.0060 - val_accuracy: 0.5568\n",
            "Epoch 44/1000\n",
            "24/24 [==============================] - 3s 136ms/step - loss: 0.0045 - accuracy: 0.6707 - val_loss: 0.0058 - val_accuracy: 0.5715\n",
            "Epoch 45/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 0.0043 - accuracy: 0.6870 - val_loss: 0.0058 - val_accuracy: 0.5765\n",
            "Epoch 46/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 0.0041 - accuracy: 0.7026 - val_loss: 0.0055 - val_accuracy: 0.5983\n",
            "Epoch 47/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 0.0039 - accuracy: 0.7208 - val_loss: 0.0053 - val_accuracy: 0.6157\n",
            "Epoch 48/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 0.0036 - accuracy: 0.7379 - val_loss: 0.0051 - val_accuracy: 0.6360\n",
            "Epoch 49/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 0.0034 - accuracy: 0.7559 - val_loss: 0.0048 - val_accuracy: 0.6505\n",
            "Epoch 50/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 0.0032 - accuracy: 0.7681 - val_loss: 0.0047 - val_accuracy: 0.6665\n",
            "Epoch 51/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 0.0030 - accuracy: 0.7829 - val_loss: 0.0046 - val_accuracy: 0.6745\n",
            "Epoch 52/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 0.0029 - accuracy: 0.7915 - val_loss: 0.0045 - val_accuracy: 0.6788\n",
            "Epoch 53/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 0.0028 - accuracy: 0.7999 - val_loss: 0.0043 - val_accuracy: 0.6920\n",
            "Epoch 54/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 0.0027 - accuracy: 0.8095 - val_loss: 0.0042 - val_accuracy: 0.7035\n",
            "Epoch 55/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 0.0026 - accuracy: 0.8186 - val_loss: 0.0042 - val_accuracy: 0.6985\n",
            "Epoch 56/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 0.0025 - accuracy: 0.8247 - val_loss: 0.0039 - val_accuracy: 0.7218\n",
            "Epoch 57/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 0.0024 - accuracy: 0.8332 - val_loss: 0.0040 - val_accuracy: 0.7175\n",
            "Epoch 58/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 0.0023 - accuracy: 0.8365 - val_loss: 0.0039 - val_accuracy: 0.7252\n",
            "Epoch 59/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 0.0022 - accuracy: 0.8425 - val_loss: 0.0038 - val_accuracy: 0.7297\n",
            "Epoch 60/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 0.0022 - accuracy: 0.8465 - val_loss: 0.0038 - val_accuracy: 0.7378\n",
            "Epoch 61/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 0.0021 - accuracy: 0.8517 - val_loss: 0.0036 - val_accuracy: 0.7450\n",
            "Epoch 62/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 0.0020 - accuracy: 0.8608 - val_loss: 0.0036 - val_accuracy: 0.7488\n",
            "Epoch 63/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 0.0019 - accuracy: 0.8674 - val_loss: 0.0035 - val_accuracy: 0.7570\n",
            "Epoch 64/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 0.0018 - accuracy: 0.8730 - val_loss: 0.0035 - val_accuracy: 0.7547\n",
            "Epoch 65/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 0.0017 - accuracy: 0.8773 - val_loss: 0.0034 - val_accuracy: 0.7592\n",
            "Epoch 66/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 0.0017 - accuracy: 0.8793 - val_loss: 0.0033 - val_accuracy: 0.7705\n",
            "Epoch 67/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 0.0016 - accuracy: 0.8845 - val_loss: 0.0033 - val_accuracy: 0.7742\n",
            "Epoch 68/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 0.0016 - accuracy: 0.8884 - val_loss: 0.0033 - val_accuracy: 0.7752\n",
            "Epoch 69/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 0.0015 - accuracy: 0.8913 - val_loss: 0.0033 - val_accuracy: 0.7738\n",
            "Epoch 70/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 0.0015 - accuracy: 0.8951 - val_loss: 0.0032 - val_accuracy: 0.7780\n",
            "Epoch 71/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 0.0014 - accuracy: 0.8975 - val_loss: 0.0031 - val_accuracy: 0.7832\n",
            "Epoch 72/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 0.0014 - accuracy: 0.9013 - val_loss: 0.0031 - val_accuracy: 0.7845\n",
            "Epoch 73/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 0.0013 - accuracy: 0.9049 - val_loss: 0.0030 - val_accuracy: 0.7928\n",
            "Epoch 74/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 0.0013 - accuracy: 0.9090 - val_loss: 0.0030 - val_accuracy: 0.7917\n",
            "Epoch 75/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 0.0013 - accuracy: 0.9121 - val_loss: 0.0029 - val_accuracy: 0.7973\n",
            "Epoch 76/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 0.0012 - accuracy: 0.9150 - val_loss: 0.0029 - val_accuracy: 0.8053\n",
            "Epoch 77/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 0.0012 - accuracy: 0.9167 - val_loss: 0.0029 - val_accuracy: 0.8048\n",
            "Epoch 78/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 0.0011 - accuracy: 0.9198 - val_loss: 0.0028 - val_accuracy: 0.8067\n",
            "Epoch 79/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 0.0011 - accuracy: 0.9224 - val_loss: 0.0028 - val_accuracy: 0.8093\n",
            "Epoch 80/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 0.0011 - accuracy: 0.9241 - val_loss: 0.0027 - val_accuracy: 0.8147\n",
            "Epoch 81/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 0.0010 - accuracy: 0.9253 - val_loss: 0.0027 - val_accuracy: 0.8125\n",
            "Epoch 82/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 9.9038e-04 - accuracy: 0.9285 - val_loss: 0.0027 - val_accuracy: 0.8147\n",
            "Epoch 83/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 9.5930e-04 - accuracy: 0.9312 - val_loss: 0.0027 - val_accuracy: 0.8197\n",
            "Epoch 84/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 9.2933e-04 - accuracy: 0.9331 - val_loss: 0.0026 - val_accuracy: 0.8215\n",
            "Epoch 85/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 9.0553e-04 - accuracy: 0.9344 - val_loss: 0.0026 - val_accuracy: 0.8233\n",
            "Epoch 86/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 8.8278e-04 - accuracy: 0.9363 - val_loss: 0.0026 - val_accuracy: 0.8212\n",
            "Epoch 87/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 8.4909e-04 - accuracy: 0.9393 - val_loss: 0.0025 - val_accuracy: 0.8312\n",
            "Epoch 88/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 8.2468e-04 - accuracy: 0.9403 - val_loss: 0.0025 - val_accuracy: 0.8323\n",
            "Epoch 89/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 8.0695e-04 - accuracy: 0.9415 - val_loss: 0.0025 - val_accuracy: 0.8318\n",
            "Epoch 90/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 7.8052e-04 - accuracy: 0.9430 - val_loss: 0.0024 - val_accuracy: 0.8352\n",
            "Epoch 91/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 7.4805e-04 - accuracy: 0.9453 - val_loss: 0.0024 - val_accuracy: 0.8347\n",
            "Epoch 92/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 7.3027e-04 - accuracy: 0.9466 - val_loss: 0.0023 - val_accuracy: 0.8423\n",
            "Epoch 93/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 7.2865e-04 - accuracy: 0.9473 - val_loss: 0.0024 - val_accuracy: 0.8392\n",
            "Epoch 94/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 7.0850e-04 - accuracy: 0.9488 - val_loss: 0.0024 - val_accuracy: 0.8385\n",
            "Epoch 95/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 7.1549e-04 - accuracy: 0.9490 - val_loss: 0.0025 - val_accuracy: 0.8343\n",
            "Epoch 96/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 7.2861e-04 - accuracy: 0.9488 - val_loss: 0.0024 - val_accuracy: 0.8418\n",
            "Epoch 97/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 6.8988e-04 - accuracy: 0.9513 - val_loss: 0.0024 - val_accuracy: 0.8405\n",
            "Epoch 98/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 6.7662e-04 - accuracy: 0.9519 - val_loss: 0.0024 - val_accuracy: 0.8397\n",
            "Epoch 99/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 6.4353e-04 - accuracy: 0.9534 - val_loss: 0.0024 - val_accuracy: 0.8392\n",
            "Epoch 100/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 6.3167e-04 - accuracy: 0.9546 - val_loss: 0.0023 - val_accuracy: 0.8472\n",
            "Epoch 101/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 6.4279e-04 - accuracy: 0.9539 - val_loss: 0.0024 - val_accuracy: 0.8437\n",
            "Epoch 102/1000\n",
            "24/24 [==============================] - 3s 136ms/step - loss: 6.2063e-04 - accuracy: 0.9554 - val_loss: 0.0023 - val_accuracy: 0.8433\n",
            "Epoch 103/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 6.0007e-04 - accuracy: 0.9562 - val_loss: 0.0023 - val_accuracy: 0.8465\n",
            "Epoch 104/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 5.8932e-04 - accuracy: 0.9565 - val_loss: 0.0023 - val_accuracy: 0.8443\n",
            "Epoch 105/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 5.9013e-04 - accuracy: 0.9567 - val_loss: 0.0023 - val_accuracy: 0.8468\n",
            "Epoch 106/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 5.9338e-04 - accuracy: 0.9567 - val_loss: 0.0023 - val_accuracy: 0.8475\n",
            "Epoch 107/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 5.8412e-04 - accuracy: 0.9578 - val_loss: 0.0023 - val_accuracy: 0.8448\n",
            "Epoch 108/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 5.7094e-04 - accuracy: 0.9585 - val_loss: 0.0022 - val_accuracy: 0.8527\n",
            "Epoch 109/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 5.5334e-04 - accuracy: 0.9595 - val_loss: 0.0023 - val_accuracy: 0.8475\n",
            "Epoch 110/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 5.4475e-04 - accuracy: 0.9598 - val_loss: 0.0022 - val_accuracy: 0.8495\n",
            "Epoch 111/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 5.2488e-04 - accuracy: 0.9605 - val_loss: 0.0022 - val_accuracy: 0.8500\n",
            "Epoch 112/1000\n",
            "24/24 [==============================] - 3s 136ms/step - loss: 5.1453e-04 - accuracy: 0.9614 - val_loss: 0.0021 - val_accuracy: 0.8560\n",
            "Epoch 113/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 5.1364e-04 - accuracy: 0.9614 - val_loss: 0.0022 - val_accuracy: 0.8568\n",
            "Epoch 114/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 5.1299e-04 - accuracy: 0.9618 - val_loss: 0.0022 - val_accuracy: 0.8538\n",
            "Epoch 115/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 5.3151e-04 - accuracy: 0.9611 - val_loss: 0.0022 - val_accuracy: 0.8545\n",
            "Epoch 116/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 5.3927e-04 - accuracy: 0.9614 - val_loss: 0.0022 - val_accuracy: 0.8537\n",
            "Epoch 117/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 5.1201e-04 - accuracy: 0.9627 - val_loss: 0.0022 - val_accuracy: 0.8553\n",
            "Epoch 118/1000\n",
            "24/24 [==============================] - 3s 136ms/step - loss: 4.9159e-04 - accuracy: 0.9640 - val_loss: 0.0021 - val_accuracy: 0.8558\n",
            "Epoch 119/1000\n",
            "24/24 [==============================] - 3s 136ms/step - loss: 4.9177e-04 - accuracy: 0.9641 - val_loss: 0.0022 - val_accuracy: 0.8553\n",
            "Epoch 120/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 4.6662e-04 - accuracy: 0.9659 - val_loss: 0.0021 - val_accuracy: 0.8582\n",
            "Epoch 121/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 4.4041e-04 - accuracy: 0.9678 - val_loss: 0.0020 - val_accuracy: 0.8637\n",
            "Epoch 122/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 4.2728e-04 - accuracy: 0.9684 - val_loss: 0.0020 - val_accuracy: 0.8647\n",
            "Epoch 123/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 4.2769e-04 - accuracy: 0.9685 - val_loss: 0.0020 - val_accuracy: 0.8607\n",
            "Epoch 124/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 4.2813e-04 - accuracy: 0.9688 - val_loss: 0.0021 - val_accuracy: 0.8628\n",
            "Epoch 125/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 4.3938e-04 - accuracy: 0.9687 - val_loss: 0.0021 - val_accuracy: 0.8572\n",
            "Epoch 126/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 4.2232e-04 - accuracy: 0.9699 - val_loss: 0.0021 - val_accuracy: 0.8573\n",
            "Epoch 127/1000\n",
            "24/24 [==============================] - 3s 136ms/step - loss: 4.8009e-04 - accuracy: 0.9672 - val_loss: 0.0023 - val_accuracy: 0.8493\n",
            "Epoch 128/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 5.1324e-04 - accuracy: 0.9644 - val_loss: 0.0022 - val_accuracy: 0.8558\n",
            "Epoch 129/1000\n",
            "24/24 [==============================] - 3s 138ms/step - loss: 6.2253e-04 - accuracy: 0.9584 - val_loss: 0.0022 - val_accuracy: 0.8525\n",
            "Epoch 130/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 5.8667e-04 - accuracy: 0.9597 - val_loss: 0.0022 - val_accuracy: 0.8572\n",
            "Epoch 131/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 5.2930e-04 - accuracy: 0.9643 - val_loss: 0.0022 - val_accuracy: 0.8538\n",
            "Epoch 132/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 5.1411e-04 - accuracy: 0.9651 - val_loss: 0.0022 - val_accuracy: 0.8585\n",
            "Epoch 133/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 4.7648e-04 - accuracy: 0.9677 - val_loss: 0.0021 - val_accuracy: 0.8633\n",
            "Epoch 134/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 4.2198e-04 - accuracy: 0.9708 - val_loss: 0.0020 - val_accuracy: 0.8663\n",
            "Epoch 135/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 4.0446e-04 - accuracy: 0.9716 - val_loss: 0.0021 - val_accuracy: 0.8608\n",
            "Epoch 136/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 4.1538e-04 - accuracy: 0.9717 - val_loss: 0.0021 - val_accuracy: 0.8642\n",
            "Epoch 137/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 3.8933e-04 - accuracy: 0.9726 - val_loss: 0.0019 - val_accuracy: 0.8750\n",
            "Epoch 138/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 3.8005e-04 - accuracy: 0.9728 - val_loss: 0.0020 - val_accuracy: 0.8700\n",
            "Epoch 139/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 3.6285e-04 - accuracy: 0.9740 - val_loss: 0.0019 - val_accuracy: 0.8740\n",
            "Epoch 140/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 3.5078e-04 - accuracy: 0.9745 - val_loss: 0.0019 - val_accuracy: 0.8722\n",
            "Epoch 141/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 3.4981e-04 - accuracy: 0.9745 - val_loss: 0.0019 - val_accuracy: 0.8758\n",
            "Epoch 142/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 3.4557e-04 - accuracy: 0.9747 - val_loss: 0.0019 - val_accuracy: 0.8745\n",
            "Epoch 143/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 3.4335e-04 - accuracy: 0.9751 - val_loss: 0.0019 - val_accuracy: 0.8785\n",
            "Epoch 144/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 3.3625e-04 - accuracy: 0.9755 - val_loss: 0.0018 - val_accuracy: 0.8795\n",
            "Epoch 145/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 3.2578e-04 - accuracy: 0.9758 - val_loss: 0.0018 - val_accuracy: 0.8805\n",
            "Epoch 146/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 3.2375e-04 - accuracy: 0.9760 - val_loss: 0.0018 - val_accuracy: 0.8805\n",
            "Epoch 147/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 3.2073e-04 - accuracy: 0.9762 - val_loss: 0.0018 - val_accuracy: 0.8827\n",
            "Epoch 148/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 3.1402e-04 - accuracy: 0.9767 - val_loss: 0.0018 - val_accuracy: 0.8837\n",
            "Epoch 149/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 3.1092e-04 - accuracy: 0.9768 - val_loss: 0.0018 - val_accuracy: 0.8825\n",
            "Epoch 150/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 3.0866e-04 - accuracy: 0.9768 - val_loss: 0.0018 - val_accuracy: 0.8850\n",
            "Epoch 151/1000\n",
            "24/24 [==============================] - 3s 136ms/step - loss: 3.0984e-04 - accuracy: 0.9770 - val_loss: 0.0018 - val_accuracy: 0.8818\n",
            "Epoch 152/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 3.1017e-04 - accuracy: 0.9770 - val_loss: 0.0018 - val_accuracy: 0.8817\n",
            "Epoch 153/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 3.0660e-04 - accuracy: 0.9771 - val_loss: 0.0017 - val_accuracy: 0.8847\n",
            "Epoch 154/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 3.0107e-04 - accuracy: 0.9772 - val_loss: 0.0017 - val_accuracy: 0.8857\n",
            "Epoch 155/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 2.9795e-04 - accuracy: 0.9774 - val_loss: 0.0017 - val_accuracy: 0.8850\n",
            "Epoch 156/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 2.9557e-04 - accuracy: 0.9773 - val_loss: 0.0017 - val_accuracy: 0.8843\n",
            "Epoch 157/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 2.9829e-04 - accuracy: 0.9773 - val_loss: 0.0018 - val_accuracy: 0.8807\n",
            "Epoch 158/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 3.0033e-04 - accuracy: 0.9772 - val_loss: 0.0018 - val_accuracy: 0.8818\n",
            "Epoch 159/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 3.0078e-04 - accuracy: 0.9773 - val_loss: 0.0018 - val_accuracy: 0.8782\n",
            "Epoch 160/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 3.0237e-04 - accuracy: 0.9775 - val_loss: 0.0018 - val_accuracy: 0.8835\n",
            "Epoch 161/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 3.0541e-04 - accuracy: 0.9776 - val_loss: 0.0018 - val_accuracy: 0.8848\n",
            "Epoch 162/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 3.0055e-04 - accuracy: 0.9776 - val_loss: 0.0017 - val_accuracy: 0.8847\n",
            "Epoch 163/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 2.9928e-04 - accuracy: 0.9777 - val_loss: 0.0018 - val_accuracy: 0.8803\n",
            "Epoch 164/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 3.0019e-04 - accuracy: 0.9780 - val_loss: 0.0018 - val_accuracy: 0.8805\n",
            "Epoch 165/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 2.9186e-04 - accuracy: 0.9783 - val_loss: 0.0017 - val_accuracy: 0.8862\n",
            "Epoch 166/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 2.9478e-04 - accuracy: 0.9784 - val_loss: 0.0018 - val_accuracy: 0.8827\n",
            "Epoch 167/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 2.9426e-04 - accuracy: 0.9784 - val_loss: 0.0018 - val_accuracy: 0.8833\n",
            "Epoch 168/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 3.0461e-04 - accuracy: 0.9778 - val_loss: 0.0019 - val_accuracy: 0.8717\n",
            "Epoch 169/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 3.1368e-04 - accuracy: 0.9778 - val_loss: 0.0019 - val_accuracy: 0.8788\n",
            "Epoch 170/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 3.3081e-04 - accuracy: 0.9769 - val_loss: 0.0020 - val_accuracy: 0.8692\n",
            "Epoch 171/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 3.8686e-04 - accuracy: 0.9739 - val_loss: 0.0020 - val_accuracy: 0.8717\n",
            "Epoch 172/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 3.7365e-04 - accuracy: 0.9750 - val_loss: 0.0019 - val_accuracy: 0.8727\n",
            "Epoch 173/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 3.6049e-04 - accuracy: 0.9760 - val_loss: 0.0020 - val_accuracy: 0.8675\n",
            "Epoch 174/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 3.4002e-04 - accuracy: 0.9770 - val_loss: 0.0019 - val_accuracy: 0.8740\n",
            "Epoch 175/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 3.3807e-04 - accuracy: 0.9775 - val_loss: 0.0019 - val_accuracy: 0.8772\n",
            "Epoch 176/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 3.4052e-04 - accuracy: 0.9770 - val_loss: 0.0020 - val_accuracy: 0.8735\n",
            "Epoch 177/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 3.3854e-04 - accuracy: 0.9772 - val_loss: 0.0019 - val_accuracy: 0.8760\n",
            "Epoch 178/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 3.2636e-04 - accuracy: 0.9778 - val_loss: 0.0018 - val_accuracy: 0.8797\n",
            "Epoch 179/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 3.2134e-04 - accuracy: 0.9782 - val_loss: 0.0018 - val_accuracy: 0.8812\n",
            "Epoch 180/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 3.0555e-04 - accuracy: 0.9790 - val_loss: 0.0021 - val_accuracy: 0.8677\n",
            "Epoch 181/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 2.7693e-04 - accuracy: 0.9812 - val_loss: 0.0018 - val_accuracy: 0.8832\n",
            "Epoch 182/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 2.6143e-04 - accuracy: 0.9824 - val_loss: 0.0018 - val_accuracy: 0.8857\n",
            "Epoch 183/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 2.5365e-04 - accuracy: 0.9830 - val_loss: 0.0017 - val_accuracy: 0.8905\n",
            "Epoch 184/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 2.4182e-04 - accuracy: 0.9838 - val_loss: 0.0017 - val_accuracy: 0.8853\n",
            "Epoch 185/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 2.3338e-04 - accuracy: 0.9840 - val_loss: 0.0017 - val_accuracy: 0.8878\n",
            "Epoch 186/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 2.5560e-04 - accuracy: 0.9828 - val_loss: 0.0018 - val_accuracy: 0.8880\n",
            "Epoch 187/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 2.3300e-04 - accuracy: 0.9843 - val_loss: 0.0017 - val_accuracy: 0.8882\n",
            "Epoch 188/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 2.2207e-04 - accuracy: 0.9850 - val_loss: 0.0017 - val_accuracy: 0.8900\n",
            "Epoch 189/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 2.2109e-04 - accuracy: 0.9850 - val_loss: 0.0017 - val_accuracy: 0.8902\n",
            "Epoch 190/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 2.2324e-04 - accuracy: 0.9852 - val_loss: 0.0017 - val_accuracy: 0.8902\n",
            "Epoch 191/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 2.2768e-04 - accuracy: 0.9847 - val_loss: 0.0017 - val_accuracy: 0.8937\n",
            "Epoch 192/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 2.2265e-04 - accuracy: 0.9849 - val_loss: 0.0017 - val_accuracy: 0.8908\n",
            "Epoch 193/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 2.1976e-04 - accuracy: 0.9852 - val_loss: 0.0016 - val_accuracy: 0.8937\n",
            "Epoch 194/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 2.1367e-04 - accuracy: 0.9854 - val_loss: 0.0017 - val_accuracy: 0.8907\n",
            "Epoch 195/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 2.1130e-04 - accuracy: 0.9855 - val_loss: 0.0016 - val_accuracy: 0.8953\n",
            "Epoch 196/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 2.1502e-04 - accuracy: 0.9850 - val_loss: 0.0017 - val_accuracy: 0.8902\n",
            "Epoch 197/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 2.1878e-04 - accuracy: 0.9850 - val_loss: 0.0017 - val_accuracy: 0.8918\n",
            "Epoch 198/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 2.1877e-04 - accuracy: 0.9850 - val_loss: 0.0016 - val_accuracy: 0.8935\n",
            "Epoch 199/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 2.1195e-04 - accuracy: 0.9856 - val_loss: 0.0017 - val_accuracy: 0.8885\n",
            "Epoch 200/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 2.0987e-04 - accuracy: 0.9858 - val_loss: 0.0017 - val_accuracy: 0.8915\n",
            "Epoch 201/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 2.2356e-04 - accuracy: 0.9850 - val_loss: 0.0017 - val_accuracy: 0.8933\n",
            "Epoch 202/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 2.2977e-04 - accuracy: 0.9849 - val_loss: 0.0018 - val_accuracy: 0.8815\n",
            "Epoch 203/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 2.5690e-04 - accuracy: 0.9831 - val_loss: 0.0020 - val_accuracy: 0.8718\n",
            "Epoch 204/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 2.7394e-04 - accuracy: 0.9825 - val_loss: 0.0017 - val_accuracy: 0.8882\n",
            "Epoch 205/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 2.4954e-04 - accuracy: 0.9839 - val_loss: 0.0017 - val_accuracy: 0.8942\n",
            "Epoch 206/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 2.3722e-04 - accuracy: 0.9843 - val_loss: 0.0018 - val_accuracy: 0.8857\n",
            "Epoch 207/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 2.3688e-04 - accuracy: 0.9840 - val_loss: 0.0017 - val_accuracy: 0.8905\n",
            "Epoch 208/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 2.3963e-04 - accuracy: 0.9841 - val_loss: 0.0017 - val_accuracy: 0.8887\n",
            "Epoch 209/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 2.2880e-04 - accuracy: 0.9852 - val_loss: 0.0017 - val_accuracy: 0.8938\n",
            "Epoch 210/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 2.1191e-04 - accuracy: 0.9860 - val_loss: 0.0016 - val_accuracy: 0.9017\n",
            "Epoch 211/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 2.0362e-04 - accuracy: 0.9865 - val_loss: 0.0016 - val_accuracy: 0.8970\n",
            "Epoch 212/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 2.0845e-04 - accuracy: 0.9861 - val_loss: 0.0016 - val_accuracy: 0.8978\n",
            "Epoch 213/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 2.0585e-04 - accuracy: 0.9866 - val_loss: 0.0016 - val_accuracy: 0.8958\n",
            "Epoch 214/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 2.2248e-04 - accuracy: 0.9855 - val_loss: 0.0017 - val_accuracy: 0.8917\n",
            "Epoch 215/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 2.1780e-04 - accuracy: 0.9857 - val_loss: 0.0019 - val_accuracy: 0.8825\n",
            "Epoch 216/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 2.5085e-04 - accuracy: 0.9840 - val_loss: 0.0017 - val_accuracy: 0.8933\n",
            "Epoch 217/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 2.2139e-04 - accuracy: 0.9854 - val_loss: 0.0017 - val_accuracy: 0.8885\n",
            "Epoch 218/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 2.2433e-04 - accuracy: 0.9853 - val_loss: 0.0018 - val_accuracy: 0.8868\n",
            "Epoch 219/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 2.1941e-04 - accuracy: 0.9855 - val_loss: 0.0019 - val_accuracy: 0.8800\n",
            "Epoch 220/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 2.1342e-04 - accuracy: 0.9861 - val_loss: 0.0018 - val_accuracy: 0.8865\n",
            "Epoch 221/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 2.1761e-04 - accuracy: 0.9858 - val_loss: 0.0017 - val_accuracy: 0.8885\n",
            "Epoch 222/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 2.2557e-04 - accuracy: 0.9852 - val_loss: 0.0017 - val_accuracy: 0.8895\n",
            "Epoch 223/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 2.1232e-04 - accuracy: 0.9864 - val_loss: 0.0016 - val_accuracy: 0.8962\n",
            "Epoch 224/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 2.1848e-04 - accuracy: 0.9860 - val_loss: 0.0019 - val_accuracy: 0.8828\n",
            "Epoch 225/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 2.1209e-04 - accuracy: 0.9867 - val_loss: 0.0017 - val_accuracy: 0.8933\n",
            "Epoch 226/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 2.0297e-04 - accuracy: 0.9868 - val_loss: 0.0018 - val_accuracy: 0.8848\n",
            "Epoch 227/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 2.2689e-04 - accuracy: 0.9852 - val_loss: 0.0019 - val_accuracy: 0.8785\n",
            "Epoch 228/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 2.4138e-04 - accuracy: 0.9841 - val_loss: 0.0019 - val_accuracy: 0.8818\n",
            "Epoch 229/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 2.2288e-04 - accuracy: 0.9855 - val_loss: 0.0019 - val_accuracy: 0.8793\n",
            "Epoch 230/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 2.3162e-04 - accuracy: 0.9850 - val_loss: 0.0016 - val_accuracy: 0.8990\n",
            "Epoch 231/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 2.0551e-04 - accuracy: 0.9865 - val_loss: 0.0015 - val_accuracy: 0.9052\n",
            "Epoch 232/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.8896e-04 - accuracy: 0.9881 - val_loss: 0.0015 - val_accuracy: 0.9043\n",
            "Epoch 233/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.8637e-04 - accuracy: 0.9879 - val_loss: 0.0015 - val_accuracy: 0.9045\n",
            "Epoch 234/1000\n",
            "24/24 [==============================] - 3s 136ms/step - loss: 1.6903e-04 - accuracy: 0.9890 - val_loss: 0.0015 - val_accuracy: 0.9060\n",
            "Epoch 235/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.6612e-04 - accuracy: 0.9894 - val_loss: 0.0016 - val_accuracy: 0.9008\n",
            "Epoch 236/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.7272e-04 - accuracy: 0.9887 - val_loss: 0.0014 - val_accuracy: 0.9085\n",
            "Epoch 237/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.7670e-04 - accuracy: 0.9886 - val_loss: 0.0015 - val_accuracy: 0.9052\n",
            "Epoch 238/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.6853e-04 - accuracy: 0.9888 - val_loss: 0.0015 - val_accuracy: 0.9033\n",
            "Epoch 239/1000\n",
            "24/24 [==============================] - 3s 136ms/step - loss: 1.6272e-04 - accuracy: 0.9895 - val_loss: 0.0015 - val_accuracy: 0.9072\n",
            "Epoch 240/1000\n",
            "24/24 [==============================] - 3s 136ms/step - loss: 1.5825e-04 - accuracy: 0.9899 - val_loss: 0.0014 - val_accuracy: 0.9125\n",
            "Epoch 241/1000\n",
            "24/24 [==============================] - 3s 136ms/step - loss: 1.4784e-04 - accuracy: 0.9903 - val_loss: 0.0014 - val_accuracy: 0.9117\n",
            "Epoch 242/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.6088e-04 - accuracy: 0.9896 - val_loss: 0.0015 - val_accuracy: 0.9058\n",
            "Epoch 243/1000\n",
            "24/24 [==============================] - 3s 136ms/step - loss: 1.6073e-04 - accuracy: 0.9895 - val_loss: 0.0016 - val_accuracy: 0.8993\n",
            "Epoch 244/1000\n",
            "24/24 [==============================] - 3s 136ms/step - loss: 1.5628e-04 - accuracy: 0.9900 - val_loss: 0.0015 - val_accuracy: 0.9053\n",
            "Epoch 245/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.5556e-04 - accuracy: 0.9900 - val_loss: 0.0015 - val_accuracy: 0.9063\n",
            "Epoch 246/1000\n",
            "24/24 [==============================] - 3s 136ms/step - loss: 1.4319e-04 - accuracy: 0.9910 - val_loss: 0.0014 - val_accuracy: 0.9127\n",
            "Epoch 247/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.4714e-04 - accuracy: 0.9905 - val_loss: 0.0015 - val_accuracy: 0.9058\n",
            "Epoch 248/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.4396e-04 - accuracy: 0.9909 - val_loss: 0.0015 - val_accuracy: 0.9073\n",
            "Epoch 249/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.4490e-04 - accuracy: 0.9908 - val_loss: 0.0015 - val_accuracy: 0.9087\n",
            "Epoch 250/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.4964e-04 - accuracy: 0.9906 - val_loss: 0.0015 - val_accuracy: 0.9050\n",
            "Epoch 251/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.5683e-04 - accuracy: 0.9900 - val_loss: 0.0015 - val_accuracy: 0.9065\n",
            "Epoch 252/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.7283e-04 - accuracy: 0.9888 - val_loss: 0.0019 - val_accuracy: 0.8818\n",
            "Epoch 253/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.9147e-04 - accuracy: 0.9877 - val_loss: 0.0016 - val_accuracy: 0.9028\n",
            "Epoch 254/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.8218e-04 - accuracy: 0.9885 - val_loss: 0.0017 - val_accuracy: 0.8957\n",
            "Epoch 255/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 2.1267e-04 - accuracy: 0.9868 - val_loss: 0.0017 - val_accuracy: 0.8952\n",
            "Epoch 256/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 2.1290e-04 - accuracy: 0.9865 - val_loss: 0.0016 - val_accuracy: 0.9025\n",
            "Epoch 257/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 2.2992e-04 - accuracy: 0.9852 - val_loss: 0.0015 - val_accuracy: 0.9050\n",
            "Epoch 258/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 2.4037e-04 - accuracy: 0.9853 - val_loss: 0.0017 - val_accuracy: 0.8977\n",
            "Epoch 259/1000\n",
            "24/24 [==============================] - 3s 136ms/step - loss: 1.8428e-04 - accuracy: 0.9886 - val_loss: 0.0014 - val_accuracy: 0.9087\n",
            "Epoch 260/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.5880e-04 - accuracy: 0.9899 - val_loss: 0.0015 - val_accuracy: 0.9107\n",
            "Epoch 261/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.5583e-04 - accuracy: 0.9904 - val_loss: 0.0014 - val_accuracy: 0.9087\n",
            "Epoch 262/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.4324e-04 - accuracy: 0.9910 - val_loss: 0.0014 - val_accuracy: 0.9083\n",
            "Epoch 263/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.4015e-04 - accuracy: 0.9910 - val_loss: 0.0014 - val_accuracy: 0.9085\n",
            "Epoch 264/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.3044e-04 - accuracy: 0.9918 - val_loss: 0.0015 - val_accuracy: 0.9097\n",
            "Epoch 265/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.3564e-04 - accuracy: 0.9916 - val_loss: 0.0015 - val_accuracy: 0.9095\n",
            "Epoch 266/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.3568e-04 - accuracy: 0.9913 - val_loss: 0.0015 - val_accuracy: 0.9102\n",
            "Epoch 267/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.3188e-04 - accuracy: 0.9918 - val_loss: 0.0014 - val_accuracy: 0.9133\n",
            "Epoch 268/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.3922e-04 - accuracy: 0.9912 - val_loss: 0.0015 - val_accuracy: 0.9075\n",
            "Epoch 269/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.4130e-04 - accuracy: 0.9913 - val_loss: 0.0015 - val_accuracy: 0.9078\n",
            "Epoch 270/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.5128e-04 - accuracy: 0.9903 - val_loss: 0.0014 - val_accuracy: 0.9112\n",
            "Epoch 271/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.4285e-04 - accuracy: 0.9910 - val_loss: 0.0014 - val_accuracy: 0.9133\n",
            "Epoch 272/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.4371e-04 - accuracy: 0.9912 - val_loss: 0.0015 - val_accuracy: 0.9065\n",
            "Epoch 273/1000\n",
            "24/24 [==============================] - 3s 136ms/step - loss: 1.3082e-04 - accuracy: 0.9919 - val_loss: 0.0014 - val_accuracy: 0.9117\n",
            "Epoch 274/1000\n",
            "24/24 [==============================] - 3s 136ms/step - loss: 1.2924e-04 - accuracy: 0.9920 - val_loss: 0.0014 - val_accuracy: 0.9148\n",
            "Epoch 275/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.2218e-04 - accuracy: 0.9924 - val_loss: 0.0014 - val_accuracy: 0.9165\n",
            "Epoch 276/1000\n",
            "24/24 [==============================] - 3s 136ms/step - loss: 1.2846e-04 - accuracy: 0.9922 - val_loss: 0.0015 - val_accuracy: 0.9092\n",
            "Epoch 277/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.2895e-04 - accuracy: 0.9919 - val_loss: 0.0015 - val_accuracy: 0.9065\n",
            "Epoch 278/1000\n",
            "24/24 [==============================] - 3s 136ms/step - loss: 1.2641e-04 - accuracy: 0.9922 - val_loss: 0.0015 - val_accuracy: 0.9078\n",
            "Epoch 279/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.2684e-04 - accuracy: 0.9922 - val_loss: 0.0015 - val_accuracy: 0.9065\n",
            "Epoch 280/1000\n",
            "24/24 [==============================] - 3s 136ms/step - loss: 1.5349e-04 - accuracy: 0.9904 - val_loss: 0.0015 - val_accuracy: 0.9073\n",
            "Epoch 281/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.7481e-04 - accuracy: 0.9888 - val_loss: 0.0016 - val_accuracy: 0.9013\n",
            "Epoch 282/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.5575e-04 - accuracy: 0.9900 - val_loss: 0.0015 - val_accuracy: 0.9065\n",
            "Epoch 283/1000\n",
            "24/24 [==============================] - 3s 136ms/step - loss: 1.3916e-04 - accuracy: 0.9915 - val_loss: 0.0016 - val_accuracy: 0.9020\n",
            "Epoch 284/1000\n",
            "24/24 [==============================] - 3s 136ms/step - loss: 1.4114e-04 - accuracy: 0.9914 - val_loss: 0.0016 - val_accuracy: 0.9040\n",
            "Epoch 285/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.3676e-04 - accuracy: 0.9917 - val_loss: 0.0014 - val_accuracy: 0.9125\n",
            "Epoch 286/1000\n",
            "24/24 [==============================] - 3s 136ms/step - loss: 1.4246e-04 - accuracy: 0.9912 - val_loss: 0.0015 - val_accuracy: 0.9093\n",
            "Epoch 287/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.5101e-04 - accuracy: 0.9908 - val_loss: 0.0014 - val_accuracy: 0.9102\n",
            "Epoch 288/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.3821e-04 - accuracy: 0.9917 - val_loss: 0.0015 - val_accuracy: 0.9090\n",
            "Epoch 289/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.3376e-04 - accuracy: 0.9918 - val_loss: 0.0014 - val_accuracy: 0.9130\n",
            "Epoch 290/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.3262e-04 - accuracy: 0.9921 - val_loss: 0.0015 - val_accuracy: 0.9087\n",
            "Epoch 291/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.5485e-04 - accuracy: 0.9903 - val_loss: 0.0015 - val_accuracy: 0.9075\n",
            "Epoch 292/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 2.5944e-04 - accuracy: 0.9841 - val_loss: 0.0016 - val_accuracy: 0.9010\n",
            "Epoch 293/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.6911e-04 - accuracy: 0.9895 - val_loss: 0.0016 - val_accuracy: 0.9035\n",
            "Epoch 294/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.5504e-04 - accuracy: 0.9905 - val_loss: 0.0015 - val_accuracy: 0.9102\n",
            "Epoch 295/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.3919e-04 - accuracy: 0.9915 - val_loss: 0.0016 - val_accuracy: 0.9032\n",
            "Epoch 296/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.4203e-04 - accuracy: 0.9913 - val_loss: 0.0015 - val_accuracy: 0.9108\n",
            "Epoch 297/1000\n",
            "24/24 [==============================] - 3s 136ms/step - loss: 1.3178e-04 - accuracy: 0.9917 - val_loss: 0.0015 - val_accuracy: 0.9095\n",
            "Epoch 298/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.3364e-04 - accuracy: 0.9918 - val_loss: 0.0014 - val_accuracy: 0.9138\n",
            "Epoch 299/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.2791e-04 - accuracy: 0.9922 - val_loss: 0.0014 - val_accuracy: 0.9113\n",
            "Epoch 300/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.3095e-04 - accuracy: 0.9921 - val_loss: 0.0014 - val_accuracy: 0.9132\n",
            "Epoch 301/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.2208e-04 - accuracy: 0.9927 - val_loss: 0.0014 - val_accuracy: 0.9130\n",
            "Epoch 302/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.2391e-04 - accuracy: 0.9926 - val_loss: 0.0014 - val_accuracy: 0.9128\n",
            "Epoch 303/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.4421e-04 - accuracy: 0.9912 - val_loss: 0.0015 - val_accuracy: 0.9097\n",
            "Epoch 304/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.4573e-04 - accuracy: 0.9912 - val_loss: 0.0015 - val_accuracy: 0.9072\n",
            "Epoch 305/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.5103e-04 - accuracy: 0.9907 - val_loss: 0.0015 - val_accuracy: 0.9092\n",
            "Epoch 306/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.4024e-04 - accuracy: 0.9914 - val_loss: 0.0015 - val_accuracy: 0.9118\n",
            "Epoch 307/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.4176e-04 - accuracy: 0.9913 - val_loss: 0.0014 - val_accuracy: 0.9128\n",
            "Epoch 308/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.4761e-04 - accuracy: 0.9910 - val_loss: 0.0014 - val_accuracy: 0.9115\n",
            "Epoch 309/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.4921e-04 - accuracy: 0.9910 - val_loss: 0.0014 - val_accuracy: 0.9153\n",
            "Epoch 310/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.5174e-04 - accuracy: 0.9905 - val_loss: 0.0016 - val_accuracy: 0.9045\n",
            "Epoch 311/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.5302e-04 - accuracy: 0.9905 - val_loss: 0.0015 - val_accuracy: 0.9050\n",
            "Epoch 312/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.5453e-04 - accuracy: 0.9904 - val_loss: 0.0015 - val_accuracy: 0.9098\n",
            "Epoch 313/1000\n",
            "24/24 [==============================] - 3s 136ms/step - loss: 1.3928e-04 - accuracy: 0.9918 - val_loss: 0.0014 - val_accuracy: 0.9150\n",
            "Epoch 314/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.3644e-04 - accuracy: 0.9915 - val_loss: 0.0015 - val_accuracy: 0.9105\n",
            "Epoch 315/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.6116e-04 - accuracy: 0.9902 - val_loss: 0.0015 - val_accuracy: 0.9097\n",
            "Epoch 316/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.4172e-04 - accuracy: 0.9912 - val_loss: 0.0015 - val_accuracy: 0.9112\n",
            "Epoch 317/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.4937e-04 - accuracy: 0.9907 - val_loss: 0.0014 - val_accuracy: 0.9117\n",
            "Epoch 318/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.3211e-04 - accuracy: 0.9919 - val_loss: 0.0014 - val_accuracy: 0.9138\n",
            "Epoch 319/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.2572e-04 - accuracy: 0.9923 - val_loss: 0.0014 - val_accuracy: 0.9145\n",
            "Epoch 320/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.4240e-04 - accuracy: 0.9911 - val_loss: 0.0014 - val_accuracy: 0.9153\n",
            "Epoch 321/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.4113e-04 - accuracy: 0.9915 - val_loss: 0.0014 - val_accuracy: 0.9127\n",
            "Epoch 322/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.3918e-04 - accuracy: 0.9915 - val_loss: 0.0014 - val_accuracy: 0.9168\n",
            "Epoch 323/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.4880e-04 - accuracy: 0.9909 - val_loss: 0.0015 - val_accuracy: 0.9057\n",
            "Epoch 324/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.3850e-04 - accuracy: 0.9918 - val_loss: 0.0014 - val_accuracy: 0.9143\n",
            "Epoch 325/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.4005e-04 - accuracy: 0.9914 - val_loss: 0.0016 - val_accuracy: 0.9052\n",
            "Epoch 326/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.4535e-04 - accuracy: 0.9915 - val_loss: 0.0014 - val_accuracy: 0.9128\n",
            "Epoch 327/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.2946e-04 - accuracy: 0.9923 - val_loss: 0.0015 - val_accuracy: 0.9067\n",
            "Epoch 328/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.2809e-04 - accuracy: 0.9925 - val_loss: 0.0014 - val_accuracy: 0.9132\n",
            "Epoch 329/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.2973e-04 - accuracy: 0.9920 - val_loss: 0.0014 - val_accuracy: 0.9123\n",
            "Epoch 330/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.2392e-04 - accuracy: 0.9927 - val_loss: 0.0014 - val_accuracy: 0.9178\n",
            "Epoch 331/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.2457e-04 - accuracy: 0.9925 - val_loss: 0.0014 - val_accuracy: 0.9127\n",
            "Epoch 332/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.2719e-04 - accuracy: 0.9924 - val_loss: 0.0014 - val_accuracy: 0.9192\n",
            "Epoch 333/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.4951e-04 - accuracy: 0.9911 - val_loss: 0.0014 - val_accuracy: 0.9142\n",
            "Epoch 334/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.4784e-04 - accuracy: 0.9910 - val_loss: 0.0014 - val_accuracy: 0.9137\n",
            "Epoch 335/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.5410e-04 - accuracy: 0.9906 - val_loss: 0.0015 - val_accuracy: 0.9112\n",
            "Epoch 336/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.8485e-04 - accuracy: 0.9887 - val_loss: 0.0016 - val_accuracy: 0.9057\n",
            "Epoch 337/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.8714e-04 - accuracy: 0.9882 - val_loss: 0.0015 - val_accuracy: 0.9112\n",
            "Epoch 338/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.6731e-04 - accuracy: 0.9899 - val_loss: 0.0015 - val_accuracy: 0.9095\n",
            "Epoch 339/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.5293e-04 - accuracy: 0.9904 - val_loss: 0.0014 - val_accuracy: 0.9128\n",
            "Epoch 340/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.6501e-04 - accuracy: 0.9902 - val_loss: 0.0015 - val_accuracy: 0.9082\n",
            "Epoch 341/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.7621e-04 - accuracy: 0.9890 - val_loss: 0.0016 - val_accuracy: 0.9035\n",
            "Epoch 342/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.6968e-04 - accuracy: 0.9896 - val_loss: 0.0015 - val_accuracy: 0.9073\n",
            "Epoch 343/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.7803e-04 - accuracy: 0.9889 - val_loss: 0.0015 - val_accuracy: 0.9083\n",
            "Epoch 344/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.5369e-04 - accuracy: 0.9904 - val_loss: 0.0014 - val_accuracy: 0.9148\n",
            "Epoch 345/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.4863e-04 - accuracy: 0.9909 - val_loss: 0.0015 - val_accuracy: 0.9075\n",
            "Epoch 346/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.4500e-04 - accuracy: 0.9914 - val_loss: 0.0013 - val_accuracy: 0.9208\n",
            "Epoch 347/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.2988e-04 - accuracy: 0.9922 - val_loss: 0.0014 - val_accuracy: 0.9167\n",
            "Epoch 348/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.2465e-04 - accuracy: 0.9928 - val_loss: 0.0014 - val_accuracy: 0.9162\n",
            "Epoch 349/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.2473e-04 - accuracy: 0.9925 - val_loss: 0.0015 - val_accuracy: 0.9098\n",
            "Epoch 350/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.2544e-04 - accuracy: 0.9924 - val_loss: 0.0014 - val_accuracy: 0.9160\n",
            "Epoch 351/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.2395e-04 - accuracy: 0.9927 - val_loss: 0.0013 - val_accuracy: 0.9177\n",
            "Epoch 352/1000\n",
            "24/24 [==============================] - 3s 136ms/step - loss: 1.2146e-04 - accuracy: 0.9930 - val_loss: 0.0013 - val_accuracy: 0.9198\n",
            "Epoch 353/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.2309e-04 - accuracy: 0.9930 - val_loss: 0.0013 - val_accuracy: 0.9175\n",
            "Epoch 354/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.2948e-04 - accuracy: 0.9925 - val_loss: 0.0013 - val_accuracy: 0.9230\n",
            "Epoch 355/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.2048e-04 - accuracy: 0.9930 - val_loss: 0.0013 - val_accuracy: 0.9207\n",
            "Epoch 356/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.2799e-04 - accuracy: 0.9925 - val_loss: 0.0013 - val_accuracy: 0.9185\n",
            "Epoch 357/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.2058e-04 - accuracy: 0.9930 - val_loss: 0.0013 - val_accuracy: 0.9187\n",
            "Epoch 358/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.1879e-04 - accuracy: 0.9930 - val_loss: 0.0014 - val_accuracy: 0.9153\n",
            "Epoch 359/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.2430e-04 - accuracy: 0.9925 - val_loss: 0.0015 - val_accuracy: 0.9127\n",
            "Epoch 360/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.0929e-04 - accuracy: 0.9932 - val_loss: 0.0015 - val_accuracy: 0.9117\n",
            "Epoch 361/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.1733e-04 - accuracy: 0.9929 - val_loss: 0.0014 - val_accuracy: 0.9153\n",
            "Epoch 362/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.0955e-04 - accuracy: 0.9935 - val_loss: 0.0013 - val_accuracy: 0.9210\n",
            "Epoch 363/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.3099e-04 - accuracy: 0.9922 - val_loss: 0.0015 - val_accuracy: 0.9112\n",
            "Epoch 364/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.3799e-04 - accuracy: 0.9917 - val_loss: 0.0014 - val_accuracy: 0.9167\n",
            "Epoch 365/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.1599e-04 - accuracy: 0.9930 - val_loss: 0.0013 - val_accuracy: 0.9213\n",
            "Epoch 366/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.1191e-04 - accuracy: 0.9934 - val_loss: 0.0014 - val_accuracy: 0.9170\n",
            "Epoch 367/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.1369e-04 - accuracy: 0.9933 - val_loss: 0.0014 - val_accuracy: 0.9165\n",
            "Epoch 368/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.1249e-04 - accuracy: 0.9934 - val_loss: 0.0014 - val_accuracy: 0.9157\n",
            "Epoch 369/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 9.9961e-05 - accuracy: 0.9942 - val_loss: 0.0014 - val_accuracy: 0.9170\n",
            "Epoch 370/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 9.8806e-05 - accuracy: 0.9944 - val_loss: 0.0014 - val_accuracy: 0.9183\n",
            "Epoch 371/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 9.9796e-05 - accuracy: 0.9943 - val_loss: 0.0012 - val_accuracy: 0.9265\n",
            "Epoch 372/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.0437e-04 - accuracy: 0.9938 - val_loss: 0.0015 - val_accuracy: 0.9110\n",
            "Epoch 373/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.1732e-04 - accuracy: 0.9928 - val_loss: 0.0013 - val_accuracy: 0.9190\n",
            "Epoch 374/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.0998e-04 - accuracy: 0.9937 - val_loss: 0.0014 - val_accuracy: 0.9188\n",
            "Epoch 375/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.0299e-04 - accuracy: 0.9943 - val_loss: 0.0014 - val_accuracy: 0.9170\n",
            "Epoch 376/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 9.9487e-05 - accuracy: 0.9942 - val_loss: 0.0013 - val_accuracy: 0.9205\n",
            "Epoch 377/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 9.5959e-05 - accuracy: 0.9947 - val_loss: 0.0013 - val_accuracy: 0.9202\n",
            "Epoch 378/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.0118e-04 - accuracy: 0.9942 - val_loss: 0.0013 - val_accuracy: 0.9217\n",
            "Epoch 379/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.1378e-04 - accuracy: 0.9934 - val_loss: 0.0015 - val_accuracy: 0.9090\n",
            "Epoch 380/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.1461e-04 - accuracy: 0.9930 - val_loss: 0.0013 - val_accuracy: 0.9202\n",
            "Epoch 381/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.1425e-04 - accuracy: 0.9932 - val_loss: 0.0014 - val_accuracy: 0.9162\n",
            "Epoch 382/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.0696e-04 - accuracy: 0.9939 - val_loss: 0.0014 - val_accuracy: 0.9147\n",
            "Epoch 383/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.0420e-04 - accuracy: 0.9938 - val_loss: 0.0013 - val_accuracy: 0.9213\n",
            "Epoch 384/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 9.6769e-05 - accuracy: 0.9946 - val_loss: 0.0014 - val_accuracy: 0.9188\n",
            "Epoch 385/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.1907e-04 - accuracy: 0.9931 - val_loss: 0.0013 - val_accuracy: 0.9235\n",
            "Epoch 386/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.8445e-04 - accuracy: 0.9884 - val_loss: 0.0015 - val_accuracy: 0.9082\n",
            "Epoch 387/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.6116e-04 - accuracy: 0.9903 - val_loss: 0.0016 - val_accuracy: 0.9045\n",
            "Epoch 388/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.5777e-04 - accuracy: 0.9905 - val_loss: 0.0014 - val_accuracy: 0.9180\n",
            "Epoch 389/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.3924e-04 - accuracy: 0.9918 - val_loss: 0.0015 - val_accuracy: 0.9112\n",
            "Epoch 390/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.3904e-04 - accuracy: 0.9915 - val_loss: 0.0015 - val_accuracy: 0.9140\n",
            "Epoch 391/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.2092e-04 - accuracy: 0.9927 - val_loss: 0.0013 - val_accuracy: 0.9205\n",
            "Epoch 392/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.1890e-04 - accuracy: 0.9929 - val_loss: 0.0014 - val_accuracy: 0.9150\n",
            "Epoch 393/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.0961e-04 - accuracy: 0.9936 - val_loss: 0.0014 - val_accuracy: 0.9185\n",
            "Epoch 394/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.0541e-04 - accuracy: 0.9940 - val_loss: 0.0015 - val_accuracy: 0.9140\n",
            "Epoch 395/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.1043e-04 - accuracy: 0.9935 - val_loss: 0.0014 - val_accuracy: 0.9183\n",
            "Epoch 396/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.0366e-04 - accuracy: 0.9940 - val_loss: 0.0015 - val_accuracy: 0.9138\n",
            "Epoch 397/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.1060e-04 - accuracy: 0.9937 - val_loss: 0.0014 - val_accuracy: 0.9187\n",
            "Epoch 398/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.0842e-04 - accuracy: 0.9936 - val_loss: 0.0013 - val_accuracy: 0.9250\n",
            "Epoch 399/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.0003e-04 - accuracy: 0.9940 - val_loss: 0.0013 - val_accuracy: 0.9257\n",
            "Epoch 400/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 9.9928e-05 - accuracy: 0.9944 - val_loss: 0.0013 - val_accuracy: 0.9203\n",
            "Epoch 401/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 9.7901e-05 - accuracy: 0.9946 - val_loss: 0.0013 - val_accuracy: 0.9257\n",
            "Epoch 402/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.0227e-04 - accuracy: 0.9942 - val_loss: 0.0015 - val_accuracy: 0.9108\n",
            "Epoch 403/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.1691e-04 - accuracy: 0.9933 - val_loss: 0.0013 - val_accuracy: 0.9232\n",
            "Epoch 404/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.0578e-04 - accuracy: 0.9940 - val_loss: 0.0013 - val_accuracy: 0.9230\n",
            "Epoch 405/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 9.7132e-05 - accuracy: 0.9943 - val_loss: 0.0013 - val_accuracy: 0.9227\n",
            "Epoch 406/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.0617e-04 - accuracy: 0.9939 - val_loss: 0.0014 - val_accuracy: 0.9187\n",
            "Epoch 407/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 9.8370e-05 - accuracy: 0.9944 - val_loss: 0.0013 - val_accuracy: 0.9240\n",
            "Epoch 408/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 9.7730e-05 - accuracy: 0.9944 - val_loss: 0.0014 - val_accuracy: 0.9183\n",
            "Epoch 409/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.0291e-04 - accuracy: 0.9942 - val_loss: 0.0014 - val_accuracy: 0.9183\n",
            "Epoch 410/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.0926e-04 - accuracy: 0.9937 - val_loss: 0.0012 - val_accuracy: 0.9283\n",
            "Epoch 411/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.0388e-04 - accuracy: 0.9941 - val_loss: 0.0013 - val_accuracy: 0.9235\n",
            "Epoch 412/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 9.9206e-05 - accuracy: 0.9944 - val_loss: 0.0013 - val_accuracy: 0.9252\n",
            "Epoch 413/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 9.5400e-05 - accuracy: 0.9947 - val_loss: 0.0012 - val_accuracy: 0.9255\n",
            "Epoch 414/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 9.3359e-05 - accuracy: 0.9948 - val_loss: 0.0012 - val_accuracy: 0.9298\n",
            "Epoch 415/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 9.3988e-05 - accuracy: 0.9947 - val_loss: 0.0012 - val_accuracy: 0.9267\n",
            "Epoch 416/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.0001e-04 - accuracy: 0.9945 - val_loss: 0.0013 - val_accuracy: 0.9238\n",
            "Epoch 417/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 9.6779e-05 - accuracy: 0.9944 - val_loss: 0.0012 - val_accuracy: 0.9257\n",
            "Epoch 418/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 9.5690e-05 - accuracy: 0.9946 - val_loss: 0.0014 - val_accuracy: 0.9170\n",
            "Epoch 419/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.0736e-04 - accuracy: 0.9937 - val_loss: 0.0013 - val_accuracy: 0.9240\n",
            "Epoch 420/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.0370e-04 - accuracy: 0.9940 - val_loss: 0.0013 - val_accuracy: 0.9238\n",
            "Epoch 421/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.1130e-04 - accuracy: 0.9935 - val_loss: 0.0014 - val_accuracy: 0.9183\n",
            "Epoch 422/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.2043e-04 - accuracy: 0.9930 - val_loss: 0.0013 - val_accuracy: 0.9227\n",
            "Epoch 423/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.0870e-04 - accuracy: 0.9935 - val_loss: 0.0014 - val_accuracy: 0.9177\n",
            "Epoch 424/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.2653e-04 - accuracy: 0.9923 - val_loss: 0.0013 - val_accuracy: 0.9242\n",
            "Epoch 425/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.1344e-04 - accuracy: 0.9933 - val_loss: 0.0013 - val_accuracy: 0.9233\n",
            "Epoch 426/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.0462e-04 - accuracy: 0.9939 - val_loss: 0.0014 - val_accuracy: 0.9152\n",
            "Epoch 427/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.3320e-04 - accuracy: 0.9921 - val_loss: 0.0014 - val_accuracy: 0.9163\n",
            "Epoch 428/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.3255e-04 - accuracy: 0.9919 - val_loss: 0.0013 - val_accuracy: 0.9237\n",
            "Epoch 429/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.2538e-04 - accuracy: 0.9927 - val_loss: 0.0014 - val_accuracy: 0.9163\n",
            "Epoch 430/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.4158e-04 - accuracy: 0.9911 - val_loss: 0.0014 - val_accuracy: 0.9148\n",
            "Epoch 431/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 1.2527e-04 - accuracy: 0.9926 - val_loss: 0.0015 - val_accuracy: 0.9092\n",
            "Epoch 432/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.3010e-04 - accuracy: 0.9921 - val_loss: 0.0014 - val_accuracy: 0.9163\n",
            "Epoch 433/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.3298e-04 - accuracy: 0.9919 - val_loss: 0.0013 - val_accuracy: 0.9232\n",
            "Epoch 434/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.1384e-04 - accuracy: 0.9935 - val_loss: 0.0012 - val_accuracy: 0.9295\n",
            "Epoch 435/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 9.5077e-05 - accuracy: 0.9945 - val_loss: 0.0012 - val_accuracy: 0.9290\n",
            "Epoch 436/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 9.3561e-05 - accuracy: 0.9946 - val_loss: 0.0012 - val_accuracy: 0.9282\n",
            "Epoch 437/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 9.4870e-05 - accuracy: 0.9947 - val_loss: 0.0011 - val_accuracy: 0.9332\n",
            "Epoch 438/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.0661e-04 - accuracy: 0.9940 - val_loss: 0.0013 - val_accuracy: 0.9212\n",
            "Epoch 439/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.2187e-04 - accuracy: 0.9930 - val_loss: 0.0017 - val_accuracy: 0.9003\n",
            "Epoch 440/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.9269e-04 - accuracy: 0.9885 - val_loss: 0.0016 - val_accuracy: 0.9098\n",
            "Epoch 441/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.6666e-04 - accuracy: 0.9899 - val_loss: 0.0016 - val_accuracy: 0.9017\n",
            "Epoch 442/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.3188e-04 - accuracy: 0.9921 - val_loss: 0.0015 - val_accuracy: 0.9112\n",
            "Epoch 443/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.3884e-04 - accuracy: 0.9918 - val_loss: 0.0014 - val_accuracy: 0.9148\n",
            "Epoch 444/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 1.1651e-04 - accuracy: 0.9933 - val_loss: 0.0014 - val_accuracy: 0.9172\n",
            "Epoch 445/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.1094e-04 - accuracy: 0.9935 - val_loss: 0.0014 - val_accuracy: 0.9183\n",
            "Epoch 446/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.0550e-04 - accuracy: 0.9938 - val_loss: 0.0014 - val_accuracy: 0.9210\n",
            "Epoch 447/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.0220e-04 - accuracy: 0.9942 - val_loss: 0.0014 - val_accuracy: 0.9183\n",
            "Epoch 448/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 9.9825e-05 - accuracy: 0.9943 - val_loss: 0.0013 - val_accuracy: 0.9220\n",
            "Epoch 449/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 9.8707e-05 - accuracy: 0.9945 - val_loss: 0.0014 - val_accuracy: 0.9205\n",
            "Epoch 450/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 9.3614e-05 - accuracy: 0.9947 - val_loss: 0.0012 - val_accuracy: 0.9258\n",
            "Epoch 451/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 9.3444e-05 - accuracy: 0.9948 - val_loss: 0.0013 - val_accuracy: 0.9195\n",
            "Epoch 452/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 9.4474e-05 - accuracy: 0.9945 - val_loss: 0.0012 - val_accuracy: 0.9278\n",
            "Epoch 453/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 9.9723e-05 - accuracy: 0.9943 - val_loss: 0.0013 - val_accuracy: 0.9233\n",
            "Epoch 454/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.0085e-04 - accuracy: 0.9941 - val_loss: 0.0012 - val_accuracy: 0.9277\n",
            "Epoch 455/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.0609e-04 - accuracy: 0.9938 - val_loss: 0.0014 - val_accuracy: 0.9178\n",
            "Epoch 456/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.1053e-04 - accuracy: 0.9934 - val_loss: 0.0013 - val_accuracy: 0.9232\n",
            "Epoch 457/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.0499e-04 - accuracy: 0.9942 - val_loss: 0.0013 - val_accuracy: 0.9265\n",
            "Epoch 458/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 9.6109e-05 - accuracy: 0.9947 - val_loss: 0.0012 - val_accuracy: 0.9315\n",
            "Epoch 459/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.1308e-04 - accuracy: 0.9935 - val_loss: 0.0014 - val_accuracy: 0.9173\n",
            "Epoch 460/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.0544e-04 - accuracy: 0.9939 - val_loss: 0.0015 - val_accuracy: 0.9082\n",
            "Epoch 461/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.1567e-04 - accuracy: 0.9933 - val_loss: 0.0013 - val_accuracy: 0.9212\n",
            "Epoch 462/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.2144e-04 - accuracy: 0.9928 - val_loss: 0.0012 - val_accuracy: 0.9300\n",
            "Epoch 463/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.2574e-04 - accuracy: 0.9930 - val_loss: 0.0016 - val_accuracy: 0.9067\n",
            "Epoch 464/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.4613e-04 - accuracy: 0.9912 - val_loss: 0.0014 - val_accuracy: 0.9147\n",
            "Epoch 465/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.2987e-04 - accuracy: 0.9922 - val_loss: 0.0014 - val_accuracy: 0.9153\n",
            "Epoch 466/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.3186e-04 - accuracy: 0.9922 - val_loss: 0.0015 - val_accuracy: 0.9130\n",
            "Epoch 467/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.1436e-04 - accuracy: 0.9933 - val_loss: 0.0015 - val_accuracy: 0.9122\n",
            "Epoch 468/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.1419e-04 - accuracy: 0.9933 - val_loss: 0.0014 - val_accuracy: 0.9212\n",
            "Epoch 469/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.0186e-04 - accuracy: 0.9942 - val_loss: 0.0014 - val_accuracy: 0.9193\n",
            "Epoch 470/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.2476e-04 - accuracy: 0.9928 - val_loss: 0.0014 - val_accuracy: 0.9198\n",
            "Epoch 471/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.2078e-04 - accuracy: 0.9932 - val_loss: 0.0012 - val_accuracy: 0.9292\n",
            "Epoch 472/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.0366e-04 - accuracy: 0.9940 - val_loss: 0.0013 - val_accuracy: 0.9225\n",
            "Epoch 473/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.2247e-04 - accuracy: 0.9930 - val_loss: 0.0016 - val_accuracy: 0.9072\n",
            "Epoch 474/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.2276e-04 - accuracy: 0.9929 - val_loss: 0.0015 - val_accuracy: 0.9157\n",
            "Epoch 475/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.1734e-04 - accuracy: 0.9933 - val_loss: 0.0013 - val_accuracy: 0.9203\n",
            "Epoch 476/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.0511e-04 - accuracy: 0.9938 - val_loss: 0.0012 - val_accuracy: 0.9283\n",
            "Epoch 477/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.0420e-04 - accuracy: 0.9937 - val_loss: 0.0013 - val_accuracy: 0.9248\n",
            "Epoch 478/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.0188e-04 - accuracy: 0.9942 - val_loss: 0.0013 - val_accuracy: 0.9252\n",
            "Epoch 479/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.0079e-04 - accuracy: 0.9943 - val_loss: 0.0013 - val_accuracy: 0.9275\n",
            "Epoch 480/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 9.2644e-05 - accuracy: 0.9948 - val_loss: 0.0014 - val_accuracy: 0.9202\n",
            "Epoch 481/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.1394e-04 - accuracy: 0.9935 - val_loss: 0.0013 - val_accuracy: 0.9232\n",
            "Epoch 482/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.0741e-04 - accuracy: 0.9938 - val_loss: 0.0013 - val_accuracy: 0.9217\n",
            "Epoch 483/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 9.4670e-05 - accuracy: 0.9945 - val_loss: 0.0013 - val_accuracy: 0.9252\n",
            "Epoch 484/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.0092e-04 - accuracy: 0.9942 - val_loss: 0.0013 - val_accuracy: 0.9242\n",
            "Epoch 485/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 8.7805e-05 - accuracy: 0.9952 - val_loss: 0.0012 - val_accuracy: 0.9288\n",
            "Epoch 486/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 9.7369e-05 - accuracy: 0.9945 - val_loss: 0.0013 - val_accuracy: 0.9220\n",
            "Epoch 487/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 9.6072e-05 - accuracy: 0.9946 - val_loss: 0.0014 - val_accuracy: 0.9155\n",
            "Epoch 488/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.0776e-04 - accuracy: 0.9938 - val_loss: 0.0014 - val_accuracy: 0.9173\n",
            "Epoch 489/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 9.8367e-05 - accuracy: 0.9943 - val_loss: 0.0012 - val_accuracy: 0.9272\n",
            "Epoch 490/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.0439e-04 - accuracy: 0.9941 - val_loss: 0.0012 - val_accuracy: 0.9278\n",
            "Epoch 491/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.0824e-04 - accuracy: 0.9937 - val_loss: 0.0014 - val_accuracy: 0.9200\n",
            "Epoch 492/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.2450e-04 - accuracy: 0.9927 - val_loss: 0.0014 - val_accuracy: 0.9203\n",
            "Epoch 493/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.3819e-04 - accuracy: 0.9918 - val_loss: 0.0014 - val_accuracy: 0.9187\n",
            "Epoch 494/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.1219e-04 - accuracy: 0.9935 - val_loss: 0.0013 - val_accuracy: 0.9263\n",
            "Epoch 495/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.0129e-04 - accuracy: 0.9940 - val_loss: 0.0013 - val_accuracy: 0.9247\n",
            "Epoch 496/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 9.4090e-05 - accuracy: 0.9947 - val_loss: 0.0012 - val_accuracy: 0.9282\n",
            "Epoch 497/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 9.5694e-05 - accuracy: 0.9946 - val_loss: 0.0012 - val_accuracy: 0.9282\n",
            "Epoch 498/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 9.4753e-05 - accuracy: 0.9948 - val_loss: 0.0014 - val_accuracy: 0.9197\n",
            "Epoch 499/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 9.0158e-05 - accuracy: 0.9951 - val_loss: 0.0012 - val_accuracy: 0.9332\n",
            "Epoch 500/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 8.7557e-05 - accuracy: 0.9951 - val_loss: 0.0012 - val_accuracy: 0.9320\n",
            "Epoch 501/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 8.8353e-05 - accuracy: 0.9952 - val_loss: 0.0011 - val_accuracy: 0.9355\n",
            "Epoch 502/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 8.9419e-05 - accuracy: 0.9950 - val_loss: 0.0012 - val_accuracy: 0.9325\n",
            "Epoch 503/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 8.5971e-05 - accuracy: 0.9952 - val_loss: 0.0011 - val_accuracy: 0.9332\n",
            "Epoch 504/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 8.7992e-05 - accuracy: 0.9952 - val_loss: 0.0011 - val_accuracy: 0.9330\n",
            "Epoch 505/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 8.7726e-05 - accuracy: 0.9952 - val_loss: 0.0011 - val_accuracy: 0.9347\n",
            "Epoch 506/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 8.3832e-05 - accuracy: 0.9953 - val_loss: 0.0012 - val_accuracy: 0.9302\n",
            "Epoch 507/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 8.5432e-05 - accuracy: 0.9952 - val_loss: 0.0012 - val_accuracy: 0.9270\n",
            "Epoch 508/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 8.9664e-05 - accuracy: 0.9951 - val_loss: 0.0011 - val_accuracy: 0.9333\n",
            "Epoch 509/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 9.1621e-05 - accuracy: 0.9948 - val_loss: 0.0012 - val_accuracy: 0.9292\n",
            "Epoch 510/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 8.7785e-05 - accuracy: 0.9952 - val_loss: 0.0011 - val_accuracy: 0.9342\n",
            "Epoch 511/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 9.3765e-05 - accuracy: 0.9948 - val_loss: 0.0013 - val_accuracy: 0.9233\n",
            "Epoch 512/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 9.6377e-05 - accuracy: 0.9947 - val_loss: 0.0013 - val_accuracy: 0.9243\n",
            "Epoch 513/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.0215e-04 - accuracy: 0.9941 - val_loss: 0.0012 - val_accuracy: 0.9277\n",
            "Epoch 514/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.0038e-04 - accuracy: 0.9942 - val_loss: 0.0013 - val_accuracy: 0.9262\n",
            "Epoch 515/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.0064e-04 - accuracy: 0.9943 - val_loss: 0.0014 - val_accuracy: 0.9185\n",
            "Epoch 516/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.0299e-04 - accuracy: 0.9942 - val_loss: 0.0013 - val_accuracy: 0.9252\n",
            "Epoch 517/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.0707e-04 - accuracy: 0.9939 - val_loss: 0.0012 - val_accuracy: 0.9282\n",
            "Epoch 518/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 9.8448e-05 - accuracy: 0.9943 - val_loss: 0.0013 - val_accuracy: 0.9260\n",
            "Epoch 519/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 9.9230e-05 - accuracy: 0.9943 - val_loss: 0.0013 - val_accuracy: 0.9242\n",
            "Epoch 520/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.5125e-04 - accuracy: 0.9912 - val_loss: 0.0014 - val_accuracy: 0.9213\n",
            "Epoch 521/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.3230e-04 - accuracy: 0.9922 - val_loss: 0.0013 - val_accuracy: 0.9227\n",
            "Epoch 522/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.5059e-04 - accuracy: 0.9909 - val_loss: 0.0014 - val_accuracy: 0.9162\n",
            "Epoch 523/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.5121e-04 - accuracy: 0.9910 - val_loss: 0.0014 - val_accuracy: 0.9192\n",
            "Epoch 524/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.2905e-04 - accuracy: 0.9927 - val_loss: 0.0013 - val_accuracy: 0.9263\n",
            "Epoch 525/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.6120e-04 - accuracy: 0.9902 - val_loss: 0.0014 - val_accuracy: 0.9188\n",
            "Epoch 526/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.5306e-04 - accuracy: 0.9906 - val_loss: 0.0013 - val_accuracy: 0.9225\n",
            "Epoch 527/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.6207e-04 - accuracy: 0.9902 - val_loss: 0.0019 - val_accuracy: 0.8885\n",
            "Epoch 528/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 2.0753e-04 - accuracy: 0.9875 - val_loss: 0.0015 - val_accuracy: 0.9095\n",
            "Epoch 529/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.8087e-04 - accuracy: 0.9893 - val_loss: 0.0015 - val_accuracy: 0.9105\n",
            "Epoch 530/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.4554e-04 - accuracy: 0.9914 - val_loss: 0.0014 - val_accuracy: 0.9210\n",
            "Epoch 531/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.1765e-04 - accuracy: 0.9933 - val_loss: 0.0014 - val_accuracy: 0.9197\n",
            "Epoch 532/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.0802e-04 - accuracy: 0.9937 - val_loss: 0.0015 - val_accuracy: 0.9143\n",
            "Epoch 533/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 9.9424e-05 - accuracy: 0.9942 - val_loss: 0.0013 - val_accuracy: 0.9270\n",
            "Epoch 534/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.1242e-04 - accuracy: 0.9935 - val_loss: 0.0013 - val_accuracy: 0.9213\n",
            "Epoch 535/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.0343e-04 - accuracy: 0.9940 - val_loss: 0.0013 - val_accuracy: 0.9247\n",
            "Epoch 536/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 9.2242e-05 - accuracy: 0.9948 - val_loss: 0.0011 - val_accuracy: 0.9320\n",
            "Epoch 537/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.1691e-04 - accuracy: 0.9933 - val_loss: 0.0016 - val_accuracy: 0.9085\n",
            "Epoch 538/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.3388e-04 - accuracy: 0.9922 - val_loss: 0.0013 - val_accuracy: 0.9227\n",
            "Epoch 539/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.1483e-04 - accuracy: 0.9934 - val_loss: 0.0013 - val_accuracy: 0.9213\n",
            "Epoch 540/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.0792e-04 - accuracy: 0.9941 - val_loss: 0.0014 - val_accuracy: 0.9225\n",
            "Epoch 541/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.0414e-04 - accuracy: 0.9941 - val_loss: 0.0013 - val_accuracy: 0.9257\n",
            "Epoch 542/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.0344e-04 - accuracy: 0.9943 - val_loss: 0.0012 - val_accuracy: 0.9298\n",
            "Epoch 543/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 9.9682e-05 - accuracy: 0.9945 - val_loss: 0.0012 - val_accuracy: 0.9267\n",
            "Epoch 544/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 9.3522e-05 - accuracy: 0.9948 - val_loss: 0.0012 - val_accuracy: 0.9303\n",
            "Epoch 545/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 9.5217e-05 - accuracy: 0.9948 - val_loss: 0.0013 - val_accuracy: 0.9265\n",
            "Epoch 546/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.0715e-04 - accuracy: 0.9942 - val_loss: 0.0013 - val_accuracy: 0.9220\n",
            "Epoch 547/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.0055e-04 - accuracy: 0.9945 - val_loss: 0.0013 - val_accuracy: 0.9237\n",
            "Epoch 548/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 9.2971e-05 - accuracy: 0.9950 - val_loss: 0.0012 - val_accuracy: 0.9310\n",
            "Epoch 549/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 9.0266e-05 - accuracy: 0.9951 - val_loss: 0.0012 - val_accuracy: 0.9282\n",
            "Epoch 550/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 8.8597e-05 - accuracy: 0.9951 - val_loss: 0.0012 - val_accuracy: 0.9305\n",
            "Epoch 551/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 9.0709e-05 - accuracy: 0.9952 - val_loss: 0.0012 - val_accuracy: 0.9302\n",
            "Epoch 552/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 8.4137e-05 - accuracy: 0.9954 - val_loss: 0.0012 - val_accuracy: 0.9300\n",
            "Epoch 553/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 8.7299e-05 - accuracy: 0.9952 - val_loss: 0.0012 - val_accuracy: 0.9307\n",
            "Epoch 554/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 9.5668e-05 - accuracy: 0.9947 - val_loss: 0.0013 - val_accuracy: 0.9242\n",
            "Epoch 555/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 9.1321e-05 - accuracy: 0.9951 - val_loss: 0.0013 - val_accuracy: 0.9262\n",
            "Epoch 556/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 8.7637e-05 - accuracy: 0.9953 - val_loss: 0.0012 - val_accuracy: 0.9300\n",
            "Epoch 557/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 8.8665e-05 - accuracy: 0.9952 - val_loss: 0.0011 - val_accuracy: 0.9363\n",
            "Epoch 558/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 8.9259e-05 - accuracy: 0.9953 - val_loss: 0.0012 - val_accuracy: 0.9312\n",
            "Epoch 559/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 9.3896e-05 - accuracy: 0.9948 - val_loss: 0.0012 - val_accuracy: 0.9293\n",
            "Epoch 560/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 9.3781e-05 - accuracy: 0.9950 - val_loss: 0.0013 - val_accuracy: 0.9253\n",
            "Epoch 561/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 9.6128e-05 - accuracy: 0.9947 - val_loss: 0.0013 - val_accuracy: 0.9268\n",
            "Epoch 562/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 9.7518e-05 - accuracy: 0.9944 - val_loss: 0.0014 - val_accuracy: 0.9213\n",
            "Epoch 563/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 9.0833e-05 - accuracy: 0.9950 - val_loss: 0.0012 - val_accuracy: 0.9313\n",
            "Epoch 564/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 9.4688e-05 - accuracy: 0.9948 - val_loss: 0.0011 - val_accuracy: 0.9350\n",
            "Epoch 565/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 9.2324e-05 - accuracy: 0.9950 - val_loss: 0.0012 - val_accuracy: 0.9320\n",
            "Epoch 566/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 9.1375e-05 - accuracy: 0.9950 - val_loss: 0.0012 - val_accuracy: 0.9305\n",
            "Epoch 567/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 9.3019e-05 - accuracy: 0.9950 - val_loss: 0.0012 - val_accuracy: 0.9287\n",
            "Epoch 568/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 9.7830e-05 - accuracy: 0.9947 - val_loss: 0.0012 - val_accuracy: 0.9288\n",
            "Epoch 569/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 9.7124e-05 - accuracy: 0.9945 - val_loss: 0.0012 - val_accuracy: 0.9295\n",
            "Epoch 570/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.0576e-04 - accuracy: 0.9942 - val_loss: 0.0013 - val_accuracy: 0.9262\n",
            "Epoch 571/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.0210e-04 - accuracy: 0.9945 - val_loss: 0.0014 - val_accuracy: 0.9233\n",
            "Epoch 572/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.0961e-04 - accuracy: 0.9939 - val_loss: 0.0012 - val_accuracy: 0.9298\n",
            "Epoch 573/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.1694e-04 - accuracy: 0.9935 - val_loss: 0.0014 - val_accuracy: 0.9213\n",
            "Epoch 574/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.1931e-04 - accuracy: 0.9933 - val_loss: 0.0013 - val_accuracy: 0.9260\n",
            "Epoch 575/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.0833e-04 - accuracy: 0.9939 - val_loss: 0.0013 - val_accuracy: 0.9272\n",
            "Epoch 576/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.1592e-04 - accuracy: 0.9935 - val_loss: 0.0013 - val_accuracy: 0.9232\n",
            "Epoch 577/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.2989e-04 - accuracy: 0.9925 - val_loss: 0.0013 - val_accuracy: 0.9245\n",
            "Epoch 578/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.1963e-04 - accuracy: 0.9934 - val_loss: 0.0013 - val_accuracy: 0.9252\n",
            "Epoch 579/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.1330e-04 - accuracy: 0.9938 - val_loss: 0.0013 - val_accuracy: 0.9228\n",
            "Epoch 580/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 9.4212e-05 - accuracy: 0.9950 - val_loss: 0.0012 - val_accuracy: 0.9320\n",
            "Epoch 581/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 8.7209e-05 - accuracy: 0.9951 - val_loss: 0.0012 - val_accuracy: 0.9307\n",
            "Epoch 582/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 8.1319e-05 - accuracy: 0.9956 - val_loss: 0.0012 - val_accuracy: 0.9318\n",
            "Epoch 583/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 7.6320e-05 - accuracy: 0.9959 - val_loss: 0.0011 - val_accuracy: 0.9350\n",
            "Epoch 584/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 8.1081e-05 - accuracy: 0.9955 - val_loss: 0.0013 - val_accuracy: 0.9257\n",
            "Epoch 585/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.2226e-04 - accuracy: 0.9930 - val_loss: 0.0014 - val_accuracy: 0.9208\n",
            "Epoch 586/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.2255e-04 - accuracy: 0.9931 - val_loss: 0.0013 - val_accuracy: 0.9223\n",
            "Epoch 587/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.2204e-04 - accuracy: 0.9930 - val_loss: 0.0012 - val_accuracy: 0.9313\n",
            "Epoch 588/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.1336e-04 - accuracy: 0.9935 - val_loss: 0.0013 - val_accuracy: 0.9262\n",
            "Epoch 589/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.1552e-04 - accuracy: 0.9933 - val_loss: 0.0012 - val_accuracy: 0.9285\n",
            "Epoch 590/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.1358e-04 - accuracy: 0.9937 - val_loss: 0.0012 - val_accuracy: 0.9282\n",
            "Epoch 591/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.3078e-04 - accuracy: 0.9926 - val_loss: 0.0012 - val_accuracy: 0.9310\n",
            "Epoch 592/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.2306e-04 - accuracy: 0.9930 - val_loss: 0.0014 - val_accuracy: 0.9175\n",
            "Epoch 593/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.3945e-04 - accuracy: 0.9918 - val_loss: 0.0012 - val_accuracy: 0.9307\n",
            "Epoch 594/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.2652e-04 - accuracy: 0.9926 - val_loss: 0.0014 - val_accuracy: 0.9178\n",
            "Epoch 595/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.0681e-04 - accuracy: 0.9937 - val_loss: 0.0014 - val_accuracy: 0.9207\n",
            "Epoch 596/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.4477e-04 - accuracy: 0.9912 - val_loss: 0.0016 - val_accuracy: 0.9120\n",
            "Epoch 597/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.3709e-04 - accuracy: 0.9920 - val_loss: 0.0014 - val_accuracy: 0.9207\n",
            "Epoch 598/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.0390e-04 - accuracy: 0.9940 - val_loss: 0.0014 - val_accuracy: 0.9218\n",
            "Epoch 599/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 9.3508e-05 - accuracy: 0.9947 - val_loss: 0.0013 - val_accuracy: 0.9282\n",
            "Epoch 600/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 8.8637e-05 - accuracy: 0.9952 - val_loss: 0.0012 - val_accuracy: 0.9308\n",
            "Epoch 601/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 8.6650e-05 - accuracy: 0.9951 - val_loss: 0.0013 - val_accuracy: 0.9245\n",
            "Epoch 602/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 8.4999e-05 - accuracy: 0.9952 - val_loss: 0.0013 - val_accuracy: 0.9278\n",
            "Epoch 603/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.8728e-05 - accuracy: 0.9956 - val_loss: 0.0012 - val_accuracy: 0.9323\n",
            "Epoch 604/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 7.8285e-05 - accuracy: 0.9958 - val_loss: 0.0012 - val_accuracy: 0.9335\n",
            "Epoch 605/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 7.2478e-05 - accuracy: 0.9961 - val_loss: 0.0011 - val_accuracy: 0.9357\n",
            "Epoch 606/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 7.4307e-05 - accuracy: 0.9959 - val_loss: 0.0011 - val_accuracy: 0.9365\n",
            "Epoch 607/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 8.0823e-05 - accuracy: 0.9955 - val_loss: 0.0011 - val_accuracy: 0.9332\n",
            "Epoch 608/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 9.7386e-05 - accuracy: 0.9945 - val_loss: 0.0013 - val_accuracy: 0.9263\n",
            "Epoch 609/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 8.3844e-05 - accuracy: 0.9955 - val_loss: 0.0012 - val_accuracy: 0.9312\n",
            "Epoch 610/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 8.2558e-05 - accuracy: 0.9955 - val_loss: 0.0011 - val_accuracy: 0.9362\n",
            "Epoch 611/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 7.2084e-05 - accuracy: 0.9961 - val_loss: 0.0012 - val_accuracy: 0.9305\n",
            "Epoch 612/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 7.0780e-05 - accuracy: 0.9963 - val_loss: 0.0011 - val_accuracy: 0.9358\n",
            "Epoch 613/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 7.7181e-05 - accuracy: 0.9959 - val_loss: 0.0011 - val_accuracy: 0.9365\n",
            "Epoch 614/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.4705e-05 - accuracy: 0.9961 - val_loss: 0.0012 - val_accuracy: 0.9290\n",
            "Epoch 615/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 8.4135e-05 - accuracy: 0.9954 - val_loss: 0.0012 - val_accuracy: 0.9285\n",
            "Epoch 616/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 9.1452e-05 - accuracy: 0.9948 - val_loss: 0.0013 - val_accuracy: 0.9237\n",
            "Epoch 617/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 8.9953e-05 - accuracy: 0.9950 - val_loss: 0.0013 - val_accuracy: 0.9255\n",
            "Epoch 618/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 9.5125e-05 - accuracy: 0.9947 - val_loss: 0.0013 - val_accuracy: 0.9282\n",
            "Epoch 619/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 8.1467e-05 - accuracy: 0.9957 - val_loss: 0.0013 - val_accuracy: 0.9280\n",
            "Epoch 620/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 7.8015e-05 - accuracy: 0.9956 - val_loss: 0.0011 - val_accuracy: 0.9347\n",
            "Epoch 621/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 8.0379e-05 - accuracy: 0.9957 - val_loss: 0.0011 - val_accuracy: 0.9353\n",
            "Epoch 622/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.4594e-05 - accuracy: 0.9960 - val_loss: 0.0012 - val_accuracy: 0.9288\n",
            "Epoch 623/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 7.2831e-05 - accuracy: 0.9962 - val_loss: 0.0012 - val_accuracy: 0.9292\n",
            "Epoch 624/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 7.4631e-05 - accuracy: 0.9960 - val_loss: 0.0011 - val_accuracy: 0.9352\n",
            "Epoch 625/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 7.7282e-05 - accuracy: 0.9957 - val_loss: 0.0012 - val_accuracy: 0.9323\n",
            "Epoch 626/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 7.3868e-05 - accuracy: 0.9960 - val_loss: 0.0012 - val_accuracy: 0.9320\n",
            "Epoch 627/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.6041e-05 - accuracy: 0.9959 - val_loss: 0.0012 - val_accuracy: 0.9303\n",
            "Epoch 628/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 7.5701e-05 - accuracy: 0.9960 - val_loss: 0.0012 - val_accuracy: 0.9303\n",
            "Epoch 629/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.3084e-05 - accuracy: 0.9961 - val_loss: 0.0011 - val_accuracy: 0.9345\n",
            "Epoch 630/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 7.3973e-05 - accuracy: 0.9960 - val_loss: 0.0011 - val_accuracy: 0.9373\n",
            "Epoch 631/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 8.4369e-05 - accuracy: 0.9955 - val_loss: 0.0012 - val_accuracy: 0.9288\n",
            "Epoch 632/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 8.7031e-05 - accuracy: 0.9951 - val_loss: 0.0013 - val_accuracy: 0.9288\n",
            "Epoch 633/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 8.7673e-05 - accuracy: 0.9950 - val_loss: 0.0011 - val_accuracy: 0.9367\n",
            "Epoch 634/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 9.9645e-05 - accuracy: 0.9942 - val_loss: 0.0013 - val_accuracy: 0.9270\n",
            "Epoch 635/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.2431e-04 - accuracy: 0.9927 - val_loss: 0.0014 - val_accuracy: 0.9192\n",
            "Epoch 636/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.1281e-04 - accuracy: 0.9934 - val_loss: 0.0013 - val_accuracy: 0.9243\n",
            "Epoch 637/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.0678e-04 - accuracy: 0.9940 - val_loss: 0.0013 - val_accuracy: 0.9243\n",
            "Epoch 638/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 9.4206e-05 - accuracy: 0.9948 - val_loss: 0.0012 - val_accuracy: 0.9292\n",
            "Epoch 639/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.0067e-04 - accuracy: 0.9944 - val_loss: 0.0012 - val_accuracy: 0.9315\n",
            "Epoch 640/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.1723e-04 - accuracy: 0.9931 - val_loss: 0.0012 - val_accuracy: 0.9322\n",
            "Epoch 641/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.4216e-04 - accuracy: 0.9918 - val_loss: 0.0014 - val_accuracy: 0.9190\n",
            "Epoch 642/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.0840e-04 - accuracy: 0.9937 - val_loss: 0.0013 - val_accuracy: 0.9245\n",
            "Epoch 643/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 9.6266e-05 - accuracy: 0.9945 - val_loss: 0.0012 - val_accuracy: 0.9333\n",
            "Epoch 644/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 9.5195e-05 - accuracy: 0.9945 - val_loss: 0.0013 - val_accuracy: 0.9252\n",
            "Epoch 645/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 9.8876e-05 - accuracy: 0.9945 - val_loss: 0.0012 - val_accuracy: 0.9302\n",
            "Epoch 646/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 8.2906e-05 - accuracy: 0.9952 - val_loss: 0.0013 - val_accuracy: 0.9258\n",
            "Epoch 647/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 8.7254e-05 - accuracy: 0.9952 - val_loss: 0.0014 - val_accuracy: 0.9213\n",
            "Epoch 648/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 8.8806e-05 - accuracy: 0.9951 - val_loss: 0.0012 - val_accuracy: 0.9320\n",
            "Epoch 649/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 8.9822e-05 - accuracy: 0.9949 - val_loss: 0.0014 - val_accuracy: 0.9222\n",
            "Epoch 650/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 9.2351e-05 - accuracy: 0.9947 - val_loss: 0.0013 - val_accuracy: 0.9253\n",
            "Epoch 651/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 8.7587e-05 - accuracy: 0.9951 - val_loss: 0.0013 - val_accuracy: 0.9252\n",
            "Epoch 652/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 8.8840e-05 - accuracy: 0.9951 - val_loss: 0.0013 - val_accuracy: 0.9263\n",
            "Epoch 653/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.8719e-05 - accuracy: 0.9958 - val_loss: 0.0012 - val_accuracy: 0.9318\n",
            "Epoch 654/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.3226e-05 - accuracy: 0.9961 - val_loss: 0.0012 - val_accuracy: 0.9318\n",
            "Epoch 655/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.1640e-05 - accuracy: 0.9962 - val_loss: 0.0012 - val_accuracy: 0.9313\n",
            "Epoch 656/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 7.7610e-05 - accuracy: 0.9958 - val_loss: 0.0013 - val_accuracy: 0.9238\n",
            "Epoch 657/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 8.1124e-05 - accuracy: 0.9956 - val_loss: 0.0012 - val_accuracy: 0.9313\n",
            "Epoch 658/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 7.4884e-05 - accuracy: 0.9959 - val_loss: 0.0012 - val_accuracy: 0.9328\n",
            "Epoch 659/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 7.5841e-05 - accuracy: 0.9959 - val_loss: 0.0012 - val_accuracy: 0.9287\n",
            "Epoch 660/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.7048e-05 - accuracy: 0.9957 - val_loss: 0.0013 - val_accuracy: 0.9287\n",
            "Epoch 661/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 9.0184e-05 - accuracy: 0.9950 - val_loss: 0.0013 - val_accuracy: 0.9270\n",
            "Epoch 662/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 9.5201e-05 - accuracy: 0.9948 - val_loss: 0.0012 - val_accuracy: 0.9287\n",
            "Epoch 663/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 8.0743e-05 - accuracy: 0.9957 - val_loss: 0.0012 - val_accuracy: 0.9318\n",
            "Epoch 664/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 7.5337e-05 - accuracy: 0.9960 - val_loss: 0.0012 - val_accuracy: 0.9292\n",
            "Epoch 665/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 8.2627e-05 - accuracy: 0.9955 - val_loss: 0.0013 - val_accuracy: 0.9263\n",
            "Epoch 666/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.8678e-05 - accuracy: 0.9958 - val_loss: 0.0012 - val_accuracy: 0.9323\n",
            "Epoch 667/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 7.9597e-05 - accuracy: 0.9958 - val_loss: 0.0012 - val_accuracy: 0.9308\n",
            "Epoch 668/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 7.4458e-05 - accuracy: 0.9959 - val_loss: 0.0012 - val_accuracy: 0.9315\n",
            "Epoch 669/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 8.2831e-05 - accuracy: 0.9954 - val_loss: 0.0011 - val_accuracy: 0.9350\n",
            "Epoch 670/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 8.6547e-05 - accuracy: 0.9953 - val_loss: 0.0012 - val_accuracy: 0.9295\n",
            "Epoch 671/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.0134e-04 - accuracy: 0.9942 - val_loss: 0.0011 - val_accuracy: 0.9382\n",
            "Epoch 672/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 8.8992e-05 - accuracy: 0.9951 - val_loss: 0.0014 - val_accuracy: 0.9197\n",
            "Epoch 673/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 8.7767e-05 - accuracy: 0.9952 - val_loss: 0.0012 - val_accuracy: 0.9300\n",
            "Epoch 674/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 7.9266e-05 - accuracy: 0.9956 - val_loss: 0.0012 - val_accuracy: 0.9293\n",
            "Epoch 675/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.7546e-05 - accuracy: 0.9958 - val_loss: 0.0011 - val_accuracy: 0.9345\n",
            "Epoch 676/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.7193e-05 - accuracy: 0.9958 - val_loss: 0.0012 - val_accuracy: 0.9317\n",
            "Epoch 677/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 8.1040e-05 - accuracy: 0.9956 - val_loss: 0.0013 - val_accuracy: 0.9297\n",
            "Epoch 678/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 8.2680e-05 - accuracy: 0.9953 - val_loss: 0.0012 - val_accuracy: 0.9278\n",
            "Epoch 679/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 8.9546e-05 - accuracy: 0.9951 - val_loss: 0.0013 - val_accuracy: 0.9277\n",
            "Epoch 680/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 8.7580e-05 - accuracy: 0.9952 - val_loss: 0.0012 - val_accuracy: 0.9310\n",
            "Epoch 681/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 9.4506e-05 - accuracy: 0.9948 - val_loss: 0.0013 - val_accuracy: 0.9242\n",
            "Epoch 682/1000\n",
            "24/24 [==============================] - 3s 136ms/step - loss: 1.1149e-04 - accuracy: 0.9937 - val_loss: 0.0014 - val_accuracy: 0.9220\n",
            "Epoch 683/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 1.0291e-04 - accuracy: 0.9943 - val_loss: 0.0014 - val_accuracy: 0.9205\n",
            "Epoch 684/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.0202e-04 - accuracy: 0.9942 - val_loss: 0.0013 - val_accuracy: 0.9285\n",
            "Epoch 685/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.0229e-04 - accuracy: 0.9944 - val_loss: 0.0013 - val_accuracy: 0.9287\n",
            "Epoch 686/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 8.8049e-05 - accuracy: 0.9952 - val_loss: 0.0013 - val_accuracy: 0.9287\n",
            "Epoch 687/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 9.6924e-05 - accuracy: 0.9946 - val_loss: 0.0013 - val_accuracy: 0.9297\n",
            "Epoch 688/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 9.1472e-05 - accuracy: 0.9949 - val_loss: 0.0013 - val_accuracy: 0.9290\n",
            "Epoch 689/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 9.0162e-05 - accuracy: 0.9950 - val_loss: 0.0012 - val_accuracy: 0.9332\n",
            "Epoch 690/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 9.6532e-05 - accuracy: 0.9944 - val_loss: 0.0012 - val_accuracy: 0.9300\n",
            "Epoch 691/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.0669e-04 - accuracy: 0.9942 - val_loss: 0.0013 - val_accuracy: 0.9252\n",
            "Epoch 692/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.1593e-04 - accuracy: 0.9934 - val_loss: 0.0012 - val_accuracy: 0.9283\n",
            "Epoch 693/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.0172e-04 - accuracy: 0.9942 - val_loss: 0.0013 - val_accuracy: 0.9258\n",
            "Epoch 694/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 9.3366e-05 - accuracy: 0.9949 - val_loss: 0.0013 - val_accuracy: 0.9285\n",
            "Epoch 695/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 8.3203e-05 - accuracy: 0.9955 - val_loss: 0.0012 - val_accuracy: 0.9338\n",
            "Epoch 696/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 8.0949e-05 - accuracy: 0.9956 - val_loss: 0.0012 - val_accuracy: 0.9303\n",
            "Epoch 697/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 8.0057e-05 - accuracy: 0.9958 - val_loss: 0.0012 - val_accuracy: 0.9320\n",
            "Epoch 698/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.3767e-05 - accuracy: 0.9960 - val_loss: 0.0012 - val_accuracy: 0.9330\n",
            "Epoch 699/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.0790e-05 - accuracy: 0.9964 - val_loss: 0.0011 - val_accuracy: 0.9375\n",
            "Epoch 700/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 7.7107e-05 - accuracy: 0.9959 - val_loss: 0.0012 - val_accuracy: 0.9330\n",
            "Epoch 701/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 8.5491e-05 - accuracy: 0.9953 - val_loss: 0.0012 - val_accuracy: 0.9317\n",
            "Epoch 702/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 8.2330e-05 - accuracy: 0.9954 - val_loss: 0.0013 - val_accuracy: 0.9238\n",
            "Epoch 703/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 9.2153e-05 - accuracy: 0.9951 - val_loss: 0.0012 - val_accuracy: 0.9335\n",
            "Epoch 704/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.0610e-04 - accuracy: 0.9940 - val_loss: 0.0014 - val_accuracy: 0.9217\n",
            "Epoch 705/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 9.9456e-05 - accuracy: 0.9942 - val_loss: 0.0012 - val_accuracy: 0.9298\n",
            "Epoch 706/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 1.0350e-04 - accuracy: 0.9942 - val_loss: 0.0013 - val_accuracy: 0.9278\n",
            "Epoch 707/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 9.5593e-05 - accuracy: 0.9948 - val_loss: 0.0013 - val_accuracy: 0.9282\n",
            "Epoch 708/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 8.9535e-05 - accuracy: 0.9951 - val_loss: 0.0012 - val_accuracy: 0.9292\n",
            "Epoch 709/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 8.1722e-05 - accuracy: 0.9955 - val_loss: 0.0011 - val_accuracy: 0.9368\n",
            "Epoch 710/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.7704e-05 - accuracy: 0.9960 - val_loss: 0.0011 - val_accuracy: 0.9392\n",
            "Epoch 711/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.1683e-05 - accuracy: 0.9961 - val_loss: 0.0011 - val_accuracy: 0.9407\n",
            "Epoch 712/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 7.2576e-05 - accuracy: 0.9962 - val_loss: 0.0011 - val_accuracy: 0.9363\n",
            "Epoch 713/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 7.2586e-05 - accuracy: 0.9961 - val_loss: 0.0011 - val_accuracy: 0.9368\n",
            "Epoch 714/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.2535e-05 - accuracy: 0.9961 - val_loss: 0.0011 - val_accuracy: 0.9358\n",
            "Epoch 715/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.5654e-05 - accuracy: 0.9960 - val_loss: 0.0011 - val_accuracy: 0.9393\n",
            "Epoch 716/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.7091e-05 - accuracy: 0.9958 - val_loss: 0.0010 - val_accuracy: 0.9415\n",
            "Epoch 717/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.0145e-05 - accuracy: 0.9964 - val_loss: 0.0011 - val_accuracy: 0.9408\n",
            "Epoch 718/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.9725e-05 - accuracy: 0.9957 - val_loss: 0.0011 - val_accuracy: 0.9365\n",
            "Epoch 719/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.3146e-05 - accuracy: 0.9960 - val_loss: 0.0011 - val_accuracy: 0.9377\n",
            "Epoch 720/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 8.1470e-05 - accuracy: 0.9956 - val_loss: 0.0012 - val_accuracy: 0.9303\n",
            "Epoch 721/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 8.2614e-05 - accuracy: 0.9956 - val_loss: 0.0012 - val_accuracy: 0.9282\n",
            "Epoch 722/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 9.1682e-05 - accuracy: 0.9948 - val_loss: 0.0011 - val_accuracy: 0.9353\n",
            "Epoch 723/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 9.1716e-05 - accuracy: 0.9949 - val_loss: 0.0011 - val_accuracy: 0.9345\n",
            "Epoch 724/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 9.4097e-05 - accuracy: 0.9949 - val_loss: 0.0012 - val_accuracy: 0.9307\n",
            "Epoch 725/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 8.9918e-05 - accuracy: 0.9950 - val_loss: 0.0013 - val_accuracy: 0.9263\n",
            "Epoch 726/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 8.5455e-05 - accuracy: 0.9953 - val_loss: 0.0011 - val_accuracy: 0.9372\n",
            "Epoch 727/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 8.0383e-05 - accuracy: 0.9957 - val_loss: 0.0012 - val_accuracy: 0.9307\n",
            "Epoch 728/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 8.5057e-05 - accuracy: 0.9953 - val_loss: 0.0013 - val_accuracy: 0.9280\n",
            "Epoch 729/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.0382e-04 - accuracy: 0.9942 - val_loss: 0.0013 - val_accuracy: 0.9292\n",
            "Epoch 730/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 1.0074e-04 - accuracy: 0.9942 - val_loss: 0.0012 - val_accuracy: 0.9308\n",
            "Epoch 731/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 1.1400e-04 - accuracy: 0.9934 - val_loss: 0.0011 - val_accuracy: 0.9382\n",
            "Epoch 732/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 1.2056e-04 - accuracy: 0.9931 - val_loss: 0.0011 - val_accuracy: 0.9350\n",
            "Epoch 733/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 9.7966e-05 - accuracy: 0.9945 - val_loss: 0.0012 - val_accuracy: 0.9313\n",
            "Epoch 734/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 9.0769e-05 - accuracy: 0.9952 - val_loss: 0.0012 - val_accuracy: 0.9342\n",
            "Epoch 735/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 9.1676e-05 - accuracy: 0.9949 - val_loss: 0.0011 - val_accuracy: 0.9375\n",
            "Epoch 736/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 9.6084e-05 - accuracy: 0.9945 - val_loss: 0.0012 - val_accuracy: 0.9298\n",
            "Epoch 737/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.0840e-04 - accuracy: 0.9938 - val_loss: 0.0013 - val_accuracy: 0.9272\n",
            "Epoch 738/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 1.2807e-04 - accuracy: 0.9929 - val_loss: 0.0011 - val_accuracy: 0.9333\n",
            "Epoch 739/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 1.1573e-04 - accuracy: 0.9935 - val_loss: 0.0013 - val_accuracy: 0.9265\n",
            "Epoch 740/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 1.0000e-04 - accuracy: 0.9946 - val_loss: 0.0012 - val_accuracy: 0.9332\n",
            "Epoch 741/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 1.0509e-04 - accuracy: 0.9940 - val_loss: 0.0013 - val_accuracy: 0.9270\n",
            "Epoch 742/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 9.8708e-05 - accuracy: 0.9944 - val_loss: 0.0012 - val_accuracy: 0.9327\n",
            "Epoch 743/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 9.4843e-05 - accuracy: 0.9947 - val_loss: 0.0011 - val_accuracy: 0.9358\n",
            "Epoch 744/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 8.6400e-05 - accuracy: 0.9953 - val_loss: 0.0012 - val_accuracy: 0.9347\n",
            "Epoch 745/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.8399e-05 - accuracy: 0.9959 - val_loss: 0.0012 - val_accuracy: 0.9320\n",
            "Epoch 746/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 8.1796e-05 - accuracy: 0.9956 - val_loss: 0.0012 - val_accuracy: 0.9308\n",
            "Epoch 747/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.7136e-05 - accuracy: 0.9959 - val_loss: 0.0011 - val_accuracy: 0.9353\n",
            "Epoch 748/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.6144e-05 - accuracy: 0.9959 - val_loss: 0.0013 - val_accuracy: 0.9272\n",
            "Epoch 749/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 8.6384e-05 - accuracy: 0.9953 - val_loss: 0.0011 - val_accuracy: 0.9360\n",
            "Epoch 750/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.8525e-05 - accuracy: 0.9957 - val_loss: 0.0011 - val_accuracy: 0.9357\n",
            "Epoch 751/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.8235e-05 - accuracy: 0.9958 - val_loss: 0.0011 - val_accuracy: 0.9395\n",
            "Epoch 752/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.5209e-05 - accuracy: 0.9961 - val_loss: 0.0011 - val_accuracy: 0.9410\n",
            "Epoch 753/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 7.4193e-05 - accuracy: 0.9960 - val_loss: 0.0011 - val_accuracy: 0.9377\n",
            "Epoch 754/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 8.3857e-05 - accuracy: 0.9953 - val_loss: 0.0012 - val_accuracy: 0.9352\n",
            "Epoch 755/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 9.1912e-05 - accuracy: 0.9950 - val_loss: 0.0011 - val_accuracy: 0.9338\n",
            "Epoch 756/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 8.2301e-05 - accuracy: 0.9956 - val_loss: 0.0011 - val_accuracy: 0.9373\n",
            "Epoch 757/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.8124e-05 - accuracy: 0.9959 - val_loss: 0.0011 - val_accuracy: 0.9383\n",
            "Epoch 758/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 6.9212e-05 - accuracy: 0.9964 - val_loss: 0.0011 - val_accuracy: 0.9373\n",
            "Epoch 759/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.1561e-05 - accuracy: 0.9962 - val_loss: 0.0011 - val_accuracy: 0.9393\n",
            "Epoch 760/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.8199e-05 - accuracy: 0.9959 - val_loss: 0.0012 - val_accuracy: 0.9342\n",
            "Epoch 761/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.5483e-05 - accuracy: 0.9960 - val_loss: 0.0012 - val_accuracy: 0.9280\n",
            "Epoch 762/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 9.3164e-05 - accuracy: 0.9948 - val_loss: 0.0012 - val_accuracy: 0.9315\n",
            "Epoch 763/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 9.4237e-05 - accuracy: 0.9945 - val_loss: 0.0014 - val_accuracy: 0.9215\n",
            "Epoch 764/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 9.9161e-05 - accuracy: 0.9944 - val_loss: 0.0013 - val_accuracy: 0.9253\n",
            "Epoch 765/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 8.5999e-05 - accuracy: 0.9952 - val_loss: 0.0012 - val_accuracy: 0.9338\n",
            "Epoch 766/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 8.0896e-05 - accuracy: 0.9956 - val_loss: 0.0013 - val_accuracy: 0.9285\n",
            "Epoch 767/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 8.1045e-05 - accuracy: 0.9956 - val_loss: 0.0011 - val_accuracy: 0.9362\n",
            "Epoch 768/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 7.7042e-05 - accuracy: 0.9959 - val_loss: 0.0011 - val_accuracy: 0.9372\n",
            "Epoch 769/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.8951e-05 - accuracy: 0.9957 - val_loss: 0.0012 - val_accuracy: 0.9318\n",
            "Epoch 770/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 9.1928e-05 - accuracy: 0.9949 - val_loss: 0.0012 - val_accuracy: 0.9335\n",
            "Epoch 771/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 8.5855e-05 - accuracy: 0.9953 - val_loss: 0.0012 - val_accuracy: 0.9322\n",
            "Epoch 772/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 1.0950e-04 - accuracy: 0.9937 - val_loss: 0.0013 - val_accuracy: 0.9247\n",
            "Epoch 773/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 1.0023e-04 - accuracy: 0.9942 - val_loss: 0.0012 - val_accuracy: 0.9308\n",
            "Epoch 774/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 9.2376e-05 - accuracy: 0.9948 - val_loss: 0.0012 - val_accuracy: 0.9298\n",
            "Epoch 775/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 8.5418e-05 - accuracy: 0.9953 - val_loss: 0.0013 - val_accuracy: 0.9292\n",
            "Epoch 776/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 8.7579e-05 - accuracy: 0.9951 - val_loss: 0.0013 - val_accuracy: 0.9265\n",
            "Epoch 777/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 8.6536e-05 - accuracy: 0.9952 - val_loss: 0.0012 - val_accuracy: 0.9342\n",
            "Epoch 778/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.2499e-05 - accuracy: 0.9962 - val_loss: 0.0011 - val_accuracy: 0.9342\n",
            "Epoch 779/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.4261e-05 - accuracy: 0.9960 - val_loss: 0.0012 - val_accuracy: 0.9318\n",
            "Epoch 780/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.5929e-05 - accuracy: 0.9959 - val_loss: 0.0011 - val_accuracy: 0.9397\n",
            "Epoch 781/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 8.4450e-05 - accuracy: 0.9953 - val_loss: 0.0012 - val_accuracy: 0.9328\n",
            "Epoch 782/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 8.6879e-05 - accuracy: 0.9954 - val_loss: 0.0012 - val_accuracy: 0.9320\n",
            "Epoch 783/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 8.4164e-05 - accuracy: 0.9955 - val_loss: 0.0013 - val_accuracy: 0.9270\n",
            "Epoch 784/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 9.9836e-05 - accuracy: 0.9943 - val_loss: 0.0011 - val_accuracy: 0.9370\n",
            "Epoch 785/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 8.3662e-05 - accuracy: 0.9955 - val_loss: 0.0012 - val_accuracy: 0.9300\n",
            "Epoch 786/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.6572e-05 - accuracy: 0.9959 - val_loss: 0.0011 - val_accuracy: 0.9367\n",
            "Epoch 787/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.3363e-05 - accuracy: 0.9960 - val_loss: 0.0011 - val_accuracy: 0.9368\n",
            "Epoch 788/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 7.6490e-05 - accuracy: 0.9959 - val_loss: 0.0011 - val_accuracy: 0.9370\n",
            "Epoch 789/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 8.9504e-05 - accuracy: 0.9949 - val_loss: 0.0012 - val_accuracy: 0.9313\n",
            "Epoch 790/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 1.0044e-04 - accuracy: 0.9945 - val_loss: 0.0012 - val_accuracy: 0.9345\n",
            "Epoch 791/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 8.5776e-05 - accuracy: 0.9952 - val_loss: 0.0011 - val_accuracy: 0.9370\n",
            "Epoch 792/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 8.2411e-05 - accuracy: 0.9955 - val_loss: 0.0011 - val_accuracy: 0.9365\n",
            "Epoch 793/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.8565e-05 - accuracy: 0.9958 - val_loss: 0.0012 - val_accuracy: 0.9287\n",
            "Epoch 794/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 9.0409e-05 - accuracy: 0.9950 - val_loss: 0.0011 - val_accuracy: 0.9360\n",
            "Epoch 795/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 9.9172e-05 - accuracy: 0.9945 - val_loss: 0.0012 - val_accuracy: 0.9328\n",
            "Epoch 796/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.8304e-05 - accuracy: 0.9958 - val_loss: 0.0012 - val_accuracy: 0.9348\n",
            "Epoch 797/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 8.1103e-05 - accuracy: 0.9956 - val_loss: 0.0012 - val_accuracy: 0.9328\n",
            "Epoch 798/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 8.5741e-05 - accuracy: 0.9953 - val_loss: 0.0012 - val_accuracy: 0.9288\n",
            "Epoch 799/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 8.0844e-05 - accuracy: 0.9956 - val_loss: 0.0012 - val_accuracy: 0.9340\n",
            "Epoch 800/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 7.3831e-05 - accuracy: 0.9961 - val_loss: 0.0012 - val_accuracy: 0.9347\n",
            "Epoch 801/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.9925e-05 - accuracy: 0.9956 - val_loss: 0.0012 - val_accuracy: 0.9325\n",
            "Epoch 802/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.9359e-05 - accuracy: 0.9958 - val_loss: 0.0011 - val_accuracy: 0.9385\n",
            "Epoch 803/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.7622e-05 - accuracy: 0.9959 - val_loss: 0.0010 - val_accuracy: 0.9423\n",
            "Epoch 804/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 7.5614e-05 - accuracy: 0.9958 - val_loss: 0.0011 - val_accuracy: 0.9350\n",
            "Epoch 805/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 8.0318e-05 - accuracy: 0.9957 - val_loss: 0.0011 - val_accuracy: 0.9358\n",
            "Epoch 806/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.4821e-05 - accuracy: 0.9960 - val_loss: 0.0011 - val_accuracy: 0.9387\n",
            "Epoch 807/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 7.8782e-05 - accuracy: 0.9956 - val_loss: 0.0011 - val_accuracy: 0.9358\n",
            "Epoch 808/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 8.3584e-05 - accuracy: 0.9955 - val_loss: 0.0013 - val_accuracy: 0.9280\n",
            "Epoch 809/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 8.1699e-05 - accuracy: 0.9955 - val_loss: 0.0011 - val_accuracy: 0.9360\n",
            "Epoch 810/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 8.4786e-05 - accuracy: 0.9954 - val_loss: 0.0012 - val_accuracy: 0.9342\n",
            "Epoch 811/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.5274e-05 - accuracy: 0.9961 - val_loss: 9.8904e-04 - val_accuracy: 0.9433\n",
            "Epoch 812/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.5404e-05 - accuracy: 0.9960 - val_loss: 0.0012 - val_accuracy: 0.9337\n",
            "Epoch 813/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.3488e-05 - accuracy: 0.9960 - val_loss: 0.0012 - val_accuracy: 0.9342\n",
            "Epoch 814/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.3396e-05 - accuracy: 0.9960 - val_loss: 0.0011 - val_accuracy: 0.9362\n",
            "Epoch 815/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.3910e-05 - accuracy: 0.9961 - val_loss: 0.0011 - val_accuracy: 0.9388\n",
            "Epoch 816/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.8900e-05 - accuracy: 0.9957 - val_loss: 0.0011 - val_accuracy: 0.9367\n",
            "Epoch 817/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 8.5844e-05 - accuracy: 0.9954 - val_loss: 0.0012 - val_accuracy: 0.9343\n",
            "Epoch 818/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 9.4181e-05 - accuracy: 0.9949 - val_loss: 0.0012 - val_accuracy: 0.9338\n",
            "Epoch 819/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 1.0491e-04 - accuracy: 0.9941 - val_loss: 0.0012 - val_accuracy: 0.9310\n",
            "Epoch 820/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 8.9406e-05 - accuracy: 0.9951 - val_loss: 0.0012 - val_accuracy: 0.9330\n",
            "Epoch 821/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 8.1705e-05 - accuracy: 0.9955 - val_loss: 0.0012 - val_accuracy: 0.9317\n",
            "Epoch 822/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.8516e-05 - accuracy: 0.9957 - val_loss: 0.0012 - val_accuracy: 0.9320\n",
            "Epoch 823/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.9492e-05 - accuracy: 0.9957 - val_loss: 0.0012 - val_accuracy: 0.9303\n",
            "Epoch 824/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 9.5486e-05 - accuracy: 0.9948 - val_loss: 0.0012 - val_accuracy: 0.9300\n",
            "Epoch 825/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 8.0195e-05 - accuracy: 0.9956 - val_loss: 0.0011 - val_accuracy: 0.9375\n",
            "Epoch 826/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 8.2837e-05 - accuracy: 0.9956 - val_loss: 0.0011 - val_accuracy: 0.9357\n",
            "Epoch 827/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 9.0921e-05 - accuracy: 0.9950 - val_loss: 0.0011 - val_accuracy: 0.9365\n",
            "Epoch 828/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 9.7734e-05 - accuracy: 0.9944 - val_loss: 0.0011 - val_accuracy: 0.9390\n",
            "Epoch 829/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 8.2086e-05 - accuracy: 0.9955 - val_loss: 0.0013 - val_accuracy: 0.9240\n",
            "Epoch 830/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 1.1294e-04 - accuracy: 0.9935 - val_loss: 0.0013 - val_accuracy: 0.9238\n",
            "Epoch 831/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 1.0487e-04 - accuracy: 0.9942 - val_loss: 0.0013 - val_accuracy: 0.9270\n",
            "Epoch 832/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 9.5035e-05 - accuracy: 0.9946 - val_loss: 0.0012 - val_accuracy: 0.9320\n",
            "Epoch 833/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 9.5449e-05 - accuracy: 0.9948 - val_loss: 0.0011 - val_accuracy: 0.9353\n",
            "Epoch 834/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 8.0973e-05 - accuracy: 0.9957 - val_loss: 0.0012 - val_accuracy: 0.9310\n",
            "Epoch 835/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 8.4850e-05 - accuracy: 0.9954 - val_loss: 0.0012 - val_accuracy: 0.9317\n",
            "Epoch 836/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 8.2442e-05 - accuracy: 0.9955 - val_loss: 0.0011 - val_accuracy: 0.9353\n",
            "Epoch 837/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 8.9731e-05 - accuracy: 0.9950 - val_loss: 0.0013 - val_accuracy: 0.9295\n",
            "Epoch 838/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 9.0999e-05 - accuracy: 0.9951 - val_loss: 0.0012 - val_accuracy: 0.9313\n",
            "Epoch 839/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 9.4090e-05 - accuracy: 0.9948 - val_loss: 0.0012 - val_accuracy: 0.9305\n",
            "Epoch 840/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 9.6533e-05 - accuracy: 0.9948 - val_loss: 0.0012 - val_accuracy: 0.9345\n",
            "Epoch 841/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 9.8068e-05 - accuracy: 0.9946 - val_loss: 0.0014 - val_accuracy: 0.9232\n",
            "Epoch 842/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 1.0295e-04 - accuracy: 0.9942 - val_loss: 0.0012 - val_accuracy: 0.9298\n",
            "Epoch 843/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 9.3752e-05 - accuracy: 0.9947 - val_loss: 0.0012 - val_accuracy: 0.9328\n",
            "Epoch 844/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 9.7038e-05 - accuracy: 0.9942 - val_loss: 0.0012 - val_accuracy: 0.9338\n",
            "Epoch 845/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 8.2868e-05 - accuracy: 0.9955 - val_loss: 0.0012 - val_accuracy: 0.9315\n",
            "Epoch 846/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 8.3645e-05 - accuracy: 0.9956 - val_loss: 0.0011 - val_accuracy: 0.9368\n",
            "Epoch 847/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 9.0626e-05 - accuracy: 0.9951 - val_loss: 0.0011 - val_accuracy: 0.9382\n",
            "Epoch 848/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.5396e-05 - accuracy: 0.9959 - val_loss: 0.0012 - val_accuracy: 0.9320\n",
            "Epoch 849/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.6914e-05 - accuracy: 0.9958 - val_loss: 0.0011 - val_accuracy: 0.9372\n",
            "Epoch 850/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.4929e-05 - accuracy: 0.9959 - val_loss: 0.0011 - val_accuracy: 0.9393\n",
            "Epoch 851/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.1384e-05 - accuracy: 0.9962 - val_loss: 0.0011 - val_accuracy: 0.9398\n",
            "Epoch 852/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.0854e-05 - accuracy: 0.9962 - val_loss: 0.0012 - val_accuracy: 0.9325\n",
            "Epoch 853/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.4555e-05 - accuracy: 0.9961 - val_loss: 0.0011 - val_accuracy: 0.9410\n",
            "Epoch 854/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.3015e-05 - accuracy: 0.9961 - val_loss: 0.0011 - val_accuracy: 0.9368\n",
            "Epoch 855/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.4734e-05 - accuracy: 0.9959 - val_loss: 0.0010 - val_accuracy: 0.9413\n",
            "Epoch 856/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 8.0279e-05 - accuracy: 0.9957 - val_loss: 0.0012 - val_accuracy: 0.9337\n",
            "Epoch 857/1000\n",
            "24/24 [==============================] - 3s 137ms/step - loss: 9.1365e-05 - accuracy: 0.9950 - val_loss: 0.0011 - val_accuracy: 0.9388\n",
            "Epoch 858/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 8.7123e-05 - accuracy: 0.9953 - val_loss: 0.0012 - val_accuracy: 0.9303\n",
            "Epoch 859/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 8.9373e-05 - accuracy: 0.9949 - val_loss: 0.0011 - val_accuracy: 0.9355\n",
            "Epoch 860/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 8.5357e-05 - accuracy: 0.9953 - val_loss: 0.0011 - val_accuracy: 0.9367\n",
            "Epoch 861/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 9.0654e-05 - accuracy: 0.9950 - val_loss: 0.0012 - val_accuracy: 0.9317\n",
            "Epoch 862/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 8.7959e-05 - accuracy: 0.9952 - val_loss: 0.0012 - val_accuracy: 0.9310\n",
            "Epoch 863/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 8.2923e-05 - accuracy: 0.9955 - val_loss: 0.0012 - val_accuracy: 0.9313\n",
            "Epoch 864/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.5911e-05 - accuracy: 0.9960 - val_loss: 0.0012 - val_accuracy: 0.9322\n",
            "Epoch 865/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.9869e-05 - accuracy: 0.9955 - val_loss: 0.0011 - val_accuracy: 0.9347\n",
            "Epoch 866/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 1.3145e-04 - accuracy: 0.9925 - val_loss: 0.0013 - val_accuracy: 0.9272\n",
            "Epoch 867/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 1.0557e-04 - accuracy: 0.9940 - val_loss: 0.0014 - val_accuracy: 0.9240\n",
            "Epoch 868/1000\n",
            "24/24 [==============================] - 3s 137ms/step - loss: 8.9527e-05 - accuracy: 0.9950 - val_loss: 0.0012 - val_accuracy: 0.9320\n",
            "Epoch 869/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 8.2463e-05 - accuracy: 0.9956 - val_loss: 0.0012 - val_accuracy: 0.9308\n",
            "Epoch 870/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.6301e-05 - accuracy: 0.9959 - val_loss: 0.0012 - val_accuracy: 0.9313\n",
            "Epoch 871/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 8.1074e-05 - accuracy: 0.9955 - val_loss: 0.0013 - val_accuracy: 0.9295\n",
            "Epoch 872/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 9.3532e-05 - accuracy: 0.9949 - val_loss: 0.0014 - val_accuracy: 0.9207\n",
            "Epoch 873/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 9.1008e-05 - accuracy: 0.9950 - val_loss: 0.0012 - val_accuracy: 0.9323\n",
            "Epoch 874/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 8.1306e-05 - accuracy: 0.9956 - val_loss: 0.0012 - val_accuracy: 0.9325\n",
            "Epoch 875/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.8058e-05 - accuracy: 0.9959 - val_loss: 0.0012 - val_accuracy: 0.9298\n",
            "Epoch 876/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.6523e-05 - accuracy: 0.9959 - val_loss: 0.0011 - val_accuracy: 0.9387\n",
            "Epoch 877/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.2191e-05 - accuracy: 0.9962 - val_loss: 9.8695e-04 - val_accuracy: 0.9445\n",
            "Epoch 878/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.3223e-05 - accuracy: 0.9962 - val_loss: 0.0012 - val_accuracy: 0.9357\n",
            "Epoch 879/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.0709e-05 - accuracy: 0.9963 - val_loss: 0.0011 - val_accuracy: 0.9392\n",
            "Epoch 880/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.0829e-05 - accuracy: 0.9963 - val_loss: 0.0011 - val_accuracy: 0.9405\n",
            "Epoch 881/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 6.9578e-05 - accuracy: 0.9964 - val_loss: 0.0011 - val_accuracy: 0.9403\n",
            "Epoch 882/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.1199e-05 - accuracy: 0.9964 - val_loss: 0.0011 - val_accuracy: 0.9413\n",
            "Epoch 883/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.1563e-05 - accuracy: 0.9963 - val_loss: 0.0010 - val_accuracy: 0.9427\n",
            "Epoch 884/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.2128e-05 - accuracy: 0.9962 - val_loss: 0.0011 - val_accuracy: 0.9392\n",
            "Epoch 885/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.4859e-05 - accuracy: 0.9961 - val_loss: 0.0012 - val_accuracy: 0.9355\n",
            "Epoch 886/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.3145e-05 - accuracy: 0.9961 - val_loss: 0.0010 - val_accuracy: 0.9418\n",
            "Epoch 887/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.1773e-05 - accuracy: 0.9962 - val_loss: 0.0013 - val_accuracy: 0.9280\n",
            "Epoch 888/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.4950e-05 - accuracy: 0.9961 - val_loss: 0.0010 - val_accuracy: 0.9427\n",
            "Epoch 889/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 7.4388e-05 - accuracy: 0.9960 - val_loss: 0.0012 - val_accuracy: 0.9325\n",
            "Epoch 890/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 8.7184e-05 - accuracy: 0.9952 - val_loss: 0.0011 - val_accuracy: 0.9393\n",
            "Epoch 891/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.9227e-05 - accuracy: 0.9958 - val_loss: 0.0011 - val_accuracy: 0.9352\n",
            "Epoch 892/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 8.7185e-05 - accuracy: 0.9952 - val_loss: 0.0012 - val_accuracy: 0.9340\n",
            "Epoch 893/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 8.5682e-05 - accuracy: 0.9953 - val_loss: 0.0011 - val_accuracy: 0.9387\n",
            "Epoch 894/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 9.2678e-05 - accuracy: 0.9948 - val_loss: 0.0012 - val_accuracy: 0.9330\n",
            "Epoch 895/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 8.1069e-05 - accuracy: 0.9955 - val_loss: 0.0011 - val_accuracy: 0.9390\n",
            "Epoch 896/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 7.8090e-05 - accuracy: 0.9959 - val_loss: 0.0011 - val_accuracy: 0.9405\n",
            "Epoch 897/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 7.5191e-05 - accuracy: 0.9961 - val_loss: 0.0010 - val_accuracy: 0.9413\n",
            "Epoch 898/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 8.4789e-05 - accuracy: 0.9954 - val_loss: 0.0012 - val_accuracy: 0.9325\n",
            "Epoch 899/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 8.1679e-05 - accuracy: 0.9953 - val_loss: 0.0012 - val_accuracy: 0.9317\n",
            "Epoch 900/1000\n",
            "24/24 [==============================] - 3s 135ms/step - loss: 9.0299e-05 - accuracy: 0.9951 - val_loss: 0.0011 - val_accuracy: 0.9388\n",
            "Epoch 901/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 8.2283e-05 - accuracy: 0.9955 - val_loss: 0.0011 - val_accuracy: 0.9378\n",
            "Epoch 902/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.9571e-05 - accuracy: 0.9957 - val_loss: 0.0012 - val_accuracy: 0.9352\n",
            "Epoch 903/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.4610e-05 - accuracy: 0.9961 - val_loss: 0.0011 - val_accuracy: 0.9392\n",
            "Epoch 904/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.6169e-05 - accuracy: 0.9959 - val_loss: 0.0011 - val_accuracy: 0.9360\n",
            "Epoch 905/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 1.0847e-04 - accuracy: 0.9941 - val_loss: 0.0012 - val_accuracy: 0.9345\n",
            "Epoch 906/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 1.0386e-04 - accuracy: 0.9942 - val_loss: 0.0011 - val_accuracy: 0.9385\n",
            "Epoch 907/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 8.1142e-05 - accuracy: 0.9956 - val_loss: 0.0010 - val_accuracy: 0.9407\n",
            "Epoch 908/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 8.2385e-05 - accuracy: 0.9954 - val_loss: 0.0011 - val_accuracy: 0.9378\n",
            "Epoch 909/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.6920e-05 - accuracy: 0.9959 - val_loss: 0.0010 - val_accuracy: 0.9417\n",
            "Epoch 910/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.4488e-05 - accuracy: 0.9961 - val_loss: 0.0010 - val_accuracy: 0.9402\n",
            "Epoch 911/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.6052e-05 - accuracy: 0.9960 - val_loss: 0.0012 - val_accuracy: 0.9288\n",
            "Epoch 912/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 8.5414e-05 - accuracy: 0.9953 - val_loss: 0.0012 - val_accuracy: 0.9340\n",
            "Epoch 913/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 8.7514e-05 - accuracy: 0.9953 - val_loss: 0.0011 - val_accuracy: 0.9373\n",
            "Epoch 914/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.6010e-05 - accuracy: 0.9960 - val_loss: 0.0012 - val_accuracy: 0.9325\n",
            "Epoch 915/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.2366e-05 - accuracy: 0.9963 - val_loss: 9.9888e-04 - val_accuracy: 0.9435\n",
            "Epoch 916/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.3365e-05 - accuracy: 0.9960 - val_loss: 0.0011 - val_accuracy: 0.9362\n",
            "Epoch 917/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.1002e-05 - accuracy: 0.9961 - val_loss: 9.8143e-04 - val_accuracy: 0.9432\n",
            "Epoch 918/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.6328e-05 - accuracy: 0.9959 - val_loss: 9.9041e-04 - val_accuracy: 0.9442\n",
            "Epoch 919/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.7144e-05 - accuracy: 0.9960 - val_loss: 0.0011 - val_accuracy: 0.9377\n",
            "Epoch 920/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.3065e-05 - accuracy: 0.9961 - val_loss: 0.0010 - val_accuracy: 0.9423\n",
            "Epoch 921/1000\n",
            "24/24 [==============================] - 3s 133ms/step - loss: 7.3210e-05 - accuracy: 0.9962 - val_loss: 0.0011 - val_accuracy: 0.9373\n",
            "Epoch 922/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.2470e-05 - accuracy: 0.9961 - val_loss: 0.0011 - val_accuracy: 0.9383\n",
            "Epoch 923/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.0056e-05 - accuracy: 0.9963 - val_loss: 0.0011 - val_accuracy: 0.9365\n",
            "Epoch 924/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.4956e-05 - accuracy: 0.9960 - val_loss: 0.0011 - val_accuracy: 0.9390\n",
            "Epoch 925/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 1.0801e-04 - accuracy: 0.9941 - val_loss: 0.0011 - val_accuracy: 0.9363\n",
            "Epoch 926/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 8.7808e-05 - accuracy: 0.9952 - val_loss: 0.0012 - val_accuracy: 0.9327\n",
            "Epoch 927/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 9.9633e-05 - accuracy: 0.9944 - val_loss: 0.0011 - val_accuracy: 0.9352\n",
            "Epoch 928/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 1.2046e-04 - accuracy: 0.9933 - val_loss: 0.0013 - val_accuracy: 0.9292\n",
            "Epoch 929/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 1.4804e-04 - accuracy: 0.9916 - val_loss: 0.0014 - val_accuracy: 0.9220\n",
            "Epoch 930/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 1.1058e-04 - accuracy: 0.9937 - val_loss: 0.0012 - val_accuracy: 0.9325\n",
            "Epoch 931/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 9.7620e-05 - accuracy: 0.9945 - val_loss: 0.0013 - val_accuracy: 0.9257\n",
            "Epoch 932/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 9.6858e-05 - accuracy: 0.9947 - val_loss: 0.0011 - val_accuracy: 0.9367\n",
            "Epoch 933/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 1.2409e-04 - accuracy: 0.9929 - val_loss: 0.0013 - val_accuracy: 0.9292\n",
            "Epoch 934/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 1.0013e-04 - accuracy: 0.9945 - val_loss: 0.0012 - val_accuracy: 0.9303\n",
            "Epoch 935/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 8.8807e-05 - accuracy: 0.9952 - val_loss: 0.0011 - val_accuracy: 0.9348\n",
            "Epoch 936/1000\n",
            "24/24 [==============================] - 3s 137ms/step - loss: 8.2643e-05 - accuracy: 0.9955 - val_loss: 0.0012 - val_accuracy: 0.9335\n",
            "Epoch 937/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 8.9263e-05 - accuracy: 0.9951 - val_loss: 0.0012 - val_accuracy: 0.9328\n",
            "Epoch 938/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 8.7596e-05 - accuracy: 0.9953 - val_loss: 0.0012 - val_accuracy: 0.9323\n",
            "Epoch 939/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 8.1776e-05 - accuracy: 0.9956 - val_loss: 0.0012 - val_accuracy: 0.9312\n",
            "Epoch 940/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.5409e-05 - accuracy: 0.9958 - val_loss: 0.0012 - val_accuracy: 0.9337\n",
            "Epoch 941/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.5014e-05 - accuracy: 0.9960 - val_loss: 0.0012 - val_accuracy: 0.9352\n",
            "Epoch 942/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.6529e-05 - accuracy: 0.9958 - val_loss: 0.0012 - val_accuracy: 0.9345\n",
            "Epoch 943/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.8799e-05 - accuracy: 0.9958 - val_loss: 0.0012 - val_accuracy: 0.9347\n",
            "Epoch 944/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.2124e-05 - accuracy: 0.9961 - val_loss: 0.0011 - val_accuracy: 0.9392\n",
            "Epoch 945/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.0874e-05 - accuracy: 0.9962 - val_loss: 0.0011 - val_accuracy: 0.9380\n",
            "Epoch 946/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.2372e-05 - accuracy: 0.9962 - val_loss: 0.0010 - val_accuracy: 0.9413\n",
            "Epoch 947/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 6.8983e-05 - accuracy: 0.9965 - val_loss: 0.0011 - val_accuracy: 0.9387\n",
            "Epoch 948/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 6.9042e-05 - accuracy: 0.9964 - val_loss: 0.0010 - val_accuracy: 0.9442\n",
            "Epoch 949/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.5875e-05 - accuracy: 0.9960 - val_loss: 0.0011 - val_accuracy: 0.9367\n",
            "Epoch 950/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.6767e-05 - accuracy: 0.9959 - val_loss: 0.0011 - val_accuracy: 0.9393\n",
            "Epoch 951/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.5683e-05 - accuracy: 0.9960 - val_loss: 0.0011 - val_accuracy: 0.9387\n",
            "Epoch 952/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.3071e-05 - accuracy: 0.9962 - val_loss: 0.0011 - val_accuracy: 0.9388\n",
            "Epoch 953/1000\n",
            "24/24 [==============================] - 3s 136ms/step - loss: 7.2572e-05 - accuracy: 0.9962 - val_loss: 9.8368e-04 - val_accuracy: 0.9443\n",
            "Epoch 954/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.2482e-05 - accuracy: 0.9962 - val_loss: 0.0012 - val_accuracy: 0.9325\n",
            "Epoch 955/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.2836e-05 - accuracy: 0.9962 - val_loss: 0.0013 - val_accuracy: 0.9295\n",
            "Epoch 956/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.8184e-05 - accuracy: 0.9958 - val_loss: 0.0013 - val_accuracy: 0.9302\n",
            "Epoch 957/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.2239e-05 - accuracy: 0.9962 - val_loss: 0.0011 - val_accuracy: 0.9378\n",
            "Epoch 958/1000\n",
            "24/24 [==============================] - 3s 133ms/step - loss: 7.3911e-05 - accuracy: 0.9962 - val_loss: 0.0011 - val_accuracy: 0.9370\n",
            "Epoch 959/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.9915e-05 - accuracy: 0.9958 - val_loss: 0.0012 - val_accuracy: 0.9330\n",
            "Epoch 960/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.2896e-05 - accuracy: 0.9962 - val_loss: 0.0010 - val_accuracy: 0.9412\n",
            "Epoch 961/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.3681e-05 - accuracy: 0.9962 - val_loss: 0.0010 - val_accuracy: 0.9420\n",
            "Epoch 962/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.1824e-05 - accuracy: 0.9962 - val_loss: 0.0012 - val_accuracy: 0.9352\n",
            "Epoch 963/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.6987e-05 - accuracy: 0.9960 - val_loss: 0.0011 - val_accuracy: 0.9370\n",
            "Epoch 964/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.2176e-05 - accuracy: 0.9962 - val_loss: 0.0011 - val_accuracy: 0.9375\n",
            "Epoch 965/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.5556e-05 - accuracy: 0.9960 - val_loss: 0.0011 - val_accuracy: 0.9385\n",
            "Epoch 966/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.0077e-05 - accuracy: 0.9963 - val_loss: 0.0011 - val_accuracy: 0.9367\n",
            "Epoch 967/1000\n",
            "24/24 [==============================] - 3s 133ms/step - loss: 7.9825e-05 - accuracy: 0.9957 - val_loss: 0.0012 - val_accuracy: 0.9342\n",
            "Epoch 968/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 9.6632e-05 - accuracy: 0.9946 - val_loss: 0.0013 - val_accuracy: 0.9280\n",
            "Epoch 969/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 8.5012e-05 - accuracy: 0.9952 - val_loss: 0.0012 - val_accuracy: 0.9347\n",
            "Epoch 970/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 8.4568e-05 - accuracy: 0.9954 - val_loss: 0.0011 - val_accuracy: 0.9392\n",
            "Epoch 971/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 9.0720e-05 - accuracy: 0.9951 - val_loss: 0.0012 - val_accuracy: 0.9347\n",
            "Epoch 972/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 1.0804e-04 - accuracy: 0.9940 - val_loss: 0.0012 - val_accuracy: 0.9358\n",
            "Epoch 973/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 9.6010e-05 - accuracy: 0.9947 - val_loss: 0.0011 - val_accuracy: 0.9360\n",
            "Epoch 974/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 8.4624e-05 - accuracy: 0.9953 - val_loss: 0.0012 - val_accuracy: 0.9342\n",
            "Epoch 975/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 8.1701e-05 - accuracy: 0.9955 - val_loss: 0.0011 - val_accuracy: 0.9398\n",
            "Epoch 976/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.4038e-05 - accuracy: 0.9960 - val_loss: 0.0012 - val_accuracy: 0.9358\n",
            "Epoch 977/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 8.7085e-05 - accuracy: 0.9953 - val_loss: 0.0011 - val_accuracy: 0.9358\n",
            "Epoch 978/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.9365e-05 - accuracy: 0.9958 - val_loss: 0.0012 - val_accuracy: 0.9317\n",
            "Epoch 979/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 9.3302e-05 - accuracy: 0.9949 - val_loss: 0.0015 - val_accuracy: 0.9165\n",
            "Epoch 980/1000\n",
            "24/24 [==============================] - 3s 133ms/step - loss: 9.6301e-05 - accuracy: 0.9948 - val_loss: 0.0012 - val_accuracy: 0.9337\n",
            "Epoch 981/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 8.3414e-05 - accuracy: 0.9954 - val_loss: 0.0011 - val_accuracy: 0.9375\n",
            "Epoch 982/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 1.2779e-04 - accuracy: 0.9930 - val_loss: 0.0017 - val_accuracy: 0.9035\n",
            "Epoch 983/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 1.1720e-04 - accuracy: 0.9934 - val_loss: 0.0013 - val_accuracy: 0.9267\n",
            "Epoch 984/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 1.8586e-04 - accuracy: 0.9892 - val_loss: 0.0013 - val_accuracy: 0.9280\n",
            "Epoch 985/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 1.2487e-04 - accuracy: 0.9929 - val_loss: 0.0013 - val_accuracy: 0.9312\n",
            "Epoch 986/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 9.7718e-05 - accuracy: 0.9945 - val_loss: 0.0013 - val_accuracy: 0.9290\n",
            "Epoch 987/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 8.5822e-05 - accuracy: 0.9952 - val_loss: 0.0013 - val_accuracy: 0.9305\n",
            "Epoch 988/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.9362e-05 - accuracy: 0.9957 - val_loss: 0.0012 - val_accuracy: 0.9337\n",
            "Epoch 989/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 8.1433e-05 - accuracy: 0.9957 - val_loss: 0.0013 - val_accuracy: 0.9292\n",
            "Epoch 990/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 8.2466e-05 - accuracy: 0.9955 - val_loss: 0.0012 - val_accuracy: 0.9310\n",
            "Epoch 991/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.2847e-05 - accuracy: 0.9961 - val_loss: 0.0012 - val_accuracy: 0.9342\n",
            "Epoch 992/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.0576e-05 - accuracy: 0.9962 - val_loss: 0.0011 - val_accuracy: 0.9352\n",
            "Epoch 993/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.3316e-05 - accuracy: 0.9961 - val_loss: 0.0012 - val_accuracy: 0.9348\n",
            "Epoch 994/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.1817e-05 - accuracy: 0.9962 - val_loss: 0.0011 - val_accuracy: 0.9370\n",
            "Epoch 995/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.0405e-05 - accuracy: 0.9962 - val_loss: 0.0012 - val_accuracy: 0.9312\n",
            "Epoch 996/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.3547e-05 - accuracy: 0.9962 - val_loss: 0.0011 - val_accuracy: 0.9353\n",
            "Epoch 997/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.3061e-05 - accuracy: 0.9962 - val_loss: 0.0011 - val_accuracy: 0.9408\n",
            "Epoch 998/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.0936e-05 - accuracy: 0.9963 - val_loss: 0.0011 - val_accuracy: 0.9405\n",
            "Epoch 999/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.1847e-05 - accuracy: 0.9963 - val_loss: 0.0011 - val_accuracy: 0.9392\n",
            "Epoch 1000/1000\n",
            "24/24 [==============================] - 3s 134ms/step - loss: 7.1100e-05 - accuracy: 0.9962 - val_loss: 0.0010 - val_accuracy: 0.9417\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "id": "HVvegAfYQjT2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98ffb756-8c69-42ff-d676-f4bf259145b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "188/188 [==============================] - 2s 10ms/step - loss: 9.6913e-04 - accuracy: 0.9452\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.0009691325831227005, 0.9451666474342346]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('bandpass_1s.h5')"
      ],
      "metadata": {
        "id": "5g_vRO0ZFYHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 그래프 분석"
      ],
      "metadata": {
        "id": "uLvm3XtMKYKg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history=history.history\n",
        "epochs = range(1, len(history['loss']) + 1)\n",
        "acc = history['accuracy']\n",
        "loss = history['loss']\n",
        "val_acc = history['val_accuracy']\n",
        "val_loss = history['val_loss']\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.plot(epochs, acc, label='accuracy')\n",
        "plt.plot(epochs, val_acc, label='val_acc')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.plot(epochs, loss, label='loss', color='g')\n",
        "plt.plot(epochs, val_loss, label='val_loss', color='r')\n",
        "plt.legend()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 701
        },
        "id": "7YRFsECl7Qp0",
        "outputId": "64da47ae-2422-4396-bd52-922d8e261cee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fc121f142e0>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 예측해보기(predict)"
      ],
      "metadata": {
        "id": "B7id_mnNKhHg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKnoTLlLBPPN",
        "outputId": "d3231455-b8db-4afc-cc5c-55aa67130b3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "188/188 [==============================] - 3s 10ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_p=pd.DataFrame(y_test)\n",
        "\n",
        "for i in range(1,101):\n",
        "  y_test_p.loc[y_test_p[i] == 1.0 , 'y_test'] = i  \n",
        "y_test_p.y_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8EWDyMdHpWR",
        "outputId": "dd093915-90df-47c6-e717-2b8d0434c7a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       75.0\n",
              "1       90.0\n",
              "2       67.0\n",
              "3       91.0\n",
              "4       84.0\n",
              "        ... \n",
              "5995    44.0\n",
              "5996    37.0\n",
              "5997    99.0\n",
              "5998    88.0\n",
              "5999    31.0\n",
              "Name: y_test, Length: 6000, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ex=np.argmax(y_pred, axis=1)\n",
        "ex=ex.astype('float64')\n",
        "ex=pd.DataFrame(ex)\n",
        "ex"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "lI3myxchcv-Q",
        "outputId": "f3d9e945-67dc-4d3a-c1e7-26f58fd8b53e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         0\n",
              "0     75.0\n",
              "1     90.0\n",
              "2     67.0\n",
              "3     91.0\n",
              "4     84.0\n",
              "...    ...\n",
              "5995  44.0\n",
              "5996  37.0\n",
              "5997  99.0\n",
              "5998  88.0\n",
              "5999  31.0\n",
              "\n",
              "[6000 rows x 1 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8bad303a-f373-476f-ad25-58f8a664d2ba\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>75.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>90.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>67.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>91.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>84.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5995</th>\n",
              "      <td>44.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5996</th>\n",
              "      <td>37.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5997</th>\n",
              "      <td>99.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5998</th>\n",
              "      <td>88.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5999</th>\n",
              "      <td>31.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6000 rows × 1 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8bad303a-f373-476f-ad25-58f8a664d2ba')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8bad303a-f373-476f-ad25-58f8a664d2ba button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8bad303a-f373-476f-ad25-58f8a664d2ba');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ch1 = pd.DataFrame(np.rint(y_pred))\n",
        "ch1['pred'] = ex\n",
        "data_test.reset_index(inplace=True, drop=True)\n",
        "predict = pd.concat([data_test,ch1['pred'], y_test_p['y_test']],axis = 1)"
      ],
      "metadata": {
        "id": "HSPb21Y4eMAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict['compare'] = np.where(predict['pred']==predict['y_test'],'yes', 'no')"
      ],
      "metadata": {
        "id": "yhe54j6XJzhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_1 = predict[predict[\"compare\"]=='no']\n",
        "predict_1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "mdgfuEehKIXX",
        "outputId": "35ded946-b41f-4202-d5d8-02489b84ca57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             0         1         2         3         4         5         6  \\\n",
              "8     0.034902  0.040244  0.046107  0.052067  0.057719  0.062743  0.066955   \n",
              "11    0.109717  0.110659  0.111964  0.114482  0.118950  0.125818  0.135116   \n",
              "26    0.014270  0.001147 -0.009664 -0.017843 -0.023431 -0.026793 -0.028535   \n",
              "27   -0.030546 -0.028344 -0.027399 -0.027423 -0.028062 -0.029064 -0.030394   \n",
              "35   -0.025196 -0.024448 -0.023728 -0.023056 -0.022438 -0.021864 -0.021319   \n",
              "...        ...       ...       ...       ...       ...       ...       ...   \n",
              "5904 -0.023422 -0.018496 -0.014607 -0.011922 -0.010307 -0.009338 -0.008411   \n",
              "5911  0.031127  0.030208  0.029620  0.029588  0.030140  0.031080  0.032028   \n",
              "5943  0.063536  0.071162  0.078900  0.086882  0.095225  0.103941  0.112881   \n",
              "5972 -0.043211 -0.041627 -0.039691 -0.037799 -0.036304 -0.035467 -0.035429   \n",
              "5986 -0.036023 -0.032761 -0.028856 -0.024080 -0.018298 -0.011573 -0.004257   \n",
              "\n",
              "             7         8         9  ...       249       250       251  \\\n",
              "8     0.070322  0.072960  0.075095  ...  0.085447  0.084625  0.083489   \n",
              "11    0.146405  0.158831  0.171282  ... -0.046100 -0.047511 -0.047318   \n",
              "26   -0.029376 -0.030009 -0.030962  ... -0.023292 -0.026665 -0.030282   \n",
              "27   -0.032242 -0.034932 -0.038768  ...  0.005832  0.002299 -0.006270   \n",
              "35   -0.020791 -0.020296 -0.019888  ...  0.045220  0.041943  0.036367   \n",
              "...        ...       ...       ...  ...       ...       ...       ...   \n",
              "5904 -0.006905 -0.004374 -0.000691  ... -0.045415 -0.049928 -0.055818   \n",
              "5911  0.032515  0.032121  0.030616  ... -0.026762 -0.027985 -0.028425   \n",
              "5943  0.121710  0.129939  0.136985  ... -0.053988 -0.052298 -0.050341   \n",
              "5972 -0.036219 -0.037772 -0.039970  ... -0.225394 -0.183337 -0.113905   \n",
              "5986  0.002999  0.009299  0.013657  ... -0.048809 -0.053204 -0.062194   \n",
              "\n",
              "           252       253       254       255  pred  y_test  compare  \n",
              "8     0.081097  0.076492  0.069015  0.058567  18.0     2.0       no  \n",
              "11   -0.046095 -0.044528 -0.043274 -0.042820   2.0    92.0       no  \n",
              "26   -0.033635 -0.035797 -0.035559 -0.031681  10.0     8.0       no  \n",
              "27   -0.020143 -0.038324 -0.058224 -0.075590  66.0    47.0       no  \n",
              "35    0.029059  0.020812  0.012487  0.004854  96.0     8.0       no  \n",
              "...        ...       ...       ...       ...   ...     ...      ...  \n",
              "5904 -0.062579 -0.069229 -0.074560 -0.077474  18.0    94.0       no  \n",
              "5911 -0.028331 -0.027923 -0.027419 -0.027036  95.0    49.0       no  \n",
              "5943 -0.048330 -0.046442 -0.044802 -0.043505  29.0    42.0       no  \n",
              "5972 -0.020055  0.090091  0.203977  0.306470  39.0    30.0       no  \n",
              "5986 -0.074844 -0.088793 -0.100251 -0.104371  22.0    12.0       no  \n",
              "\n",
              "[432 rows x 259 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b2b6d5b4-0b70-466c-8aa3-8305d2eb7bcc\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>249</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "      <th>pred</th>\n",
              "      <th>y_test</th>\n",
              "      <th>compare</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.034902</td>\n",
              "      <td>0.040244</td>\n",
              "      <td>0.046107</td>\n",
              "      <td>0.052067</td>\n",
              "      <td>0.057719</td>\n",
              "      <td>0.062743</td>\n",
              "      <td>0.066955</td>\n",
              "      <td>0.070322</td>\n",
              "      <td>0.072960</td>\n",
              "      <td>0.075095</td>\n",
              "      <td>...</td>\n",
              "      <td>0.085447</td>\n",
              "      <td>0.084625</td>\n",
              "      <td>0.083489</td>\n",
              "      <td>0.081097</td>\n",
              "      <td>0.076492</td>\n",
              "      <td>0.069015</td>\n",
              "      <td>0.058567</td>\n",
              "      <td>18.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.109717</td>\n",
              "      <td>0.110659</td>\n",
              "      <td>0.111964</td>\n",
              "      <td>0.114482</td>\n",
              "      <td>0.118950</td>\n",
              "      <td>0.125818</td>\n",
              "      <td>0.135116</td>\n",
              "      <td>0.146405</td>\n",
              "      <td>0.158831</td>\n",
              "      <td>0.171282</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.046100</td>\n",
              "      <td>-0.047511</td>\n",
              "      <td>-0.047318</td>\n",
              "      <td>-0.046095</td>\n",
              "      <td>-0.044528</td>\n",
              "      <td>-0.043274</td>\n",
              "      <td>-0.042820</td>\n",
              "      <td>2.0</td>\n",
              "      <td>92.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.014270</td>\n",
              "      <td>0.001147</td>\n",
              "      <td>-0.009664</td>\n",
              "      <td>-0.017843</td>\n",
              "      <td>-0.023431</td>\n",
              "      <td>-0.026793</td>\n",
              "      <td>-0.028535</td>\n",
              "      <td>-0.029376</td>\n",
              "      <td>-0.030009</td>\n",
              "      <td>-0.030962</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.023292</td>\n",
              "      <td>-0.026665</td>\n",
              "      <td>-0.030282</td>\n",
              "      <td>-0.033635</td>\n",
              "      <td>-0.035797</td>\n",
              "      <td>-0.035559</td>\n",
              "      <td>-0.031681</td>\n",
              "      <td>10.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>-0.030546</td>\n",
              "      <td>-0.028344</td>\n",
              "      <td>-0.027399</td>\n",
              "      <td>-0.027423</td>\n",
              "      <td>-0.028062</td>\n",
              "      <td>-0.029064</td>\n",
              "      <td>-0.030394</td>\n",
              "      <td>-0.032242</td>\n",
              "      <td>-0.034932</td>\n",
              "      <td>-0.038768</td>\n",
              "      <td>...</td>\n",
              "      <td>0.005832</td>\n",
              "      <td>0.002299</td>\n",
              "      <td>-0.006270</td>\n",
              "      <td>-0.020143</td>\n",
              "      <td>-0.038324</td>\n",
              "      <td>-0.058224</td>\n",
              "      <td>-0.075590</td>\n",
              "      <td>66.0</td>\n",
              "      <td>47.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>-0.025196</td>\n",
              "      <td>-0.024448</td>\n",
              "      <td>-0.023728</td>\n",
              "      <td>-0.023056</td>\n",
              "      <td>-0.022438</td>\n",
              "      <td>-0.021864</td>\n",
              "      <td>-0.021319</td>\n",
              "      <td>-0.020791</td>\n",
              "      <td>-0.020296</td>\n",
              "      <td>-0.019888</td>\n",
              "      <td>...</td>\n",
              "      <td>0.045220</td>\n",
              "      <td>0.041943</td>\n",
              "      <td>0.036367</td>\n",
              "      <td>0.029059</td>\n",
              "      <td>0.020812</td>\n",
              "      <td>0.012487</td>\n",
              "      <td>0.004854</td>\n",
              "      <td>96.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5904</th>\n",
              "      <td>-0.023422</td>\n",
              "      <td>-0.018496</td>\n",
              "      <td>-0.014607</td>\n",
              "      <td>-0.011922</td>\n",
              "      <td>-0.010307</td>\n",
              "      <td>-0.009338</td>\n",
              "      <td>-0.008411</td>\n",
              "      <td>-0.006905</td>\n",
              "      <td>-0.004374</td>\n",
              "      <td>-0.000691</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.045415</td>\n",
              "      <td>-0.049928</td>\n",
              "      <td>-0.055818</td>\n",
              "      <td>-0.062579</td>\n",
              "      <td>-0.069229</td>\n",
              "      <td>-0.074560</td>\n",
              "      <td>-0.077474</td>\n",
              "      <td>18.0</td>\n",
              "      <td>94.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5911</th>\n",
              "      <td>0.031127</td>\n",
              "      <td>0.030208</td>\n",
              "      <td>0.029620</td>\n",
              "      <td>0.029588</td>\n",
              "      <td>0.030140</td>\n",
              "      <td>0.031080</td>\n",
              "      <td>0.032028</td>\n",
              "      <td>0.032515</td>\n",
              "      <td>0.032121</td>\n",
              "      <td>0.030616</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.026762</td>\n",
              "      <td>-0.027985</td>\n",
              "      <td>-0.028425</td>\n",
              "      <td>-0.028331</td>\n",
              "      <td>-0.027923</td>\n",
              "      <td>-0.027419</td>\n",
              "      <td>-0.027036</td>\n",
              "      <td>95.0</td>\n",
              "      <td>49.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5943</th>\n",
              "      <td>0.063536</td>\n",
              "      <td>0.071162</td>\n",
              "      <td>0.078900</td>\n",
              "      <td>0.086882</td>\n",
              "      <td>0.095225</td>\n",
              "      <td>0.103941</td>\n",
              "      <td>0.112881</td>\n",
              "      <td>0.121710</td>\n",
              "      <td>0.129939</td>\n",
              "      <td>0.136985</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.053988</td>\n",
              "      <td>-0.052298</td>\n",
              "      <td>-0.050341</td>\n",
              "      <td>-0.048330</td>\n",
              "      <td>-0.046442</td>\n",
              "      <td>-0.044802</td>\n",
              "      <td>-0.043505</td>\n",
              "      <td>29.0</td>\n",
              "      <td>42.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5972</th>\n",
              "      <td>-0.043211</td>\n",
              "      <td>-0.041627</td>\n",
              "      <td>-0.039691</td>\n",
              "      <td>-0.037799</td>\n",
              "      <td>-0.036304</td>\n",
              "      <td>-0.035467</td>\n",
              "      <td>-0.035429</td>\n",
              "      <td>-0.036219</td>\n",
              "      <td>-0.037772</td>\n",
              "      <td>-0.039970</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.225394</td>\n",
              "      <td>-0.183337</td>\n",
              "      <td>-0.113905</td>\n",
              "      <td>-0.020055</td>\n",
              "      <td>0.090091</td>\n",
              "      <td>0.203977</td>\n",
              "      <td>0.306470</td>\n",
              "      <td>39.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5986</th>\n",
              "      <td>-0.036023</td>\n",
              "      <td>-0.032761</td>\n",
              "      <td>-0.028856</td>\n",
              "      <td>-0.024080</td>\n",
              "      <td>-0.018298</td>\n",
              "      <td>-0.011573</td>\n",
              "      <td>-0.004257</td>\n",
              "      <td>0.002999</td>\n",
              "      <td>0.009299</td>\n",
              "      <td>0.013657</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.048809</td>\n",
              "      <td>-0.053204</td>\n",
              "      <td>-0.062194</td>\n",
              "      <td>-0.074844</td>\n",
              "      <td>-0.088793</td>\n",
              "      <td>-0.100251</td>\n",
              "      <td>-0.104371</td>\n",
              "      <td>22.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>432 rows × 259 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b2b6d5b4-0b70-466c-8aa3-8305d2eb7bcc')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b2b6d5b4-0b70-466c-8aa3-8305d2eb7bcc button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b2b6d5b4-0b70-466c-8aa3-8305d2eb7bcc');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 불러오기"
      ],
      "metadata": {
        "id": "i23UaEsLj8l9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "band_model=load_model('/content/drive/MyDrive/bandpass_1s.h5')\n",
        "band_model.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDiBfj9Sj-_L",
        "outputId": "23fabeb7-7f44-4be8-9ea1-72ec7acf546f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "188/188 [==============================] - 3s 11ms/step - loss: 9.6913e-04 - accuracy: 0.9452\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.0009691325831227005, 0.9451666474342346]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = band_model.predict(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CU5WHkbRlPL4",
        "outputId": "1a0178a6-ca2f-41b9-8cae-73c9988fb271"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "188/188 [==============================] - 3s 10ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_p=pd.DataFrame(y_test)\n",
        "\n",
        "for i in range(1,101):\n",
        "  y_test_p.loc[y_test_p[i] == 1.0 , 'y_test'] = i  \n",
        "y_test_p.y_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJBmbMsDl3S3",
        "outputId": "3d64a189-d98b-418c-d6a6-e9aa44b24e41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       42.0\n",
              "1       52.0\n",
              "2       99.0\n",
              "3       25.0\n",
              "4       20.0\n",
              "        ... \n",
              "5995    45.0\n",
              "5996    20.0\n",
              "5997    56.0\n",
              "5998     9.0\n",
              "5999    41.0\n",
              "Name: y_test, Length: 6000, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ex=np.argmax(y_pred, axis=1)\n",
        "ex=ex.astype('float64')\n",
        "ex=pd.DataFrame(ex)\n",
        "ex[1]=y_test_p.y_test"
      ],
      "metadata": {
        "id": "e2Lkh7Lwlp7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ex.columns=['pred', 'y_test']"
      ],
      "metadata": {
        "id": "HtuU7pX5mfKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ex"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "H-EvwhEBmIhm",
        "outputId": "60d0951c-6186-491f-fc51-1fb960f42181"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      pred  y_test\n",
              "0     42.0    42.0\n",
              "1     52.0    52.0\n",
              "2     99.0    99.0\n",
              "3     25.0    25.0\n",
              "4     20.0    20.0\n",
              "...    ...     ...\n",
              "5995  38.0    45.0\n",
              "5996  20.0    20.0\n",
              "5997  56.0    56.0\n",
              "5998   9.0     9.0\n",
              "5999  41.0    41.0\n",
              "\n",
              "[6000 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-dcd8f53b-50f7-4a12-9e82-ff368749f655\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pred</th>\n",
              "      <th>y_test</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>42.0</td>\n",
              "      <td>42.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>52.0</td>\n",
              "      <td>52.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>99.0</td>\n",
              "      <td>99.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>25.0</td>\n",
              "      <td>25.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5995</th>\n",
              "      <td>38.0</td>\n",
              "      <td>45.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5996</th>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5997</th>\n",
              "      <td>56.0</td>\n",
              "      <td>56.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5998</th>\n",
              "      <td>9.0</td>\n",
              "      <td>9.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5999</th>\n",
              "      <td>41.0</td>\n",
              "      <td>41.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6000 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dcd8f53b-50f7-4a12-9e82-ff368749f655')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-dcd8f53b-50f7-4a12-9e82-ff368749f655 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-dcd8f53b-50f7-4a12-9e82-ff368749f655');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.highpass"
      ],
      "metadata": {
        "id": "3SxPZjD4u6eC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46bf1084-0e1c-4a75-faae-05630c1f105b",
        "id": "sT99AEGYu9a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip '/content/drive/MyDrive/model_line.zip' -d '/content/drive/MyDrive/model_line'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c6ba509-4e6d-42b3-8562-1567bcc35d70",
        "id": "JUiDqyLzu9a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/model_line.zip\n",
            "   creating: /content/drive/MyDrive/model_line/model_line/\n",
            "   creating: /content/drive/MyDrive/model_line/model_line/band/\n",
            "  inflating: /content/drive/MyDrive/model_line/model_line/band/test_band_line.csv  \n",
            "  inflating: /content/drive/MyDrive/model_line/model_line/band/train_band_line.csv  \n",
            "  inflating: /content/drive/MyDrive/model_line/model_line/band/valid_band_line.csv  \n",
            "   creating: /content/drive/MyDrive/model_line/model_line/high/\n",
            "  inflating: /content/drive/MyDrive/model_line/model_line/high/test_high_line.csv  \n",
            "  inflating: /content/drive/MyDrive/model_line/model_line/high/train_high_line.csv  \n",
            "  inflating: /content/drive/MyDrive/model_line/model_line/high/valid_high_line.csv  \n",
            "   creating: /content/drive/MyDrive/model_line/model_line/low/\n",
            "  inflating: /content/drive/MyDrive/model_line/model_line/low/test_low_line.csv  \n",
            "  inflating: /content/drive/MyDrive/model_line/model_line/low/train_low_line.csv  \n",
            "  inflating: /content/drive/MyDrive/model_line/model_line/low/valid_low_line.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqmjel0bu9a2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import itertools\n",
        "import time\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import (CosineAnnealingLR,\n",
        "                                      CosineAnnealingWarmRestarts,\n",
        "                                      StepLR,\n",
        "                                      ExponentialLR)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import accuracy_score, auc, f1_score, precision_score, recall_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm, re, sys\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "from keras.layers import Conv2D, Conv1D, MaxPooling2D, MaxPooling1D, Flatten, BatchNormalization, Dense\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.callbacks import CSVLogger, ModelCheckpoint"
      ],
      "metadata": {
        "id": "hMywic9Fu9a2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train = pd.read_csv('/content/drive/MyDrive/model_line/model_line/high/train_high_line.csv', header=None)\n",
        "data_test = pd.read_csv('/content/drive/MyDrive/model_line/model_line/high/test_high_line.csv', header=None)\n",
        "data_val= pd.read_csv('/content/drive/MyDrive/model_line/model_line/high/valid_high_line.csv', header=None)"
      ],
      "metadata": {
        "id": "UOBpnPrqu9a2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "outputId": "78c13791-5b09-4e5d-84b9-65a45a2ced17",
        "id": "JVpsDpVDu9a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           0         1         2         3         4         5         6    \\\n",
              "0     0.004293 -0.108721 -0.025203  0.050075  0.041975 -0.079525 -0.019323   \n",
              "1    -0.178025 -0.129949 -0.154714 -0.190879 -0.147544 -0.149849 -0.139385   \n",
              "2    -0.118486 -0.135978 -0.021999  0.014580 -0.094641 -0.135181 -0.068231   \n",
              "3     0.595354  0.768205  0.751377  0.729010  0.837712  0.799024  0.553267   \n",
              "4    -0.107180 -0.029394 -0.003439 -0.114145 -0.110911 -0.047808  0.014365   \n",
              "...        ...       ...       ...       ...       ...       ...       ...   \n",
              "5995 -0.006009  0.035463  0.031765  0.034116  0.041276  0.040276  0.043814   \n",
              "5996 -0.004209 -0.021269  0.014421  0.066451  0.034102  0.022313  0.039954   \n",
              "5997 -0.052598 -0.055633 -0.055577 -0.032002 -0.095826 -0.056251 -0.088205   \n",
              "5998 -0.039242 -0.044322 -0.024571 -0.017190 -0.044918 -0.038277 -0.066724   \n",
              "5999 -0.001325 -0.136461 -0.178526 -0.261551 -0.222405 -0.167719 -0.153012   \n",
              "\n",
              "           7         8         9    ...       246       247       248  \\\n",
              "0     0.040809  0.003013 -0.044322  ...  0.025460  0.090296 -0.007490   \n",
              "1    -0.184831 -0.160077 -0.125704  ... -0.158946 -0.192521 -0.104194   \n",
              "2     0.024199 -0.080811 -0.151051  ... -0.012902 -0.057865 -0.169278   \n",
              "3     0.214589  0.034092  0.025435  ... -0.060304 -0.051033  0.071048   \n",
              "4    -0.074943 -0.107671 -0.060639  ... -0.104624 -0.173071 -0.158586   \n",
              "...        ...       ...       ...  ...       ...       ...       ...   \n",
              "5995  0.049392  0.014919  0.082215  ...  0.032174  0.025207  0.017123   \n",
              "5996  0.015126  0.016828  0.095201  ... -0.002060 -0.007017 -0.001635   \n",
              "5997 -0.003379 -0.041393 -0.062797  ... -0.038018 -0.060236 -0.027453   \n",
              "5998 -0.033992 -0.049339 -0.018386  ... -0.089395 -0.019348 -0.078060   \n",
              "5999 -0.143434 -0.108636 -0.056848  ...  0.127477  0.115293  0.154681   \n",
              "\n",
              "           249       250       251       252       253       254       255  \n",
              "0    -0.026310  0.053336  0.110380  0.044460 -0.027323  0.009700  0.105980  \n",
              "1    -0.074016 -0.121687 -0.121997 -0.143646 -0.113273 -0.137490 -0.137856  \n",
              "2    -0.139103 -0.051629  0.003184 -0.126743 -0.173762 -0.090042 -0.026942  \n",
              "3     0.039479 -0.059981 -0.111832  0.002407  0.031495 -0.068237 -0.108500  \n",
              "4    -0.065081 -0.066074 -0.202336 -0.198327 -0.087227 -0.061546 -0.158424  \n",
              "...        ...       ...       ...       ...       ...       ...       ...  \n",
              "5995 -0.047797 -0.036055 -0.080359 -0.080849 -0.131536 -0.108450 -0.119801  \n",
              "5996  0.073427  0.119448  0.241599  0.262261  0.279691  0.237252  0.092732  \n",
              "5997 -0.048409 -0.078653 -0.040536 -0.018418 -0.062418 -0.006057  0.010865  \n",
              "5998 -0.082112 -0.081604 -0.070175 -0.080676 -0.072666  0.004694 -0.046835  \n",
              "5999  0.142378  0.226287  0.199556  0.225905  0.209185  0.220656  0.204857  \n",
              "\n",
              "[6000 rows x 256 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-89b8e1f0-83c5-4ce9-8b6e-9354eabea76e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>246</th>\n",
              "      <th>247</th>\n",
              "      <th>248</th>\n",
              "      <th>249</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.004293</td>\n",
              "      <td>-0.108721</td>\n",
              "      <td>-0.025203</td>\n",
              "      <td>0.050075</td>\n",
              "      <td>0.041975</td>\n",
              "      <td>-0.079525</td>\n",
              "      <td>-0.019323</td>\n",
              "      <td>0.040809</td>\n",
              "      <td>0.003013</td>\n",
              "      <td>-0.044322</td>\n",
              "      <td>...</td>\n",
              "      <td>0.025460</td>\n",
              "      <td>0.090296</td>\n",
              "      <td>-0.007490</td>\n",
              "      <td>-0.026310</td>\n",
              "      <td>0.053336</td>\n",
              "      <td>0.110380</td>\n",
              "      <td>0.044460</td>\n",
              "      <td>-0.027323</td>\n",
              "      <td>0.009700</td>\n",
              "      <td>0.105980</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.178025</td>\n",
              "      <td>-0.129949</td>\n",
              "      <td>-0.154714</td>\n",
              "      <td>-0.190879</td>\n",
              "      <td>-0.147544</td>\n",
              "      <td>-0.149849</td>\n",
              "      <td>-0.139385</td>\n",
              "      <td>-0.184831</td>\n",
              "      <td>-0.160077</td>\n",
              "      <td>-0.125704</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.158946</td>\n",
              "      <td>-0.192521</td>\n",
              "      <td>-0.104194</td>\n",
              "      <td>-0.074016</td>\n",
              "      <td>-0.121687</td>\n",
              "      <td>-0.121997</td>\n",
              "      <td>-0.143646</td>\n",
              "      <td>-0.113273</td>\n",
              "      <td>-0.137490</td>\n",
              "      <td>-0.137856</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.118486</td>\n",
              "      <td>-0.135978</td>\n",
              "      <td>-0.021999</td>\n",
              "      <td>0.014580</td>\n",
              "      <td>-0.094641</td>\n",
              "      <td>-0.135181</td>\n",
              "      <td>-0.068231</td>\n",
              "      <td>0.024199</td>\n",
              "      <td>-0.080811</td>\n",
              "      <td>-0.151051</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.012902</td>\n",
              "      <td>-0.057865</td>\n",
              "      <td>-0.169278</td>\n",
              "      <td>-0.139103</td>\n",
              "      <td>-0.051629</td>\n",
              "      <td>0.003184</td>\n",
              "      <td>-0.126743</td>\n",
              "      <td>-0.173762</td>\n",
              "      <td>-0.090042</td>\n",
              "      <td>-0.026942</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.595354</td>\n",
              "      <td>0.768205</td>\n",
              "      <td>0.751377</td>\n",
              "      <td>0.729010</td>\n",
              "      <td>0.837712</td>\n",
              "      <td>0.799024</td>\n",
              "      <td>0.553267</td>\n",
              "      <td>0.214589</td>\n",
              "      <td>0.034092</td>\n",
              "      <td>0.025435</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.060304</td>\n",
              "      <td>-0.051033</td>\n",
              "      <td>0.071048</td>\n",
              "      <td>0.039479</td>\n",
              "      <td>-0.059981</td>\n",
              "      <td>-0.111832</td>\n",
              "      <td>0.002407</td>\n",
              "      <td>0.031495</td>\n",
              "      <td>-0.068237</td>\n",
              "      <td>-0.108500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.107180</td>\n",
              "      <td>-0.029394</td>\n",
              "      <td>-0.003439</td>\n",
              "      <td>-0.114145</td>\n",
              "      <td>-0.110911</td>\n",
              "      <td>-0.047808</td>\n",
              "      <td>0.014365</td>\n",
              "      <td>-0.074943</td>\n",
              "      <td>-0.107671</td>\n",
              "      <td>-0.060639</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.104624</td>\n",
              "      <td>-0.173071</td>\n",
              "      <td>-0.158586</td>\n",
              "      <td>-0.065081</td>\n",
              "      <td>-0.066074</td>\n",
              "      <td>-0.202336</td>\n",
              "      <td>-0.198327</td>\n",
              "      <td>-0.087227</td>\n",
              "      <td>-0.061546</td>\n",
              "      <td>-0.158424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5995</th>\n",
              "      <td>-0.006009</td>\n",
              "      <td>0.035463</td>\n",
              "      <td>0.031765</td>\n",
              "      <td>0.034116</td>\n",
              "      <td>0.041276</td>\n",
              "      <td>0.040276</td>\n",
              "      <td>0.043814</td>\n",
              "      <td>0.049392</td>\n",
              "      <td>0.014919</td>\n",
              "      <td>0.082215</td>\n",
              "      <td>...</td>\n",
              "      <td>0.032174</td>\n",
              "      <td>0.025207</td>\n",
              "      <td>0.017123</td>\n",
              "      <td>-0.047797</td>\n",
              "      <td>-0.036055</td>\n",
              "      <td>-0.080359</td>\n",
              "      <td>-0.080849</td>\n",
              "      <td>-0.131536</td>\n",
              "      <td>-0.108450</td>\n",
              "      <td>-0.119801</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5996</th>\n",
              "      <td>-0.004209</td>\n",
              "      <td>-0.021269</td>\n",
              "      <td>0.014421</td>\n",
              "      <td>0.066451</td>\n",
              "      <td>0.034102</td>\n",
              "      <td>0.022313</td>\n",
              "      <td>0.039954</td>\n",
              "      <td>0.015126</td>\n",
              "      <td>0.016828</td>\n",
              "      <td>0.095201</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.002060</td>\n",
              "      <td>-0.007017</td>\n",
              "      <td>-0.001635</td>\n",
              "      <td>0.073427</td>\n",
              "      <td>0.119448</td>\n",
              "      <td>0.241599</td>\n",
              "      <td>0.262261</td>\n",
              "      <td>0.279691</td>\n",
              "      <td>0.237252</td>\n",
              "      <td>0.092732</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5997</th>\n",
              "      <td>-0.052598</td>\n",
              "      <td>-0.055633</td>\n",
              "      <td>-0.055577</td>\n",
              "      <td>-0.032002</td>\n",
              "      <td>-0.095826</td>\n",
              "      <td>-0.056251</td>\n",
              "      <td>-0.088205</td>\n",
              "      <td>-0.003379</td>\n",
              "      <td>-0.041393</td>\n",
              "      <td>-0.062797</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.038018</td>\n",
              "      <td>-0.060236</td>\n",
              "      <td>-0.027453</td>\n",
              "      <td>-0.048409</td>\n",
              "      <td>-0.078653</td>\n",
              "      <td>-0.040536</td>\n",
              "      <td>-0.018418</td>\n",
              "      <td>-0.062418</td>\n",
              "      <td>-0.006057</td>\n",
              "      <td>0.010865</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5998</th>\n",
              "      <td>-0.039242</td>\n",
              "      <td>-0.044322</td>\n",
              "      <td>-0.024571</td>\n",
              "      <td>-0.017190</td>\n",
              "      <td>-0.044918</td>\n",
              "      <td>-0.038277</td>\n",
              "      <td>-0.066724</td>\n",
              "      <td>-0.033992</td>\n",
              "      <td>-0.049339</td>\n",
              "      <td>-0.018386</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.089395</td>\n",
              "      <td>-0.019348</td>\n",
              "      <td>-0.078060</td>\n",
              "      <td>-0.082112</td>\n",
              "      <td>-0.081604</td>\n",
              "      <td>-0.070175</td>\n",
              "      <td>-0.080676</td>\n",
              "      <td>-0.072666</td>\n",
              "      <td>0.004694</td>\n",
              "      <td>-0.046835</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5999</th>\n",
              "      <td>-0.001325</td>\n",
              "      <td>-0.136461</td>\n",
              "      <td>-0.178526</td>\n",
              "      <td>-0.261551</td>\n",
              "      <td>-0.222405</td>\n",
              "      <td>-0.167719</td>\n",
              "      <td>-0.153012</td>\n",
              "      <td>-0.143434</td>\n",
              "      <td>-0.108636</td>\n",
              "      <td>-0.056848</td>\n",
              "      <td>...</td>\n",
              "      <td>0.127477</td>\n",
              "      <td>0.115293</td>\n",
              "      <td>0.154681</td>\n",
              "      <td>0.142378</td>\n",
              "      <td>0.226287</td>\n",
              "      <td>0.199556</td>\n",
              "      <td>0.225905</td>\n",
              "      <td>0.209185</td>\n",
              "      <td>0.220656</td>\n",
              "      <td>0.204857</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6000 rows × 256 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-89b8e1f0-83c5-4ce9-8b6e-9354eabea76e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-89b8e1f0-83c5-4ce9-8b6e-9354eabea76e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-89b8e1f0-83c5-4ce9-8b6e-9354eabea76e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3498e76-d0e3-4861-b405-14f8b925db75",
        "id": "70QtguG8u9a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(24000, 256)"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 라벨생성"
      ],
      "metadata": {
        "id": "qWUEooCuu9a3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "arr = np.array([])\n",
        "\n",
        "for i in range(1,101):\n",
        "  for j in range(240):\n",
        "    arr = np.append(arr, np.array([i]))"
      ],
      "metadata": {
        "id": "U8dg-Mkru9a3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train[256]=arr"
      ],
      "metadata": {
        "id": "0VSRZdHsu9a3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "outputId": "350f2101-e4b1-4bc5-cf5c-8781d4417497",
        "id": "JwrdroqGu9a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            0         1         2         3         4         5         6    \\\n",
              "0     -0.011566  0.077466  0.083038  0.015121 -0.024216  0.026618  0.112943   \n",
              "1     -0.105411  0.006199  0.035219 -0.038238 -0.087385 -0.047431  0.057264   \n",
              "2     -0.124635 -0.101820 -0.004986 -0.009573 -0.097080 -0.078287 -0.033934   \n",
              "3     -0.070665 -0.110059 -0.041584  0.011091 -0.073945 -0.116371 -0.065348   \n",
              "4      0.164484  0.137004  0.165505  0.176745  0.186086  0.149056  0.190527   \n",
              "...         ...       ...       ...       ...       ...       ...       ...   \n",
              "23995  0.047641  0.025272  0.001593  0.008873 -0.022388 -0.055809 -0.067971   \n",
              "23996 -0.017653  0.005880 -0.008398 -0.053956 -0.045114 -0.050892 -0.014791   \n",
              "23997  0.020267  0.062224 -0.014968  0.012301  0.073691  0.025622  0.056645   \n",
              "23998 -0.091136 -0.072359 -0.082491 -0.096773 -0.108624 -0.106044 -0.096813   \n",
              "23999 -0.081848 -0.092246 -0.069385 -0.066354 -0.034663 -0.035583 -0.052443   \n",
              "\n",
              "            7         8         9    ...       247       248       249  \\\n",
              "0      0.067018 -0.022367  0.035899  ... -0.080380 -0.088511  0.005901   \n",
              "1      0.011541 -0.063622 -0.054354  ... -0.102607 -0.121740 -0.038993   \n",
              "2      0.011788 -0.100740 -0.105788  ... -0.087241 -0.110820 -0.037730   \n",
              "3      0.005895 -0.075523 -0.138241  ... -0.074093 -0.113909 -0.113364   \n",
              "4      0.256168  0.278809  0.242770  ...  0.168813  0.158065  0.156737   \n",
              "...         ...       ...       ...  ...       ...       ...       ...   \n",
              "23995 -0.044163 -0.045987 -0.032190  ... -0.062754  0.002815 -0.032905   \n",
              "23996 -0.025570 -0.044989 -0.027848  ... -0.017953  0.000676  0.008635   \n",
              "23997  0.040058  0.073452  0.061997  ... -0.057440 -0.065800 -0.044000   \n",
              "23998 -0.094621 -0.049099 -0.103425  ... -0.095215 -0.045811 -0.093347   \n",
              "23999 -0.033073 -0.060994 -0.040115  ... -0.064874 -0.019088 -0.028823   \n",
              "\n",
              "            250       251       252       253       254       255    256  \n",
              "0     -0.000657 -0.066214 -0.061900 -0.011674  0.064162 -0.020820    1.0  \n",
              "1     -0.017367 -0.083201 -0.108995 -0.055099  0.004076 -0.050829    1.0  \n",
              "2      0.000719 -0.066072 -0.119163 -0.068725  0.003512 -0.040881    1.0  \n",
              "3     -0.053677 -0.015389 -0.088530 -0.122749 -0.019986 -0.033522    1.0  \n",
              "4      0.236019  0.242662  0.237836  0.242839  0.272923  0.371568    1.0  \n",
              "...         ...       ...       ...       ...       ...       ...    ...  \n",
              "23995 -0.016336 -0.045078  0.004621 -0.020431  0.005698 -0.048605  100.0  \n",
              "23996 -0.010605  0.004606  0.032398  0.011841 -0.004365  0.032981  100.0  \n",
              "23997 -0.082399 -0.083447 -0.065335 -0.088911 -0.080017 -0.076062  100.0  \n",
              "23998 -0.052513 -0.086620 -0.069087 -0.056044 -0.095822 -0.080069  100.0  \n",
              "23999 -0.049239 -0.026504 -0.037821 -0.061718 -0.066845 -0.059923  100.0  \n",
              "\n",
              "[24000 rows x 257 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2bd29241-a2fb-4ec5-8c53-6b7695b2ea82\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>247</th>\n",
              "      <th>248</th>\n",
              "      <th>249</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "      <th>256</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.011566</td>\n",
              "      <td>0.077466</td>\n",
              "      <td>0.083038</td>\n",
              "      <td>0.015121</td>\n",
              "      <td>-0.024216</td>\n",
              "      <td>0.026618</td>\n",
              "      <td>0.112943</td>\n",
              "      <td>0.067018</td>\n",
              "      <td>-0.022367</td>\n",
              "      <td>0.035899</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.080380</td>\n",
              "      <td>-0.088511</td>\n",
              "      <td>0.005901</td>\n",
              "      <td>-0.000657</td>\n",
              "      <td>-0.066214</td>\n",
              "      <td>-0.061900</td>\n",
              "      <td>-0.011674</td>\n",
              "      <td>0.064162</td>\n",
              "      <td>-0.020820</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.105411</td>\n",
              "      <td>0.006199</td>\n",
              "      <td>0.035219</td>\n",
              "      <td>-0.038238</td>\n",
              "      <td>-0.087385</td>\n",
              "      <td>-0.047431</td>\n",
              "      <td>0.057264</td>\n",
              "      <td>0.011541</td>\n",
              "      <td>-0.063622</td>\n",
              "      <td>-0.054354</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.102607</td>\n",
              "      <td>-0.121740</td>\n",
              "      <td>-0.038993</td>\n",
              "      <td>-0.017367</td>\n",
              "      <td>-0.083201</td>\n",
              "      <td>-0.108995</td>\n",
              "      <td>-0.055099</td>\n",
              "      <td>0.004076</td>\n",
              "      <td>-0.050829</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.124635</td>\n",
              "      <td>-0.101820</td>\n",
              "      <td>-0.004986</td>\n",
              "      <td>-0.009573</td>\n",
              "      <td>-0.097080</td>\n",
              "      <td>-0.078287</td>\n",
              "      <td>-0.033934</td>\n",
              "      <td>0.011788</td>\n",
              "      <td>-0.100740</td>\n",
              "      <td>-0.105788</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.087241</td>\n",
              "      <td>-0.110820</td>\n",
              "      <td>-0.037730</td>\n",
              "      <td>0.000719</td>\n",
              "      <td>-0.066072</td>\n",
              "      <td>-0.119163</td>\n",
              "      <td>-0.068725</td>\n",
              "      <td>0.003512</td>\n",
              "      <td>-0.040881</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.070665</td>\n",
              "      <td>-0.110059</td>\n",
              "      <td>-0.041584</td>\n",
              "      <td>0.011091</td>\n",
              "      <td>-0.073945</td>\n",
              "      <td>-0.116371</td>\n",
              "      <td>-0.065348</td>\n",
              "      <td>0.005895</td>\n",
              "      <td>-0.075523</td>\n",
              "      <td>-0.138241</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.074093</td>\n",
              "      <td>-0.113909</td>\n",
              "      <td>-0.113364</td>\n",
              "      <td>-0.053677</td>\n",
              "      <td>-0.015389</td>\n",
              "      <td>-0.088530</td>\n",
              "      <td>-0.122749</td>\n",
              "      <td>-0.019986</td>\n",
              "      <td>-0.033522</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.164484</td>\n",
              "      <td>0.137004</td>\n",
              "      <td>0.165505</td>\n",
              "      <td>0.176745</td>\n",
              "      <td>0.186086</td>\n",
              "      <td>0.149056</td>\n",
              "      <td>0.190527</td>\n",
              "      <td>0.256168</td>\n",
              "      <td>0.278809</td>\n",
              "      <td>0.242770</td>\n",
              "      <td>...</td>\n",
              "      <td>0.168813</td>\n",
              "      <td>0.158065</td>\n",
              "      <td>0.156737</td>\n",
              "      <td>0.236019</td>\n",
              "      <td>0.242662</td>\n",
              "      <td>0.237836</td>\n",
              "      <td>0.242839</td>\n",
              "      <td>0.272923</td>\n",
              "      <td>0.371568</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23995</th>\n",
              "      <td>0.047641</td>\n",
              "      <td>0.025272</td>\n",
              "      <td>0.001593</td>\n",
              "      <td>0.008873</td>\n",
              "      <td>-0.022388</td>\n",
              "      <td>-0.055809</td>\n",
              "      <td>-0.067971</td>\n",
              "      <td>-0.044163</td>\n",
              "      <td>-0.045987</td>\n",
              "      <td>-0.032190</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.062754</td>\n",
              "      <td>0.002815</td>\n",
              "      <td>-0.032905</td>\n",
              "      <td>-0.016336</td>\n",
              "      <td>-0.045078</td>\n",
              "      <td>0.004621</td>\n",
              "      <td>-0.020431</td>\n",
              "      <td>0.005698</td>\n",
              "      <td>-0.048605</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23996</th>\n",
              "      <td>-0.017653</td>\n",
              "      <td>0.005880</td>\n",
              "      <td>-0.008398</td>\n",
              "      <td>-0.053956</td>\n",
              "      <td>-0.045114</td>\n",
              "      <td>-0.050892</td>\n",
              "      <td>-0.014791</td>\n",
              "      <td>-0.025570</td>\n",
              "      <td>-0.044989</td>\n",
              "      <td>-0.027848</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.017953</td>\n",
              "      <td>0.000676</td>\n",
              "      <td>0.008635</td>\n",
              "      <td>-0.010605</td>\n",
              "      <td>0.004606</td>\n",
              "      <td>0.032398</td>\n",
              "      <td>0.011841</td>\n",
              "      <td>-0.004365</td>\n",
              "      <td>0.032981</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23997</th>\n",
              "      <td>0.020267</td>\n",
              "      <td>0.062224</td>\n",
              "      <td>-0.014968</td>\n",
              "      <td>0.012301</td>\n",
              "      <td>0.073691</td>\n",
              "      <td>0.025622</td>\n",
              "      <td>0.056645</td>\n",
              "      <td>0.040058</td>\n",
              "      <td>0.073452</td>\n",
              "      <td>0.061997</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.057440</td>\n",
              "      <td>-0.065800</td>\n",
              "      <td>-0.044000</td>\n",
              "      <td>-0.082399</td>\n",
              "      <td>-0.083447</td>\n",
              "      <td>-0.065335</td>\n",
              "      <td>-0.088911</td>\n",
              "      <td>-0.080017</td>\n",
              "      <td>-0.076062</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23998</th>\n",
              "      <td>-0.091136</td>\n",
              "      <td>-0.072359</td>\n",
              "      <td>-0.082491</td>\n",
              "      <td>-0.096773</td>\n",
              "      <td>-0.108624</td>\n",
              "      <td>-0.106044</td>\n",
              "      <td>-0.096813</td>\n",
              "      <td>-0.094621</td>\n",
              "      <td>-0.049099</td>\n",
              "      <td>-0.103425</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.095215</td>\n",
              "      <td>-0.045811</td>\n",
              "      <td>-0.093347</td>\n",
              "      <td>-0.052513</td>\n",
              "      <td>-0.086620</td>\n",
              "      <td>-0.069087</td>\n",
              "      <td>-0.056044</td>\n",
              "      <td>-0.095822</td>\n",
              "      <td>-0.080069</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23999</th>\n",
              "      <td>-0.081848</td>\n",
              "      <td>-0.092246</td>\n",
              "      <td>-0.069385</td>\n",
              "      <td>-0.066354</td>\n",
              "      <td>-0.034663</td>\n",
              "      <td>-0.035583</td>\n",
              "      <td>-0.052443</td>\n",
              "      <td>-0.033073</td>\n",
              "      <td>-0.060994</td>\n",
              "      <td>-0.040115</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.064874</td>\n",
              "      <td>-0.019088</td>\n",
              "      <td>-0.028823</td>\n",
              "      <td>-0.049239</td>\n",
              "      <td>-0.026504</td>\n",
              "      <td>-0.037821</td>\n",
              "      <td>-0.061718</td>\n",
              "      <td>-0.066845</td>\n",
              "      <td>-0.059923</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>24000 rows × 257 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2bd29241-a2fb-4ec5-8c53-6b7695b2ea82')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2bd29241-a2fb-4ec5-8c53-6b7695b2ea82 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2bd29241-a2fb-4ec5-8c53-6b7695b2ea82');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "arr_t = np.array([])\n",
        "\n",
        "for i in range(1,101):\n",
        "  for j in range(60):\n",
        "    arr_t = np.append(arr_t, np.array([i]))"
      ],
      "metadata": {
        "id": "WFjZ9noiu9a4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arr_v = np.array([])\n",
        "\n",
        "for i in range(1,101):\n",
        "  for j in range(60):\n",
        "    arr_v = np.append(arr_v, np.array([i]))"
      ],
      "metadata": {
        "id": "mkJ_shJTu9a4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_test[256]=arr_t\n",
        "data_val[256]=arr_v\n",
        "data_val"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "outputId": "4dbf70a0-18c5-4fba-c13a-db363ebc3be1",
        "id": "AVrjkGSnu9a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           0         1         2         3         4         5         6    \\\n",
              "0    -0.088677 -0.125790 -0.030982  0.007508 -0.029261 -0.106478 -0.080494   \n",
              "1    -0.166172 -0.207399 -0.221436 -0.207283 -0.178781 -0.161000 -0.193089   \n",
              "2    -0.145358 -0.148342 -0.020535 -0.017398 -0.130030 -0.147643 -0.060204   \n",
              "3     0.071963  0.015509 -0.112935 -0.062499  0.079877 -0.016757 -0.127871   \n",
              "4    -0.041662  0.030685 -0.020187 -0.129210 -0.085054  0.044992  0.021448   \n",
              "...        ...       ...       ...       ...       ...       ...       ...   \n",
              "5995  0.032231  0.030567 -0.020890  0.050221  0.017189 -0.008196 -0.015203   \n",
              "5996 -0.196898 -0.177660 -0.151701 -0.170922 -0.196274 -0.164125 -0.167807   \n",
              "5997  0.014198 -0.011697 -0.009543 -0.048490 -0.005837  0.002504 -0.030444   \n",
              "5998 -0.048497 -0.030079 -0.042822 -0.063725 -0.028908 -0.011291 -0.015405   \n",
              "5999 -0.040551 -0.032769 -0.024399 -0.043038 -0.040068 -0.059028 -0.083389   \n",
              "\n",
              "           7         8         9    ...       247       248       249  \\\n",
              "0    -0.011038 -0.021531 -0.121852  ...  0.017174 -0.070759 -0.096122   \n",
              "1    -0.206398 -0.157608 -0.133839  ... -0.207781 -0.165102 -0.191124   \n",
              "2     0.013275 -0.095876 -0.130336  ... -0.085840 -0.182494 -0.152887   \n",
              "3    -0.039045  0.030101  0.042338  ... -0.033357  0.015213 -0.075836   \n",
              "4    -0.110557 -0.113222 -0.015198  ... -0.124194 -0.064253 -0.014054   \n",
              "...        ...       ...       ...  ...       ...       ...       ...   \n",
              "5995 -0.032563 -0.038065 -0.040280  ... -0.038393 -0.042812 -0.032542   \n",
              "5996 -0.145068 -0.153429 -0.147951  ... -0.014796  0.048950  0.007776   \n",
              "5997  0.011946  0.002635  0.027564  ... -0.081514 -0.086519 -0.102934   \n",
              "5998 -0.037688 -0.033962 -0.027736  ... -0.023651 -0.016484 -0.058906   \n",
              "5999 -0.066660 -0.039522 -0.092334  ... -0.051936 -0.021097  0.032533   \n",
              "\n",
              "           250       251       252       253       254       255    256  \n",
              "0     0.007007 -0.004853 -0.026522 -0.062090  0.001993  0.019837    1.0  \n",
              "1    -0.196126 -0.205478 -0.184471 -0.151804 -0.194417 -0.193371    1.0  \n",
              "2    -0.014291 -0.043764 -0.122897 -0.128260 -0.017722 -0.028884    1.0  \n",
              "3    -0.154705 -0.100604 -0.008633  0.015829  0.039830  0.248372    1.0  \n",
              "4    -0.089064 -0.151946 -0.088977 -0.031070 -0.035743 -0.117336    1.0  \n",
              "...        ...       ...       ...       ...       ...       ...    ...  \n",
              "5995  0.037526 -0.009246  0.006291  0.019648 -0.020407 -0.015023  100.0  \n",
              "5996  0.021993  0.008940  0.032578  0.035396  0.003384  0.046122  100.0  \n",
              "5997 -0.041559 -0.065864 -0.047559 -0.085704 -0.048828 -0.085453  100.0  \n",
              "5998  0.005502 -0.050809 -0.055031 -0.001622 -0.067492 -0.039172  100.0  \n",
              "5999  0.108753  0.185233  0.263694  0.291106  0.295209  0.143481  100.0  \n",
              "\n",
              "[6000 rows x 257 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b3c7adab-7b8a-4e98-9489-37a900c0fb56\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>247</th>\n",
              "      <th>248</th>\n",
              "      <th>249</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "      <th>256</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.088677</td>\n",
              "      <td>-0.125790</td>\n",
              "      <td>-0.030982</td>\n",
              "      <td>0.007508</td>\n",
              "      <td>-0.029261</td>\n",
              "      <td>-0.106478</td>\n",
              "      <td>-0.080494</td>\n",
              "      <td>-0.011038</td>\n",
              "      <td>-0.021531</td>\n",
              "      <td>-0.121852</td>\n",
              "      <td>...</td>\n",
              "      <td>0.017174</td>\n",
              "      <td>-0.070759</td>\n",
              "      <td>-0.096122</td>\n",
              "      <td>0.007007</td>\n",
              "      <td>-0.004853</td>\n",
              "      <td>-0.026522</td>\n",
              "      <td>-0.062090</td>\n",
              "      <td>0.001993</td>\n",
              "      <td>0.019837</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.166172</td>\n",
              "      <td>-0.207399</td>\n",
              "      <td>-0.221436</td>\n",
              "      <td>-0.207283</td>\n",
              "      <td>-0.178781</td>\n",
              "      <td>-0.161000</td>\n",
              "      <td>-0.193089</td>\n",
              "      <td>-0.206398</td>\n",
              "      <td>-0.157608</td>\n",
              "      <td>-0.133839</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.207781</td>\n",
              "      <td>-0.165102</td>\n",
              "      <td>-0.191124</td>\n",
              "      <td>-0.196126</td>\n",
              "      <td>-0.205478</td>\n",
              "      <td>-0.184471</td>\n",
              "      <td>-0.151804</td>\n",
              "      <td>-0.194417</td>\n",
              "      <td>-0.193371</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.145358</td>\n",
              "      <td>-0.148342</td>\n",
              "      <td>-0.020535</td>\n",
              "      <td>-0.017398</td>\n",
              "      <td>-0.130030</td>\n",
              "      <td>-0.147643</td>\n",
              "      <td>-0.060204</td>\n",
              "      <td>0.013275</td>\n",
              "      <td>-0.095876</td>\n",
              "      <td>-0.130336</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.085840</td>\n",
              "      <td>-0.182494</td>\n",
              "      <td>-0.152887</td>\n",
              "      <td>-0.014291</td>\n",
              "      <td>-0.043764</td>\n",
              "      <td>-0.122897</td>\n",
              "      <td>-0.128260</td>\n",
              "      <td>-0.017722</td>\n",
              "      <td>-0.028884</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.071963</td>\n",
              "      <td>0.015509</td>\n",
              "      <td>-0.112935</td>\n",
              "      <td>-0.062499</td>\n",
              "      <td>0.079877</td>\n",
              "      <td>-0.016757</td>\n",
              "      <td>-0.127871</td>\n",
              "      <td>-0.039045</td>\n",
              "      <td>0.030101</td>\n",
              "      <td>0.042338</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.033357</td>\n",
              "      <td>0.015213</td>\n",
              "      <td>-0.075836</td>\n",
              "      <td>-0.154705</td>\n",
              "      <td>-0.100604</td>\n",
              "      <td>-0.008633</td>\n",
              "      <td>0.015829</td>\n",
              "      <td>0.039830</td>\n",
              "      <td>0.248372</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.041662</td>\n",
              "      <td>0.030685</td>\n",
              "      <td>-0.020187</td>\n",
              "      <td>-0.129210</td>\n",
              "      <td>-0.085054</td>\n",
              "      <td>0.044992</td>\n",
              "      <td>0.021448</td>\n",
              "      <td>-0.110557</td>\n",
              "      <td>-0.113222</td>\n",
              "      <td>-0.015198</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.124194</td>\n",
              "      <td>-0.064253</td>\n",
              "      <td>-0.014054</td>\n",
              "      <td>-0.089064</td>\n",
              "      <td>-0.151946</td>\n",
              "      <td>-0.088977</td>\n",
              "      <td>-0.031070</td>\n",
              "      <td>-0.035743</td>\n",
              "      <td>-0.117336</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5995</th>\n",
              "      <td>0.032231</td>\n",
              "      <td>0.030567</td>\n",
              "      <td>-0.020890</td>\n",
              "      <td>0.050221</td>\n",
              "      <td>0.017189</td>\n",
              "      <td>-0.008196</td>\n",
              "      <td>-0.015203</td>\n",
              "      <td>-0.032563</td>\n",
              "      <td>-0.038065</td>\n",
              "      <td>-0.040280</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.038393</td>\n",
              "      <td>-0.042812</td>\n",
              "      <td>-0.032542</td>\n",
              "      <td>0.037526</td>\n",
              "      <td>-0.009246</td>\n",
              "      <td>0.006291</td>\n",
              "      <td>0.019648</td>\n",
              "      <td>-0.020407</td>\n",
              "      <td>-0.015023</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5996</th>\n",
              "      <td>-0.196898</td>\n",
              "      <td>-0.177660</td>\n",
              "      <td>-0.151701</td>\n",
              "      <td>-0.170922</td>\n",
              "      <td>-0.196274</td>\n",
              "      <td>-0.164125</td>\n",
              "      <td>-0.167807</td>\n",
              "      <td>-0.145068</td>\n",
              "      <td>-0.153429</td>\n",
              "      <td>-0.147951</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.014796</td>\n",
              "      <td>0.048950</td>\n",
              "      <td>0.007776</td>\n",
              "      <td>0.021993</td>\n",
              "      <td>0.008940</td>\n",
              "      <td>0.032578</td>\n",
              "      <td>0.035396</td>\n",
              "      <td>0.003384</td>\n",
              "      <td>0.046122</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5997</th>\n",
              "      <td>0.014198</td>\n",
              "      <td>-0.011697</td>\n",
              "      <td>-0.009543</td>\n",
              "      <td>-0.048490</td>\n",
              "      <td>-0.005837</td>\n",
              "      <td>0.002504</td>\n",
              "      <td>-0.030444</td>\n",
              "      <td>0.011946</td>\n",
              "      <td>0.002635</td>\n",
              "      <td>0.027564</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.081514</td>\n",
              "      <td>-0.086519</td>\n",
              "      <td>-0.102934</td>\n",
              "      <td>-0.041559</td>\n",
              "      <td>-0.065864</td>\n",
              "      <td>-0.047559</td>\n",
              "      <td>-0.085704</td>\n",
              "      <td>-0.048828</td>\n",
              "      <td>-0.085453</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5998</th>\n",
              "      <td>-0.048497</td>\n",
              "      <td>-0.030079</td>\n",
              "      <td>-0.042822</td>\n",
              "      <td>-0.063725</td>\n",
              "      <td>-0.028908</td>\n",
              "      <td>-0.011291</td>\n",
              "      <td>-0.015405</td>\n",
              "      <td>-0.037688</td>\n",
              "      <td>-0.033962</td>\n",
              "      <td>-0.027736</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.023651</td>\n",
              "      <td>-0.016484</td>\n",
              "      <td>-0.058906</td>\n",
              "      <td>0.005502</td>\n",
              "      <td>-0.050809</td>\n",
              "      <td>-0.055031</td>\n",
              "      <td>-0.001622</td>\n",
              "      <td>-0.067492</td>\n",
              "      <td>-0.039172</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5999</th>\n",
              "      <td>-0.040551</td>\n",
              "      <td>-0.032769</td>\n",
              "      <td>-0.024399</td>\n",
              "      <td>-0.043038</td>\n",
              "      <td>-0.040068</td>\n",
              "      <td>-0.059028</td>\n",
              "      <td>-0.083389</td>\n",
              "      <td>-0.066660</td>\n",
              "      <td>-0.039522</td>\n",
              "      <td>-0.092334</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.051936</td>\n",
              "      <td>-0.021097</td>\n",
              "      <td>0.032533</td>\n",
              "      <td>0.108753</td>\n",
              "      <td>0.185233</td>\n",
              "      <td>0.263694</td>\n",
              "      <td>0.291106</td>\n",
              "      <td>0.295209</td>\n",
              "      <td>0.143481</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6000 rows × 257 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b3c7adab-7b8a-4e98-9489-37a900c0fb56')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b3c7adab-7b8a-4e98-9489-37a900c0fb56 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b3c7adab-7b8a-4e98-9489-37a900c0fb56');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터 전체 섞기"
      ],
      "metadata": {
        "id": "sadv6pVlu9a4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_total=pd.concat([data_train, data_test, data_val])\n",
        "data_total"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "outputId": "12347800-84c7-44e3-f315-9264498fa629",
        "id": "K_cN21DRu9a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           0         1         2         3         4         5         6    \\\n",
              "0    -0.011566  0.077466  0.083038  0.015121 -0.024216  0.026618  0.112943   \n",
              "1    -0.105411  0.006199  0.035219 -0.038238 -0.087385 -0.047431  0.057264   \n",
              "2    -0.124635 -0.101820 -0.004986 -0.009573 -0.097080 -0.078287 -0.033934   \n",
              "3    -0.070665 -0.110059 -0.041584  0.011091 -0.073945 -0.116371 -0.065348   \n",
              "4     0.164484  0.137004  0.165505  0.176745  0.186086  0.149056  0.190527   \n",
              "...        ...       ...       ...       ...       ...       ...       ...   \n",
              "5995  0.032231  0.030567 -0.020890  0.050221  0.017189 -0.008196 -0.015203   \n",
              "5996 -0.196898 -0.177660 -0.151701 -0.170922 -0.196274 -0.164125 -0.167807   \n",
              "5997  0.014198 -0.011697 -0.009543 -0.048490 -0.005837  0.002504 -0.030444   \n",
              "5998 -0.048497 -0.030079 -0.042822 -0.063725 -0.028908 -0.011291 -0.015405   \n",
              "5999 -0.040551 -0.032769 -0.024399 -0.043038 -0.040068 -0.059028 -0.083389   \n",
              "\n",
              "           7         8         9    ...       247       248       249  \\\n",
              "0     0.067018 -0.022367  0.035899  ... -0.080380 -0.088511  0.005901   \n",
              "1     0.011541 -0.063622 -0.054354  ... -0.102607 -0.121740 -0.038993   \n",
              "2     0.011788 -0.100740 -0.105788  ... -0.087241 -0.110820 -0.037730   \n",
              "3     0.005895 -0.075523 -0.138241  ... -0.074093 -0.113909 -0.113364   \n",
              "4     0.256168  0.278809  0.242770  ...  0.168813  0.158065  0.156737   \n",
              "...        ...       ...       ...  ...       ...       ...       ...   \n",
              "5995 -0.032563 -0.038065 -0.040280  ... -0.038393 -0.042812 -0.032542   \n",
              "5996 -0.145068 -0.153429 -0.147951  ... -0.014796  0.048950  0.007776   \n",
              "5997  0.011946  0.002635  0.027564  ... -0.081514 -0.086519 -0.102934   \n",
              "5998 -0.037688 -0.033962 -0.027736  ... -0.023651 -0.016484 -0.058906   \n",
              "5999 -0.066660 -0.039522 -0.092334  ... -0.051936 -0.021097  0.032533   \n",
              "\n",
              "           250       251       252       253       254       255    256  \n",
              "0    -0.000657 -0.066214 -0.061900 -0.011674  0.064162 -0.020820    1.0  \n",
              "1    -0.017367 -0.083201 -0.108995 -0.055099  0.004076 -0.050829    1.0  \n",
              "2     0.000719 -0.066072 -0.119163 -0.068725  0.003512 -0.040881    1.0  \n",
              "3    -0.053677 -0.015389 -0.088530 -0.122749 -0.019986 -0.033522    1.0  \n",
              "4     0.236019  0.242662  0.237836  0.242839  0.272923  0.371568    1.0  \n",
              "...        ...       ...       ...       ...       ...       ...    ...  \n",
              "5995  0.037526 -0.009246  0.006291  0.019648 -0.020407 -0.015023  100.0  \n",
              "5996  0.021993  0.008940  0.032578  0.035396  0.003384  0.046122  100.0  \n",
              "5997 -0.041559 -0.065864 -0.047559 -0.085704 -0.048828 -0.085453  100.0  \n",
              "5998  0.005502 -0.050809 -0.055031 -0.001622 -0.067492 -0.039172  100.0  \n",
              "5999  0.108753  0.185233  0.263694  0.291106  0.295209  0.143481  100.0  \n",
              "\n",
              "[36000 rows x 257 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2c2b49d9-a02b-40f8-b0d3-c98583a6dbfc\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>247</th>\n",
              "      <th>248</th>\n",
              "      <th>249</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "      <th>256</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.011566</td>\n",
              "      <td>0.077466</td>\n",
              "      <td>0.083038</td>\n",
              "      <td>0.015121</td>\n",
              "      <td>-0.024216</td>\n",
              "      <td>0.026618</td>\n",
              "      <td>0.112943</td>\n",
              "      <td>0.067018</td>\n",
              "      <td>-0.022367</td>\n",
              "      <td>0.035899</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.080380</td>\n",
              "      <td>-0.088511</td>\n",
              "      <td>0.005901</td>\n",
              "      <td>-0.000657</td>\n",
              "      <td>-0.066214</td>\n",
              "      <td>-0.061900</td>\n",
              "      <td>-0.011674</td>\n",
              "      <td>0.064162</td>\n",
              "      <td>-0.020820</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.105411</td>\n",
              "      <td>0.006199</td>\n",
              "      <td>0.035219</td>\n",
              "      <td>-0.038238</td>\n",
              "      <td>-0.087385</td>\n",
              "      <td>-0.047431</td>\n",
              "      <td>0.057264</td>\n",
              "      <td>0.011541</td>\n",
              "      <td>-0.063622</td>\n",
              "      <td>-0.054354</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.102607</td>\n",
              "      <td>-0.121740</td>\n",
              "      <td>-0.038993</td>\n",
              "      <td>-0.017367</td>\n",
              "      <td>-0.083201</td>\n",
              "      <td>-0.108995</td>\n",
              "      <td>-0.055099</td>\n",
              "      <td>0.004076</td>\n",
              "      <td>-0.050829</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.124635</td>\n",
              "      <td>-0.101820</td>\n",
              "      <td>-0.004986</td>\n",
              "      <td>-0.009573</td>\n",
              "      <td>-0.097080</td>\n",
              "      <td>-0.078287</td>\n",
              "      <td>-0.033934</td>\n",
              "      <td>0.011788</td>\n",
              "      <td>-0.100740</td>\n",
              "      <td>-0.105788</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.087241</td>\n",
              "      <td>-0.110820</td>\n",
              "      <td>-0.037730</td>\n",
              "      <td>0.000719</td>\n",
              "      <td>-0.066072</td>\n",
              "      <td>-0.119163</td>\n",
              "      <td>-0.068725</td>\n",
              "      <td>0.003512</td>\n",
              "      <td>-0.040881</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.070665</td>\n",
              "      <td>-0.110059</td>\n",
              "      <td>-0.041584</td>\n",
              "      <td>0.011091</td>\n",
              "      <td>-0.073945</td>\n",
              "      <td>-0.116371</td>\n",
              "      <td>-0.065348</td>\n",
              "      <td>0.005895</td>\n",
              "      <td>-0.075523</td>\n",
              "      <td>-0.138241</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.074093</td>\n",
              "      <td>-0.113909</td>\n",
              "      <td>-0.113364</td>\n",
              "      <td>-0.053677</td>\n",
              "      <td>-0.015389</td>\n",
              "      <td>-0.088530</td>\n",
              "      <td>-0.122749</td>\n",
              "      <td>-0.019986</td>\n",
              "      <td>-0.033522</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.164484</td>\n",
              "      <td>0.137004</td>\n",
              "      <td>0.165505</td>\n",
              "      <td>0.176745</td>\n",
              "      <td>0.186086</td>\n",
              "      <td>0.149056</td>\n",
              "      <td>0.190527</td>\n",
              "      <td>0.256168</td>\n",
              "      <td>0.278809</td>\n",
              "      <td>0.242770</td>\n",
              "      <td>...</td>\n",
              "      <td>0.168813</td>\n",
              "      <td>0.158065</td>\n",
              "      <td>0.156737</td>\n",
              "      <td>0.236019</td>\n",
              "      <td>0.242662</td>\n",
              "      <td>0.237836</td>\n",
              "      <td>0.242839</td>\n",
              "      <td>0.272923</td>\n",
              "      <td>0.371568</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5995</th>\n",
              "      <td>0.032231</td>\n",
              "      <td>0.030567</td>\n",
              "      <td>-0.020890</td>\n",
              "      <td>0.050221</td>\n",
              "      <td>0.017189</td>\n",
              "      <td>-0.008196</td>\n",
              "      <td>-0.015203</td>\n",
              "      <td>-0.032563</td>\n",
              "      <td>-0.038065</td>\n",
              "      <td>-0.040280</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.038393</td>\n",
              "      <td>-0.042812</td>\n",
              "      <td>-0.032542</td>\n",
              "      <td>0.037526</td>\n",
              "      <td>-0.009246</td>\n",
              "      <td>0.006291</td>\n",
              "      <td>0.019648</td>\n",
              "      <td>-0.020407</td>\n",
              "      <td>-0.015023</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5996</th>\n",
              "      <td>-0.196898</td>\n",
              "      <td>-0.177660</td>\n",
              "      <td>-0.151701</td>\n",
              "      <td>-0.170922</td>\n",
              "      <td>-0.196274</td>\n",
              "      <td>-0.164125</td>\n",
              "      <td>-0.167807</td>\n",
              "      <td>-0.145068</td>\n",
              "      <td>-0.153429</td>\n",
              "      <td>-0.147951</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.014796</td>\n",
              "      <td>0.048950</td>\n",
              "      <td>0.007776</td>\n",
              "      <td>0.021993</td>\n",
              "      <td>0.008940</td>\n",
              "      <td>0.032578</td>\n",
              "      <td>0.035396</td>\n",
              "      <td>0.003384</td>\n",
              "      <td>0.046122</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5997</th>\n",
              "      <td>0.014198</td>\n",
              "      <td>-0.011697</td>\n",
              "      <td>-0.009543</td>\n",
              "      <td>-0.048490</td>\n",
              "      <td>-0.005837</td>\n",
              "      <td>0.002504</td>\n",
              "      <td>-0.030444</td>\n",
              "      <td>0.011946</td>\n",
              "      <td>0.002635</td>\n",
              "      <td>0.027564</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.081514</td>\n",
              "      <td>-0.086519</td>\n",
              "      <td>-0.102934</td>\n",
              "      <td>-0.041559</td>\n",
              "      <td>-0.065864</td>\n",
              "      <td>-0.047559</td>\n",
              "      <td>-0.085704</td>\n",
              "      <td>-0.048828</td>\n",
              "      <td>-0.085453</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5998</th>\n",
              "      <td>-0.048497</td>\n",
              "      <td>-0.030079</td>\n",
              "      <td>-0.042822</td>\n",
              "      <td>-0.063725</td>\n",
              "      <td>-0.028908</td>\n",
              "      <td>-0.011291</td>\n",
              "      <td>-0.015405</td>\n",
              "      <td>-0.037688</td>\n",
              "      <td>-0.033962</td>\n",
              "      <td>-0.027736</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.023651</td>\n",
              "      <td>-0.016484</td>\n",
              "      <td>-0.058906</td>\n",
              "      <td>0.005502</td>\n",
              "      <td>-0.050809</td>\n",
              "      <td>-0.055031</td>\n",
              "      <td>-0.001622</td>\n",
              "      <td>-0.067492</td>\n",
              "      <td>-0.039172</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5999</th>\n",
              "      <td>-0.040551</td>\n",
              "      <td>-0.032769</td>\n",
              "      <td>-0.024399</td>\n",
              "      <td>-0.043038</td>\n",
              "      <td>-0.040068</td>\n",
              "      <td>-0.059028</td>\n",
              "      <td>-0.083389</td>\n",
              "      <td>-0.066660</td>\n",
              "      <td>-0.039522</td>\n",
              "      <td>-0.092334</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.051936</td>\n",
              "      <td>-0.021097</td>\n",
              "      <td>0.032533</td>\n",
              "      <td>0.108753</td>\n",
              "      <td>0.185233</td>\n",
              "      <td>0.263694</td>\n",
              "      <td>0.291106</td>\n",
              "      <td>0.295209</td>\n",
              "      <td>0.143481</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>36000 rows × 257 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2c2b49d9-a02b-40f8-b0d3-c98583a6dbfc')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2c2b49d9-a02b-40f8-b0d3-c98583a6dbfc button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2c2b49d9-a02b-40f8-b0d3-c98583a6dbfc');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_total=data_total.sample(frac=1)"
      ],
      "metadata": {
        "id": "LvcMuX_Su9a4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train=data_total.iloc[0:24000, :]\n",
        "data_val=data_total.iloc[24000:30000, :]\n",
        "data_test=data_total.iloc[30000:36000,:]"
      ],
      "metadata": {
        "id": "AvtkEzJWu9a6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "outputId": "a693ebf0-1ba9-4a26-8994-e85bec29fcf0",
        "id": "gVebQZwdu9a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            0         1         2         3         4         5         6    \\\n",
              "2160  -0.067399 -0.055313 -0.074646 -0.114919 -0.088871 -0.111103 -0.079715   \n",
              "20919 -0.039001 -0.049287 -0.064703 -0.075447 -0.068599 -0.085801 -0.038972   \n",
              "973    0.013016 -0.011119 -0.039795  0.015698 -0.013839 -0.056547 -0.028326   \n",
              "1238  -0.012839 -0.007891 -0.000335 -0.017209 -0.042385  0.004278 -0.018220   \n",
              "4149   0.162093  0.259302  0.278531  0.214980  0.245281  0.184952  0.160723   \n",
              "...         ...       ...       ...       ...       ...       ...       ...   \n",
              "8202  -0.006040 -0.023679 -0.003979  0.004199  0.014016  0.003021 -0.013575   \n",
              "5298  -0.234748 -0.194218 -0.143088 -0.116647 -0.096146 -0.075835 -0.060663   \n",
              "4371  -0.042573 -0.026527 -0.038931 -0.040005 -0.065119 -0.035112  0.002794   \n",
              "3576   0.039162 -0.001734  0.004371 -0.030724  0.003882  0.129788  0.027695   \n",
              "2246  -0.037558 -0.028723 -0.035768  0.002306 -0.049590 -0.022046  0.006337   \n",
              "\n",
              "            7         8         9    ...       247       248       249  \\\n",
              "2160  -0.080806 -0.070167 -0.029497  ...  0.017222  0.009899 -0.004473   \n",
              "20919 -0.034171 -0.073489 -0.067426  ... -0.050403 -0.042220 -0.078167   \n",
              "973    0.008265 -0.002105 -0.010835  ... -0.108325 -0.112022 -0.108531   \n",
              "1238   0.001170 -0.015320 -0.020522  ... -0.041861 -0.045602 -0.031203   \n",
              "4149   0.035515 -0.023882 -0.040479  ... -0.051584 -0.040018 -0.069031   \n",
              "...         ...       ...       ...  ...       ...       ...       ...   \n",
              "8202  -0.007312 -0.046171 -0.016271  ... -0.066009 -0.075146 -0.032192   \n",
              "5298  -0.047532 -0.003239 -0.032327  ... -0.004386 -0.045173 -0.044530   \n",
              "4371  -0.054849 -0.045523 -0.051146  ...  0.046142  0.003203  0.054244   \n",
              "3576   0.035203  0.010411 -0.036581  ... -0.120116 -0.078387 -0.139557   \n",
              "2246  -0.037121 -0.009859 -0.001557  ...  0.073934  0.034510  0.027490   \n",
              "\n",
              "            250       251       252       253       254       255   256  \n",
              "2160   0.059794  0.014011 -0.047852 -0.048926 -0.075550 -0.053034  37.0  \n",
              "20919 -0.022235 -0.015043 -0.028401 -0.049329 -0.014087 -0.047666  88.0  \n",
              "973   -0.119081 -0.121462 -0.090844 -0.160078 -0.088232 -0.114928  17.0  \n",
              "1238   0.011176 -0.056984 -0.035815 -0.028645 -0.029925 -0.026754   6.0  \n",
              "4149  -0.051542 -0.050382 -0.028941 -0.031798 -0.047034 -0.036189  70.0  \n",
              "...         ...       ...       ...       ...       ...       ...   ...  \n",
              "8202   0.024472  0.087386  0.153660  0.207485  0.299180  0.282465  35.0  \n",
              "5298  -0.004445 -0.020140 -0.004285 -0.043359 -0.019732 -0.003674  89.0  \n",
              "4371   0.003735  0.000176 -0.011942  0.004859  0.008150 -0.002779  73.0  \n",
              "3576  -0.167027 -0.096697 -0.065567 -0.172236 -0.129005 -0.096974  15.0  \n",
              "2246  -0.025038  0.002396  0.033644 -0.058196 -0.029304 -0.041908  38.0  \n",
              "\n",
              "[24000 rows x 257 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-30e4542f-fe0b-4d28-b890-48c380d9e7bf\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>247</th>\n",
              "      <th>248</th>\n",
              "      <th>249</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "      <th>256</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2160</th>\n",
              "      <td>-0.067399</td>\n",
              "      <td>-0.055313</td>\n",
              "      <td>-0.074646</td>\n",
              "      <td>-0.114919</td>\n",
              "      <td>-0.088871</td>\n",
              "      <td>-0.111103</td>\n",
              "      <td>-0.079715</td>\n",
              "      <td>-0.080806</td>\n",
              "      <td>-0.070167</td>\n",
              "      <td>-0.029497</td>\n",
              "      <td>...</td>\n",
              "      <td>0.017222</td>\n",
              "      <td>0.009899</td>\n",
              "      <td>-0.004473</td>\n",
              "      <td>0.059794</td>\n",
              "      <td>0.014011</td>\n",
              "      <td>-0.047852</td>\n",
              "      <td>-0.048926</td>\n",
              "      <td>-0.075550</td>\n",
              "      <td>-0.053034</td>\n",
              "      <td>37.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20919</th>\n",
              "      <td>-0.039001</td>\n",
              "      <td>-0.049287</td>\n",
              "      <td>-0.064703</td>\n",
              "      <td>-0.075447</td>\n",
              "      <td>-0.068599</td>\n",
              "      <td>-0.085801</td>\n",
              "      <td>-0.038972</td>\n",
              "      <td>-0.034171</td>\n",
              "      <td>-0.073489</td>\n",
              "      <td>-0.067426</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.050403</td>\n",
              "      <td>-0.042220</td>\n",
              "      <td>-0.078167</td>\n",
              "      <td>-0.022235</td>\n",
              "      <td>-0.015043</td>\n",
              "      <td>-0.028401</td>\n",
              "      <td>-0.049329</td>\n",
              "      <td>-0.014087</td>\n",
              "      <td>-0.047666</td>\n",
              "      <td>88.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>973</th>\n",
              "      <td>0.013016</td>\n",
              "      <td>-0.011119</td>\n",
              "      <td>-0.039795</td>\n",
              "      <td>0.015698</td>\n",
              "      <td>-0.013839</td>\n",
              "      <td>-0.056547</td>\n",
              "      <td>-0.028326</td>\n",
              "      <td>0.008265</td>\n",
              "      <td>-0.002105</td>\n",
              "      <td>-0.010835</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.108325</td>\n",
              "      <td>-0.112022</td>\n",
              "      <td>-0.108531</td>\n",
              "      <td>-0.119081</td>\n",
              "      <td>-0.121462</td>\n",
              "      <td>-0.090844</td>\n",
              "      <td>-0.160078</td>\n",
              "      <td>-0.088232</td>\n",
              "      <td>-0.114928</td>\n",
              "      <td>17.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1238</th>\n",
              "      <td>-0.012839</td>\n",
              "      <td>-0.007891</td>\n",
              "      <td>-0.000335</td>\n",
              "      <td>-0.017209</td>\n",
              "      <td>-0.042385</td>\n",
              "      <td>0.004278</td>\n",
              "      <td>-0.018220</td>\n",
              "      <td>0.001170</td>\n",
              "      <td>-0.015320</td>\n",
              "      <td>-0.020522</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.041861</td>\n",
              "      <td>-0.045602</td>\n",
              "      <td>-0.031203</td>\n",
              "      <td>0.011176</td>\n",
              "      <td>-0.056984</td>\n",
              "      <td>-0.035815</td>\n",
              "      <td>-0.028645</td>\n",
              "      <td>-0.029925</td>\n",
              "      <td>-0.026754</td>\n",
              "      <td>6.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4149</th>\n",
              "      <td>0.162093</td>\n",
              "      <td>0.259302</td>\n",
              "      <td>0.278531</td>\n",
              "      <td>0.214980</td>\n",
              "      <td>0.245281</td>\n",
              "      <td>0.184952</td>\n",
              "      <td>0.160723</td>\n",
              "      <td>0.035515</td>\n",
              "      <td>-0.023882</td>\n",
              "      <td>-0.040479</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.051584</td>\n",
              "      <td>-0.040018</td>\n",
              "      <td>-0.069031</td>\n",
              "      <td>-0.051542</td>\n",
              "      <td>-0.050382</td>\n",
              "      <td>-0.028941</td>\n",
              "      <td>-0.031798</td>\n",
              "      <td>-0.047034</td>\n",
              "      <td>-0.036189</td>\n",
              "      <td>70.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8202</th>\n",
              "      <td>-0.006040</td>\n",
              "      <td>-0.023679</td>\n",
              "      <td>-0.003979</td>\n",
              "      <td>0.004199</td>\n",
              "      <td>0.014016</td>\n",
              "      <td>0.003021</td>\n",
              "      <td>-0.013575</td>\n",
              "      <td>-0.007312</td>\n",
              "      <td>-0.046171</td>\n",
              "      <td>-0.016271</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.066009</td>\n",
              "      <td>-0.075146</td>\n",
              "      <td>-0.032192</td>\n",
              "      <td>0.024472</td>\n",
              "      <td>0.087386</td>\n",
              "      <td>0.153660</td>\n",
              "      <td>0.207485</td>\n",
              "      <td>0.299180</td>\n",
              "      <td>0.282465</td>\n",
              "      <td>35.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5298</th>\n",
              "      <td>-0.234748</td>\n",
              "      <td>-0.194218</td>\n",
              "      <td>-0.143088</td>\n",
              "      <td>-0.116647</td>\n",
              "      <td>-0.096146</td>\n",
              "      <td>-0.075835</td>\n",
              "      <td>-0.060663</td>\n",
              "      <td>-0.047532</td>\n",
              "      <td>-0.003239</td>\n",
              "      <td>-0.032327</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.004386</td>\n",
              "      <td>-0.045173</td>\n",
              "      <td>-0.044530</td>\n",
              "      <td>-0.004445</td>\n",
              "      <td>-0.020140</td>\n",
              "      <td>-0.004285</td>\n",
              "      <td>-0.043359</td>\n",
              "      <td>-0.019732</td>\n",
              "      <td>-0.003674</td>\n",
              "      <td>89.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4371</th>\n",
              "      <td>-0.042573</td>\n",
              "      <td>-0.026527</td>\n",
              "      <td>-0.038931</td>\n",
              "      <td>-0.040005</td>\n",
              "      <td>-0.065119</td>\n",
              "      <td>-0.035112</td>\n",
              "      <td>0.002794</td>\n",
              "      <td>-0.054849</td>\n",
              "      <td>-0.045523</td>\n",
              "      <td>-0.051146</td>\n",
              "      <td>...</td>\n",
              "      <td>0.046142</td>\n",
              "      <td>0.003203</td>\n",
              "      <td>0.054244</td>\n",
              "      <td>0.003735</td>\n",
              "      <td>0.000176</td>\n",
              "      <td>-0.011942</td>\n",
              "      <td>0.004859</td>\n",
              "      <td>0.008150</td>\n",
              "      <td>-0.002779</td>\n",
              "      <td>73.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3576</th>\n",
              "      <td>0.039162</td>\n",
              "      <td>-0.001734</td>\n",
              "      <td>0.004371</td>\n",
              "      <td>-0.030724</td>\n",
              "      <td>0.003882</td>\n",
              "      <td>0.129788</td>\n",
              "      <td>0.027695</td>\n",
              "      <td>0.035203</td>\n",
              "      <td>0.010411</td>\n",
              "      <td>-0.036581</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.120116</td>\n",
              "      <td>-0.078387</td>\n",
              "      <td>-0.139557</td>\n",
              "      <td>-0.167027</td>\n",
              "      <td>-0.096697</td>\n",
              "      <td>-0.065567</td>\n",
              "      <td>-0.172236</td>\n",
              "      <td>-0.129005</td>\n",
              "      <td>-0.096974</td>\n",
              "      <td>15.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2246</th>\n",
              "      <td>-0.037558</td>\n",
              "      <td>-0.028723</td>\n",
              "      <td>-0.035768</td>\n",
              "      <td>0.002306</td>\n",
              "      <td>-0.049590</td>\n",
              "      <td>-0.022046</td>\n",
              "      <td>0.006337</td>\n",
              "      <td>-0.037121</td>\n",
              "      <td>-0.009859</td>\n",
              "      <td>-0.001557</td>\n",
              "      <td>...</td>\n",
              "      <td>0.073934</td>\n",
              "      <td>0.034510</td>\n",
              "      <td>0.027490</td>\n",
              "      <td>-0.025038</td>\n",
              "      <td>0.002396</td>\n",
              "      <td>0.033644</td>\n",
              "      <td>-0.058196</td>\n",
              "      <td>-0.029304</td>\n",
              "      <td>-0.041908</td>\n",
              "      <td>38.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>24000 rows × 257 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-30e4542f-fe0b-4d28-b890-48c380d9e7bf')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-30e4542f-fe0b-4d28-b890-48c380d9e7bf button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-30e4542f-fe0b-4d28-b890-48c380d9e7bf');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 전처리"
      ],
      "metadata": {
        "id": "YwbRDs03u9a7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = to_categorical(data_train[256])\n",
        "y_test = to_categorical(data_test[256])\n",
        "y_val = to_categorical(data_val[256])\n",
        "y_train[23999]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0dc6d115-4dba-4585-bbc4-b9cbb86368b8",
        "id": "vmLBSA6mu9a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_train.drop([256], axis=1, inplace=True)\n",
        "data_test.drop([256], axis=1, inplace=True)\n",
        "data_val.drop([256], axis=1, inplace=True)\n",
        "data_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "outputId": "24efc099-547d-468b-9675-73fb218863cb",
        "id": "63uCXv0Mu9a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py:4906: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  return super().drop(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            0         1         2         3         4         5         6    \\\n",
              "2160  -0.067399 -0.055313 -0.074646 -0.114919 -0.088871 -0.111103 -0.079715   \n",
              "20919 -0.039001 -0.049287 -0.064703 -0.075447 -0.068599 -0.085801 -0.038972   \n",
              "973    0.013016 -0.011119 -0.039795  0.015698 -0.013839 -0.056547 -0.028326   \n",
              "1238  -0.012839 -0.007891 -0.000335 -0.017209 -0.042385  0.004278 -0.018220   \n",
              "4149   0.162093  0.259302  0.278531  0.214980  0.245281  0.184952  0.160723   \n",
              "...         ...       ...       ...       ...       ...       ...       ...   \n",
              "8202  -0.006040 -0.023679 -0.003979  0.004199  0.014016  0.003021 -0.013575   \n",
              "5298  -0.234748 -0.194218 -0.143088 -0.116647 -0.096146 -0.075835 -0.060663   \n",
              "4371  -0.042573 -0.026527 -0.038931 -0.040005 -0.065119 -0.035112  0.002794   \n",
              "3576   0.039162 -0.001734  0.004371 -0.030724  0.003882  0.129788  0.027695   \n",
              "2246  -0.037558 -0.028723 -0.035768  0.002306 -0.049590 -0.022046  0.006337   \n",
              "\n",
              "            7         8         9    ...       246       247       248  \\\n",
              "2160  -0.080806 -0.070167 -0.029497  ...  0.018093  0.017222  0.009899   \n",
              "20919 -0.034171 -0.073489 -0.067426  ... -0.120486 -0.050403 -0.042220   \n",
              "973    0.008265 -0.002105 -0.010835  ... -0.084478 -0.108325 -0.112022   \n",
              "1238   0.001170 -0.015320 -0.020522  ... -0.037809 -0.041861 -0.045602   \n",
              "4149   0.035515 -0.023882 -0.040479  ...  0.028871 -0.051584 -0.040018   \n",
              "...         ...       ...       ...  ...       ...       ...       ...   \n",
              "8202  -0.007312 -0.046171 -0.016271  ... -0.098162 -0.066009 -0.075146   \n",
              "5298  -0.047532 -0.003239 -0.032327  ... -0.024578 -0.004386 -0.045173   \n",
              "4371  -0.054849 -0.045523 -0.051146  ...  0.037911  0.046142  0.003203   \n",
              "3576   0.035203  0.010411 -0.036581  ... -0.109445 -0.120116 -0.078387   \n",
              "2246  -0.037121 -0.009859 -0.001557  ...  0.059359  0.073934  0.034510   \n",
              "\n",
              "            249       250       251       252       253       254       255  \n",
              "2160  -0.004473  0.059794  0.014011 -0.047852 -0.048926 -0.075550 -0.053034  \n",
              "20919 -0.078167 -0.022235 -0.015043 -0.028401 -0.049329 -0.014087 -0.047666  \n",
              "973   -0.108531 -0.119081 -0.121462 -0.090844 -0.160078 -0.088232 -0.114928  \n",
              "1238  -0.031203  0.011176 -0.056984 -0.035815 -0.028645 -0.029925 -0.026754  \n",
              "4149  -0.069031 -0.051542 -0.050382 -0.028941 -0.031798 -0.047034 -0.036189  \n",
              "...         ...       ...       ...       ...       ...       ...       ...  \n",
              "8202  -0.032192  0.024472  0.087386  0.153660  0.207485  0.299180  0.282465  \n",
              "5298  -0.044530 -0.004445 -0.020140 -0.004285 -0.043359 -0.019732 -0.003674  \n",
              "4371   0.054244  0.003735  0.000176 -0.011942  0.004859  0.008150 -0.002779  \n",
              "3576  -0.139557 -0.167027 -0.096697 -0.065567 -0.172236 -0.129005 -0.096974  \n",
              "2246   0.027490 -0.025038  0.002396  0.033644 -0.058196 -0.029304 -0.041908  \n",
              "\n",
              "[24000 rows x 256 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c3ef7109-733c-4108-8534-3b18bf5b43ea\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>246</th>\n",
              "      <th>247</th>\n",
              "      <th>248</th>\n",
              "      <th>249</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2160</th>\n",
              "      <td>-0.067399</td>\n",
              "      <td>-0.055313</td>\n",
              "      <td>-0.074646</td>\n",
              "      <td>-0.114919</td>\n",
              "      <td>-0.088871</td>\n",
              "      <td>-0.111103</td>\n",
              "      <td>-0.079715</td>\n",
              "      <td>-0.080806</td>\n",
              "      <td>-0.070167</td>\n",
              "      <td>-0.029497</td>\n",
              "      <td>...</td>\n",
              "      <td>0.018093</td>\n",
              "      <td>0.017222</td>\n",
              "      <td>0.009899</td>\n",
              "      <td>-0.004473</td>\n",
              "      <td>0.059794</td>\n",
              "      <td>0.014011</td>\n",
              "      <td>-0.047852</td>\n",
              "      <td>-0.048926</td>\n",
              "      <td>-0.075550</td>\n",
              "      <td>-0.053034</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20919</th>\n",
              "      <td>-0.039001</td>\n",
              "      <td>-0.049287</td>\n",
              "      <td>-0.064703</td>\n",
              "      <td>-0.075447</td>\n",
              "      <td>-0.068599</td>\n",
              "      <td>-0.085801</td>\n",
              "      <td>-0.038972</td>\n",
              "      <td>-0.034171</td>\n",
              "      <td>-0.073489</td>\n",
              "      <td>-0.067426</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.120486</td>\n",
              "      <td>-0.050403</td>\n",
              "      <td>-0.042220</td>\n",
              "      <td>-0.078167</td>\n",
              "      <td>-0.022235</td>\n",
              "      <td>-0.015043</td>\n",
              "      <td>-0.028401</td>\n",
              "      <td>-0.049329</td>\n",
              "      <td>-0.014087</td>\n",
              "      <td>-0.047666</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>973</th>\n",
              "      <td>0.013016</td>\n",
              "      <td>-0.011119</td>\n",
              "      <td>-0.039795</td>\n",
              "      <td>0.015698</td>\n",
              "      <td>-0.013839</td>\n",
              "      <td>-0.056547</td>\n",
              "      <td>-0.028326</td>\n",
              "      <td>0.008265</td>\n",
              "      <td>-0.002105</td>\n",
              "      <td>-0.010835</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.084478</td>\n",
              "      <td>-0.108325</td>\n",
              "      <td>-0.112022</td>\n",
              "      <td>-0.108531</td>\n",
              "      <td>-0.119081</td>\n",
              "      <td>-0.121462</td>\n",
              "      <td>-0.090844</td>\n",
              "      <td>-0.160078</td>\n",
              "      <td>-0.088232</td>\n",
              "      <td>-0.114928</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1238</th>\n",
              "      <td>-0.012839</td>\n",
              "      <td>-0.007891</td>\n",
              "      <td>-0.000335</td>\n",
              "      <td>-0.017209</td>\n",
              "      <td>-0.042385</td>\n",
              "      <td>0.004278</td>\n",
              "      <td>-0.018220</td>\n",
              "      <td>0.001170</td>\n",
              "      <td>-0.015320</td>\n",
              "      <td>-0.020522</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.037809</td>\n",
              "      <td>-0.041861</td>\n",
              "      <td>-0.045602</td>\n",
              "      <td>-0.031203</td>\n",
              "      <td>0.011176</td>\n",
              "      <td>-0.056984</td>\n",
              "      <td>-0.035815</td>\n",
              "      <td>-0.028645</td>\n",
              "      <td>-0.029925</td>\n",
              "      <td>-0.026754</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4149</th>\n",
              "      <td>0.162093</td>\n",
              "      <td>0.259302</td>\n",
              "      <td>0.278531</td>\n",
              "      <td>0.214980</td>\n",
              "      <td>0.245281</td>\n",
              "      <td>0.184952</td>\n",
              "      <td>0.160723</td>\n",
              "      <td>0.035515</td>\n",
              "      <td>-0.023882</td>\n",
              "      <td>-0.040479</td>\n",
              "      <td>...</td>\n",
              "      <td>0.028871</td>\n",
              "      <td>-0.051584</td>\n",
              "      <td>-0.040018</td>\n",
              "      <td>-0.069031</td>\n",
              "      <td>-0.051542</td>\n",
              "      <td>-0.050382</td>\n",
              "      <td>-0.028941</td>\n",
              "      <td>-0.031798</td>\n",
              "      <td>-0.047034</td>\n",
              "      <td>-0.036189</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8202</th>\n",
              "      <td>-0.006040</td>\n",
              "      <td>-0.023679</td>\n",
              "      <td>-0.003979</td>\n",
              "      <td>0.004199</td>\n",
              "      <td>0.014016</td>\n",
              "      <td>0.003021</td>\n",
              "      <td>-0.013575</td>\n",
              "      <td>-0.007312</td>\n",
              "      <td>-0.046171</td>\n",
              "      <td>-0.016271</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.098162</td>\n",
              "      <td>-0.066009</td>\n",
              "      <td>-0.075146</td>\n",
              "      <td>-0.032192</td>\n",
              "      <td>0.024472</td>\n",
              "      <td>0.087386</td>\n",
              "      <td>0.153660</td>\n",
              "      <td>0.207485</td>\n",
              "      <td>0.299180</td>\n",
              "      <td>0.282465</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5298</th>\n",
              "      <td>-0.234748</td>\n",
              "      <td>-0.194218</td>\n",
              "      <td>-0.143088</td>\n",
              "      <td>-0.116647</td>\n",
              "      <td>-0.096146</td>\n",
              "      <td>-0.075835</td>\n",
              "      <td>-0.060663</td>\n",
              "      <td>-0.047532</td>\n",
              "      <td>-0.003239</td>\n",
              "      <td>-0.032327</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.024578</td>\n",
              "      <td>-0.004386</td>\n",
              "      <td>-0.045173</td>\n",
              "      <td>-0.044530</td>\n",
              "      <td>-0.004445</td>\n",
              "      <td>-0.020140</td>\n",
              "      <td>-0.004285</td>\n",
              "      <td>-0.043359</td>\n",
              "      <td>-0.019732</td>\n",
              "      <td>-0.003674</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4371</th>\n",
              "      <td>-0.042573</td>\n",
              "      <td>-0.026527</td>\n",
              "      <td>-0.038931</td>\n",
              "      <td>-0.040005</td>\n",
              "      <td>-0.065119</td>\n",
              "      <td>-0.035112</td>\n",
              "      <td>0.002794</td>\n",
              "      <td>-0.054849</td>\n",
              "      <td>-0.045523</td>\n",
              "      <td>-0.051146</td>\n",
              "      <td>...</td>\n",
              "      <td>0.037911</td>\n",
              "      <td>0.046142</td>\n",
              "      <td>0.003203</td>\n",
              "      <td>0.054244</td>\n",
              "      <td>0.003735</td>\n",
              "      <td>0.000176</td>\n",
              "      <td>-0.011942</td>\n",
              "      <td>0.004859</td>\n",
              "      <td>0.008150</td>\n",
              "      <td>-0.002779</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3576</th>\n",
              "      <td>0.039162</td>\n",
              "      <td>-0.001734</td>\n",
              "      <td>0.004371</td>\n",
              "      <td>-0.030724</td>\n",
              "      <td>0.003882</td>\n",
              "      <td>0.129788</td>\n",
              "      <td>0.027695</td>\n",
              "      <td>0.035203</td>\n",
              "      <td>0.010411</td>\n",
              "      <td>-0.036581</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.109445</td>\n",
              "      <td>-0.120116</td>\n",
              "      <td>-0.078387</td>\n",
              "      <td>-0.139557</td>\n",
              "      <td>-0.167027</td>\n",
              "      <td>-0.096697</td>\n",
              "      <td>-0.065567</td>\n",
              "      <td>-0.172236</td>\n",
              "      <td>-0.129005</td>\n",
              "      <td>-0.096974</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2246</th>\n",
              "      <td>-0.037558</td>\n",
              "      <td>-0.028723</td>\n",
              "      <td>-0.035768</td>\n",
              "      <td>0.002306</td>\n",
              "      <td>-0.049590</td>\n",
              "      <td>-0.022046</td>\n",
              "      <td>0.006337</td>\n",
              "      <td>-0.037121</td>\n",
              "      <td>-0.009859</td>\n",
              "      <td>-0.001557</td>\n",
              "      <td>...</td>\n",
              "      <td>0.059359</td>\n",
              "      <td>0.073934</td>\n",
              "      <td>0.034510</td>\n",
              "      <td>0.027490</td>\n",
              "      <td>-0.025038</td>\n",
              "      <td>0.002396</td>\n",
              "      <td>0.033644</td>\n",
              "      <td>-0.058196</td>\n",
              "      <td>-0.029304</td>\n",
              "      <td>-0.041908</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>24000 rows × 256 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c3ef7109-733c-4108-8534-3b18bf5b43ea')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c3ef7109-733c-4108-8534-3b18bf5b43ea button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c3ef7109-733c-4108-8534-3b18bf5b43ea');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train=data_train.values\n",
        "X_test=data_test.values\n",
        "X_val=data_val.values"
      ],
      "metadata": {
        "id": "PytXfxUzu9a7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(X_val.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)\n",
        "print(y_val.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17cc8578-61ba-4dd4-81e1-dd084aa0e3ab",
        "id": "rkKYvKTOu9a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(24000, 256)\n",
            "(6000, 256)\n",
            "(6000, 256)\n",
            "(24000, 101)\n",
            "(6000, 101)\n",
            "(6000, 101)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X_train = X_train.reshape(24000, 256, 1)\n",
        "X_test = X_test.reshape(6000, 256, 1)\n",
        "X_val = X_val.reshape(6000, 256, 1)\n",
        "X_train.shape, X_test.shape, X_val.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf10b8f0-1b07-4548-b56a-f43571e6ad5f",
        "id": "zWwjM0s4u9a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((24000, 256, 1), (6000, 256, 1), (6000, 256, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***gru모델(이 모델을 실행했습니다)***"
      ],
      "metadata": {
        "id": "q1pJydE3u9a8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(GRU(units=100, return_sequences=True, input_shape=(256,1))) #shape[1]=열\n",
        "model.add(GRU(units=50, return_sequences=True))\n",
        "model.add(GRU(units=50, return_sequences=True))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Dense(101, activation='softmax'))\n",
        "model.compile(optimizer='adam', loss='mse',metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a91bcb1-a338-461c-c221-8087ce00e0cd",
        "id": "90hodE8Tu9a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_12\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " gru_34 (GRU)                (None, 256, 100)          30900     \n",
            "                                                                 \n",
            " gru_35 (GRU)                (None, 256, 50)           22800     \n",
            "                                                                 \n",
            " gru_36 (GRU)                (None, 256, 50)           15300     \n",
            "                                                                 \n",
            " flatten_7 (Flatten)         (None, 12800)             0         \n",
            "                                                                 \n",
            " dropout_15 (Dropout)        (None, 12800)             0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 101)               1292901   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,361,901\n",
            "Trainable params: 1,361,901\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## cnn"
      ],
      "metadata": {
        "id": "_Pp5u4elu9a7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model():\n",
        "    model = Sequential()\n",
        "    model.add(Conv1D(filters=64, kernel_size=6, activation='relu', \n",
        "                    padding='same', input_shape=(256, 1)))\n",
        "    model.add(BatchNormalization())\n",
        "    \n",
        "    # adding a pooling layer\n",
        "    model.add(MaxPooling1D(pool_size=(3), strides=2, padding='same'))\n",
        "    \n",
        "    model.add(Conv1D(filters=64, kernel_size=6, activation='relu', \n",
        "                    padding='same', input_shape=(256, 1)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling1D(pool_size=(3), strides=2, padding='same'))\n",
        "    \n",
        "    model.add(Conv1D(filters=64, kernel_size=6, activation='relu', \n",
        "                    padding='same', input_shape=(256, 1)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling1D(pool_size=(3), strides=2, padding='same'))\n",
        "    \n",
        "    model.add(Flatten())\n",
        "    #model.add(Dense(64, activation='relu'))\n",
        "    #model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(101, activation='softmax'))\n",
        "    \n",
        "    model.compile(optimizer='adam', loss='mse',metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "ZM1vFO65u9a8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model()\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2397946-5a7a-4f1f-e09b-f35229db7ab7",
        "id": "cIjsuzyhu9a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_18\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_17 (Conv1D)          (None, 256, 64)           448       \n",
            "                                                                 \n",
            " batch_normalization_12 (Bat  (None, 256, 64)          256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling1d_17 (MaxPoolin  (None, 128, 64)          0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_18 (Conv1D)          (None, 128, 64)           24640     \n",
            "                                                                 \n",
            " batch_normalization_13 (Bat  (None, 128, 64)          256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling1d_18 (MaxPoolin  (None, 64, 64)           0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_19 (Conv1D)          (None, 64, 64)            24640     \n",
            "                                                                 \n",
            " batch_normalization_14 (Bat  (None, 64, 64)           256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling1d_19 (MaxPoolin  (None, 32, 64)           0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " flatten_13 (Flatten)        (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_24 (Dense)            (None, 100)               204900    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 255,396\n",
            "Trainable params: 255,012\n",
            "Non-trainable params: 384\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## cnn-gru"
      ],
      "metadata": {
        "id": "uolWtZKZu9a8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Input, Embedding, GRU, LSTM, MaxPooling1D, GlobalMaxPool1D, CuDNNGRU\n",
        "from keras.layers import Dropout, Dense, Activation, Flatten, Conv1D, SpatialDropout1D\n",
        "from keras.models import Sequential\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv1D(filters=64, kernel_size=6, activation='relu', padding='same', input_shape=(256, 1)))\n",
        "model.add(SpatialDropout1D(0.2))\n",
        "model.add(MaxPooling1D(4))\n",
        "model.add(CuDNNGRU(64))\n",
        "\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Dense(101, activation='softmax'))\n",
        "model.compile(optimizer='adam', loss='mse',metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33f77877-e599-4ded-8400-373f5cec14bc",
        "id": "XM37Hpdlu9a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_13 (Conv1D)          (None, 256, 64)           448       \n",
            "                                                                 \n",
            " spatial_dropout1d_4 (Spatia  (None, 256, 64)          0         \n",
            " lDropout1D)                                                     \n",
            "                                                                 \n",
            " max_pooling1d_13 (MaxPoolin  (None, 64, 64)           0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " cu_dnngru_4 (CuDNNGRU)      (None, 64)                24960     \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 101)               6565      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 31,973\n",
            "Trainable params: 31,973\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSTM"
      ],
      "metadata": {
        "id": "vDWxnMP_u9a8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(units=100, return_sequences=True, input_shape=(256,1))) #shape[1]=열\n",
        "model.add(LSTM(units=50, return_sequences=True))\n",
        "model.add(LSTM(units=50, return_sequences=True))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Dense(101, activation='softmax'))\n",
        "model.compile(optimizer='adam', loss='mse',metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8055881e-f18e-4941-e7d3-1d853b39d2e3",
        "id": "J1TLq8qju9a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_3 (LSTM)               (None, 256, 100)          40800     \n",
            "                                                                 \n",
            " lstm_4 (LSTM)               (None, 256, 50)           30200     \n",
            "                                                                 \n",
            " lstm_5 (LSTM)               (None, 256, 50)           20200     \n",
            "                                                                 \n",
            " flatten_6 (Flatten)         (None, 12800)             0         \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 12800)             0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 101)               1292901   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,384,101\n",
            "Trainable params: 1,384,101\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RNN"
      ],
      "metadata": {
        "id": "NCK6kHrRu9a9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Input, Embedding, GRU, LSTM, SimpleRNN, MaxPooling1D, GlobalMaxPool1D, CuDNNGRU\n",
        "from keras.layers import Dropout, Dense, Activation, Flatten, Conv1D, SpatialDropout1D\n",
        "from keras.models import Sequential\n",
        "\n",
        "model = Sequential()\n",
        "model.add(SimpleRNN(units=100, return_sequences=True, input_shape=(256,1))) #shape[1]=열\n",
        "model.add(SimpleRNN(units=50, return_sequences=True))\n",
        "model.add(SimpleRNN(units=50, return_sequences=True))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(101, activation='softmax'))\n",
        "model.compile(optimizer='adam', loss='mse',metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f56aa04-4c34-4860-fc44-299596d4f96f",
        "id": "3_RikgPFu9a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " simple_rnn_12 (SimpleRNN)   (None, 256, 100)          10200     \n",
            "                                                                 \n",
            " simple_rnn_13 (SimpleRNN)   (None, 256, 50)           7550      \n",
            "                                                                 \n",
            " simple_rnn_14 (SimpleRNN)   (None, 256, 50)           5050      \n",
            "                                                                 \n",
            " flatten_4 (Flatten)         (None, 12800)             0         \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 12800)             0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 101)               1292901   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,315,701\n",
            "Trainable params: 1,315,701\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델학습/평가"
      ],
      "metadata": {
        "id": "ESqvgOV9u9a9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train, y_train, batch_size = 512, epochs = 1000, verbose = 1, validation_data=(X_val, y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ba770aa-03a5-42d9-8c36-94c14223cf28",
        "id": "EBLzTT1du9a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "47/47 [==============================] - 8s 92ms/step - loss: 0.0098 - accuracy: 0.0096 - val_loss: 0.0098 - val_accuracy: 0.0120\n",
            "Epoch 2/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0098 - accuracy: 0.0097 - val_loss: 0.0098 - val_accuracy: 0.0120\n",
            "Epoch 3/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0098 - accuracy: 0.0122 - val_loss: 0.0098 - val_accuracy: 0.0107\n",
            "Epoch 4/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0098 - accuracy: 0.0127 - val_loss: 0.0098 - val_accuracy: 0.0118\n",
            "Epoch 5/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 0.0098 - accuracy: 0.0175 - val_loss: 0.0098 - val_accuracy: 0.0160\n",
            "Epoch 6/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 0.0098 - accuracy: 0.0265 - val_loss: 0.0098 - val_accuracy: 0.0245\n",
            "Epoch 7/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 0.0097 - accuracy: 0.0429 - val_loss: 0.0098 - val_accuracy: 0.0290\n",
            "Epoch 8/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 0.0097 - accuracy: 0.0541 - val_loss: 0.0098 - val_accuracy: 0.0432\n",
            "Epoch 9/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 0.0097 - accuracy: 0.0633 - val_loss: 0.0097 - val_accuracy: 0.0455\n",
            "Epoch 10/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 0.0096 - accuracy: 0.0799 - val_loss: 0.0098 - val_accuracy: 0.0548\n",
            "Epoch 11/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 0.0096 - accuracy: 0.0955 - val_loss: 0.0097 - val_accuracy: 0.0662\n",
            "Epoch 12/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0095 - accuracy: 0.1093 - val_loss: 0.0097 - val_accuracy: 0.0710\n",
            "Epoch 13/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0094 - accuracy: 0.1283 - val_loss: 0.0096 - val_accuracy: 0.1058\n",
            "Epoch 14/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0093 - accuracy: 0.1513 - val_loss: 0.0096 - val_accuracy: 0.1112\n",
            "Epoch 15/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0092 - accuracy: 0.1754 - val_loss: 0.0095 - val_accuracy: 0.1372\n",
            "Epoch 16/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0090 - accuracy: 0.1988 - val_loss: 0.0095 - val_accuracy: 0.1482\n",
            "Epoch 17/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0089 - accuracy: 0.2200 - val_loss: 0.0093 - val_accuracy: 0.1613\n",
            "Epoch 18/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0087 - accuracy: 0.2433 - val_loss: 0.0093 - val_accuracy: 0.1823\n",
            "Epoch 19/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0086 - accuracy: 0.2642 - val_loss: 0.0091 - val_accuracy: 0.1993\n",
            "Epoch 20/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0083 - accuracy: 0.2905 - val_loss: 0.0090 - val_accuracy: 0.2217\n",
            "Epoch 21/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0081 - accuracy: 0.3211 - val_loss: 0.0089 - val_accuracy: 0.2472\n",
            "Epoch 22/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0079 - accuracy: 0.3493 - val_loss: 0.0086 - val_accuracy: 0.2775\n",
            "Epoch 23/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0076 - accuracy: 0.3808 - val_loss: 0.0084 - val_accuracy: 0.3012\n",
            "Epoch 24/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0073 - accuracy: 0.4144 - val_loss: 0.0083 - val_accuracy: 0.3247\n",
            "Epoch 25/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0070 - accuracy: 0.4455 - val_loss: 0.0080 - val_accuracy: 0.3633\n",
            "Epoch 26/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0066 - accuracy: 0.4857 - val_loss: 0.0077 - val_accuracy: 0.3933\n",
            "Epoch 27/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 0.0062 - accuracy: 0.5212 - val_loss: 0.0074 - val_accuracy: 0.4297\n",
            "Epoch 28/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 0.0059 - accuracy: 0.5512 - val_loss: 0.0072 - val_accuracy: 0.4493\n",
            "Epoch 29/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 0.0054 - accuracy: 0.5902 - val_loss: 0.0066 - val_accuracy: 0.5018\n",
            "Epoch 30/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 0.0049 - accuracy: 0.6308 - val_loss: 0.0060 - val_accuracy: 0.5542\n",
            "Epoch 31/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 0.0044 - accuracy: 0.6702 - val_loss: 0.0056 - val_accuracy: 0.5882\n",
            "Epoch 32/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0040 - accuracy: 0.7034 - val_loss: 0.0053 - val_accuracy: 0.6163\n",
            "Epoch 33/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 0.0037 - accuracy: 0.7305 - val_loss: 0.0048 - val_accuracy: 0.6525\n",
            "Epoch 34/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 0.0032 - accuracy: 0.7677 - val_loss: 0.0044 - val_accuracy: 0.6858\n",
            "Epoch 35/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0028 - accuracy: 0.7983 - val_loss: 0.0041 - val_accuracy: 0.7048\n",
            "Epoch 36/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 0.0026 - accuracy: 0.8177 - val_loss: 0.0039 - val_accuracy: 0.7230\n",
            "Epoch 37/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0023 - accuracy: 0.8354 - val_loss: 0.0036 - val_accuracy: 0.7415\n",
            "Epoch 38/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 0.0020 - accuracy: 0.8554 - val_loss: 0.0034 - val_accuracy: 0.7593\n",
            "Epoch 39/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0018 - accuracy: 0.8733 - val_loss: 0.0032 - val_accuracy: 0.7727\n",
            "Epoch 40/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0017 - accuracy: 0.8823 - val_loss: 0.0030 - val_accuracy: 0.7897\n",
            "Epoch 41/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0015 - accuracy: 0.8930 - val_loss: 0.0030 - val_accuracy: 0.7908\n",
            "Epoch 42/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0015 - accuracy: 0.8978 - val_loss: 0.0028 - val_accuracy: 0.8022\n",
            "Epoch 43/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0013 - accuracy: 0.9126 - val_loss: 0.0026 - val_accuracy: 0.8217\n",
            "Epoch 44/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0011 - accuracy: 0.9213 - val_loss: 0.0025 - val_accuracy: 0.8247\n",
            "Epoch 45/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0010 - accuracy: 0.9287 - val_loss: 0.0026 - val_accuracy: 0.8163\n",
            "Epoch 46/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.7937e-04 - accuracy: 0.9317 - val_loss: 0.0023 - val_accuracy: 0.8408\n",
            "Epoch 47/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.9661e-04 - accuracy: 0.9376 - val_loss: 0.0025 - val_accuracy: 0.8307\n",
            "Epoch 48/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.1827e-04 - accuracy: 0.9431 - val_loss: 0.0022 - val_accuracy: 0.8477\n",
            "Epoch 49/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.5891e-04 - accuracy: 0.9475 - val_loss: 0.0022 - val_accuracy: 0.8510\n",
            "Epoch 50/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.0182e-04 - accuracy: 0.9513 - val_loss: 0.0021 - val_accuracy: 0.8538\n",
            "Epoch 51/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.1998e-04 - accuracy: 0.9565 - val_loss: 0.0020 - val_accuracy: 0.8652\n",
            "Epoch 52/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.6651e-04 - accuracy: 0.9598 - val_loss: 0.0020 - val_accuracy: 0.8678\n",
            "Epoch 53/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.5514e-04 - accuracy: 0.9614 - val_loss: 0.0019 - val_accuracy: 0.8658\n",
            "Epoch 54/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.3023e-04 - accuracy: 0.9634 - val_loss: 0.0020 - val_accuracy: 0.8700\n",
            "Epoch 55/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.0445e-04 - accuracy: 0.9654 - val_loss: 0.0019 - val_accuracy: 0.8695\n",
            "Epoch 56/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.6726e-04 - accuracy: 0.9676 - val_loss: 0.0018 - val_accuracy: 0.8770\n",
            "Epoch 57/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.2098e-04 - accuracy: 0.9707 - val_loss: 0.0019 - val_accuracy: 0.8742\n",
            "Epoch 58/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.0217e-04 - accuracy: 0.9721 - val_loss: 0.0019 - val_accuracy: 0.8737\n",
            "Epoch 59/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 4.0362e-04 - accuracy: 0.9718 - val_loss: 0.0018 - val_accuracy: 0.8818\n",
            "Epoch 60/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.9923e-04 - accuracy: 0.9726 - val_loss: 0.0018 - val_accuracy: 0.8740\n",
            "Epoch 61/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.8791e-04 - accuracy: 0.9732 - val_loss: 0.0017 - val_accuracy: 0.8843\n",
            "Epoch 62/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.4990e-04 - accuracy: 0.9753 - val_loss: 0.0017 - val_accuracy: 0.8848\n",
            "Epoch 63/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.4841e-04 - accuracy: 0.9758 - val_loss: 0.0017 - val_accuracy: 0.8812\n",
            "Epoch 64/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.3722e-04 - accuracy: 0.9761 - val_loss: 0.0018 - val_accuracy: 0.8815\n",
            "Epoch 65/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.4255e-04 - accuracy: 0.9758 - val_loss: 0.0018 - val_accuracy: 0.8797\n",
            "Epoch 66/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.4006e-04 - accuracy: 0.9764 - val_loss: 0.0018 - val_accuracy: 0.8788\n",
            "Epoch 67/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.7079e-04 - accuracy: 0.9748 - val_loss: 0.0019 - val_accuracy: 0.8740\n",
            "Epoch 68/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.6680e-04 - accuracy: 0.9753 - val_loss: 0.0019 - val_accuracy: 0.8738\n",
            "Epoch 69/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.5435e-04 - accuracy: 0.9768 - val_loss: 0.0019 - val_accuracy: 0.8755\n",
            "Epoch 70/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.4221e-04 - accuracy: 0.9770 - val_loss: 0.0018 - val_accuracy: 0.8828\n",
            "Epoch 71/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.2525e-04 - accuracy: 0.9781 - val_loss: 0.0019 - val_accuracy: 0.8765\n",
            "Epoch 72/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.2558e-04 - accuracy: 0.9785 - val_loss: 0.0018 - val_accuracy: 0.8837\n",
            "Epoch 73/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.2418e-04 - accuracy: 0.9778 - val_loss: 0.0018 - val_accuracy: 0.8827\n",
            "Epoch 74/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.2007e-04 - accuracy: 0.9783 - val_loss: 0.0018 - val_accuracy: 0.8790\n",
            "Epoch 75/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.0660e-04 - accuracy: 0.9790 - val_loss: 0.0018 - val_accuracy: 0.8828\n",
            "Epoch 76/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.9242e-04 - accuracy: 0.9804 - val_loss: 0.0017 - val_accuracy: 0.8892\n",
            "Epoch 77/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.1408e-04 - accuracy: 0.9790 - val_loss: 0.0017 - val_accuracy: 0.8890\n",
            "Epoch 78/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.1773e-04 - accuracy: 0.9791 - val_loss: 0.0018 - val_accuracy: 0.8788\n",
            "Epoch 79/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.1876e-04 - accuracy: 0.9787 - val_loss: 0.0017 - val_accuracy: 0.8882\n",
            "Epoch 80/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.9859e-04 - accuracy: 0.9801 - val_loss: 0.0017 - val_accuracy: 0.8888\n",
            "Epoch 81/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.8276e-04 - accuracy: 0.9815 - val_loss: 0.0017 - val_accuracy: 0.8895\n",
            "Epoch 82/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.6011e-04 - accuracy: 0.9825 - val_loss: 0.0018 - val_accuracy: 0.8845\n",
            "Epoch 83/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.7311e-04 - accuracy: 0.9820 - val_loss: 0.0017 - val_accuracy: 0.8855\n",
            "Epoch 84/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.7079e-04 - accuracy: 0.9820 - val_loss: 0.0016 - val_accuracy: 0.8937\n",
            "Epoch 85/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.6553e-04 - accuracy: 0.9825 - val_loss: 0.0016 - val_accuracy: 0.8948\n",
            "Epoch 86/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.7730e-04 - accuracy: 0.9821 - val_loss: 0.0018 - val_accuracy: 0.8828\n",
            "Epoch 87/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.7884e-04 - accuracy: 0.9824 - val_loss: 0.0017 - val_accuracy: 0.8903\n",
            "Epoch 88/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.5807e-04 - accuracy: 0.9828 - val_loss: 0.0016 - val_accuracy: 0.8972\n",
            "Epoch 89/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.5160e-04 - accuracy: 0.9834 - val_loss: 0.0016 - val_accuracy: 0.8917\n",
            "Epoch 90/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.4367e-04 - accuracy: 0.9841 - val_loss: 0.0016 - val_accuracy: 0.8943\n",
            "Epoch 91/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.5359e-04 - accuracy: 0.9835 - val_loss: 0.0016 - val_accuracy: 0.8947\n",
            "Epoch 92/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.2460e-04 - accuracy: 0.9854 - val_loss: 0.0016 - val_accuracy: 0.8992\n",
            "Epoch 93/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.0515e-04 - accuracy: 0.9865 - val_loss: 0.0016 - val_accuracy: 0.8992\n",
            "Epoch 94/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.2423e-04 - accuracy: 0.9857 - val_loss: 0.0017 - val_accuracy: 0.8895\n",
            "Epoch 95/1000\n",
            "47/47 [==============================] - 3s 75ms/step - loss: 2.2769e-04 - accuracy: 0.9854 - val_loss: 0.0017 - val_accuracy: 0.8895\n",
            "Epoch 96/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3920e-04 - accuracy: 0.9849 - val_loss: 0.0016 - val_accuracy: 0.8952\n",
            "Epoch 97/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 2.4052e-04 - accuracy: 0.9846 - val_loss: 0.0017 - val_accuracy: 0.8887\n",
            "Epoch 98/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.7287e-04 - accuracy: 0.9825 - val_loss: 0.0017 - val_accuracy: 0.8928\n",
            "Epoch 99/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3957e-04 - accuracy: 0.9852 - val_loss: 0.0015 - val_accuracy: 0.9013\n",
            "Epoch 100/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3645e-04 - accuracy: 0.9847 - val_loss: 0.0017 - val_accuracy: 0.8883\n",
            "Epoch 101/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.6348e-04 - accuracy: 0.9835 - val_loss: 0.0017 - val_accuracy: 0.8890\n",
            "Epoch 102/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.4708e-04 - accuracy: 0.9845 - val_loss: 0.0017 - val_accuracy: 0.8902\n",
            "Epoch 103/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.5403e-04 - accuracy: 0.9837 - val_loss: 0.0016 - val_accuracy: 0.8945\n",
            "Epoch 104/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 2.8466e-04 - accuracy: 0.9820 - val_loss: 0.0020 - val_accuracy: 0.8733\n",
            "Epoch 105/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.6373e-04 - accuracy: 0.9835 - val_loss: 0.0017 - val_accuracy: 0.8938\n",
            "Epoch 106/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.4498e-04 - accuracy: 0.9847 - val_loss: 0.0017 - val_accuracy: 0.8900\n",
            "Epoch 107/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3919e-04 - accuracy: 0.9848 - val_loss: 0.0017 - val_accuracy: 0.8963\n",
            "Epoch 108/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.9926e-04 - accuracy: 0.9877 - val_loss: 0.0016 - val_accuracy: 0.8970\n",
            "Epoch 109/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.8599e-04 - accuracy: 0.9883 - val_loss: 0.0015 - val_accuracy: 0.9045\n",
            "Epoch 110/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.7188e-04 - accuracy: 0.9892 - val_loss: 0.0016 - val_accuracy: 0.9007\n",
            "Epoch 111/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.9446e-04 - accuracy: 0.9880 - val_loss: 0.0016 - val_accuracy: 0.8970\n",
            "Epoch 112/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.8132e-04 - accuracy: 0.9888 - val_loss: 0.0016 - val_accuracy: 0.9000\n",
            "Epoch 113/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.7433e-04 - accuracy: 0.9890 - val_loss: 0.0016 - val_accuracy: 0.9012\n",
            "Epoch 114/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.6613e-04 - accuracy: 0.9898 - val_loss: 0.0015 - val_accuracy: 0.9097\n",
            "Epoch 115/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.5932e-04 - accuracy: 0.9902 - val_loss: 0.0014 - val_accuracy: 0.9137\n",
            "Epoch 116/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.5464e-04 - accuracy: 0.9905 - val_loss: 0.0014 - val_accuracy: 0.9090\n",
            "Epoch 117/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.6016e-04 - accuracy: 0.9899 - val_loss: 0.0015 - val_accuracy: 0.9043\n",
            "Epoch 118/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.6950e-04 - accuracy: 0.9894 - val_loss: 0.0015 - val_accuracy: 0.9035\n",
            "Epoch 119/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.7468e-04 - accuracy: 0.9892 - val_loss: 0.0016 - val_accuracy: 0.8998\n",
            "Epoch 120/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.7714e-04 - accuracy: 0.9895 - val_loss: 0.0015 - val_accuracy: 0.9043\n",
            "Epoch 121/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.8942e-04 - accuracy: 0.9885 - val_loss: 0.0015 - val_accuracy: 0.9040\n",
            "Epoch 122/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.9476e-04 - accuracy: 0.9880 - val_loss: 0.0015 - val_accuracy: 0.9043\n",
            "Epoch 123/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 2.4419e-04 - accuracy: 0.9847 - val_loss: 0.0016 - val_accuracy: 0.8990\n",
            "Epoch 124/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.0250e-04 - accuracy: 0.9876 - val_loss: 0.0015 - val_accuracy: 0.9020\n",
            "Epoch 125/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3819e-04 - accuracy: 0.9853 - val_loss: 0.0015 - val_accuracy: 0.9035\n",
            "Epoch 126/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.5645e-04 - accuracy: 0.9840 - val_loss: 0.0018 - val_accuracy: 0.8875\n",
            "Epoch 127/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3832e-04 - accuracy: 0.9853 - val_loss: 0.0017 - val_accuracy: 0.8957\n",
            "Epoch 128/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.1069e-04 - accuracy: 0.9870 - val_loss: 0.0016 - val_accuracy: 0.9032\n",
            "Epoch 129/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.8682e-04 - accuracy: 0.9887 - val_loss: 0.0015 - val_accuracy: 0.9063\n",
            "Epoch 130/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.6462e-04 - accuracy: 0.9898 - val_loss: 0.0015 - val_accuracy: 0.9028\n",
            "Epoch 131/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.6234e-04 - accuracy: 0.9898 - val_loss: 0.0015 - val_accuracy: 0.9042\n",
            "Epoch 132/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.5673e-04 - accuracy: 0.9905 - val_loss: 0.0015 - val_accuracy: 0.9072\n",
            "Epoch 133/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.5212e-04 - accuracy: 0.9910 - val_loss: 0.0014 - val_accuracy: 0.9137\n",
            "Epoch 134/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.5818e-04 - accuracy: 0.9904 - val_loss: 0.0015 - val_accuracy: 0.9043\n",
            "Epoch 135/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.5609e-04 - accuracy: 0.9905 - val_loss: 0.0014 - val_accuracy: 0.9128\n",
            "Epoch 136/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.6300e-04 - accuracy: 0.9902 - val_loss: 0.0015 - val_accuracy: 0.9108\n",
            "Epoch 137/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.5167e-04 - accuracy: 0.9910 - val_loss: 0.0014 - val_accuracy: 0.9135\n",
            "Epoch 138/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.5092e-04 - accuracy: 0.9910 - val_loss: 0.0015 - val_accuracy: 0.9107\n",
            "Epoch 139/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.5551e-04 - accuracy: 0.9907 - val_loss: 0.0016 - val_accuracy: 0.9028\n",
            "Epoch 140/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.5738e-04 - accuracy: 0.9907 - val_loss: 0.0016 - val_accuracy: 0.8992\n",
            "Epoch 141/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.5710e-04 - accuracy: 0.9907 - val_loss: 0.0015 - val_accuracy: 0.9097\n",
            "Epoch 142/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.7885e-04 - accuracy: 0.9892 - val_loss: 0.0018 - val_accuracy: 0.8907\n",
            "Epoch 143/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.9754e-04 - accuracy: 0.9878 - val_loss: 0.0017 - val_accuracy: 0.8943\n",
            "Epoch 144/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3592e-04 - accuracy: 0.9850 - val_loss: 0.0018 - val_accuracy: 0.8872\n",
            "Epoch 145/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.9483e-04 - accuracy: 0.9881 - val_loss: 0.0015 - val_accuracy: 0.9093\n",
            "Epoch 146/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.8972e-04 - accuracy: 0.9883 - val_loss: 0.0015 - val_accuracy: 0.9070\n",
            "Epoch 147/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.7858e-04 - accuracy: 0.9889 - val_loss: 0.0015 - val_accuracy: 0.9050\n",
            "Epoch 148/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.8502e-04 - accuracy: 0.9889 - val_loss: 0.0017 - val_accuracy: 0.8980\n",
            "Epoch 149/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.2659e-04 - accuracy: 0.9860 - val_loss: 0.0015 - val_accuracy: 0.9065\n",
            "Epoch 150/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.7110e-04 - accuracy: 0.9897 - val_loss: 0.0017 - val_accuracy: 0.8962\n",
            "Epoch 151/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.8039e-04 - accuracy: 0.9890 - val_loss: 0.0014 - val_accuracy: 0.9145\n",
            "Epoch 152/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.7287e-04 - accuracy: 0.9893 - val_loss: 0.0016 - val_accuracy: 0.9043\n",
            "Epoch 153/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.5849e-04 - accuracy: 0.9902 - val_loss: 0.0014 - val_accuracy: 0.9128\n",
            "Epoch 154/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.7110e-04 - accuracy: 0.9890 - val_loss: 0.0017 - val_accuracy: 0.8943\n",
            "Epoch 155/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.8194e-04 - accuracy: 0.9885 - val_loss: 0.0014 - val_accuracy: 0.9103\n",
            "Epoch 156/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.6649e-04 - accuracy: 0.9898 - val_loss: 0.0016 - val_accuracy: 0.9043\n",
            "Epoch 157/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.2677e-04 - accuracy: 0.9924 - val_loss: 0.0014 - val_accuracy: 0.9158\n",
            "Epoch 158/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.2306e-04 - accuracy: 0.9923 - val_loss: 0.0015 - val_accuracy: 0.9107\n",
            "Epoch 159/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.0830e-04 - accuracy: 0.9936 - val_loss: 0.0015 - val_accuracy: 0.9100\n",
            "Epoch 160/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.1232e-04 - accuracy: 0.9932 - val_loss: 0.0014 - val_accuracy: 0.9115\n",
            "Epoch 161/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.5768e-04 - accuracy: 0.9900 - val_loss: 0.0017 - val_accuracy: 0.8998\n",
            "Epoch 162/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.6236e-05 - accuracy: 0.9942 - val_loss: 0.0013 - val_accuracy: 0.9218\n",
            "Epoch 163/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.7193e-05 - accuracy: 0.9957 - val_loss: 0.0014 - val_accuracy: 0.9173\n",
            "Epoch 164/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.7312e-05 - accuracy: 0.9950 - val_loss: 0.0013 - val_accuracy: 0.9173\n",
            "Epoch 165/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.7419e-05 - accuracy: 0.9957 - val_loss: 0.0013 - val_accuracy: 0.9215\n",
            "Epoch 166/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.2291e-05 - accuracy: 0.9959 - val_loss: 0.0012 - val_accuracy: 0.9238\n",
            "Epoch 167/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.3505e-05 - accuracy: 0.9952 - val_loss: 0.0013 - val_accuracy: 0.9167\n",
            "Epoch 168/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.6014e-05 - accuracy: 0.9949 - val_loss: 0.0013 - val_accuracy: 0.9217\n",
            "Epoch 169/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 8.9799e-05 - accuracy: 0.9950 - val_loss: 0.0013 - val_accuracy: 0.9227\n",
            "Epoch 170/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.9187e-05 - accuracy: 0.9947 - val_loss: 0.0014 - val_accuracy: 0.9172\n",
            "Epoch 171/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.0261e-04 - accuracy: 0.9939 - val_loss: 0.0014 - val_accuracy: 0.9132\n",
            "Epoch 172/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.8487e-05 - accuracy: 0.9950 - val_loss: 0.0014 - val_accuracy: 0.9172\n",
            "Epoch 173/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.4828e-05 - accuracy: 0.9958 - val_loss: 0.0014 - val_accuracy: 0.9118\n",
            "Epoch 174/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.2066e-05 - accuracy: 0.9960 - val_loss: 0.0013 - val_accuracy: 0.9172\n",
            "Epoch 175/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 5.9962e-05 - accuracy: 0.9967 - val_loss: 0.0014 - val_accuracy: 0.9180\n",
            "Epoch 176/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 9.0771e-05 - accuracy: 0.9945 - val_loss: 0.0015 - val_accuracy: 0.9077\n",
            "Epoch 177/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.0860e-04 - accuracy: 0.9935 - val_loss: 0.0015 - val_accuracy: 0.9095\n",
            "Epoch 178/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.0557e-04 - accuracy: 0.9938 - val_loss: 0.0015 - val_accuracy: 0.9117\n",
            "Epoch 179/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.0298e-04 - accuracy: 0.9938 - val_loss: 0.0014 - val_accuracy: 0.9183\n",
            "Epoch 180/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.9130e-05 - accuracy: 0.9955 - val_loss: 0.0013 - val_accuracy: 0.9212\n",
            "Epoch 181/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.7982e-05 - accuracy: 0.9964 - val_loss: 0.0014 - val_accuracy: 0.9160\n",
            "Epoch 182/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.5097e-05 - accuracy: 0.9950 - val_loss: 0.0013 - val_accuracy: 0.9197\n",
            "Epoch 183/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.5883e-05 - accuracy: 0.9958 - val_loss: 0.0013 - val_accuracy: 0.9193\n",
            "Epoch 184/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.0042e-04 - accuracy: 0.9944 - val_loss: 0.0014 - val_accuracy: 0.9198\n",
            "Epoch 185/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.0502e-04 - accuracy: 0.9935 - val_loss: 0.0014 - val_accuracy: 0.9125\n",
            "Epoch 186/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 9.6350e-05 - accuracy: 0.9945 - val_loss: 0.0014 - val_accuracy: 0.9165\n",
            "Epoch 187/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.7580e-05 - accuracy: 0.9948 - val_loss: 0.0014 - val_accuracy: 0.9165\n",
            "Epoch 188/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.4027e-04 - accuracy: 0.9915 - val_loss: 0.0015 - val_accuracy: 0.9100\n",
            "Epoch 189/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.3068e-04 - accuracy: 0.9918 - val_loss: 0.0016 - val_accuracy: 0.9068\n",
            "Epoch 190/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.1131e-04 - accuracy: 0.9934 - val_loss: 0.0013 - val_accuracy: 0.9198\n",
            "Epoch 191/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.1602e-05 - accuracy: 0.9947 - val_loss: 0.0015 - val_accuracy: 0.9100\n",
            "Epoch 192/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.5734e-05 - accuracy: 0.9951 - val_loss: 0.0013 - val_accuracy: 0.9202\n",
            "Epoch 193/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.9514e-05 - accuracy: 0.9954 - val_loss: 0.0012 - val_accuracy: 0.9243\n",
            "Epoch 194/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.2386e-05 - accuracy: 0.9959 - val_loss: 0.0013 - val_accuracy: 0.9207\n",
            "Epoch 195/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.0145e-05 - accuracy: 0.9952 - val_loss: 0.0015 - val_accuracy: 0.9108\n",
            "Epoch 196/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.6036e-05 - accuracy: 0.9951 - val_loss: 0.0013 - val_accuracy: 0.9192\n",
            "Epoch 197/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 8.7886e-05 - accuracy: 0.9950 - val_loss: 0.0014 - val_accuracy: 0.9142\n",
            "Epoch 198/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.9866e-05 - accuracy: 0.9948 - val_loss: 0.0013 - val_accuracy: 0.9208\n",
            "Epoch 199/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.1685e-05 - accuracy: 0.9953 - val_loss: 0.0014 - val_accuracy: 0.9152\n",
            "Epoch 200/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.1058e-04 - accuracy: 0.9935 - val_loss: 0.0014 - val_accuracy: 0.9152\n",
            "Epoch 201/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.1347e-04 - accuracy: 0.9934 - val_loss: 0.0015 - val_accuracy: 0.9108\n",
            "Epoch 202/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.0967e-04 - accuracy: 0.9934 - val_loss: 0.0013 - val_accuracy: 0.9223\n",
            "Epoch 203/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.5304e-05 - accuracy: 0.9956 - val_loss: 0.0014 - val_accuracy: 0.9178\n",
            "Epoch 204/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.1930e-05 - accuracy: 0.9955 - val_loss: 0.0014 - val_accuracy: 0.9178\n",
            "Epoch 205/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.7463e-05 - accuracy: 0.9964 - val_loss: 0.0013 - val_accuracy: 0.9220\n",
            "Epoch 206/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.5396e-05 - accuracy: 0.9958 - val_loss: 0.0013 - val_accuracy: 0.9233\n",
            "Epoch 207/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.5261e-05 - accuracy: 0.9962 - val_loss: 0.0013 - val_accuracy: 0.9223\n",
            "Epoch 208/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.5854e-05 - accuracy: 0.9957 - val_loss: 0.0012 - val_accuracy: 0.9277\n",
            "Epoch 209/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.3510e-05 - accuracy: 0.9953 - val_loss: 0.0015 - val_accuracy: 0.9123\n",
            "Epoch 210/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.2272e-05 - accuracy: 0.9951 - val_loss: 0.0014 - val_accuracy: 0.9180\n",
            "Epoch 211/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.7804e-05 - accuracy: 0.9957 - val_loss: 0.0013 - val_accuracy: 0.9220\n",
            "Epoch 212/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 6.9592e-05 - accuracy: 0.9961 - val_loss: 0.0012 - val_accuracy: 0.9248\n",
            "Epoch 213/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.7888e-05 - accuracy: 0.9950 - val_loss: 0.0014 - val_accuracy: 0.9165\n",
            "Epoch 214/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.8601e-05 - accuracy: 0.9952 - val_loss: 0.0013 - val_accuracy: 0.9245\n",
            "Epoch 215/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.5986e-05 - accuracy: 0.9941 - val_loss: 0.0013 - val_accuracy: 0.9245\n",
            "Epoch 216/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.8710e-05 - accuracy: 0.9948 - val_loss: 0.0013 - val_accuracy: 0.9258\n",
            "Epoch 217/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.0117e-04 - accuracy: 0.9943 - val_loss: 0.0014 - val_accuracy: 0.9188\n",
            "Epoch 218/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.8240e-05 - accuracy: 0.9955 - val_loss: 0.0014 - val_accuracy: 0.9185\n",
            "Epoch 219/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.0493e-04 - accuracy: 0.9936 - val_loss: 0.0016 - val_accuracy: 0.9068\n",
            "Epoch 220/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.1001e-04 - accuracy: 0.9934 - val_loss: 0.0014 - val_accuracy: 0.9162\n",
            "Epoch 221/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.5808e-05 - accuracy: 0.9949 - val_loss: 0.0012 - val_accuracy: 0.9277\n",
            "Epoch 222/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.4323e-05 - accuracy: 0.9951 - val_loss: 0.0013 - val_accuracy: 0.9240\n",
            "Epoch 223/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.4929e-05 - accuracy: 0.9958 - val_loss: 0.0013 - val_accuracy: 0.9228\n",
            "Epoch 224/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 6.2759e-05 - accuracy: 0.9965 - val_loss: 0.0012 - val_accuracy: 0.9295\n",
            "Epoch 225/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 7.0968e-05 - accuracy: 0.9961 - val_loss: 0.0015 - val_accuracy: 0.9113\n",
            "Epoch 226/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.5439e-05 - accuracy: 0.9955 - val_loss: 0.0013 - val_accuracy: 0.9193\n",
            "Epoch 227/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.2875e-05 - accuracy: 0.9959 - val_loss: 0.0013 - val_accuracy: 0.9260\n",
            "Epoch 228/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 6.3101e-05 - accuracy: 0.9965 - val_loss: 0.0013 - val_accuracy: 0.9245\n",
            "Epoch 229/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.5849e-05 - accuracy: 0.9949 - val_loss: 0.0018 - val_accuracy: 0.8917\n",
            "Epoch 230/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.1514e-04 - accuracy: 0.9933 - val_loss: 0.0013 - val_accuracy: 0.9270\n",
            "Epoch 231/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.1550e-04 - accuracy: 0.9932 - val_loss: 0.0015 - val_accuracy: 0.9112\n",
            "Epoch 232/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.1417e-04 - accuracy: 0.9933 - val_loss: 0.0015 - val_accuracy: 0.9103\n",
            "Epoch 233/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.0053e-04 - accuracy: 0.9937 - val_loss: 0.0012 - val_accuracy: 0.9262\n",
            "Epoch 234/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 8.1890e-05 - accuracy: 0.9951 - val_loss: 0.0013 - val_accuracy: 0.9242\n",
            "Epoch 235/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.0171e-04 - accuracy: 0.9939 - val_loss: 0.0014 - val_accuracy: 0.9205\n",
            "Epoch 236/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.1321e-04 - accuracy: 0.9931 - val_loss: 0.0013 - val_accuracy: 0.9218\n",
            "Epoch 237/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.9257e-05 - accuracy: 0.9943 - val_loss: 0.0017 - val_accuracy: 0.9000\n",
            "Epoch 238/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.0733e-04 - accuracy: 0.9935 - val_loss: 0.0016 - val_accuracy: 0.9108\n",
            "Epoch 239/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.0937e-04 - accuracy: 0.9937 - val_loss: 0.0013 - val_accuracy: 0.9220\n",
            "Epoch 240/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.0575e-04 - accuracy: 0.9939 - val_loss: 0.0013 - val_accuracy: 0.9232\n",
            "Epoch 241/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 7.0027e-05 - accuracy: 0.9959 - val_loss: 0.0013 - val_accuracy: 0.9253\n",
            "Epoch 242/1000\n",
            "47/47 [==============================] - 3s 75ms/step - loss: 5.9777e-05 - accuracy: 0.9968 - val_loss: 0.0012 - val_accuracy: 0.9295\n",
            "Epoch 243/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.6807e-05 - accuracy: 0.9969 - val_loss: 0.0013 - val_accuracy: 0.9265\n",
            "Epoch 244/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.0781e-05 - accuracy: 0.9967 - val_loss: 0.0012 - val_accuracy: 0.9328\n",
            "Epoch 245/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.3428e-05 - accuracy: 0.9972 - val_loss: 0.0012 - val_accuracy: 0.9275\n",
            "Epoch 246/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.1944e-05 - accuracy: 0.9972 - val_loss: 0.0011 - val_accuracy: 0.9342\n",
            "Epoch 247/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 5.1751e-05 - accuracy: 0.9972 - val_loss: 0.0013 - val_accuracy: 0.9262\n",
            "Epoch 248/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 7.0054e-05 - accuracy: 0.9959 - val_loss: 0.0012 - val_accuracy: 0.9305\n",
            "Epoch 249/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 6.8167e-05 - accuracy: 0.9961 - val_loss: 0.0012 - val_accuracy: 0.9285\n",
            "Epoch 250/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.7461e-05 - accuracy: 0.9960 - val_loss: 0.0014 - val_accuracy: 0.9207\n",
            "Epoch 251/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.9469e-05 - accuracy: 0.9948 - val_loss: 0.0019 - val_accuracy: 0.8908\n",
            "Epoch 252/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.0549e-04 - accuracy: 0.9939 - val_loss: 0.0014 - val_accuracy: 0.9203\n",
            "Epoch 253/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.6837e-05 - accuracy: 0.9942 - val_loss: 0.0013 - val_accuracy: 0.9260\n",
            "Epoch 254/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.9271e-05 - accuracy: 0.9942 - val_loss: 0.0016 - val_accuracy: 0.9093\n",
            "Epoch 255/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.9487e-05 - accuracy: 0.9943 - val_loss: 0.0016 - val_accuracy: 0.9093\n",
            "Epoch 256/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.0017e-04 - accuracy: 0.9940 - val_loss: 0.0014 - val_accuracy: 0.9192\n",
            "Epoch 257/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.2281e-05 - accuracy: 0.9946 - val_loss: 0.0015 - val_accuracy: 0.9097\n",
            "Epoch 258/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.6876e-05 - accuracy: 0.9949 - val_loss: 0.0013 - val_accuracy: 0.9248\n",
            "Epoch 259/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.0362e-05 - accuracy: 0.9959 - val_loss: 0.0012 - val_accuracy: 0.9305\n",
            "Epoch 260/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.3020e-05 - accuracy: 0.9959 - val_loss: 0.0013 - val_accuracy: 0.9262\n",
            "Epoch 261/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.2245e-05 - accuracy: 0.9957 - val_loss: 0.0012 - val_accuracy: 0.9310\n",
            "Epoch 262/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.4975e-05 - accuracy: 0.9959 - val_loss: 0.0012 - val_accuracy: 0.9283\n",
            "Epoch 263/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.4915e-05 - accuracy: 0.9958 - val_loss: 0.0012 - val_accuracy: 0.9298\n",
            "Epoch 264/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.5975e-05 - accuracy: 0.9956 - val_loss: 0.0013 - val_accuracy: 0.9258\n",
            "Epoch 265/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.3942e-05 - accuracy: 0.9950 - val_loss: 0.0013 - val_accuracy: 0.9232\n",
            "Epoch 266/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.3981e-05 - accuracy: 0.9958 - val_loss: 0.0013 - val_accuracy: 0.9225\n",
            "Epoch 267/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.4900e-05 - accuracy: 0.9965 - val_loss: 0.0012 - val_accuracy: 0.9282\n",
            "Epoch 268/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.9964e-05 - accuracy: 0.9967 - val_loss: 0.0014 - val_accuracy: 0.9188\n",
            "Epoch 269/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.5415e-05 - accuracy: 0.9964 - val_loss: 0.0013 - val_accuracy: 0.9250\n",
            "Epoch 270/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.9484e-05 - accuracy: 0.9974 - val_loss: 0.0012 - val_accuracy: 0.9322\n",
            "Epoch 271/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.0418e-05 - accuracy: 0.9973 - val_loss: 0.0013 - val_accuracy: 0.9235\n",
            "Epoch 272/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.8972e-05 - accuracy: 0.9955 - val_loss: 0.0014 - val_accuracy: 0.9188\n",
            "Epoch 273/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.7867e-05 - accuracy: 0.9955 - val_loss: 0.0013 - val_accuracy: 0.9245\n",
            "Epoch 274/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.9854e-05 - accuracy: 0.9947 - val_loss: 0.0013 - val_accuracy: 0.9210\n",
            "Epoch 275/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.0441e-04 - accuracy: 0.9937 - val_loss: 0.0014 - val_accuracy: 0.9195\n",
            "Epoch 276/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.5154e-05 - accuracy: 0.9942 - val_loss: 0.0011 - val_accuracy: 0.9340\n",
            "Epoch 277/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.8155e-05 - accuracy: 0.9941 - val_loss: 0.0012 - val_accuracy: 0.9262\n",
            "Epoch 278/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.7252e-05 - accuracy: 0.9964 - val_loss: 0.0012 - val_accuracy: 0.9300\n",
            "Epoch 279/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.6573e-05 - accuracy: 0.9955 - val_loss: 0.0013 - val_accuracy: 0.9272\n",
            "Epoch 280/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.2065e-05 - accuracy: 0.9953 - val_loss: 0.0014 - val_accuracy: 0.9182\n",
            "Epoch 281/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.9483e-05 - accuracy: 0.9962 - val_loss: 0.0013 - val_accuracy: 0.9228\n",
            "Epoch 282/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.6905e-05 - accuracy: 0.9964 - val_loss: 0.0013 - val_accuracy: 0.9260\n",
            "Epoch 283/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.4675e-05 - accuracy: 0.9963 - val_loss: 0.0014 - val_accuracy: 0.9175\n",
            "Epoch 284/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.3081e-05 - accuracy: 0.9952 - val_loss: 0.0014 - val_accuracy: 0.9183\n",
            "Epoch 285/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.4984e-05 - accuracy: 0.9952 - val_loss: 0.0013 - val_accuracy: 0.9272\n",
            "Epoch 286/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.6516e-05 - accuracy: 0.9950 - val_loss: 0.0015 - val_accuracy: 0.9160\n",
            "Epoch 287/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.5142e-05 - accuracy: 0.9957 - val_loss: 0.0012 - val_accuracy: 0.9320\n",
            "Epoch 288/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.6961e-05 - accuracy: 0.9967 - val_loss: 0.0012 - val_accuracy: 0.9290\n",
            "Epoch 289/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.0759e-05 - accuracy: 0.9946 - val_loss: 0.0014 - val_accuracy: 0.9157\n",
            "Epoch 290/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.7333e-05 - accuracy: 0.9949 - val_loss: 0.0015 - val_accuracy: 0.9148\n",
            "Epoch 291/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.2163e-05 - accuracy: 0.9960 - val_loss: 0.0012 - val_accuracy: 0.9303\n",
            "Epoch 292/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.2496e-05 - accuracy: 0.9971 - val_loss: 0.0013 - val_accuracy: 0.9267\n",
            "Epoch 293/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.7289e-05 - accuracy: 0.9968 - val_loss: 0.0012 - val_accuracy: 0.9332\n",
            "Epoch 294/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.2963e-05 - accuracy: 0.9970 - val_loss: 0.0011 - val_accuracy: 0.9383\n",
            "Epoch 295/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.8243e-05 - accuracy: 0.9974 - val_loss: 0.0011 - val_accuracy: 0.9342\n",
            "Epoch 296/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.1827e-05 - accuracy: 0.9965 - val_loss: 0.0011 - val_accuracy: 0.9348\n",
            "Epoch 297/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.0498e-05 - accuracy: 0.9946 - val_loss: 0.0012 - val_accuracy: 0.9287\n",
            "Epoch 298/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 9.6776e-05 - accuracy: 0.9942 - val_loss: 0.0013 - val_accuracy: 0.9272\n",
            "Epoch 299/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.4538e-05 - accuracy: 0.9951 - val_loss: 0.0013 - val_accuracy: 0.9248\n",
            "Epoch 300/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.6220e-05 - accuracy: 0.9956 - val_loss: 0.0012 - val_accuracy: 0.9297\n",
            "Epoch 301/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.0916e-05 - accuracy: 0.9959 - val_loss: 0.0012 - val_accuracy: 0.9292\n",
            "Epoch 302/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.4986e-05 - accuracy: 0.9965 - val_loss: 0.0013 - val_accuracy: 0.9270\n",
            "Epoch 303/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.3515e-05 - accuracy: 0.9958 - val_loss: 0.0014 - val_accuracy: 0.9182\n",
            "Epoch 304/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.0642e-04 - accuracy: 0.9937 - val_loss: 0.0011 - val_accuracy: 0.9360\n",
            "Epoch 305/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.0563e-05 - accuracy: 0.9960 - val_loss: 0.0012 - val_accuracy: 0.9332\n",
            "Epoch 306/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.5840e-05 - accuracy: 0.9964 - val_loss: 0.0011 - val_accuracy: 0.9377\n",
            "Epoch 307/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.8875e-05 - accuracy: 0.9966 - val_loss: 0.0011 - val_accuracy: 0.9382\n",
            "Epoch 308/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.1125e-05 - accuracy: 0.9973 - val_loss: 0.0011 - val_accuracy: 0.9367\n",
            "Epoch 309/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.2063e-05 - accuracy: 0.9973 - val_loss: 0.0012 - val_accuracy: 0.9300\n",
            "Epoch 310/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.6391e-05 - accuracy: 0.9970 - val_loss: 0.0011 - val_accuracy: 0.9350\n",
            "Epoch 311/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.5840e-05 - accuracy: 0.9962 - val_loss: 0.0011 - val_accuracy: 0.9362\n",
            "Epoch 312/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.3647e-05 - accuracy: 0.9959 - val_loss: 0.0013 - val_accuracy: 0.9250\n",
            "Epoch 313/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.3620e-05 - accuracy: 0.9960 - val_loss: 0.0012 - val_accuracy: 0.9315\n",
            "Epoch 314/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.3984e-05 - accuracy: 0.9964 - val_loss: 0.0012 - val_accuracy: 0.9313\n",
            "Epoch 315/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.7688e-05 - accuracy: 0.9961 - val_loss: 0.0014 - val_accuracy: 0.9222\n",
            "Epoch 316/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.1456e-05 - accuracy: 0.9954 - val_loss: 0.0013 - val_accuracy: 0.9263\n",
            "Epoch 317/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.5923e-05 - accuracy: 0.9945 - val_loss: 0.0013 - val_accuracy: 0.9225\n",
            "Epoch 318/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.6736e-05 - accuracy: 0.9950 - val_loss: 0.0012 - val_accuracy: 0.9302\n",
            "Epoch 319/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.1188e-04 - accuracy: 0.9934 - val_loss: 0.0015 - val_accuracy: 0.9115\n",
            "Epoch 320/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 8.4087e-05 - accuracy: 0.9954 - val_loss: 0.0014 - val_accuracy: 0.9228\n",
            "Epoch 321/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.3219e-05 - accuracy: 0.9964 - val_loss: 0.0013 - val_accuracy: 0.9242\n",
            "Epoch 322/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.7708e-05 - accuracy: 0.9969 - val_loss: 0.0011 - val_accuracy: 0.9350\n",
            "Epoch 323/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.5671e-05 - accuracy: 0.9970 - val_loss: 0.0012 - val_accuracy: 0.9297\n",
            "Epoch 324/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.5114e-05 - accuracy: 0.9964 - val_loss: 0.0013 - val_accuracy: 0.9280\n",
            "Epoch 325/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.5134e-05 - accuracy: 0.9970 - val_loss: 0.0012 - val_accuracy: 0.9340\n",
            "Epoch 326/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.0977e-05 - accuracy: 0.9973 - val_loss: 0.0010 - val_accuracy: 0.9395\n",
            "Epoch 327/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.6425e-05 - accuracy: 0.9977 - val_loss: 0.0010 - val_accuracy: 0.9407\n",
            "Epoch 328/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.7517e-05 - accuracy: 0.9975 - val_loss: 0.0010 - val_accuracy: 0.9402\n",
            "Epoch 329/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.5738e-05 - accuracy: 0.9976 - val_loss: 0.0012 - val_accuracy: 0.9345\n",
            "Epoch 330/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.7562e-05 - accuracy: 0.9970 - val_loss: 0.0011 - val_accuracy: 0.9362\n",
            "Epoch 331/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.6618e-05 - accuracy: 0.9949 - val_loss: 0.0013 - val_accuracy: 0.9273\n",
            "Epoch 332/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.0289e-04 - accuracy: 0.9938 - val_loss: 0.0015 - val_accuracy: 0.9150\n",
            "Epoch 333/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.0252e-04 - accuracy: 0.9941 - val_loss: 0.0013 - val_accuracy: 0.9252\n",
            "Epoch 334/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.1176e-04 - accuracy: 0.9936 - val_loss: 0.0013 - val_accuracy: 0.9235\n",
            "Epoch 335/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.3506e-05 - accuracy: 0.9959 - val_loss: 0.0012 - val_accuracy: 0.9303\n",
            "Epoch 336/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.7905e-05 - accuracy: 0.9955 - val_loss: 0.0012 - val_accuracy: 0.9293\n",
            "Epoch 337/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.5992e-05 - accuracy: 0.9962 - val_loss: 0.0012 - val_accuracy: 0.9282\n",
            "Epoch 338/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.8178e-05 - accuracy: 0.9962 - val_loss: 0.0012 - val_accuracy: 0.9317\n",
            "Epoch 339/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.4204e-05 - accuracy: 0.9972 - val_loss: 0.0011 - val_accuracy: 0.9387\n",
            "Epoch 340/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.9567e-05 - accuracy: 0.9957 - val_loss: 0.0018 - val_accuracy: 0.8998\n",
            "Epoch 341/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.8205e-04 - accuracy: 0.9893 - val_loss: 0.0015 - val_accuracy: 0.9153\n",
            "Epoch 342/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.5696e-04 - accuracy: 0.9906 - val_loss: 0.0015 - val_accuracy: 0.9137\n",
            "Epoch 343/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.3598e-04 - accuracy: 0.9921 - val_loss: 0.0014 - val_accuracy: 0.9215\n",
            "Epoch 344/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.3894e-05 - accuracy: 0.9946 - val_loss: 0.0013 - val_accuracy: 0.9237\n",
            "Epoch 345/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.1192e-05 - accuracy: 0.9949 - val_loss: 0.0013 - val_accuracy: 0.9300\n",
            "Epoch 346/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.5992e-05 - accuracy: 0.9963 - val_loss: 0.0012 - val_accuracy: 0.9325\n",
            "Epoch 347/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.3186e-05 - accuracy: 0.9971 - val_loss: 0.0012 - val_accuracy: 0.9310\n",
            "Epoch 348/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.2279e-05 - accuracy: 0.9972 - val_loss: 0.0012 - val_accuracy: 0.9325\n",
            "Epoch 349/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.1404e-05 - accuracy: 0.9966 - val_loss: 0.0012 - val_accuracy: 0.9347\n",
            "Epoch 350/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.9412e-05 - accuracy: 0.9967 - val_loss: 0.0012 - val_accuracy: 0.9310\n",
            "Epoch 351/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.0473e-05 - accuracy: 0.9972 - val_loss: 0.0011 - val_accuracy: 0.9360\n",
            "Epoch 352/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.1186e-05 - accuracy: 0.9972 - val_loss: 0.0012 - val_accuracy: 0.9342\n",
            "Epoch 353/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.8295e-05 - accuracy: 0.9968 - val_loss: 0.0013 - val_accuracy: 0.9285\n",
            "Epoch 354/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.7532e-05 - accuracy: 0.9962 - val_loss: 0.0011 - val_accuracy: 0.9368\n",
            "Epoch 355/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.4029e-05 - accuracy: 0.9965 - val_loss: 0.0011 - val_accuracy: 0.9347\n",
            "Epoch 356/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.9870e-05 - accuracy: 0.9967 - val_loss: 0.0013 - val_accuracy: 0.9283\n",
            "Epoch 357/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.4125e-05 - accuracy: 0.9964 - val_loss: 0.0011 - val_accuracy: 0.9385\n",
            "Epoch 358/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.9271e-05 - accuracy: 0.9973 - val_loss: 0.0011 - val_accuracy: 0.9390\n",
            "Epoch 359/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.3161e-05 - accuracy: 0.9971 - val_loss: 0.0012 - val_accuracy: 0.9303\n",
            "Epoch 360/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.2245e-05 - accuracy: 0.9959 - val_loss: 0.0013 - val_accuracy: 0.9257\n",
            "Epoch 361/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.1433e-05 - accuracy: 0.9960 - val_loss: 0.0012 - val_accuracy: 0.9315\n",
            "Epoch 362/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.1715e-05 - accuracy: 0.9967 - val_loss: 0.0013 - val_accuracy: 0.9258\n",
            "Epoch 363/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.0445e-04 - accuracy: 0.9940 - val_loss: 0.0014 - val_accuracy: 0.9228\n",
            "Epoch 364/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.6371e-05 - accuracy: 0.9944 - val_loss: 0.0013 - val_accuracy: 0.9273\n",
            "Epoch 365/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.2839e-05 - accuracy: 0.9952 - val_loss: 0.0015 - val_accuracy: 0.9135\n",
            "Epoch 366/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.6806e-05 - accuracy: 0.9959 - val_loss: 0.0013 - val_accuracy: 0.9222\n",
            "Epoch 367/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.5521e-05 - accuracy: 0.9955 - val_loss: 0.0013 - val_accuracy: 0.9280\n",
            "Epoch 368/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.0881e-05 - accuracy: 0.9960 - val_loss: 0.0012 - val_accuracy: 0.9328\n",
            "Epoch 369/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.2101e-05 - accuracy: 0.9959 - val_loss: 0.0014 - val_accuracy: 0.9167\n",
            "Epoch 370/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.5486e-05 - accuracy: 0.9958 - val_loss: 0.0012 - val_accuracy: 0.9315\n",
            "Epoch 371/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.1008e-05 - accuracy: 0.9960 - val_loss: 0.0013 - val_accuracy: 0.9262\n",
            "Epoch 372/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.8653e-05 - accuracy: 0.9955 - val_loss: 0.0013 - val_accuracy: 0.9298\n",
            "Epoch 373/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.9501e-05 - accuracy: 0.9967 - val_loss: 0.0012 - val_accuracy: 0.9310\n",
            "Epoch 374/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.3836e-05 - accuracy: 0.9970 - val_loss: 0.0013 - val_accuracy: 0.9237\n",
            "Epoch 375/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.1737e-05 - accuracy: 0.9967 - val_loss: 0.0013 - val_accuracy: 0.9270\n",
            "Epoch 376/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 5.8931e-05 - accuracy: 0.9967 - val_loss: 0.0012 - val_accuracy: 0.9288\n",
            "Epoch 377/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.0438e-05 - accuracy: 0.9960 - val_loss: 0.0012 - val_accuracy: 0.9290\n",
            "Epoch 378/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.0725e-05 - accuracy: 0.9968 - val_loss: 0.0011 - val_accuracy: 0.9360\n",
            "Epoch 379/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.8938e-05 - accuracy: 0.9960 - val_loss: 0.0012 - val_accuracy: 0.9320\n",
            "Epoch 380/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.4148e-05 - accuracy: 0.9970 - val_loss: 0.0012 - val_accuracy: 0.9323\n",
            "Epoch 381/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.0719e-05 - accuracy: 0.9973 - val_loss: 0.0011 - val_accuracy: 0.9388\n",
            "Epoch 382/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.7782e-05 - accuracy: 0.9975 - val_loss: 0.0011 - val_accuracy: 0.9368\n",
            "Epoch 383/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.4133e-05 - accuracy: 0.9977 - val_loss: 0.0010 - val_accuracy: 0.9418\n",
            "Epoch 384/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.2487e-05 - accuracy: 0.9978 - val_loss: 9.9868e-04 - val_accuracy: 0.9425\n",
            "Epoch 385/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.6074e-05 - accuracy: 0.9975 - val_loss: 0.0010 - val_accuracy: 0.9413\n",
            "Epoch 386/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.0815e-05 - accuracy: 0.9973 - val_loss: 0.0012 - val_accuracy: 0.9347\n",
            "Epoch 387/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.2559e-05 - accuracy: 0.9966 - val_loss: 0.0011 - val_accuracy: 0.9370\n",
            "Epoch 388/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.0533e-05 - accuracy: 0.9965 - val_loss: 0.0015 - val_accuracy: 0.9162\n",
            "Epoch 389/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.2168e-05 - accuracy: 0.9949 - val_loss: 0.0013 - val_accuracy: 0.9270\n",
            "Epoch 390/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.8243e-05 - accuracy: 0.9955 - val_loss: 0.0014 - val_accuracy: 0.9225\n",
            "Epoch 391/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.6074e-05 - accuracy: 0.9955 - val_loss: 0.0012 - val_accuracy: 0.9322\n",
            "Epoch 392/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.1366e-05 - accuracy: 0.9967 - val_loss: 0.0013 - val_accuracy: 0.9300\n",
            "Epoch 393/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.6115e-05 - accuracy: 0.9945 - val_loss: 0.0013 - val_accuracy: 0.9268\n",
            "Epoch 394/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.1236e-04 - accuracy: 0.9935 - val_loss: 0.0013 - val_accuracy: 0.9252\n",
            "Epoch 395/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.3576e-04 - accuracy: 0.9921 - val_loss: 0.0017 - val_accuracy: 0.9053\n",
            "Epoch 396/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.2207e-04 - accuracy: 0.9930 - val_loss: 0.0012 - val_accuracy: 0.9348\n",
            "Epoch 397/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.6955e-05 - accuracy: 0.9956 - val_loss: 0.0014 - val_accuracy: 0.9213\n",
            "Epoch 398/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.2900e-05 - accuracy: 0.9945 - val_loss: 0.0011 - val_accuracy: 0.9368\n",
            "Epoch 399/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.2968e-05 - accuracy: 0.9965 - val_loss: 0.0012 - val_accuracy: 0.9318\n",
            "Epoch 400/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.1731e-05 - accuracy: 0.9965 - val_loss: 0.0010 - val_accuracy: 0.9422\n",
            "Epoch 401/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.5302e-05 - accuracy: 0.9969 - val_loss: 0.0012 - val_accuracy: 0.9343\n",
            "Epoch 402/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.9708e-05 - accuracy: 0.9973 - val_loss: 0.0010 - val_accuracy: 0.9440\n",
            "Epoch 403/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.3606e-05 - accuracy: 0.9970 - val_loss: 0.0012 - val_accuracy: 0.9343\n",
            "Epoch 404/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.6168e-05 - accuracy: 0.9975 - val_loss: 0.0011 - val_accuracy: 0.9412\n",
            "Epoch 405/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.5814e-05 - accuracy: 0.9975 - val_loss: 9.5647e-04 - val_accuracy: 0.9455\n",
            "Epoch 406/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.3103e-05 - accuracy: 0.9977 - val_loss: 0.0011 - val_accuracy: 0.9390\n",
            "Epoch 407/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.4642e-05 - accuracy: 0.9977 - val_loss: 0.0011 - val_accuracy: 0.9407\n",
            "Epoch 408/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.8103e-05 - accuracy: 0.9975 - val_loss: 0.0011 - val_accuracy: 0.9402\n",
            "Epoch 409/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.5219e-05 - accuracy: 0.9970 - val_loss: 0.0011 - val_accuracy: 0.9398\n",
            "Epoch 410/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.5385e-05 - accuracy: 0.9945 - val_loss: 0.0016 - val_accuracy: 0.9082\n",
            "Epoch 411/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.0417e-04 - accuracy: 0.9938 - val_loss: 0.0011 - val_accuracy: 0.9383\n",
            "Epoch 412/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.3316e-05 - accuracy: 0.9948 - val_loss: 0.0012 - val_accuracy: 0.9293\n",
            "Epoch 413/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.6122e-05 - accuracy: 0.9957 - val_loss: 0.0011 - val_accuracy: 0.9373\n",
            "Epoch 414/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.9469e-05 - accuracy: 0.9967 - val_loss: 0.0011 - val_accuracy: 0.9368\n",
            "Epoch 415/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.0024e-05 - accuracy: 0.9967 - val_loss: 0.0011 - val_accuracy: 0.9382\n",
            "Epoch 416/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.4636e-05 - accuracy: 0.9969 - val_loss: 0.0012 - val_accuracy: 0.9343\n",
            "Epoch 417/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 6.9657e-05 - accuracy: 0.9962 - val_loss: 0.0013 - val_accuracy: 0.9260\n",
            "Epoch 418/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.9853e-05 - accuracy: 0.9974 - val_loss: 0.0010 - val_accuracy: 0.9428\n",
            "Epoch 419/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.8756e-05 - accuracy: 0.9975 - val_loss: 0.0012 - val_accuracy: 0.9337\n",
            "Epoch 420/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.0537e-05 - accuracy: 0.9965 - val_loss: 0.0012 - val_accuracy: 0.9303\n",
            "Epoch 421/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.9450e-05 - accuracy: 0.9966 - val_loss: 0.0011 - val_accuracy: 0.9372\n",
            "Epoch 422/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.3962e-05 - accuracy: 0.9970 - val_loss: 0.0012 - val_accuracy: 0.9327\n",
            "Epoch 423/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.7620e-05 - accuracy: 0.9968 - val_loss: 0.0014 - val_accuracy: 0.9192\n",
            "Epoch 424/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.9889e-05 - accuracy: 0.9960 - val_loss: 0.0013 - val_accuracy: 0.9285\n",
            "Epoch 425/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.6676e-05 - accuracy: 0.9963 - val_loss: 0.0012 - val_accuracy: 0.9327\n",
            "Epoch 426/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.5442e-05 - accuracy: 0.9955 - val_loss: 0.0012 - val_accuracy: 0.9343\n",
            "Epoch 427/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.0302e-04 - accuracy: 0.9943 - val_loss: 0.0013 - val_accuracy: 0.9285\n",
            "Epoch 428/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.9030e-05 - accuracy: 0.9960 - val_loss: 0.0012 - val_accuracy: 0.9315\n",
            "Epoch 429/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.1355e-05 - accuracy: 0.9952 - val_loss: 0.0011 - val_accuracy: 0.9387\n",
            "Epoch 430/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.0957e-04 - accuracy: 0.9938 - val_loss: 0.0013 - val_accuracy: 0.9252\n",
            "Epoch 431/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.0656e-05 - accuracy: 0.9948 - val_loss: 0.0013 - val_accuracy: 0.9283\n",
            "Epoch 432/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.4307e-05 - accuracy: 0.9945 - val_loss: 0.0013 - val_accuracy: 0.9292\n",
            "Epoch 433/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.6535e-05 - accuracy: 0.9958 - val_loss: 0.0011 - val_accuracy: 0.9342\n",
            "Epoch 434/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.4209e-05 - accuracy: 0.9962 - val_loss: 0.0012 - val_accuracy: 0.9322\n",
            "Epoch 435/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.6810e-05 - accuracy: 0.9962 - val_loss: 0.0011 - val_accuracy: 0.9400\n",
            "Epoch 436/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.8854e-05 - accuracy: 0.9968 - val_loss: 0.0011 - val_accuracy: 0.9380\n",
            "Epoch 437/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.3855e-05 - accuracy: 0.9970 - val_loss: 0.0011 - val_accuracy: 0.9347\n",
            "Epoch 438/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.5629e-05 - accuracy: 0.9976 - val_loss: 0.0012 - val_accuracy: 0.9337\n",
            "Epoch 439/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.8556e-05 - accuracy: 0.9974 - val_loss: 0.0010 - val_accuracy: 0.9442\n",
            "Epoch 440/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.5945e-05 - accuracy: 0.9975 - val_loss: 0.0012 - val_accuracy: 0.9318\n",
            "Epoch 441/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.6680e-05 - accuracy: 0.9968 - val_loss: 0.0010 - val_accuracy: 0.9412\n",
            "Epoch 442/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.5208e-05 - accuracy: 0.9977 - val_loss: 0.0010 - val_accuracy: 0.9420\n",
            "Epoch 443/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.2390e-05 - accuracy: 0.9978 - val_loss: 0.0010 - val_accuracy: 0.9422\n",
            "Epoch 444/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.2258e-05 - accuracy: 0.9978 - val_loss: 0.0010 - val_accuracy: 0.9430\n",
            "Epoch 445/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.0615e-05 - accuracy: 0.9968 - val_loss: 0.0011 - val_accuracy: 0.9387\n",
            "Epoch 446/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.3686e-05 - accuracy: 0.9945 - val_loss: 0.0013 - val_accuracy: 0.9262\n",
            "Epoch 447/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.3624e-05 - accuracy: 0.9965 - val_loss: 0.0012 - val_accuracy: 0.9338\n",
            "Epoch 448/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 5.9914e-05 - accuracy: 0.9969 - val_loss: 0.0012 - val_accuracy: 0.9347\n",
            "Epoch 449/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.9088e-05 - accuracy: 0.9967 - val_loss: 0.0011 - val_accuracy: 0.9355\n",
            "Epoch 450/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.6035e-05 - accuracy: 0.9970 - val_loss: 0.0012 - val_accuracy: 0.9345\n",
            "Epoch 451/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.4126e-05 - accuracy: 0.9948 - val_loss: 0.0013 - val_accuracy: 0.9248\n",
            "Epoch 452/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.5229e-05 - accuracy: 0.9964 - val_loss: 0.0011 - val_accuracy: 0.9368\n",
            "Epoch 453/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.6134e-05 - accuracy: 0.9971 - val_loss: 0.0011 - val_accuracy: 0.9408\n",
            "Epoch 454/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.4239e-05 - accuracy: 0.9971 - val_loss: 0.0011 - val_accuracy: 0.9375\n",
            "Epoch 455/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.0623e-05 - accuracy: 0.9961 - val_loss: 0.0011 - val_accuracy: 0.9382\n",
            "Epoch 456/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.0125e-05 - accuracy: 0.9956 - val_loss: 0.0010 - val_accuracy: 0.9425\n",
            "Epoch 457/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.4114e-05 - accuracy: 0.9960 - val_loss: 0.0012 - val_accuracy: 0.9335\n",
            "Epoch 458/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.1450e-05 - accuracy: 0.9966 - val_loss: 0.0011 - val_accuracy: 0.9383\n",
            "Epoch 459/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.1437e-05 - accuracy: 0.9973 - val_loss: 0.0010 - val_accuracy: 0.9420\n",
            "Epoch 460/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.1558e-05 - accuracy: 0.9961 - val_loss: 0.0013 - val_accuracy: 0.9273\n",
            "Epoch 461/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.5170e-05 - accuracy: 0.9970 - val_loss: 0.0012 - val_accuracy: 0.9343\n",
            "Epoch 462/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.2041e-05 - accuracy: 0.9967 - val_loss: 0.0012 - val_accuracy: 0.9325\n",
            "Epoch 463/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.2785e-05 - accuracy: 0.9965 - val_loss: 0.0011 - val_accuracy: 0.9345\n",
            "Epoch 464/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.8634e-05 - accuracy: 0.9960 - val_loss: 0.0015 - val_accuracy: 0.9177\n",
            "Epoch 465/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.7563e-05 - accuracy: 0.9963 - val_loss: 0.0012 - val_accuracy: 0.9320\n",
            "Epoch 466/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.0063e-05 - accuracy: 0.9969 - val_loss: 0.0011 - val_accuracy: 0.9367\n",
            "Epoch 467/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.1658e-05 - accuracy: 0.9966 - val_loss: 0.0012 - val_accuracy: 0.9350\n",
            "Epoch 468/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 5.2032e-05 - accuracy: 0.9971 - val_loss: 0.0011 - val_accuracy: 0.9358\n",
            "Epoch 469/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.8962e-05 - accuracy: 0.9968 - val_loss: 0.0012 - val_accuracy: 0.9343\n",
            "Epoch 470/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.3099e-05 - accuracy: 0.9958 - val_loss: 0.0012 - val_accuracy: 0.9310\n",
            "Epoch 471/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.7325e-05 - accuracy: 0.9969 - val_loss: 0.0012 - val_accuracy: 0.9338\n",
            "Epoch 472/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.8288e-05 - accuracy: 0.9975 - val_loss: 0.0011 - val_accuracy: 0.9383\n",
            "Epoch 473/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.8948e-05 - accuracy: 0.9973 - val_loss: 0.0011 - val_accuracy: 0.9398\n",
            "Epoch 474/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.0906e-05 - accuracy: 0.9965 - val_loss: 0.0014 - val_accuracy: 0.9245\n",
            "Epoch 475/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.9127e-05 - accuracy: 0.9960 - val_loss: 0.0011 - val_accuracy: 0.9407\n",
            "Epoch 476/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.6049e-05 - accuracy: 0.9958 - val_loss: 0.0013 - val_accuracy: 0.9255\n",
            "Epoch 477/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 6.5847e-05 - accuracy: 0.9965 - val_loss: 0.0011 - val_accuracy: 0.9367\n",
            "Epoch 478/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.3465e-05 - accuracy: 0.9946 - val_loss: 0.0013 - val_accuracy: 0.9258\n",
            "Epoch 479/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.8403e-05 - accuracy: 0.9958 - val_loss: 0.0013 - val_accuracy: 0.9278\n",
            "Epoch 480/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.7585e-05 - accuracy: 0.9969 - val_loss: 0.0012 - val_accuracy: 0.9352\n",
            "Epoch 481/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.0780e-05 - accuracy: 0.9967 - val_loss: 0.0012 - val_accuracy: 0.9352\n",
            "Epoch 482/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.2264e-05 - accuracy: 0.9973 - val_loss: 0.0013 - val_accuracy: 0.9287\n",
            "Epoch 483/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.7344e-05 - accuracy: 0.9964 - val_loss: 0.0012 - val_accuracy: 0.9322\n",
            "Epoch 484/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.8389e-05 - accuracy: 0.9968 - val_loss: 0.0011 - val_accuracy: 0.9348\n",
            "Epoch 485/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.9590e-05 - accuracy: 0.9967 - val_loss: 0.0012 - val_accuracy: 0.9328\n",
            "Epoch 486/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.0631e-05 - accuracy: 0.9967 - val_loss: 0.0010 - val_accuracy: 0.9413\n",
            "Epoch 487/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.4005e-05 - accuracy: 0.9972 - val_loss: 9.9902e-04 - val_accuracy: 0.9438\n",
            "Epoch 488/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.5515e-05 - accuracy: 0.9970 - val_loss: 0.0012 - val_accuracy: 0.9350\n",
            "Epoch 489/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.6041e-05 - accuracy: 0.9963 - val_loss: 0.0012 - val_accuracy: 0.9343\n",
            "Epoch 490/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.9774e-05 - accuracy: 0.9955 - val_loss: 0.0014 - val_accuracy: 0.9217\n",
            "Epoch 491/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.0587e-05 - accuracy: 0.9968 - val_loss: 0.0012 - val_accuracy: 0.9330\n",
            "Epoch 492/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.9346e-05 - accuracy: 0.9973 - val_loss: 0.0013 - val_accuracy: 0.9237\n",
            "Epoch 493/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.0932e-05 - accuracy: 0.9960 - val_loss: 0.0014 - val_accuracy: 0.9233\n",
            "Epoch 494/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.8656e-05 - accuracy: 0.9950 - val_loss: 0.0016 - val_accuracy: 0.9132\n",
            "Epoch 495/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.2857e-04 - accuracy: 0.9927 - val_loss: 0.0013 - val_accuracy: 0.9265\n",
            "Epoch 496/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.2641e-04 - accuracy: 0.9927 - val_loss: 0.0014 - val_accuracy: 0.9237\n",
            "Epoch 497/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.5383e-05 - accuracy: 0.9951 - val_loss: 0.0013 - val_accuracy: 0.9290\n",
            "Epoch 498/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.3953e-05 - accuracy: 0.9966 - val_loss: 0.0012 - val_accuracy: 0.9353\n",
            "Epoch 499/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.9769e-05 - accuracy: 0.9967 - val_loss: 0.0014 - val_accuracy: 0.9217\n",
            "Epoch 500/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.2839e-05 - accuracy: 0.9973 - val_loss: 0.0010 - val_accuracy: 0.9437\n",
            "Epoch 501/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.6748e-05 - accuracy: 0.9976 - val_loss: 9.8774e-04 - val_accuracy: 0.9443\n",
            "Epoch 502/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.6625e-05 - accuracy: 0.9975 - val_loss: 9.9468e-04 - val_accuracy: 0.9443\n",
            "Epoch 503/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.0712e-05 - accuracy: 0.9973 - val_loss: 0.0012 - val_accuracy: 0.9335\n",
            "Epoch 504/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.6126e-05 - accuracy: 0.9962 - val_loss: 0.0012 - val_accuracy: 0.9323\n",
            "Epoch 505/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.8346e-05 - accuracy: 0.9960 - val_loss: 0.0012 - val_accuracy: 0.9315\n",
            "Epoch 506/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.1210e-05 - accuracy: 0.9967 - val_loss: 0.0012 - val_accuracy: 0.9353\n",
            "Epoch 507/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.0902e-05 - accuracy: 0.9966 - val_loss: 9.5959e-04 - val_accuracy: 0.9453\n",
            "Epoch 508/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.8854e-05 - accuracy: 0.9974 - val_loss: 0.0014 - val_accuracy: 0.9228\n",
            "Epoch 509/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.8640e-05 - accuracy: 0.9968 - val_loss: 0.0011 - val_accuracy: 0.9415\n",
            "Epoch 510/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.1063e-05 - accuracy: 0.9972 - val_loss: 0.0011 - val_accuracy: 0.9378\n",
            "Epoch 511/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.3392e-05 - accuracy: 0.9971 - val_loss: 0.0011 - val_accuracy: 0.9385\n",
            "Epoch 512/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.2859e-05 - accuracy: 0.9973 - val_loss: 0.0011 - val_accuracy: 0.9407\n",
            "Epoch 513/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.7927e-05 - accuracy: 0.9975 - val_loss: 0.0010 - val_accuracy: 0.9440\n",
            "Epoch 514/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.7772e-05 - accuracy: 0.9975 - val_loss: 0.0010 - val_accuracy: 0.9443\n",
            "Epoch 515/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.7962e-05 - accuracy: 0.9975 - val_loss: 0.0010 - val_accuracy: 0.9438\n",
            "Epoch 516/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.9709e-05 - accuracy: 0.9974 - val_loss: 0.0010 - val_accuracy: 0.9417\n",
            "Epoch 517/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.4065e-05 - accuracy: 0.9972 - val_loss: 9.3048e-04 - val_accuracy: 0.9487\n",
            "Epoch 518/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.0708e-05 - accuracy: 0.9973 - val_loss: 0.0010 - val_accuracy: 0.9447\n",
            "Epoch 519/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.7886e-05 - accuracy: 0.9975 - val_loss: 0.0012 - val_accuracy: 0.9325\n",
            "Epoch 520/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 4.8926e-05 - accuracy: 0.9975 - val_loss: 0.0010 - val_accuracy: 0.9443\n",
            "Epoch 521/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.1264e-05 - accuracy: 0.9973 - val_loss: 0.0012 - val_accuracy: 0.9325\n",
            "Epoch 522/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.9177e-05 - accuracy: 0.9948 - val_loss: 0.0014 - val_accuracy: 0.9217\n",
            "Epoch 523/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.5170e-05 - accuracy: 0.9951 - val_loss: 0.0014 - val_accuracy: 0.9253\n",
            "Epoch 524/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.8683e-05 - accuracy: 0.9943 - val_loss: 0.0012 - val_accuracy: 0.9323\n",
            "Epoch 525/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.7480e-05 - accuracy: 0.9957 - val_loss: 0.0015 - val_accuracy: 0.9130\n",
            "Epoch 526/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.7318e-05 - accuracy: 0.9958 - val_loss: 0.0012 - val_accuracy: 0.9348\n",
            "Epoch 527/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.5101e-05 - accuracy: 0.9952 - val_loss: 0.0014 - val_accuracy: 0.9218\n",
            "Epoch 528/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.1218e-05 - accuracy: 0.9954 - val_loss: 0.0012 - val_accuracy: 0.9327\n",
            "Epoch 529/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.3121e-05 - accuracy: 0.9953 - val_loss: 0.0013 - val_accuracy: 0.9295\n",
            "Epoch 530/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.3271e-05 - accuracy: 0.9953 - val_loss: 0.0011 - val_accuracy: 0.9352\n",
            "Epoch 531/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.2154e-05 - accuracy: 0.9959 - val_loss: 0.0011 - val_accuracy: 0.9393\n",
            "Epoch 532/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.6924e-05 - accuracy: 0.9957 - val_loss: 0.0013 - val_accuracy: 0.9310\n",
            "Epoch 533/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.7909e-05 - accuracy: 0.9963 - val_loss: 0.0011 - val_accuracy: 0.9368\n",
            "Epoch 534/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.2800e-05 - accuracy: 0.9972 - val_loss: 0.0010 - val_accuracy: 0.9433\n",
            "Epoch 535/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.4645e-05 - accuracy: 0.9965 - val_loss: 0.0012 - val_accuracy: 0.9333\n",
            "Epoch 536/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.6936e-05 - accuracy: 0.9970 - val_loss: 9.8179e-04 - val_accuracy: 0.9457\n",
            "Epoch 537/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.5353e-05 - accuracy: 0.9977 - val_loss: 0.0010 - val_accuracy: 0.9428\n",
            "Epoch 538/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.6818e-05 - accuracy: 0.9975 - val_loss: 0.0011 - val_accuracy: 0.9405\n",
            "Epoch 539/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.5683e-05 - accuracy: 0.9976 - val_loss: 9.6265e-04 - val_accuracy: 0.9463\n",
            "Epoch 540/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.6474e-05 - accuracy: 0.9976 - val_loss: 8.7686e-04 - val_accuracy: 0.9523\n",
            "Epoch 541/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.3649e-05 - accuracy: 0.9977 - val_loss: 9.5896e-04 - val_accuracy: 0.9462\n",
            "Epoch 542/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.7533e-05 - accuracy: 0.9975 - val_loss: 9.4723e-04 - val_accuracy: 0.9470\n",
            "Epoch 543/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.5550e-05 - accuracy: 0.9976 - val_loss: 0.0010 - val_accuracy: 0.9433\n",
            "Epoch 544/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.3793e-05 - accuracy: 0.9978 - val_loss: 8.8064e-04 - val_accuracy: 0.9508\n",
            "Epoch 545/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.2257e-05 - accuracy: 0.9978 - val_loss: 8.8358e-04 - val_accuracy: 0.9495\n",
            "Epoch 546/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.2184e-05 - accuracy: 0.9979 - val_loss: 0.0010 - val_accuracy: 0.9433\n",
            "Epoch 547/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.5939e-05 - accuracy: 0.9976 - val_loss: 9.5982e-04 - val_accuracy: 0.9467\n",
            "Epoch 548/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.4702e-05 - accuracy: 0.9970 - val_loss: 9.9939e-04 - val_accuracy: 0.9447\n",
            "Epoch 549/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.3951e-05 - accuracy: 0.9965 - val_loss: 0.0015 - val_accuracy: 0.9145\n",
            "Epoch 550/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.3954e-05 - accuracy: 0.9964 - val_loss: 0.0010 - val_accuracy: 0.9427\n",
            "Epoch 551/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.7967e-05 - accuracy: 0.9964 - val_loss: 0.0011 - val_accuracy: 0.9408\n",
            "Epoch 552/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.6073e-05 - accuracy: 0.9968 - val_loss: 0.0012 - val_accuracy: 0.9353\n",
            "Epoch 553/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.3993e-05 - accuracy: 0.9971 - val_loss: 0.0010 - val_accuracy: 0.9420\n",
            "Epoch 554/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.2551e-05 - accuracy: 0.9960 - val_loss: 0.0010 - val_accuracy: 0.9425\n",
            "Epoch 555/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.6630e-05 - accuracy: 0.9969 - val_loss: 0.0012 - val_accuracy: 0.9360\n",
            "Epoch 556/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.0070e-05 - accuracy: 0.9967 - val_loss: 0.0013 - val_accuracy: 0.9277\n",
            "Epoch 557/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.9480e-05 - accuracy: 0.9967 - val_loss: 0.0012 - val_accuracy: 0.9330\n",
            "Epoch 558/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.7511e-05 - accuracy: 0.9975 - val_loss: 0.0011 - val_accuracy: 0.9407\n",
            "Epoch 559/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.5374e-05 - accuracy: 0.9970 - val_loss: 0.0010 - val_accuracy: 0.9433\n",
            "Epoch 560/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.5386e-05 - accuracy: 0.9971 - val_loss: 9.7450e-04 - val_accuracy: 0.9453\n",
            "Epoch 561/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.6091e-05 - accuracy: 0.9951 - val_loss: 0.0013 - val_accuracy: 0.9267\n",
            "Epoch 562/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.2821e-05 - accuracy: 0.9954 - val_loss: 0.0011 - val_accuracy: 0.9355\n",
            "Epoch 563/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.7226e-05 - accuracy: 0.9945 - val_loss: 0.0017 - val_accuracy: 0.9057\n",
            "Epoch 564/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.1865e-04 - accuracy: 0.9931 - val_loss: 0.0013 - val_accuracy: 0.9263\n",
            "Epoch 565/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.1244e-04 - accuracy: 0.9937 - val_loss: 0.0012 - val_accuracy: 0.9317\n",
            "Epoch 566/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.7959e-05 - accuracy: 0.9956 - val_loss: 0.0014 - val_accuracy: 0.9213\n",
            "Epoch 567/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.7625e-05 - accuracy: 0.9952 - val_loss: 0.0015 - val_accuracy: 0.9182\n",
            "Epoch 568/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.6702e-05 - accuracy: 0.9958 - val_loss: 0.0011 - val_accuracy: 0.9395\n",
            "Epoch 569/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.1484e-05 - accuracy: 0.9966 - val_loss: 9.6888e-04 - val_accuracy: 0.9467\n",
            "Epoch 570/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.0135e-05 - accuracy: 0.9974 - val_loss: 9.9785e-04 - val_accuracy: 0.9448\n",
            "Epoch 571/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.0483e-05 - accuracy: 0.9973 - val_loss: 9.7521e-04 - val_accuracy: 0.9465\n",
            "Epoch 572/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.0462e-05 - accuracy: 0.9973 - val_loss: 0.0010 - val_accuracy: 0.9413\n",
            "Epoch 573/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.7741e-05 - accuracy: 0.9962 - val_loss: 0.0012 - val_accuracy: 0.9358\n",
            "Epoch 574/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.0343e-05 - accuracy: 0.9968 - val_loss: 0.0011 - val_accuracy: 0.9377\n",
            "Epoch 575/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.9526e-05 - accuracy: 0.9960 - val_loss: 0.0011 - val_accuracy: 0.9395\n",
            "Epoch 576/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.9803e-05 - accuracy: 0.9974 - val_loss: 0.0011 - val_accuracy: 0.9398\n",
            "Epoch 577/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.9650e-05 - accuracy: 0.9973 - val_loss: 9.4523e-04 - val_accuracy: 0.9473\n",
            "Epoch 578/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.3249e-05 - accuracy: 0.9965 - val_loss: 0.0011 - val_accuracy: 0.9417\n",
            "Epoch 579/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.2651e-05 - accuracy: 0.9973 - val_loss: 0.0012 - val_accuracy: 0.9357\n",
            "Epoch 580/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.3963e-05 - accuracy: 0.9971 - val_loss: 0.0011 - val_accuracy: 0.9408\n",
            "Epoch 581/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.2842e-05 - accuracy: 0.9971 - val_loss: 9.9600e-04 - val_accuracy: 0.9453\n",
            "Epoch 582/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.0246e-05 - accuracy: 0.9973 - val_loss: 9.6788e-04 - val_accuracy: 0.9460\n",
            "Epoch 583/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.5384e-05 - accuracy: 0.9977 - val_loss: 9.0046e-04 - val_accuracy: 0.9502\n",
            "Epoch 584/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.7834e-05 - accuracy: 0.9975 - val_loss: 0.0010 - val_accuracy: 0.9417\n",
            "Epoch 585/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.2782e-05 - accuracy: 0.9960 - val_loss: 0.0013 - val_accuracy: 0.9247\n",
            "Epoch 586/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.0020e-05 - accuracy: 0.9962 - val_loss: 0.0012 - val_accuracy: 0.9322\n",
            "Epoch 587/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.9993e-05 - accuracy: 0.9962 - val_loss: 0.0011 - val_accuracy: 0.9383\n",
            "Epoch 588/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.9900e-05 - accuracy: 0.9967 - val_loss: 0.0011 - val_accuracy: 0.9373\n",
            "Epoch 589/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.2575e-05 - accuracy: 0.9965 - val_loss: 0.0011 - val_accuracy: 0.9402\n",
            "Epoch 590/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.0577e-05 - accuracy: 0.9974 - val_loss: 0.0011 - val_accuracy: 0.9385\n",
            "Epoch 591/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.2543e-05 - accuracy: 0.9973 - val_loss: 0.0010 - val_accuracy: 0.9413\n",
            "Epoch 592/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.6462e-05 - accuracy: 0.9970 - val_loss: 9.9391e-04 - val_accuracy: 0.9452\n",
            "Epoch 593/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.6602e-05 - accuracy: 0.9976 - val_loss: 9.7687e-04 - val_accuracy: 0.9453\n",
            "Epoch 594/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.5893e-05 - accuracy: 0.9976 - val_loss: 9.2593e-04 - val_accuracy: 0.9488\n",
            "Epoch 595/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.9720e-05 - accuracy: 0.9974 - val_loss: 0.0011 - val_accuracy: 0.9405\n",
            "Epoch 596/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.9569e-05 - accuracy: 0.9968 - val_loss: 0.0010 - val_accuracy: 0.9430\n",
            "Epoch 597/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.5012e-05 - accuracy: 0.9970 - val_loss: 0.0010 - val_accuracy: 0.9448\n",
            "Epoch 598/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.9904e-05 - accuracy: 0.9957 - val_loss: 0.0010 - val_accuracy: 0.9447\n",
            "Epoch 599/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.9327e-05 - accuracy: 0.9975 - val_loss: 9.3098e-04 - val_accuracy: 0.9483\n",
            "Epoch 600/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.1365e-05 - accuracy: 0.9972 - val_loss: 0.0010 - val_accuracy: 0.9435\n",
            "Epoch 601/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.2819e-05 - accuracy: 0.9972 - val_loss: 0.0011 - val_accuracy: 0.9418\n",
            "Epoch 602/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.0348e-05 - accuracy: 0.9974 - val_loss: 0.0011 - val_accuracy: 0.9420\n",
            "Epoch 603/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.9816e-05 - accuracy: 0.9974 - val_loss: 0.0011 - val_accuracy: 0.9382\n",
            "Epoch 604/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 6.1671e-05 - accuracy: 0.9966 - val_loss: 0.0012 - val_accuracy: 0.9350\n",
            "Epoch 605/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.3663e-05 - accuracy: 0.9966 - val_loss: 0.0010 - val_accuracy: 0.9427\n",
            "Epoch 606/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.4316e-05 - accuracy: 0.9958 - val_loss: 0.0011 - val_accuracy: 0.9387\n",
            "Epoch 607/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.8349e-05 - accuracy: 0.9950 - val_loss: 0.0011 - val_accuracy: 0.9373\n",
            "Epoch 608/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.3225e-05 - accuracy: 0.9949 - val_loss: 0.0011 - val_accuracy: 0.9403\n",
            "Epoch 609/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.0790e-05 - accuracy: 0.9961 - val_loss: 0.0011 - val_accuracy: 0.9415\n",
            "Epoch 610/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.0477e-05 - accuracy: 0.9967 - val_loss: 0.0011 - val_accuracy: 0.9397\n",
            "Epoch 611/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.1012e-05 - accuracy: 0.9967 - val_loss: 0.0014 - val_accuracy: 0.9253\n",
            "Epoch 612/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.5116e-05 - accuracy: 0.9964 - val_loss: 0.0013 - val_accuracy: 0.9297\n",
            "Epoch 613/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.0744e-05 - accuracy: 0.9974 - val_loss: 0.0010 - val_accuracy: 0.9438\n",
            "Epoch 614/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.9263e-05 - accuracy: 0.9974 - val_loss: 0.0011 - val_accuracy: 0.9390\n",
            "Epoch 615/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.0916e-05 - accuracy: 0.9973 - val_loss: 0.0010 - val_accuracy: 0.9412\n",
            "Epoch 616/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.9778e-05 - accuracy: 0.9973 - val_loss: 0.0010 - val_accuracy: 0.9430\n",
            "Epoch 617/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.4125e-05 - accuracy: 0.9970 - val_loss: 0.0012 - val_accuracy: 0.9317\n",
            "Epoch 618/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.4056e-05 - accuracy: 0.9955 - val_loss: 0.0012 - val_accuracy: 0.9347\n",
            "Epoch 619/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.3100e-05 - accuracy: 0.9965 - val_loss: 0.0011 - val_accuracy: 0.9382\n",
            "Epoch 620/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.5689e-05 - accuracy: 0.9958 - val_loss: 0.0011 - val_accuracy: 0.9380\n",
            "Epoch 621/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.6241e-05 - accuracy: 0.9962 - val_loss: 0.0011 - val_accuracy: 0.9372\n",
            "Epoch 622/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.4225e-05 - accuracy: 0.9971 - val_loss: 0.0011 - val_accuracy: 0.9368\n",
            "Epoch 623/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.9614e-05 - accuracy: 0.9962 - val_loss: 0.0010 - val_accuracy: 0.9423\n",
            "Epoch 624/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.2438e-05 - accuracy: 0.9972 - val_loss: 0.0011 - val_accuracy: 0.9392\n",
            "Epoch 625/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.9430e-05 - accuracy: 0.9968 - val_loss: 0.0010 - val_accuracy: 0.9437\n",
            "Epoch 626/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.2056e-05 - accuracy: 0.9967 - val_loss: 0.0010 - val_accuracy: 0.9423\n",
            "Epoch 627/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.3460e-05 - accuracy: 0.9972 - val_loss: 0.0011 - val_accuracy: 0.9418\n",
            "Epoch 628/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.1979e-05 - accuracy: 0.9967 - val_loss: 0.0012 - val_accuracy: 0.9348\n",
            "Epoch 629/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.7693e-05 - accuracy: 0.9963 - val_loss: 0.0012 - val_accuracy: 0.9333\n",
            "Epoch 630/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.3292e-05 - accuracy: 0.9965 - val_loss: 0.0012 - val_accuracy: 0.9323\n",
            "Epoch 631/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.1980e-05 - accuracy: 0.9972 - val_loss: 9.9793e-04 - val_accuracy: 0.9450\n",
            "Epoch 632/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.8636e-05 - accuracy: 0.9974 - val_loss: 9.8925e-04 - val_accuracy: 0.9448\n",
            "Epoch 633/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.6711e-05 - accuracy: 0.9969 - val_loss: 0.0010 - val_accuracy: 0.9432\n",
            "Epoch 634/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.0057e-05 - accuracy: 0.9966 - val_loss: 0.0011 - val_accuracy: 0.9398\n",
            "Epoch 635/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.1211e-05 - accuracy: 0.9972 - val_loss: 9.6078e-04 - val_accuracy: 0.9467\n",
            "Epoch 636/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.0912e-05 - accuracy: 0.9967 - val_loss: 0.0014 - val_accuracy: 0.9247\n",
            "Epoch 637/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.8757e-05 - accuracy: 0.9945 - val_loss: 0.0012 - val_accuracy: 0.9338\n",
            "Epoch 638/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.9884e-05 - accuracy: 0.9956 - val_loss: 0.0012 - val_accuracy: 0.9322\n",
            "Epoch 639/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.9981e-05 - accuracy: 0.9968 - val_loss: 0.0011 - val_accuracy: 0.9398\n",
            "Epoch 640/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.5372e-05 - accuracy: 0.9964 - val_loss: 0.0013 - val_accuracy: 0.9295\n",
            "Epoch 641/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.8211e-05 - accuracy: 0.9957 - val_loss: 0.0013 - val_accuracy: 0.9298\n",
            "Epoch 642/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.2710e-05 - accuracy: 0.9973 - val_loss: 9.2688e-04 - val_accuracy: 0.9485\n",
            "Epoch 643/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.2129e-05 - accuracy: 0.9972 - val_loss: 9.5943e-04 - val_accuracy: 0.9470\n",
            "Epoch 644/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.1273e-05 - accuracy: 0.9973 - val_loss: 0.0011 - val_accuracy: 0.9390\n",
            "Epoch 645/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.5243e-05 - accuracy: 0.9976 - val_loss: 0.0011 - val_accuracy: 0.9408\n",
            "Epoch 646/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.3746e-05 - accuracy: 0.9977 - val_loss: 8.9389e-04 - val_accuracy: 0.9505\n",
            "Epoch 647/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.6688e-05 - accuracy: 0.9975 - val_loss: 0.0012 - val_accuracy: 0.9345\n",
            "Epoch 648/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.0059e-05 - accuracy: 0.9971 - val_loss: 9.7025e-04 - val_accuracy: 0.9452\n",
            "Epoch 649/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.7714e-05 - accuracy: 0.9969 - val_loss: 9.7182e-04 - val_accuracy: 0.9462\n",
            "Epoch 650/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.4299e-05 - accuracy: 0.9977 - val_loss: 0.0011 - val_accuracy: 0.9405\n",
            "Epoch 651/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.4654e-05 - accuracy: 0.9977 - val_loss: 9.8600e-04 - val_accuracy: 0.9460\n",
            "Epoch 652/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.3932e-05 - accuracy: 0.9977 - val_loss: 0.0011 - val_accuracy: 0.9423\n",
            "Epoch 653/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.7587e-05 - accuracy: 0.9975 - val_loss: 9.2894e-04 - val_accuracy: 0.9487\n",
            "Epoch 654/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.1141e-05 - accuracy: 0.9979 - val_loss: 9.0579e-04 - val_accuracy: 0.9495\n",
            "Epoch 655/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.3563e-05 - accuracy: 0.9978 - val_loss: 9.6469e-04 - val_accuracy: 0.9468\n",
            "Epoch 656/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 3.9772e-05 - accuracy: 0.9980 - val_loss: 9.3842e-04 - val_accuracy: 0.9478\n",
            "Epoch 657/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.9406e-05 - accuracy: 0.9980 - val_loss: 9.6600e-04 - val_accuracy: 0.9470\n",
            "Epoch 658/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.9699e-05 - accuracy: 0.9980 - val_loss: 0.0010 - val_accuracy: 0.9450\n",
            "Epoch 659/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.9615e-05 - accuracy: 0.9980 - val_loss: 9.3685e-04 - val_accuracy: 0.9492\n",
            "Epoch 660/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.1535e-05 - accuracy: 0.9979 - val_loss: 9.4962e-04 - val_accuracy: 0.9477\n",
            "Epoch 661/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.3977e-05 - accuracy: 0.9977 - val_loss: 9.9401e-04 - val_accuracy: 0.9443\n",
            "Epoch 662/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.8108e-05 - accuracy: 0.9964 - val_loss: 0.0015 - val_accuracy: 0.9192\n",
            "Epoch 663/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.6142e-05 - accuracy: 0.9948 - val_loss: 0.0011 - val_accuracy: 0.9403\n",
            "Epoch 664/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.2269e-05 - accuracy: 0.9962 - val_loss: 0.0012 - val_accuracy: 0.9338\n",
            "Epoch 665/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.8011e-05 - accuracy: 0.9964 - val_loss: 9.9344e-04 - val_accuracy: 0.9450\n",
            "Epoch 666/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.6469e-05 - accuracy: 0.9963 - val_loss: 0.0013 - val_accuracy: 0.9263\n",
            "Epoch 667/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.2867e-05 - accuracy: 0.9958 - val_loss: 0.0012 - val_accuracy: 0.9347\n",
            "Epoch 668/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.9456e-05 - accuracy: 0.9962 - val_loss: 0.0011 - val_accuracy: 0.9390\n",
            "Epoch 669/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.2843e-05 - accuracy: 0.9971 - val_loss: 0.0011 - val_accuracy: 0.9370\n",
            "Epoch 670/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.4863e-05 - accuracy: 0.9977 - val_loss: 0.0011 - val_accuracy: 0.9372\n",
            "Epoch 671/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.4969e-05 - accuracy: 0.9971 - val_loss: 0.0013 - val_accuracy: 0.9308\n",
            "Epoch 672/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 7.8208e-05 - accuracy: 0.9958 - val_loss: 0.0010 - val_accuracy: 0.9455\n",
            "Epoch 673/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.0188e-05 - accuracy: 0.9962 - val_loss: 0.0013 - val_accuracy: 0.9262\n",
            "Epoch 674/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.0679e-05 - accuracy: 0.9961 - val_loss: 0.0012 - val_accuracy: 0.9335\n",
            "Epoch 675/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.0140e-04 - accuracy: 0.9942 - val_loss: 0.0014 - val_accuracy: 0.9212\n",
            "Epoch 676/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.0289e-04 - accuracy: 0.9942 - val_loss: 0.0011 - val_accuracy: 0.9413\n",
            "Epoch 677/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 7.2143e-05 - accuracy: 0.9960 - val_loss: 0.0011 - val_accuracy: 0.9413\n",
            "Epoch 678/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.8176e-05 - accuracy: 0.9963 - val_loss: 0.0011 - val_accuracy: 0.9375\n",
            "Epoch 679/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.3120e-05 - accuracy: 0.9971 - val_loss: 9.8461e-04 - val_accuracy: 0.9455\n",
            "Epoch 680/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.0957e-05 - accuracy: 0.9972 - val_loss: 0.0010 - val_accuracy: 0.9430\n",
            "Epoch 681/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.9686e-05 - accuracy: 0.9974 - val_loss: 9.9116e-04 - val_accuracy: 0.9460\n",
            "Epoch 682/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.2580e-05 - accuracy: 0.9973 - val_loss: 0.0011 - val_accuracy: 0.9383\n",
            "Epoch 683/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.1261e-05 - accuracy: 0.9973 - val_loss: 0.0010 - val_accuracy: 0.9433\n",
            "Epoch 684/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.9915e-05 - accuracy: 0.9974 - val_loss: 0.0010 - val_accuracy: 0.9447\n",
            "Epoch 685/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.8725e-05 - accuracy: 0.9973 - val_loss: 0.0011 - val_accuracy: 0.9373\n",
            "Epoch 686/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.2772e-05 - accuracy: 0.9972 - val_loss: 9.7800e-04 - val_accuracy: 0.9458\n",
            "Epoch 687/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.0360e-05 - accuracy: 0.9980 - val_loss: 9.2098e-04 - val_accuracy: 0.9488\n",
            "Epoch 688/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.0805e-05 - accuracy: 0.9979 - val_loss: 9.8645e-04 - val_accuracy: 0.9463\n",
            "Epoch 689/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.2326e-05 - accuracy: 0.9977 - val_loss: 8.8741e-04 - val_accuracy: 0.9513\n",
            "Epoch 690/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.5501e-05 - accuracy: 0.9976 - val_loss: 8.8724e-04 - val_accuracy: 0.9512\n",
            "Epoch 691/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.3457e-05 - accuracy: 0.9978 - val_loss: 9.1775e-04 - val_accuracy: 0.9492\n",
            "Epoch 692/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.2935e-05 - accuracy: 0.9978 - val_loss: 8.3229e-04 - val_accuracy: 0.9545\n",
            "Epoch 693/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.2277e-05 - accuracy: 0.9978 - val_loss: 9.1057e-04 - val_accuracy: 0.9500\n",
            "Epoch 694/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.3050e-05 - accuracy: 0.9978 - val_loss: 9.5167e-04 - val_accuracy: 0.9477\n",
            "Epoch 695/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.1182e-05 - accuracy: 0.9979 - val_loss: 9.6500e-04 - val_accuracy: 0.9468\n",
            "Epoch 696/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.0628e-05 - accuracy: 0.9980 - val_loss: 8.8505e-04 - val_accuracy: 0.9510\n",
            "Epoch 697/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.0396e-05 - accuracy: 0.9980 - val_loss: 8.3402e-04 - val_accuracy: 0.9537\n",
            "Epoch 698/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.9480e-05 - accuracy: 0.9980 - val_loss: 8.3937e-04 - val_accuracy: 0.9525\n",
            "Epoch 699/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.1155e-05 - accuracy: 0.9979 - val_loss: 9.0755e-04 - val_accuracy: 0.9495\n",
            "Epoch 700/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.6433e-05 - accuracy: 0.9975 - val_loss: 8.5921e-04 - val_accuracy: 0.9522\n",
            "Epoch 701/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.4625e-05 - accuracy: 0.9977 - val_loss: 0.0010 - val_accuracy: 0.9455\n",
            "Epoch 702/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.3686e-05 - accuracy: 0.9977 - val_loss: 9.1775e-04 - val_accuracy: 0.9497\n",
            "Epoch 703/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.3488e-05 - accuracy: 0.9972 - val_loss: 0.0011 - val_accuracy: 0.9403\n",
            "Epoch 704/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.3471e-05 - accuracy: 0.9961 - val_loss: 0.0012 - val_accuracy: 0.9333\n",
            "Epoch 705/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.2550e-05 - accuracy: 0.9967 - val_loss: 0.0012 - val_accuracy: 0.9335\n",
            "Epoch 706/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.8090e-05 - accuracy: 0.9967 - val_loss: 0.0011 - val_accuracy: 0.9383\n",
            "Epoch 707/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.3383e-05 - accuracy: 0.9953 - val_loss: 0.0013 - val_accuracy: 0.9307\n",
            "Epoch 708/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.2187e-05 - accuracy: 0.9954 - val_loss: 0.0013 - val_accuracy: 0.9253\n",
            "Epoch 709/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 8.5390e-05 - accuracy: 0.9953 - val_loss: 0.0012 - val_accuracy: 0.9328\n",
            "Epoch 710/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.7722e-05 - accuracy: 0.9968 - val_loss: 0.0011 - val_accuracy: 0.9370\n",
            "Epoch 711/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.6998e-05 - accuracy: 0.9976 - val_loss: 0.0011 - val_accuracy: 0.9390\n",
            "Epoch 712/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.5240e-05 - accuracy: 0.9970 - val_loss: 0.0011 - val_accuracy: 0.9383\n",
            "Epoch 713/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.1556e-05 - accuracy: 0.9973 - val_loss: 0.0013 - val_accuracy: 0.9295\n",
            "Epoch 714/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.6869e-05 - accuracy: 0.9970 - val_loss: 0.0010 - val_accuracy: 0.9453\n",
            "Epoch 715/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.7179e-05 - accuracy: 0.9969 - val_loss: 9.6748e-04 - val_accuracy: 0.9478\n",
            "Epoch 716/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.4593e-05 - accuracy: 0.9971 - val_loss: 0.0011 - val_accuracy: 0.9387\n",
            "Epoch 717/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.3189e-05 - accuracy: 0.9972 - val_loss: 9.1388e-04 - val_accuracy: 0.9503\n",
            "Epoch 718/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.8094e-05 - accuracy: 0.9974 - val_loss: 0.0010 - val_accuracy: 0.9435\n",
            "Epoch 719/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 6.2228e-05 - accuracy: 0.9966 - val_loss: 0.0011 - val_accuracy: 0.9408\n",
            "Epoch 720/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.8060e-05 - accuracy: 0.9975 - val_loss: 9.7128e-04 - val_accuracy: 0.9465\n",
            "Epoch 721/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.3394e-05 - accuracy: 0.9977 - val_loss: 9.3782e-04 - val_accuracy: 0.9487\n",
            "Epoch 722/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.3089e-05 - accuracy: 0.9978 - val_loss: 9.6698e-04 - val_accuracy: 0.9467\n",
            "Epoch 723/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.3044e-05 - accuracy: 0.9977 - val_loss: 0.0011 - val_accuracy: 0.9417\n",
            "Epoch 724/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 7.8474e-05 - accuracy: 0.9955 - val_loss: 0.0011 - val_accuracy: 0.9368\n",
            "Epoch 725/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.0068e-05 - accuracy: 0.9967 - val_loss: 0.0013 - val_accuracy: 0.9315\n",
            "Epoch 726/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.5272e-05 - accuracy: 0.9958 - val_loss: 0.0012 - val_accuracy: 0.9365\n",
            "Epoch 727/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.3287e-05 - accuracy: 0.9960 - val_loss: 0.0013 - val_accuracy: 0.9292\n",
            "Epoch 728/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 7.0171e-05 - accuracy: 0.9960 - val_loss: 0.0011 - val_accuracy: 0.9385\n",
            "Epoch 729/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.1812e-05 - accuracy: 0.9966 - val_loss: 0.0011 - val_accuracy: 0.9420\n",
            "Epoch 730/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.7390e-05 - accuracy: 0.9957 - val_loss: 0.0012 - val_accuracy: 0.9363\n",
            "Epoch 731/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.5950e-05 - accuracy: 0.9965 - val_loss: 0.0011 - val_accuracy: 0.9418\n",
            "Epoch 732/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.5554e-05 - accuracy: 0.9970 - val_loss: 0.0011 - val_accuracy: 0.9397\n",
            "Epoch 733/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.8373e-05 - accuracy: 0.9970 - val_loss: 0.0011 - val_accuracy: 0.9395\n",
            "Epoch 734/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 6.5315e-05 - accuracy: 0.9965 - val_loss: 9.0883e-04 - val_accuracy: 0.9503\n",
            "Epoch 735/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.5287e-05 - accuracy: 0.9971 - val_loss: 0.0010 - val_accuracy: 0.9430\n",
            "Epoch 736/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.5598e-05 - accuracy: 0.9959 - val_loss: 0.0012 - val_accuracy: 0.9340\n",
            "Epoch 737/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.5559e-05 - accuracy: 0.9971 - val_loss: 9.3415e-04 - val_accuracy: 0.9492\n",
            "Epoch 738/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.9832e-05 - accuracy: 0.9973 - val_loss: 0.0010 - val_accuracy: 0.9427\n",
            "Epoch 739/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.1606e-05 - accuracy: 0.9972 - val_loss: 9.2003e-04 - val_accuracy: 0.9498\n",
            "Epoch 740/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.9814e-05 - accuracy: 0.9973 - val_loss: 0.0012 - val_accuracy: 0.9340\n",
            "Epoch 741/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.6545e-05 - accuracy: 0.9970 - val_loss: 9.4281e-04 - val_accuracy: 0.9478\n",
            "Epoch 742/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.8387e-05 - accuracy: 0.9973 - val_loss: 9.2006e-04 - val_accuracy: 0.9497\n",
            "Epoch 743/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.8519e-05 - accuracy: 0.9974 - val_loss: 9.4186e-04 - val_accuracy: 0.9480\n",
            "Epoch 744/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.4549e-05 - accuracy: 0.9977 - val_loss: 0.0010 - val_accuracy: 0.9450\n",
            "Epoch 745/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.4274e-05 - accuracy: 0.9977 - val_loss: 8.4707e-04 - val_accuracy: 0.9517\n",
            "Epoch 746/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.4818e-05 - accuracy: 0.9976 - val_loss: 0.0010 - val_accuracy: 0.9425\n",
            "Epoch 747/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.2958e-05 - accuracy: 0.9978 - val_loss: 0.0010 - val_accuracy: 0.9430\n",
            "Epoch 748/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.5102e-05 - accuracy: 0.9977 - val_loss: 0.0011 - val_accuracy: 0.9380\n",
            "Epoch 749/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.5708e-05 - accuracy: 0.9970 - val_loss: 9.4179e-04 - val_accuracy: 0.9478\n",
            "Epoch 750/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.5191e-05 - accuracy: 0.9970 - val_loss: 0.0011 - val_accuracy: 0.9403\n",
            "Epoch 751/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 6.0836e-05 - accuracy: 0.9967 - val_loss: 0.0010 - val_accuracy: 0.9443\n",
            "Epoch 752/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 8.9464e-05 - accuracy: 0.9952 - val_loss: 0.0011 - val_accuracy: 0.9377\n",
            "Epoch 753/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.0607e-05 - accuracy: 0.9968 - val_loss: 0.0012 - val_accuracy: 0.9360\n",
            "Epoch 754/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.4896e-05 - accuracy: 0.9970 - val_loss: 0.0012 - val_accuracy: 0.9338\n",
            "Epoch 755/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 9.0680e-05 - accuracy: 0.9950 - val_loss: 0.0014 - val_accuracy: 0.9237\n",
            "Epoch 756/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.7790e-05 - accuracy: 0.9964 - val_loss: 0.0014 - val_accuracy: 0.9212\n",
            "Epoch 757/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.0246e-04 - accuracy: 0.9942 - val_loss: 0.0012 - val_accuracy: 0.9365\n",
            "Epoch 758/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.5044e-05 - accuracy: 0.9965 - val_loss: 0.0010 - val_accuracy: 0.9423\n",
            "Epoch 759/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 6.1251e-05 - accuracy: 0.9967 - val_loss: 0.0012 - val_accuracy: 0.9342\n",
            "Epoch 760/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.3970e-05 - accuracy: 0.9966 - val_loss: 0.0012 - val_accuracy: 0.9370\n",
            "Epoch 761/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.6604e-05 - accuracy: 0.9970 - val_loss: 0.0011 - val_accuracy: 0.9397\n",
            "Epoch 762/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.6377e-05 - accuracy: 0.9975 - val_loss: 9.7380e-04 - val_accuracy: 0.9458\n",
            "Epoch 763/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.6990e-05 - accuracy: 0.9975 - val_loss: 0.0011 - val_accuracy: 0.9417\n",
            "Epoch 764/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.5554e-05 - accuracy: 0.9977 - val_loss: 9.8763e-04 - val_accuracy: 0.9458\n",
            "Epoch 765/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.2933e-05 - accuracy: 0.9978 - val_loss: 8.9810e-04 - val_accuracy: 0.9497\n",
            "Epoch 766/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.6258e-05 - accuracy: 0.9976 - val_loss: 0.0010 - val_accuracy: 0.9427\n",
            "Epoch 767/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.1481e-05 - accuracy: 0.9966 - val_loss: 0.0011 - val_accuracy: 0.9375\n",
            "Epoch 768/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.5804e-05 - accuracy: 0.9971 - val_loss: 0.0010 - val_accuracy: 0.9457\n",
            "Epoch 769/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 6.3595e-05 - accuracy: 0.9965 - val_loss: 0.0013 - val_accuracy: 0.9302\n",
            "Epoch 770/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.4190e-05 - accuracy: 0.9971 - val_loss: 0.0011 - val_accuracy: 0.9407\n",
            "Epoch 771/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.4887e-05 - accuracy: 0.9977 - val_loss: 0.0011 - val_accuracy: 0.9407\n",
            "Epoch 772/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.9643e-05 - accuracy: 0.9973 - val_loss: 9.7865e-04 - val_accuracy: 0.9460\n",
            "Epoch 773/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 6.6591e-05 - accuracy: 0.9964 - val_loss: 0.0011 - val_accuracy: 0.9375\n",
            "Epoch 774/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.8913e-05 - accuracy: 0.9974 - val_loss: 0.0010 - val_accuracy: 0.9430\n",
            "Epoch 775/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 7.3916e-05 - accuracy: 0.9960 - val_loss: 0.0013 - val_accuracy: 0.9292\n",
            "Epoch 776/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 6.9554e-05 - accuracy: 0.9961 - val_loss: 0.0012 - val_accuracy: 0.9350\n",
            "Epoch 777/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.5963e-05 - accuracy: 0.9970 - val_loss: 9.5951e-04 - val_accuracy: 0.9467\n",
            "Epoch 778/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.4888e-05 - accuracy: 0.9977 - val_loss: 9.3081e-04 - val_accuracy: 0.9490\n",
            "Epoch 779/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.8699e-05 - accuracy: 0.9974 - val_loss: 9.5171e-04 - val_accuracy: 0.9483\n",
            "Epoch 780/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.6675e-05 - accuracy: 0.9975 - val_loss: 0.0011 - val_accuracy: 0.9390\n",
            "Epoch 781/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 6.0755e-05 - accuracy: 0.9968 - val_loss: 9.1967e-04 - val_accuracy: 0.9498\n",
            "Epoch 782/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.3212e-05 - accuracy: 0.9971 - val_loss: 0.0012 - val_accuracy: 0.9358\n",
            "Epoch 783/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.3668e-05 - accuracy: 0.9972 - val_loss: 0.0011 - val_accuracy: 0.9372\n",
            "Epoch 784/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.0372e-05 - accuracy: 0.9973 - val_loss: 0.0014 - val_accuracy: 0.9215\n",
            "Epoch 785/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.7151e-05 - accuracy: 0.9969 - val_loss: 9.7768e-04 - val_accuracy: 0.9472\n",
            "Epoch 786/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 6.9330e-05 - accuracy: 0.9962 - val_loss: 9.4774e-04 - val_accuracy: 0.9472\n",
            "Epoch 787/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.8787e-05 - accuracy: 0.9973 - val_loss: 9.1333e-04 - val_accuracy: 0.9493\n",
            "Epoch 788/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.0005e-05 - accuracy: 0.9974 - val_loss: 9.7331e-04 - val_accuracy: 0.9457\n",
            "Epoch 789/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.5010e-05 - accuracy: 0.9976 - val_loss: 9.0879e-04 - val_accuracy: 0.9508\n",
            "Epoch 790/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.2624e-05 - accuracy: 0.9977 - val_loss: 0.0010 - val_accuracy: 0.9430\n",
            "Epoch 791/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.5433e-05 - accuracy: 0.9970 - val_loss: 0.0011 - val_accuracy: 0.9400\n",
            "Epoch 792/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 6.2392e-05 - accuracy: 0.9967 - val_loss: 9.8150e-04 - val_accuracy: 0.9462\n",
            "Epoch 793/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.6794e-05 - accuracy: 0.9970 - val_loss: 9.5975e-04 - val_accuracy: 0.9467\n",
            "Epoch 794/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.5417e-05 - accuracy: 0.9977 - val_loss: 8.8354e-04 - val_accuracy: 0.9513\n",
            "Epoch 795/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.2886e-05 - accuracy: 0.9978 - val_loss: 9.8757e-04 - val_accuracy: 0.9455\n",
            "Epoch 796/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.8616e-05 - accuracy: 0.9974 - val_loss: 9.4167e-04 - val_accuracy: 0.9478\n",
            "Epoch 797/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.7781e-05 - accuracy: 0.9975 - val_loss: 0.0011 - val_accuracy: 0.9378\n",
            "Epoch 798/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.9749e-05 - accuracy: 0.9968 - val_loss: 0.0011 - val_accuracy: 0.9400\n",
            "Epoch 799/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.8434e-05 - accuracy: 0.9962 - val_loss: 0.0012 - val_accuracy: 0.9323\n",
            "Epoch 800/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.1208e-05 - accuracy: 0.9961 - val_loss: 0.0011 - val_accuracy: 0.9420\n",
            "Epoch 801/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.4251e-05 - accuracy: 0.9971 - val_loss: 0.0012 - val_accuracy: 0.9345\n",
            "Epoch 802/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.3207e-05 - accuracy: 0.9971 - val_loss: 0.0012 - val_accuracy: 0.9325\n",
            "Epoch 803/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.8319e-05 - accuracy: 0.9969 - val_loss: 9.5635e-04 - val_accuracy: 0.9478\n",
            "Epoch 804/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.7939e-05 - accuracy: 0.9974 - val_loss: 9.5161e-04 - val_accuracy: 0.9478\n",
            "Epoch 805/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.8636e-05 - accuracy: 0.9974 - val_loss: 9.3523e-04 - val_accuracy: 0.9493\n",
            "Epoch 806/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.8651e-05 - accuracy: 0.9975 - val_loss: 8.2805e-04 - val_accuracy: 0.9548\n",
            "Epoch 807/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.2071e-05 - accuracy: 0.9978 - val_loss: 9.1707e-04 - val_accuracy: 0.9502\n",
            "Epoch 808/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.9237e-05 - accuracy: 0.9974 - val_loss: 9.2787e-04 - val_accuracy: 0.9485\n",
            "Epoch 809/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.3823e-05 - accuracy: 0.9977 - val_loss: 9.6197e-04 - val_accuracy: 0.9470\n",
            "Epoch 810/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.4401e-05 - accuracy: 0.9977 - val_loss: 8.6751e-04 - val_accuracy: 0.9520\n",
            "Epoch 811/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.4221e-05 - accuracy: 0.9977 - val_loss: 8.3895e-04 - val_accuracy: 0.9540\n",
            "Epoch 812/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 3.9948e-05 - accuracy: 0.9980 - val_loss: 8.3057e-04 - val_accuracy: 0.9535\n",
            "Epoch 813/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 3.8686e-05 - accuracy: 0.9980 - val_loss: 7.5917e-04 - val_accuracy: 0.9585\n",
            "Epoch 814/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 3.9046e-05 - accuracy: 0.9980 - val_loss: 9.1509e-04 - val_accuracy: 0.9498\n",
            "Epoch 815/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 3.9188e-05 - accuracy: 0.9980 - val_loss: 8.2309e-04 - val_accuracy: 0.9542\n",
            "Epoch 816/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.4989e-05 - accuracy: 0.9976 - val_loss: 8.7403e-04 - val_accuracy: 0.9515\n",
            "Epoch 817/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.8752e-05 - accuracy: 0.9975 - val_loss: 8.9972e-04 - val_accuracy: 0.9513\n",
            "Epoch 818/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.6186e-05 - accuracy: 0.9976 - val_loss: 0.0011 - val_accuracy: 0.9383\n",
            "Epoch 819/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.4397e-05 - accuracy: 0.9977 - val_loss: 9.7962e-04 - val_accuracy: 0.9458\n",
            "Epoch 820/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.2371e-05 - accuracy: 0.9979 - val_loss: 9.8388e-04 - val_accuracy: 0.9458\n",
            "Epoch 821/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.9830e-05 - accuracy: 0.9980 - val_loss: 8.7434e-04 - val_accuracy: 0.9523\n",
            "Epoch 822/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.2263e-05 - accuracy: 0.9977 - val_loss: 9.1336e-04 - val_accuracy: 0.9512\n",
            "Epoch 823/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 6.6186e-05 - accuracy: 0.9964 - val_loss: 0.0011 - val_accuracy: 0.9407\n",
            "Epoch 824/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 9.9777e-05 - accuracy: 0.9946 - val_loss: 0.0013 - val_accuracy: 0.9303\n",
            "Epoch 825/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 1.0991e-04 - accuracy: 0.9939 - val_loss: 0.0013 - val_accuracy: 0.9292\n",
            "Epoch 826/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 6.8840e-05 - accuracy: 0.9962 - val_loss: 0.0011 - val_accuracy: 0.9413\n",
            "Epoch 827/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 7.1306e-05 - accuracy: 0.9962 - val_loss: 0.0010 - val_accuracy: 0.9435\n",
            "Epoch 828/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.8352e-05 - accuracy: 0.9975 - val_loss: 0.0010 - val_accuracy: 0.9450\n",
            "Epoch 829/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.0373e-05 - accuracy: 0.9973 - val_loss: 0.0010 - val_accuracy: 0.9438\n",
            "Epoch 830/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.3360e-05 - accuracy: 0.9977 - val_loss: 0.0011 - val_accuracy: 0.9407\n",
            "Epoch 831/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.1484e-05 - accuracy: 0.9973 - val_loss: 8.6773e-04 - val_accuracy: 0.9527\n",
            "Epoch 832/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.8694e-05 - accuracy: 0.9974 - val_loss: 9.1620e-04 - val_accuracy: 0.9497\n",
            "Epoch 833/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.8177e-05 - accuracy: 0.9973 - val_loss: 0.0010 - val_accuracy: 0.9443\n",
            "Epoch 834/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 6.0197e-05 - accuracy: 0.9969 - val_loss: 9.8167e-04 - val_accuracy: 0.9468\n",
            "Epoch 835/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.1826e-05 - accuracy: 0.9978 - val_loss: 9.6303e-04 - val_accuracy: 0.9470\n",
            "Epoch 836/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.1863e-05 - accuracy: 0.9978 - val_loss: 9.9386e-04 - val_accuracy: 0.9457\n",
            "Epoch 837/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.9393e-05 - accuracy: 0.9969 - val_loss: 0.0010 - val_accuracy: 0.9425\n",
            "Epoch 838/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.9759e-05 - accuracy: 0.9973 - val_loss: 9.7183e-04 - val_accuracy: 0.9460\n",
            "Epoch 839/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.7187e-05 - accuracy: 0.9975 - val_loss: 9.1164e-04 - val_accuracy: 0.9500\n",
            "Epoch 840/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.6946e-05 - accuracy: 0.9975 - val_loss: 8.7859e-04 - val_accuracy: 0.9522\n",
            "Epoch 841/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.8529e-05 - accuracy: 0.9974 - val_loss: 9.4086e-04 - val_accuracy: 0.9470\n",
            "Epoch 842/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.4415e-05 - accuracy: 0.9977 - val_loss: 8.7362e-04 - val_accuracy: 0.9525\n",
            "Epoch 843/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.0012e-05 - accuracy: 0.9980 - val_loss: 9.4230e-04 - val_accuracy: 0.9497\n",
            "Epoch 844/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.6135e-05 - accuracy: 0.9975 - val_loss: 9.2255e-04 - val_accuracy: 0.9490\n",
            "Epoch 845/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.1993e-05 - accuracy: 0.9971 - val_loss: 0.0011 - val_accuracy: 0.9415\n",
            "Epoch 846/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.1686e-05 - accuracy: 0.9967 - val_loss: 0.0010 - val_accuracy: 0.9445\n",
            "Epoch 847/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.3517e-05 - accuracy: 0.9972 - val_loss: 0.0011 - val_accuracy: 0.9387\n",
            "Epoch 848/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.4653e-05 - accuracy: 0.9976 - val_loss: 0.0011 - val_accuracy: 0.9378\n",
            "Epoch 849/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.6916e-05 - accuracy: 0.9970 - val_loss: 0.0011 - val_accuracy: 0.9385\n",
            "Epoch 850/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 1.0506e-04 - accuracy: 0.9942 - val_loss: 0.0011 - val_accuracy: 0.9417\n",
            "Epoch 851/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 7.6657e-05 - accuracy: 0.9959 - val_loss: 0.0013 - val_accuracy: 0.9307\n",
            "Epoch 852/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 6.7484e-05 - accuracy: 0.9964 - val_loss: 0.0010 - val_accuracy: 0.9440\n",
            "Epoch 853/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.6985e-05 - accuracy: 0.9968 - val_loss: 0.0010 - val_accuracy: 0.9430\n",
            "Epoch 854/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.0255e-05 - accuracy: 0.9974 - val_loss: 9.6459e-04 - val_accuracy: 0.9478\n",
            "Epoch 855/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.3052e-05 - accuracy: 0.9978 - val_loss: 8.3207e-04 - val_accuracy: 0.9545\n",
            "Epoch 856/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.1649e-05 - accuracy: 0.9979 - val_loss: 8.2996e-04 - val_accuracy: 0.9553\n",
            "Epoch 857/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.9938e-05 - accuracy: 0.9973 - val_loss: 0.0012 - val_accuracy: 0.9367\n",
            "Epoch 858/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.5573e-05 - accuracy: 0.9970 - val_loss: 9.9340e-04 - val_accuracy: 0.9450\n",
            "Epoch 859/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 7.1625e-05 - accuracy: 0.9962 - val_loss: 0.0012 - val_accuracy: 0.9365\n",
            "Epoch 860/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 6.7964e-05 - accuracy: 0.9963 - val_loss: 0.0011 - val_accuracy: 0.9368\n",
            "Epoch 861/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.4836e-05 - accuracy: 0.9971 - val_loss: 0.0010 - val_accuracy: 0.9437\n",
            "Epoch 862/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.5156e-05 - accuracy: 0.9976 - val_loss: 8.8903e-04 - val_accuracy: 0.9510\n",
            "Epoch 863/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.6761e-05 - accuracy: 0.9975 - val_loss: 9.5539e-04 - val_accuracy: 0.9472\n",
            "Epoch 864/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.1865e-05 - accuracy: 0.9978 - val_loss: 9.6016e-04 - val_accuracy: 0.9482\n",
            "Epoch 865/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.1968e-05 - accuracy: 0.9972 - val_loss: 0.0010 - val_accuracy: 0.9417\n",
            "Epoch 866/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.5621e-05 - accuracy: 0.9976 - val_loss: 9.8557e-04 - val_accuracy: 0.9457\n",
            "Epoch 867/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.6941e-05 - accuracy: 0.9975 - val_loss: 0.0012 - val_accuracy: 0.9347\n",
            "Epoch 868/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.2069e-05 - accuracy: 0.9974 - val_loss: 9.4368e-04 - val_accuracy: 0.9493\n",
            "Epoch 869/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.7035e-05 - accuracy: 0.9975 - val_loss: 0.0011 - val_accuracy: 0.9420\n",
            "Epoch 870/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.9020e-05 - accuracy: 0.9974 - val_loss: 0.0011 - val_accuracy: 0.9378\n",
            "Epoch 871/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.2058e-05 - accuracy: 0.9973 - val_loss: 9.5898e-04 - val_accuracy: 0.9473\n",
            "Epoch 872/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.5862e-05 - accuracy: 0.9970 - val_loss: 0.0013 - val_accuracy: 0.9288\n",
            "Epoch 873/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 6.1216e-05 - accuracy: 0.9967 - val_loss: 0.0010 - val_accuracy: 0.9455\n",
            "Epoch 874/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.7993e-05 - accuracy: 0.9975 - val_loss: 0.0011 - val_accuracy: 0.9393\n",
            "Epoch 875/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.5430e-05 - accuracy: 0.9965 - val_loss: 0.0013 - val_accuracy: 0.9267\n",
            "Epoch 876/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 6.1762e-05 - accuracy: 0.9967 - val_loss: 9.6197e-04 - val_accuracy: 0.9462\n",
            "Epoch 877/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.0919e-05 - accuracy: 0.9979 - val_loss: 9.4800e-04 - val_accuracy: 0.9473\n",
            "Epoch 878/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.2135e-05 - accuracy: 0.9978 - val_loss: 9.1227e-04 - val_accuracy: 0.9495\n",
            "Epoch 879/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 3.9424e-05 - accuracy: 0.9980 - val_loss: 9.1252e-04 - val_accuracy: 0.9495\n",
            "Epoch 880/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.1859e-05 - accuracy: 0.9978 - val_loss: 0.0010 - val_accuracy: 0.9410\n",
            "Epoch 881/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.7898e-05 - accuracy: 0.9975 - val_loss: 0.0010 - val_accuracy: 0.9438\n",
            "Epoch 882/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.5704e-05 - accuracy: 0.9977 - val_loss: 9.4205e-04 - val_accuracy: 0.9485\n",
            "Epoch 883/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.0346e-05 - accuracy: 0.9979 - val_loss: 9.0219e-04 - val_accuracy: 0.9503\n",
            "Epoch 884/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.4096e-05 - accuracy: 0.9977 - val_loss: 9.2383e-04 - val_accuracy: 0.9510\n",
            "Epoch 885/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.6041e-05 - accuracy: 0.9976 - val_loss: 9.6302e-04 - val_accuracy: 0.9473\n",
            "Epoch 886/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.5145e-05 - accuracy: 0.9971 - val_loss: 0.0011 - val_accuracy: 0.9383\n",
            "Epoch 887/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 6.0943e-05 - accuracy: 0.9967 - val_loss: 0.0012 - val_accuracy: 0.9367\n",
            "Epoch 888/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 7.1440e-05 - accuracy: 0.9961 - val_loss: 0.0013 - val_accuracy: 0.9300\n",
            "Epoch 889/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 6.5648e-05 - accuracy: 0.9965 - val_loss: 0.0011 - val_accuracy: 0.9403\n",
            "Epoch 890/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.1324e-05 - accuracy: 0.9973 - val_loss: 0.0012 - val_accuracy: 0.9360\n",
            "Epoch 891/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.8293e-05 - accuracy: 0.9975 - val_loss: 9.1863e-04 - val_accuracy: 0.9498\n",
            "Epoch 892/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.3175e-05 - accuracy: 0.9971 - val_loss: 0.0012 - val_accuracy: 0.9325\n",
            "Epoch 893/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.1636e-05 - accuracy: 0.9967 - val_loss: 0.0011 - val_accuracy: 0.9388\n",
            "Epoch 894/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.5016e-05 - accuracy: 0.9977 - val_loss: 9.2592e-04 - val_accuracy: 0.9493\n",
            "Epoch 895/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.5747e-05 - accuracy: 0.9976 - val_loss: 0.0013 - val_accuracy: 0.9285\n",
            "Epoch 896/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.9502e-05 - accuracy: 0.9950 - val_loss: 0.0011 - val_accuracy: 0.9375\n",
            "Epoch 897/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 1.3523e-04 - accuracy: 0.9926 - val_loss: 0.0012 - val_accuracy: 0.9357\n",
            "Epoch 898/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 8.0587e-05 - accuracy: 0.9956 - val_loss: 0.0012 - val_accuracy: 0.9332\n",
            "Epoch 899/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 8.1233e-05 - accuracy: 0.9955 - val_loss: 9.0528e-04 - val_accuracy: 0.9490\n",
            "Epoch 900/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 6.5477e-05 - accuracy: 0.9964 - val_loss: 0.0012 - val_accuracy: 0.9362\n",
            "Epoch 901/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 6.8234e-05 - accuracy: 0.9963 - val_loss: 0.0010 - val_accuracy: 0.9462\n",
            "Epoch 902/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.5950e-05 - accuracy: 0.9970 - val_loss: 9.5861e-04 - val_accuracy: 0.9485\n",
            "Epoch 903/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.3750e-05 - accuracy: 0.9977 - val_loss: 0.0010 - val_accuracy: 0.9435\n",
            "Epoch 904/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.3291e-05 - accuracy: 0.9977 - val_loss: 9.5858e-04 - val_accuracy: 0.9488\n",
            "Epoch 905/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.0388e-05 - accuracy: 0.9980 - val_loss: 9.7243e-04 - val_accuracy: 0.9470\n",
            "Epoch 906/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.5973e-05 - accuracy: 0.9975 - val_loss: 0.0011 - val_accuracy: 0.9377\n",
            "Epoch 907/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.6376e-05 - accuracy: 0.9970 - val_loss: 0.0011 - val_accuracy: 0.9410\n",
            "Epoch 908/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.7172e-05 - accuracy: 0.9975 - val_loss: 9.9552e-04 - val_accuracy: 0.9463\n",
            "Epoch 909/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.9911e-05 - accuracy: 0.9974 - val_loss: 9.5442e-04 - val_accuracy: 0.9465\n",
            "Epoch 910/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.8196e-05 - accuracy: 0.9974 - val_loss: 9.6607e-04 - val_accuracy: 0.9477\n",
            "Epoch 911/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.2208e-05 - accuracy: 0.9978 - val_loss: 9.5206e-04 - val_accuracy: 0.9472\n",
            "Epoch 912/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.8163e-05 - accuracy: 0.9969 - val_loss: 0.0011 - val_accuracy: 0.9423\n",
            "Epoch 913/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 5.2301e-05 - accuracy: 0.9972 - val_loss: 0.0011 - val_accuracy: 0.9385\n",
            "Epoch 914/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.6281e-05 - accuracy: 0.9970 - val_loss: 9.9736e-04 - val_accuracy: 0.9457\n",
            "Epoch 915/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.4426e-05 - accuracy: 0.9965 - val_loss: 0.0010 - val_accuracy: 0.9428\n",
            "Epoch 916/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.2409e-05 - accuracy: 0.9973 - val_loss: 0.0010 - val_accuracy: 0.9438\n",
            "Epoch 917/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.2271e-05 - accuracy: 0.9973 - val_loss: 9.0502e-04 - val_accuracy: 0.9500\n",
            "Epoch 918/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.2799e-05 - accuracy: 0.9977 - val_loss: 9.7486e-04 - val_accuracy: 0.9467\n",
            "Epoch 919/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.3411e-05 - accuracy: 0.9971 - val_loss: 0.0010 - val_accuracy: 0.9430\n",
            "Epoch 920/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.0804e-05 - accuracy: 0.9979 - val_loss: 9.0959e-04 - val_accuracy: 0.9515\n",
            "Epoch 921/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.0579e-05 - accuracy: 0.9979 - val_loss: 9.4265e-04 - val_accuracy: 0.9495\n",
            "Epoch 922/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 3.7894e-05 - accuracy: 0.9981 - val_loss: 9.2417e-04 - val_accuracy: 0.9493\n",
            "Epoch 923/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 3.6212e-05 - accuracy: 0.9982 - val_loss: 9.0698e-04 - val_accuracy: 0.9503\n",
            "Epoch 924/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 3.6253e-05 - accuracy: 0.9982 - val_loss: 9.3649e-04 - val_accuracy: 0.9493\n",
            "Epoch 925/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 3.6441e-05 - accuracy: 0.9982 - val_loss: 0.0010 - val_accuracy: 0.9438\n",
            "Epoch 926/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 3.9160e-05 - accuracy: 0.9980 - val_loss: 8.8750e-04 - val_accuracy: 0.9515\n",
            "Epoch 927/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.9652e-05 - accuracy: 0.9979 - val_loss: 0.0010 - val_accuracy: 0.9445\n",
            "Epoch 928/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.0453e-05 - accuracy: 0.9979 - val_loss: 9.8799e-04 - val_accuracy: 0.9468\n",
            "Epoch 929/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.1806e-05 - accuracy: 0.9973 - val_loss: 0.0011 - val_accuracy: 0.9413\n",
            "Epoch 930/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.8168e-05 - accuracy: 0.9976 - val_loss: 8.4204e-04 - val_accuracy: 0.9540\n",
            "Epoch 931/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 3.8787e-05 - accuracy: 0.9980 - val_loss: 8.5780e-04 - val_accuracy: 0.9533\n",
            "Epoch 932/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 3.7684e-05 - accuracy: 0.9981 - val_loss: 8.5755e-04 - val_accuracy: 0.9522\n",
            "Epoch 933/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 3.6599e-05 - accuracy: 0.9981 - val_loss: 9.0800e-04 - val_accuracy: 0.9503\n",
            "Epoch 934/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.5013e-05 - accuracy: 0.9977 - val_loss: 9.5502e-04 - val_accuracy: 0.9473\n",
            "Epoch 935/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 3.8342e-05 - accuracy: 0.9980 - val_loss: 9.2618e-04 - val_accuracy: 0.9495\n",
            "Epoch 936/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.6188e-05 - accuracy: 0.9975 - val_loss: 8.6475e-04 - val_accuracy: 0.9515\n",
            "Epoch 937/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 6.0325e-05 - accuracy: 0.9967 - val_loss: 0.0012 - val_accuracy: 0.9380\n",
            "Epoch 938/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 6.8165e-05 - accuracy: 0.9963 - val_loss: 0.0013 - val_accuracy: 0.9318\n",
            "Epoch 939/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 6.7767e-05 - accuracy: 0.9962 - val_loss: 0.0010 - val_accuracy: 0.9432\n",
            "Epoch 940/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.5106e-05 - accuracy: 0.9970 - val_loss: 0.0012 - val_accuracy: 0.9363\n",
            "Epoch 941/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 7.6083e-05 - accuracy: 0.9957 - val_loss: 0.0010 - val_accuracy: 0.9435\n",
            "Epoch 942/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 8.2676e-05 - accuracy: 0.9956 - val_loss: 0.0011 - val_accuracy: 0.9377\n",
            "Epoch 943/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 6.9013e-05 - accuracy: 0.9962 - val_loss: 0.0011 - val_accuracy: 0.9405\n",
            "Epoch 944/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.8642e-05 - accuracy: 0.9968 - val_loss: 9.5860e-04 - val_accuracy: 0.9480\n",
            "Epoch 945/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 6.7366e-05 - accuracy: 0.9964 - val_loss: 0.0011 - val_accuracy: 0.9395\n",
            "Epoch 946/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.3770e-05 - accuracy: 0.9972 - val_loss: 0.0011 - val_accuracy: 0.9423\n",
            "Epoch 947/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.5895e-05 - accuracy: 0.9976 - val_loss: 9.8844e-04 - val_accuracy: 0.9462\n",
            "Epoch 948/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 6.2375e-05 - accuracy: 0.9967 - val_loss: 9.3854e-04 - val_accuracy: 0.9493\n",
            "Epoch 949/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.2144e-05 - accuracy: 0.9978 - val_loss: 0.0010 - val_accuracy: 0.9442\n",
            "Epoch 950/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.3427e-05 - accuracy: 0.9977 - val_loss: 8.6589e-04 - val_accuracy: 0.9525\n",
            "Epoch 951/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.1531e-05 - accuracy: 0.9979 - val_loss: 0.0010 - val_accuracy: 0.9457\n",
            "Epoch 952/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.0423e-05 - accuracy: 0.9979 - val_loss: 8.4937e-04 - val_accuracy: 0.9535\n",
            "Epoch 953/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 3.7038e-05 - accuracy: 0.9981 - val_loss: 8.8561e-04 - val_accuracy: 0.9528\n",
            "Epoch 954/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 3.9340e-05 - accuracy: 0.9980 - val_loss: 8.5819e-04 - val_accuracy: 0.9518\n",
            "Epoch 955/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 3.8706e-05 - accuracy: 0.9980 - val_loss: 9.2262e-04 - val_accuracy: 0.9488\n",
            "Epoch 956/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 3.6381e-05 - accuracy: 0.9982 - val_loss: 9.1570e-04 - val_accuracy: 0.9493\n",
            "Epoch 957/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 3.6848e-05 - accuracy: 0.9981 - val_loss: 9.3574e-04 - val_accuracy: 0.9493\n",
            "Epoch 958/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 3.9445e-05 - accuracy: 0.9980 - val_loss: 9.5334e-04 - val_accuracy: 0.9488\n",
            "Epoch 959/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.2769e-05 - accuracy: 0.9977 - val_loss: 9.2262e-04 - val_accuracy: 0.9490\n",
            "Epoch 960/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.0765e-05 - accuracy: 0.9979 - val_loss: 0.0011 - val_accuracy: 0.9380\n",
            "Epoch 961/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 1.0384e-04 - accuracy: 0.9943 - val_loss: 0.0012 - val_accuracy: 0.9342\n",
            "Epoch 962/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 8.0642e-05 - accuracy: 0.9957 - val_loss: 0.0012 - val_accuracy: 0.9323\n",
            "Epoch 963/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 7.0841e-05 - accuracy: 0.9963 - val_loss: 0.0012 - val_accuracy: 0.9380\n",
            "Epoch 964/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.2464e-05 - accuracy: 0.9978 - val_loss: 0.0010 - val_accuracy: 0.9455\n",
            "Epoch 965/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.3940e-05 - accuracy: 0.9977 - val_loss: 0.0010 - val_accuracy: 0.9453\n",
            "Epoch 966/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.5460e-05 - accuracy: 0.9971 - val_loss: 0.0010 - val_accuracy: 0.9447\n",
            "Epoch 967/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.3242e-05 - accuracy: 0.9966 - val_loss: 9.7767e-04 - val_accuracy: 0.9467\n",
            "Epoch 968/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.6647e-05 - accuracy: 0.9975 - val_loss: 0.0011 - val_accuracy: 0.9412\n",
            "Epoch 969/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.2756e-05 - accuracy: 0.9977 - val_loss: 0.0010 - val_accuracy: 0.9443\n",
            "Epoch 970/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.3916e-05 - accuracy: 0.9977 - val_loss: 9.1271e-04 - val_accuracy: 0.9502\n",
            "Epoch 971/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.6540e-05 - accuracy: 0.9975 - val_loss: 9.9451e-04 - val_accuracy: 0.9458\n",
            "Epoch 972/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.7271e-05 - accuracy: 0.9975 - val_loss: 8.8109e-04 - val_accuracy: 0.9530\n",
            "Epoch 973/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 3.8810e-05 - accuracy: 0.9980 - val_loss: 8.2463e-04 - val_accuracy: 0.9543\n",
            "Epoch 974/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 3.8100e-05 - accuracy: 0.9980 - val_loss: 8.9490e-04 - val_accuracy: 0.9508\n",
            "Epoch 975/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 3.8340e-05 - accuracy: 0.9980 - val_loss: 9.5481e-04 - val_accuracy: 0.9482\n",
            "Epoch 976/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.4466e-05 - accuracy: 0.9976 - val_loss: 0.0010 - val_accuracy: 0.9445\n",
            "Epoch 977/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.9783e-05 - accuracy: 0.9974 - val_loss: 9.2156e-04 - val_accuracy: 0.9502\n",
            "Epoch 978/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.5450e-05 - accuracy: 0.9971 - val_loss: 9.6557e-04 - val_accuracy: 0.9475\n",
            "Epoch 979/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.3129e-05 - accuracy: 0.9977 - val_loss: 0.0010 - val_accuracy: 0.9453\n",
            "Epoch 980/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.4860e-05 - accuracy: 0.9977 - val_loss: 8.9842e-04 - val_accuracy: 0.9505\n",
            "Epoch 981/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.3890e-05 - accuracy: 0.9977 - val_loss: 9.7635e-04 - val_accuracy: 0.9472\n",
            "Epoch 982/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 6.4062e-05 - accuracy: 0.9965 - val_loss: 0.0013 - val_accuracy: 0.9272\n",
            "Epoch 983/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 6.9655e-05 - accuracy: 0.9962 - val_loss: 0.0010 - val_accuracy: 0.9435\n",
            "Epoch 984/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 7.1552e-05 - accuracy: 0.9960 - val_loss: 0.0012 - val_accuracy: 0.9357\n",
            "Epoch 985/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 7.8210e-05 - accuracy: 0.9956 - val_loss: 0.0011 - val_accuracy: 0.9383\n",
            "Epoch 986/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.3876e-05 - accuracy: 0.9977 - val_loss: 0.0011 - val_accuracy: 0.9428\n",
            "Epoch 987/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.9319e-05 - accuracy: 0.9973 - val_loss: 9.4259e-04 - val_accuracy: 0.9485\n",
            "Epoch 988/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.9653e-05 - accuracy: 0.9980 - val_loss: 0.0011 - val_accuracy: 0.9425\n",
            "Epoch 989/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.5070e-05 - accuracy: 0.9977 - val_loss: 9.4364e-04 - val_accuracy: 0.9487\n",
            "Epoch 990/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.9383e-05 - accuracy: 0.9973 - val_loss: 9.1829e-04 - val_accuracy: 0.9503\n",
            "Epoch 991/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.2108e-05 - accuracy: 0.9971 - val_loss: 9.5645e-04 - val_accuracy: 0.9477\n",
            "Epoch 992/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.0859e-05 - accuracy: 0.9974 - val_loss: 0.0011 - val_accuracy: 0.9405\n",
            "Epoch 993/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.6724e-05 - accuracy: 0.9975 - val_loss: 0.0010 - val_accuracy: 0.9452\n",
            "Epoch 994/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.2501e-05 - accuracy: 0.9978 - val_loss: 9.9506e-04 - val_accuracy: 0.9452\n",
            "Epoch 995/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.4546e-05 - accuracy: 0.9977 - val_loss: 9.6412e-04 - val_accuracy: 0.9477\n",
            "Epoch 996/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.3995e-05 - accuracy: 0.9977 - val_loss: 9.1801e-04 - val_accuracy: 0.9495\n",
            "Epoch 997/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.1158e-05 - accuracy: 0.9979 - val_loss: 9.4431e-04 - val_accuracy: 0.9487\n",
            "Epoch 998/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.7962e-05 - accuracy: 0.9975 - val_loss: 0.0010 - val_accuracy: 0.9443\n",
            "Epoch 999/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 6.7224e-05 - accuracy: 0.9963 - val_loss: 0.0012 - val_accuracy: 0.9342\n",
            "Epoch 1000/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.8537e-05 - accuracy: 0.9964 - val_loss: 0.0014 - val_accuracy: 0.9258\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49a41793-25a2-4cef-e40f-ed052ac4dc8a",
        "id": "BidnGe07u9a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "188/188 [==============================] - 2s 9ms/step - loss: 0.0014 - accuracy: 0.9223\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.0014241042081266642, 0.9223333597183228]"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('highpass_1s.h5')"
      ],
      "metadata": {
        "id": "wQoXmZ0EFg_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 그래프 분석"
      ],
      "metadata": {
        "id": "m79hkJfuu9a9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history=history.history\n",
        "epochs = range(1, len(history['loss']) + 1)\n",
        "acc = history['accuracy']\n",
        "loss = history['loss']\n",
        "val_acc = history['val_accuracy']\n",
        "val_loss = history['val_loss']\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.plot(epochs, acc, label='accuracy')\n",
        "plt.plot(epochs, val_acc, label='val_acc')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.plot(epochs, loss, label='loss', color='g')\n",
        "plt.plot(epochs, val_loss, label='val_loss', color='r')\n",
        "plt.legend()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 701
        },
        "outputId": "0db4cc8b-fc09-4807-ad1c-74720c0ffb54",
        "id": "ztdlucKyu9a-"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f10d6c9f5b0>"
            ]
          },
          "metadata": {},
          "execution_count": 83
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 예측해보기(predict)"
      ],
      "metadata": {
        "id": "7hsZEv6au9a_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e06685b-4c91-4d7b-804a-f36f83b0daf8",
        "id": "i9_Xzkqzu9a_"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "188/188 [==============================] - 2s 9ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[44,  0,  0, ...,  0,  0,  0],\n",
              "       [ 0, 57,  0, ...,  0,  0,  0],\n",
              "       [ 0,  0, 65, ...,  0,  0,  0],\n",
              "       ...,\n",
              "       [ 0,  0,  0, ..., 68,  0,  0],\n",
              "       [ 0,  0,  0, ...,  0, 67,  0],\n",
              "       [ 0,  0,  0, ...,  0,  0, 57]])"
            ]
          },
          "metadata": {},
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_p=pd.DataFrame(y_test)\n",
        "\n",
        "for i in range(1,101):\n",
        "  y_test_p.loc[y_test_p[i] == 1.0 , 'y_test'] = i  \n",
        "y_test_p.y_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "348ddc8c-b4b0-4b45-9b9b-a83b87cbce28",
        "id": "U35lO3JDu9a_"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       59.0\n",
              "1       68.0\n",
              "2       72.0\n",
              "3        2.0\n",
              "4       57.0\n",
              "        ... \n",
              "5995     3.0\n",
              "5996    87.0\n",
              "5997    40.0\n",
              "5998    68.0\n",
              "5999    37.0\n",
              "Name: y_test, Length: 6000, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ex=np.argmax(y_pred, axis=1)\n",
        "ex=ex.astype('float64')\n",
        "ex=pd.DataFrame(ex)\n",
        "ex"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "22f2ac13-d7ce-409c-db36-bf92c9ca0cad",
        "id": "kWobvWivu9a_"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         0\n",
              "0     59.0\n",
              "1     68.0\n",
              "2     72.0\n",
              "3      2.0\n",
              "4     57.0\n",
              "...    ...\n",
              "5995   3.0\n",
              "5996  87.0\n",
              "5997  40.0\n",
              "5998  68.0\n",
              "5999  45.0\n",
              "\n",
              "[6000 rows x 1 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-33a2dfcf-6ed7-497f-9b45-70f7cc7db973\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>59.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>68.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>72.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>57.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5995</th>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5996</th>\n",
              "      <td>87.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5997</th>\n",
              "      <td>40.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5998</th>\n",
              "      <td>68.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5999</th>\n",
              "      <td>45.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6000 rows × 1 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-33a2dfcf-6ed7-497f-9b45-70f7cc7db973')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-33a2dfcf-6ed7-497f-9b45-70f7cc7db973 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-33a2dfcf-6ed7-497f-9b45-70f7cc7db973');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ch1 = pd.DataFrame(np.rint(y_pred))\n",
        "ch1['pred'] = ex\n",
        "data_test.reset_index(inplace=True, drop=True)\n",
        "predict = pd.concat([data_test,ch1['pred'], y_test_p['y_test']],axis = 1)"
      ],
      "metadata": {
        "id": "a0vVXNj2u9a_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict['compare'] = np.where(predict['pred']==predict['y_test'],'yes', 'no')"
      ],
      "metadata": {
        "id": "RuxfqPeJu9bA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_1 = predict[predict[\"compare\"]=='no']\n",
        "predict_1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "outputId": "833df983-0f21-460a-de46-668f491d18e1",
        "id": "RmByZaU2u9bA"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             0         1         2         3         4         5         6  \\\n",
              "30    0.001909  0.008317  0.014489  0.020412  0.025995  0.031137  0.035801   \n",
              "46    0.096114  0.095362  0.095047  0.095447  0.096676  0.098677  0.101247   \n",
              "54   -0.041972 -0.048778 -0.055266 -0.061636 -0.067728 -0.073065 -0.076978   \n",
              "55    0.050686  0.056807  0.061922  0.066330  0.070463  0.074831  0.079964   \n",
              "71   -0.511413 -0.507140 -0.469849 -0.408462 -0.333769 -0.256404 -0.185178   \n",
              "...        ...       ...       ...       ...       ...       ...       ...   \n",
              "5971  0.005315 -0.043084 -0.083200 -0.110473 -0.122973 -0.121490 -0.109050   \n",
              "5981 -0.005281 -0.018280 -0.031669 -0.042939 -0.050464 -0.053648 -0.052817   \n",
              "5992 -0.084734 -0.091664 -0.098196 -0.101411 -0.098148 -0.085744 -0.062829   \n",
              "5994  0.008461  0.011468  0.012707  0.012096  0.009875  0.006558  0.002841   \n",
              "5999  0.052145  0.038833  0.024907  0.010402 -0.004429 -0.019087 -0.032867   \n",
              "\n",
              "             7         8         9  ...       249       250       251  \\\n",
              "30    0.040075  0.044195  0.048538  ...  0.044631  0.050399  0.057432   \n",
              "46    0.104083  0.106844  0.109202  ... -0.004398 -0.008214 -0.013081   \n",
              "54   -0.078796 -0.078050 -0.074630  ... -0.102289 -0.108650 -0.114708   \n",
              "55    0.086349  0.094365  0.104239  ...  0.125449  0.136371  0.147691   \n",
              "71   -0.126038 -0.081724 -0.052058  ...  0.031533  0.034874  0.038518   \n",
              "...        ...       ...       ...  ...       ...       ...       ...   \n",
              "5971 -0.090022 -0.069069 -0.050204  ... -0.027012 -0.029614 -0.031356   \n",
              "5981 -0.048958 -0.043371 -0.037307  ... -0.016826 -0.013496 -0.011275   \n",
              "5992 -0.030004  0.009808  0.051498  ...  0.072150  0.081214  0.090895   \n",
              "5994 -0.000513 -0.002782 -0.003375  ... -0.048723 -0.051678 -0.052151   \n",
              "5999 -0.044991 -0.054774 -0.061789  ... -0.056943 -0.054300 -0.054324   \n",
              "\n",
              "           252       253       254       255   pred  y_test  compare  \n",
              "30    0.066046  0.076371  0.088327  0.101648   85.0    36.0       no  \n",
              "46   -0.019006 -0.025674 -0.032320 -0.037693   83.0    80.0       no  \n",
              "54   -0.119718 -0.122885 -0.123496 -0.121095   20.0    71.0       no  \n",
              "55    0.159697  0.172642  0.186692  0.201860   41.0    50.0       no  \n",
              "71    0.042180  0.045745  0.049241  0.052764    4.0    93.0       no  \n",
              "...        ...       ...       ...       ...    ...     ...      ...  \n",
              "5971 -0.033020 -0.035244 -0.038329 -0.042129   95.0    23.0       no  \n",
              "5981 -0.010594 -0.011830 -0.015208 -0.020697   52.0    40.0       no  \n",
              "5992  0.101388  0.112932  0.125713  0.139775  100.0    83.0       no  \n",
              "5994 -0.050205 -0.046318 -0.041326 -0.036264   95.0    49.0       no  \n",
              "5999 -0.058068 -0.065599 -0.075635 -0.085448   45.0    37.0       no  \n",
              "\n",
              "[545 rows x 259 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-22dffb56-3200-49eb-89c1-97f537806f07\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>249</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "      <th>pred</th>\n",
              "      <th>y_test</th>\n",
              "      <th>compare</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.001909</td>\n",
              "      <td>0.008317</td>\n",
              "      <td>0.014489</td>\n",
              "      <td>0.020412</td>\n",
              "      <td>0.025995</td>\n",
              "      <td>0.031137</td>\n",
              "      <td>0.035801</td>\n",
              "      <td>0.040075</td>\n",
              "      <td>0.044195</td>\n",
              "      <td>0.048538</td>\n",
              "      <td>...</td>\n",
              "      <td>0.044631</td>\n",
              "      <td>0.050399</td>\n",
              "      <td>0.057432</td>\n",
              "      <td>0.066046</td>\n",
              "      <td>0.076371</td>\n",
              "      <td>0.088327</td>\n",
              "      <td>0.101648</td>\n",
              "      <td>85.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>0.096114</td>\n",
              "      <td>0.095362</td>\n",
              "      <td>0.095047</td>\n",
              "      <td>0.095447</td>\n",
              "      <td>0.096676</td>\n",
              "      <td>0.098677</td>\n",
              "      <td>0.101247</td>\n",
              "      <td>0.104083</td>\n",
              "      <td>0.106844</td>\n",
              "      <td>0.109202</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.004398</td>\n",
              "      <td>-0.008214</td>\n",
              "      <td>-0.013081</td>\n",
              "      <td>-0.019006</td>\n",
              "      <td>-0.025674</td>\n",
              "      <td>-0.032320</td>\n",
              "      <td>-0.037693</td>\n",
              "      <td>83.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>-0.041972</td>\n",
              "      <td>-0.048778</td>\n",
              "      <td>-0.055266</td>\n",
              "      <td>-0.061636</td>\n",
              "      <td>-0.067728</td>\n",
              "      <td>-0.073065</td>\n",
              "      <td>-0.076978</td>\n",
              "      <td>-0.078796</td>\n",
              "      <td>-0.078050</td>\n",
              "      <td>-0.074630</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.102289</td>\n",
              "      <td>-0.108650</td>\n",
              "      <td>-0.114708</td>\n",
              "      <td>-0.119718</td>\n",
              "      <td>-0.122885</td>\n",
              "      <td>-0.123496</td>\n",
              "      <td>-0.121095</td>\n",
              "      <td>20.0</td>\n",
              "      <td>71.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>0.050686</td>\n",
              "      <td>0.056807</td>\n",
              "      <td>0.061922</td>\n",
              "      <td>0.066330</td>\n",
              "      <td>0.070463</td>\n",
              "      <td>0.074831</td>\n",
              "      <td>0.079964</td>\n",
              "      <td>0.086349</td>\n",
              "      <td>0.094365</td>\n",
              "      <td>0.104239</td>\n",
              "      <td>...</td>\n",
              "      <td>0.125449</td>\n",
              "      <td>0.136371</td>\n",
              "      <td>0.147691</td>\n",
              "      <td>0.159697</td>\n",
              "      <td>0.172642</td>\n",
              "      <td>0.186692</td>\n",
              "      <td>0.201860</td>\n",
              "      <td>41.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>-0.511413</td>\n",
              "      <td>-0.507140</td>\n",
              "      <td>-0.469849</td>\n",
              "      <td>-0.408462</td>\n",
              "      <td>-0.333769</td>\n",
              "      <td>-0.256404</td>\n",
              "      <td>-0.185178</td>\n",
              "      <td>-0.126038</td>\n",
              "      <td>-0.081724</td>\n",
              "      <td>-0.052058</td>\n",
              "      <td>...</td>\n",
              "      <td>0.031533</td>\n",
              "      <td>0.034874</td>\n",
              "      <td>0.038518</td>\n",
              "      <td>0.042180</td>\n",
              "      <td>0.045745</td>\n",
              "      <td>0.049241</td>\n",
              "      <td>0.052764</td>\n",
              "      <td>4.0</td>\n",
              "      <td>93.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5971</th>\n",
              "      <td>0.005315</td>\n",
              "      <td>-0.043084</td>\n",
              "      <td>-0.083200</td>\n",
              "      <td>-0.110473</td>\n",
              "      <td>-0.122973</td>\n",
              "      <td>-0.121490</td>\n",
              "      <td>-0.109050</td>\n",
              "      <td>-0.090022</td>\n",
              "      <td>-0.069069</td>\n",
              "      <td>-0.050204</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.027012</td>\n",
              "      <td>-0.029614</td>\n",
              "      <td>-0.031356</td>\n",
              "      <td>-0.033020</td>\n",
              "      <td>-0.035244</td>\n",
              "      <td>-0.038329</td>\n",
              "      <td>-0.042129</td>\n",
              "      <td>95.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5981</th>\n",
              "      <td>-0.005281</td>\n",
              "      <td>-0.018280</td>\n",
              "      <td>-0.031669</td>\n",
              "      <td>-0.042939</td>\n",
              "      <td>-0.050464</td>\n",
              "      <td>-0.053648</td>\n",
              "      <td>-0.052817</td>\n",
              "      <td>-0.048958</td>\n",
              "      <td>-0.043371</td>\n",
              "      <td>-0.037307</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.016826</td>\n",
              "      <td>-0.013496</td>\n",
              "      <td>-0.011275</td>\n",
              "      <td>-0.010594</td>\n",
              "      <td>-0.011830</td>\n",
              "      <td>-0.015208</td>\n",
              "      <td>-0.020697</td>\n",
              "      <td>52.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5992</th>\n",
              "      <td>-0.084734</td>\n",
              "      <td>-0.091664</td>\n",
              "      <td>-0.098196</td>\n",
              "      <td>-0.101411</td>\n",
              "      <td>-0.098148</td>\n",
              "      <td>-0.085744</td>\n",
              "      <td>-0.062829</td>\n",
              "      <td>-0.030004</td>\n",
              "      <td>0.009808</td>\n",
              "      <td>0.051498</td>\n",
              "      <td>...</td>\n",
              "      <td>0.072150</td>\n",
              "      <td>0.081214</td>\n",
              "      <td>0.090895</td>\n",
              "      <td>0.101388</td>\n",
              "      <td>0.112932</td>\n",
              "      <td>0.125713</td>\n",
              "      <td>0.139775</td>\n",
              "      <td>100.0</td>\n",
              "      <td>83.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5994</th>\n",
              "      <td>0.008461</td>\n",
              "      <td>0.011468</td>\n",
              "      <td>0.012707</td>\n",
              "      <td>0.012096</td>\n",
              "      <td>0.009875</td>\n",
              "      <td>0.006558</td>\n",
              "      <td>0.002841</td>\n",
              "      <td>-0.000513</td>\n",
              "      <td>-0.002782</td>\n",
              "      <td>-0.003375</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.048723</td>\n",
              "      <td>-0.051678</td>\n",
              "      <td>-0.052151</td>\n",
              "      <td>-0.050205</td>\n",
              "      <td>-0.046318</td>\n",
              "      <td>-0.041326</td>\n",
              "      <td>-0.036264</td>\n",
              "      <td>95.0</td>\n",
              "      <td>49.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5999</th>\n",
              "      <td>0.052145</td>\n",
              "      <td>0.038833</td>\n",
              "      <td>0.024907</td>\n",
              "      <td>0.010402</td>\n",
              "      <td>-0.004429</td>\n",
              "      <td>-0.019087</td>\n",
              "      <td>-0.032867</td>\n",
              "      <td>-0.044991</td>\n",
              "      <td>-0.054774</td>\n",
              "      <td>-0.061789</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.056943</td>\n",
              "      <td>-0.054300</td>\n",
              "      <td>-0.054324</td>\n",
              "      <td>-0.058068</td>\n",
              "      <td>-0.065599</td>\n",
              "      <td>-0.075635</td>\n",
              "      <td>-0.085448</td>\n",
              "      <td>45.0</td>\n",
              "      <td>37.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>545 rows × 259 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-22dffb56-3200-49eb-89c1-97f537806f07')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-22dffb56-3200-49eb-89c1-97f537806f07 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-22dffb56-3200-49eb-89c1-97f537806f07');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 163
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.lowpass"
      ],
      "metadata": {
        "id": "C3YzYPPIOiqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46bf1084-0e1c-4a75-faae-05630c1f105b",
        "id": "lCgPKknoOiqX"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip '/content/drive/MyDrive/model_line.zip' -d '/content/drive/MyDrive/model_line'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c6ba509-4e6d-42b3-8562-1567bcc35d70",
        "id": "iYG2TuBLOiqX"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/model_line.zip\n",
            "   creating: /content/drive/MyDrive/model_line/model_line/\n",
            "   creating: /content/drive/MyDrive/model_line/model_line/band/\n",
            "  inflating: /content/drive/MyDrive/model_line/model_line/band/test_band_line.csv  \n",
            "  inflating: /content/drive/MyDrive/model_line/model_line/band/train_band_line.csv  \n",
            "  inflating: /content/drive/MyDrive/model_line/model_line/band/valid_band_line.csv  \n",
            "   creating: /content/drive/MyDrive/model_line/model_line/high/\n",
            "  inflating: /content/drive/MyDrive/model_line/model_line/high/test_high_line.csv  \n",
            "  inflating: /content/drive/MyDrive/model_line/model_line/high/train_high_line.csv  \n",
            "  inflating: /content/drive/MyDrive/model_line/model_line/high/valid_high_line.csv  \n",
            "   creating: /content/drive/MyDrive/model_line/model_line/low/\n",
            "  inflating: /content/drive/MyDrive/model_line/model_line/low/test_low_line.csv  \n",
            "  inflating: /content/drive/MyDrive/model_line/model_line/low/train_low_line.csv  \n",
            "  inflating: /content/drive/MyDrive/model_line/model_line/low/valid_low_line.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9IwB13flOiqX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import itertools\n",
        "import time\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import (CosineAnnealingLR,\n",
        "                                      CosineAnnealingWarmRestarts,\n",
        "                                      StepLR,\n",
        "                                      ExponentialLR)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import accuracy_score, auc, f1_score, precision_score, recall_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm, re, sys\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "from keras.layers import Conv2D, Conv1D, MaxPooling2D, MaxPooling1D, Flatten, BatchNormalization, Dense\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.callbacks import CSVLogger, ModelCheckpoint"
      ],
      "metadata": {
        "id": "3omMrMIkOiqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train = pd.read_csv('/content/drive/MyDrive/model_line/model_line/low/train_low_line.csv', header=None)\n",
        "data_test = pd.read_csv('/content/drive/MyDrive/model_line/model_line/low/test_low_line.csv', header=None)\n",
        "data_val= pd.read_csv('/content/drive/MyDrive/model_line/model_line/low/valid_low_line.csv', header=None)"
      ],
      "metadata": {
        "id": "3Sqlcw_OOiqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "outputId": "e50556ee-4dcd-4ce8-ccd3-13f7e6ace318",
        "id": "WY-yP6RdOiqY"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           0         1         2         3         4         5         6    \\\n",
              "0     1.754140  1.756590  1.758298  1.759022  1.758643  1.757236  1.755108   \n",
              "1     1.590544  1.594114  1.597217  1.599791  1.601832  1.603404  1.604643   \n",
              "2     1.706668  1.708791  1.709560  1.709006  1.707349  1.704986  1.702446   \n",
              "3     2.295298  2.411576  2.487373  2.511645  2.480919  2.400025  2.281178   \n",
              "4     1.705033  1.707353  1.709552  1.711737  1.714067  1.716690  1.719673   \n",
              "...        ...       ...       ...       ...       ...       ...       ...   \n",
              "5995  1.613963  1.619528  1.627568  1.636760  1.645732  1.653393  1.659145   \n",
              "5996  1.696360  1.696858  1.698535  1.701395  1.705272  1.709867  1.714797   \n",
              "5997  1.547967  1.549320  1.550668  1.552017  1.553337  1.554553  1.555551   \n",
              "5998  1.615198  1.615168  1.615529  1.616101  1.616700  1.617167  1.617373   \n",
              "5999  1.692739  1.619039  1.551620  1.499212  1.467524  1.458448  1.470120   \n",
              "\n",
              "           7         8         9    ...       246       247       248  \\\n",
              "0     1.752784  1.750938  1.750271  ...  1.698640  1.701614  1.704703   \n",
              "1     1.605734  1.606893  1.608334  ...  1.656219  1.659175  1.662269   \n",
              "2     1.700306  1.699091  1.699173  ...  1.656174  1.659317  1.661245   \n",
              "3     2.141574  2.000075  1.873748  ...  1.688096  1.699816  1.720896   \n",
              "4     1.722960  1.726364  1.729587  ...  1.692391  1.691363  1.688594   \n",
              "...        ...       ...       ...  ...       ...       ...       ...   \n",
              "5995  1.662945  1.665238  1.666797  ...  1.784428  1.755844  1.730389   \n",
              "5996  1.719637  1.723966  1.727412  ...  1.661005  1.699782  1.744423   \n",
              "5997  1.556214  1.556469  1.556341  ...  1.596446  1.592691  1.591059   \n",
              "5998  1.617227  1.616682  1.615755  ...  1.597461  1.595455  1.593952   \n",
              "5999  1.497716  1.534729  1.574390  ...  1.790030  1.801024  1.812046   \n",
              "\n",
              "           249       250       251       252       253       254       255  \n",
              "0     1.707794  1.710813  1.713725  1.716508  1.719121  1.721462  1.723356  \n",
              "1     1.665042  1.667068  1.668064  1.667958  1.666907  1.665254  1.663453  \n",
              "2     1.662187  1.662586  1.662967  1.663795  1.665342  1.667614  1.670345  \n",
              "3     1.742862  1.759763  1.768718  1.769686  1.764689  1.756791  1.749115  \n",
              "4     1.684654  1.680340  1.676541  1.674078  1.673557  1.675256  1.679065  \n",
              "...        ...       ...       ...       ...       ...       ...       ...  \n",
              "5995  1.708044  1.688545  1.671541  1.656708  1.643799  1.632639  1.623080  \n",
              "5996  1.789141  1.826705  1.849760  1.852416  1.831714  1.788555  1.727819  \n",
              "5997  1.591456  1.593591  1.597004  1.601126  1.605314  1.608896  1.611187  \n",
              "5998  1.593380  1.593925  1.595443  1.597461  1.599283  1.600176  1.599565  \n",
              "5999  1.822807  1.833107  1.842928  1.852499  1.862298  1.872954  1.885091  \n",
              "\n",
              "[6000 rows x 256 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-67a5ee3c-a9d1-4cd7-bc6f-8b530374de78\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>246</th>\n",
              "      <th>247</th>\n",
              "      <th>248</th>\n",
              "      <th>249</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.754140</td>\n",
              "      <td>1.756590</td>\n",
              "      <td>1.758298</td>\n",
              "      <td>1.759022</td>\n",
              "      <td>1.758643</td>\n",
              "      <td>1.757236</td>\n",
              "      <td>1.755108</td>\n",
              "      <td>1.752784</td>\n",
              "      <td>1.750938</td>\n",
              "      <td>1.750271</td>\n",
              "      <td>...</td>\n",
              "      <td>1.698640</td>\n",
              "      <td>1.701614</td>\n",
              "      <td>1.704703</td>\n",
              "      <td>1.707794</td>\n",
              "      <td>1.710813</td>\n",
              "      <td>1.713725</td>\n",
              "      <td>1.716508</td>\n",
              "      <td>1.719121</td>\n",
              "      <td>1.721462</td>\n",
              "      <td>1.723356</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.590544</td>\n",
              "      <td>1.594114</td>\n",
              "      <td>1.597217</td>\n",
              "      <td>1.599791</td>\n",
              "      <td>1.601832</td>\n",
              "      <td>1.603404</td>\n",
              "      <td>1.604643</td>\n",
              "      <td>1.605734</td>\n",
              "      <td>1.606893</td>\n",
              "      <td>1.608334</td>\n",
              "      <td>...</td>\n",
              "      <td>1.656219</td>\n",
              "      <td>1.659175</td>\n",
              "      <td>1.662269</td>\n",
              "      <td>1.665042</td>\n",
              "      <td>1.667068</td>\n",
              "      <td>1.668064</td>\n",
              "      <td>1.667958</td>\n",
              "      <td>1.666907</td>\n",
              "      <td>1.665254</td>\n",
              "      <td>1.663453</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.706668</td>\n",
              "      <td>1.708791</td>\n",
              "      <td>1.709560</td>\n",
              "      <td>1.709006</td>\n",
              "      <td>1.707349</td>\n",
              "      <td>1.704986</td>\n",
              "      <td>1.702446</td>\n",
              "      <td>1.700306</td>\n",
              "      <td>1.699091</td>\n",
              "      <td>1.699173</td>\n",
              "      <td>...</td>\n",
              "      <td>1.656174</td>\n",
              "      <td>1.659317</td>\n",
              "      <td>1.661245</td>\n",
              "      <td>1.662187</td>\n",
              "      <td>1.662586</td>\n",
              "      <td>1.662967</td>\n",
              "      <td>1.663795</td>\n",
              "      <td>1.665342</td>\n",
              "      <td>1.667614</td>\n",
              "      <td>1.670345</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2.295298</td>\n",
              "      <td>2.411576</td>\n",
              "      <td>2.487373</td>\n",
              "      <td>2.511645</td>\n",
              "      <td>2.480919</td>\n",
              "      <td>2.400025</td>\n",
              "      <td>2.281178</td>\n",
              "      <td>2.141574</td>\n",
              "      <td>2.000075</td>\n",
              "      <td>1.873748</td>\n",
              "      <td>...</td>\n",
              "      <td>1.688096</td>\n",
              "      <td>1.699816</td>\n",
              "      <td>1.720896</td>\n",
              "      <td>1.742862</td>\n",
              "      <td>1.759763</td>\n",
              "      <td>1.768718</td>\n",
              "      <td>1.769686</td>\n",
              "      <td>1.764689</td>\n",
              "      <td>1.756791</td>\n",
              "      <td>1.749115</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.705033</td>\n",
              "      <td>1.707353</td>\n",
              "      <td>1.709552</td>\n",
              "      <td>1.711737</td>\n",
              "      <td>1.714067</td>\n",
              "      <td>1.716690</td>\n",
              "      <td>1.719673</td>\n",
              "      <td>1.722960</td>\n",
              "      <td>1.726364</td>\n",
              "      <td>1.729587</td>\n",
              "      <td>...</td>\n",
              "      <td>1.692391</td>\n",
              "      <td>1.691363</td>\n",
              "      <td>1.688594</td>\n",
              "      <td>1.684654</td>\n",
              "      <td>1.680340</td>\n",
              "      <td>1.676541</td>\n",
              "      <td>1.674078</td>\n",
              "      <td>1.673557</td>\n",
              "      <td>1.675256</td>\n",
              "      <td>1.679065</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5995</th>\n",
              "      <td>1.613963</td>\n",
              "      <td>1.619528</td>\n",
              "      <td>1.627568</td>\n",
              "      <td>1.636760</td>\n",
              "      <td>1.645732</td>\n",
              "      <td>1.653393</td>\n",
              "      <td>1.659145</td>\n",
              "      <td>1.662945</td>\n",
              "      <td>1.665238</td>\n",
              "      <td>1.666797</td>\n",
              "      <td>...</td>\n",
              "      <td>1.784428</td>\n",
              "      <td>1.755844</td>\n",
              "      <td>1.730389</td>\n",
              "      <td>1.708044</td>\n",
              "      <td>1.688545</td>\n",
              "      <td>1.671541</td>\n",
              "      <td>1.656708</td>\n",
              "      <td>1.643799</td>\n",
              "      <td>1.632639</td>\n",
              "      <td>1.623080</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5996</th>\n",
              "      <td>1.696360</td>\n",
              "      <td>1.696858</td>\n",
              "      <td>1.698535</td>\n",
              "      <td>1.701395</td>\n",
              "      <td>1.705272</td>\n",
              "      <td>1.709867</td>\n",
              "      <td>1.714797</td>\n",
              "      <td>1.719637</td>\n",
              "      <td>1.723966</td>\n",
              "      <td>1.727412</td>\n",
              "      <td>...</td>\n",
              "      <td>1.661005</td>\n",
              "      <td>1.699782</td>\n",
              "      <td>1.744423</td>\n",
              "      <td>1.789141</td>\n",
              "      <td>1.826705</td>\n",
              "      <td>1.849760</td>\n",
              "      <td>1.852416</td>\n",
              "      <td>1.831714</td>\n",
              "      <td>1.788555</td>\n",
              "      <td>1.727819</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5997</th>\n",
              "      <td>1.547967</td>\n",
              "      <td>1.549320</td>\n",
              "      <td>1.550668</td>\n",
              "      <td>1.552017</td>\n",
              "      <td>1.553337</td>\n",
              "      <td>1.554553</td>\n",
              "      <td>1.555551</td>\n",
              "      <td>1.556214</td>\n",
              "      <td>1.556469</td>\n",
              "      <td>1.556341</td>\n",
              "      <td>...</td>\n",
              "      <td>1.596446</td>\n",
              "      <td>1.592691</td>\n",
              "      <td>1.591059</td>\n",
              "      <td>1.591456</td>\n",
              "      <td>1.593591</td>\n",
              "      <td>1.597004</td>\n",
              "      <td>1.601126</td>\n",
              "      <td>1.605314</td>\n",
              "      <td>1.608896</td>\n",
              "      <td>1.611187</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5998</th>\n",
              "      <td>1.615198</td>\n",
              "      <td>1.615168</td>\n",
              "      <td>1.615529</td>\n",
              "      <td>1.616101</td>\n",
              "      <td>1.616700</td>\n",
              "      <td>1.617167</td>\n",
              "      <td>1.617373</td>\n",
              "      <td>1.617227</td>\n",
              "      <td>1.616682</td>\n",
              "      <td>1.615755</td>\n",
              "      <td>...</td>\n",
              "      <td>1.597461</td>\n",
              "      <td>1.595455</td>\n",
              "      <td>1.593952</td>\n",
              "      <td>1.593380</td>\n",
              "      <td>1.593925</td>\n",
              "      <td>1.595443</td>\n",
              "      <td>1.597461</td>\n",
              "      <td>1.599283</td>\n",
              "      <td>1.600176</td>\n",
              "      <td>1.599565</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5999</th>\n",
              "      <td>1.692739</td>\n",
              "      <td>1.619039</td>\n",
              "      <td>1.551620</td>\n",
              "      <td>1.499212</td>\n",
              "      <td>1.467524</td>\n",
              "      <td>1.458448</td>\n",
              "      <td>1.470120</td>\n",
              "      <td>1.497716</td>\n",
              "      <td>1.534729</td>\n",
              "      <td>1.574390</td>\n",
              "      <td>...</td>\n",
              "      <td>1.790030</td>\n",
              "      <td>1.801024</td>\n",
              "      <td>1.812046</td>\n",
              "      <td>1.822807</td>\n",
              "      <td>1.833107</td>\n",
              "      <td>1.842928</td>\n",
              "      <td>1.852499</td>\n",
              "      <td>1.862298</td>\n",
              "      <td>1.872954</td>\n",
              "      <td>1.885091</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6000 rows × 256 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-67a5ee3c-a9d1-4cd7-bc6f-8b530374de78')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-67a5ee3c-a9d1-4cd7-bc6f-8b530374de78 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-67a5ee3c-a9d1-4cd7-bc6f-8b530374de78');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a29733d-62fb-4012-c140-8fc0f17f46db",
        "id": "MarJi45zOiqY"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(24000, 256)"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 라벨생성"
      ],
      "metadata": {
        "id": "yNxaWPtzOiqY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "arr = np.array([])\n",
        "\n",
        "for i in range(1,101):\n",
        "  for j in range(240):\n",
        "    arr = np.append(arr, np.array([i]))"
      ],
      "metadata": {
        "id": "D5OBuZ1gOiqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train[256]=arr"
      ],
      "metadata": {
        "id": "Ns5w6anOOiqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "outputId": "1800d58a-d0d8-4d3d-8f1b-8f7acaf260a9",
        "id": "Pls7pK6GOiqY"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            0         1         2         3         4         5         6    \\\n",
              "0      1.697478  1.713703  1.728594  1.741144  1.750717  1.757105  1.760490   \n",
              "1      1.748610  1.747925  1.746924  1.746097  1.745874  1.746535  1.748158   \n",
              "2      1.709674  1.710317  1.711171  1.712183  1.713227  1.714166  1.714897   \n",
              "3      1.726979  1.726187  1.724966  1.723451  1.721925  1.720776  1.720402   \n",
              "4      1.889883  1.889929  1.891485  1.895719  1.903396  1.914786  1.929653   \n",
              "...         ...       ...       ...       ...       ...       ...       ...   \n",
              "23995  1.664913  1.642295  1.622705  1.606331  1.593275  1.583516  1.576874   \n",
              "23996  1.520984  1.516451  1.512894  1.510777  1.510324  1.511420  1.513570   \n",
              "23997  1.656066  1.659062  1.662113  1.665637  1.669814  1.674541  1.679471   \n",
              "23998  1.575260  1.571225  1.568130  1.566590  1.567050  1.569659  1.574187   \n",
              "23999  1.568698  1.573141  1.578992  1.585492  1.591738  1.596858  1.600196   \n",
              "\n",
              "            7         8         9    ...       247       248       249  \\\n",
              "0      1.761353  1.760349  1.758191  ...  1.733012  1.733423  1.734708   \n",
              "1      1.750626  1.753678  1.756978  ...  1.694451  1.696899  1.699745   \n",
              "2      1.715389  1.715680  1.715858  ...  1.722441  1.724729  1.726115   \n",
              "3      1.721109  1.723015  1.726001  ...  1.739826  1.739925  1.740406   \n",
              "4      1.947339  1.966906  1.987309  ...  1.914361  1.928167  1.944398   \n",
              "...         ...       ...       ...  ...       ...       ...       ...   \n",
              "23995  1.572981  1.571289  1.571116  ...  1.609903  1.611269  1.612058   \n",
              "23996  1.515967  1.517666  1.517848  ...  1.613789  1.614391  1.617356   \n",
              "23997  1.684122  1.688029  1.690891  ...  1.600670  1.597018  1.594806   \n",
              "23998  1.580032  1.586320  1.592084  ...  1.579022  1.578733  1.577374   \n",
              "23999  1.601434  1.600648  1.598281  ...  1.592516  1.596574  1.599401   \n",
              "\n",
              "            250       251       252       253       254       255    256  \n",
              "0      1.736879  1.739700  1.742739  1.745491  1.747515  1.748556    1.0  \n",
              "1      1.702543  1.704941  1.706748  1.707952  1.708690  1.709187    1.0  \n",
              "2      1.726820  1.727122  1.727259  1.727362  1.727430  1.727353    1.0  \n",
              "3      1.741383  1.742927  1.745059  1.747751  1.750910  1.754365    1.0  \n",
              "4      1.962799  1.982890  2.004016  2.025433  2.046399  2.066255    1.0  \n",
              "...         ...       ...       ...       ...       ...       ...    ...  \n",
              "23995  1.612049  1.611103  1.609213  1.606538  1.603402  1.600248  100.0  \n",
              "23996  1.622495  1.629054  1.636056  1.642622  1.648187  1.652595  100.0  \n",
              "23997  1.593269  1.591762  1.589812  1.587139  1.583680  1.579595  100.0  \n",
              "23998  1.575056  1.572128  1.569131  1.566724  1.565555  1.566133  100.0  \n",
              "23999  1.600918  1.601328  1.601033  1.600493  1.600067  1.599890  100.0  \n",
              "\n",
              "[24000 rows x 257 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-48f6dccc-83ac-43a0-8d89-812d61aa6fd3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>247</th>\n",
              "      <th>248</th>\n",
              "      <th>249</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "      <th>256</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.697478</td>\n",
              "      <td>1.713703</td>\n",
              "      <td>1.728594</td>\n",
              "      <td>1.741144</td>\n",
              "      <td>1.750717</td>\n",
              "      <td>1.757105</td>\n",
              "      <td>1.760490</td>\n",
              "      <td>1.761353</td>\n",
              "      <td>1.760349</td>\n",
              "      <td>1.758191</td>\n",
              "      <td>...</td>\n",
              "      <td>1.733012</td>\n",
              "      <td>1.733423</td>\n",
              "      <td>1.734708</td>\n",
              "      <td>1.736879</td>\n",
              "      <td>1.739700</td>\n",
              "      <td>1.742739</td>\n",
              "      <td>1.745491</td>\n",
              "      <td>1.747515</td>\n",
              "      <td>1.748556</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.748610</td>\n",
              "      <td>1.747925</td>\n",
              "      <td>1.746924</td>\n",
              "      <td>1.746097</td>\n",
              "      <td>1.745874</td>\n",
              "      <td>1.746535</td>\n",
              "      <td>1.748158</td>\n",
              "      <td>1.750626</td>\n",
              "      <td>1.753678</td>\n",
              "      <td>1.756978</td>\n",
              "      <td>...</td>\n",
              "      <td>1.694451</td>\n",
              "      <td>1.696899</td>\n",
              "      <td>1.699745</td>\n",
              "      <td>1.702543</td>\n",
              "      <td>1.704941</td>\n",
              "      <td>1.706748</td>\n",
              "      <td>1.707952</td>\n",
              "      <td>1.708690</td>\n",
              "      <td>1.709187</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.709674</td>\n",
              "      <td>1.710317</td>\n",
              "      <td>1.711171</td>\n",
              "      <td>1.712183</td>\n",
              "      <td>1.713227</td>\n",
              "      <td>1.714166</td>\n",
              "      <td>1.714897</td>\n",
              "      <td>1.715389</td>\n",
              "      <td>1.715680</td>\n",
              "      <td>1.715858</td>\n",
              "      <td>...</td>\n",
              "      <td>1.722441</td>\n",
              "      <td>1.724729</td>\n",
              "      <td>1.726115</td>\n",
              "      <td>1.726820</td>\n",
              "      <td>1.727122</td>\n",
              "      <td>1.727259</td>\n",
              "      <td>1.727362</td>\n",
              "      <td>1.727430</td>\n",
              "      <td>1.727353</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.726979</td>\n",
              "      <td>1.726187</td>\n",
              "      <td>1.724966</td>\n",
              "      <td>1.723451</td>\n",
              "      <td>1.721925</td>\n",
              "      <td>1.720776</td>\n",
              "      <td>1.720402</td>\n",
              "      <td>1.721109</td>\n",
              "      <td>1.723015</td>\n",
              "      <td>1.726001</td>\n",
              "      <td>...</td>\n",
              "      <td>1.739826</td>\n",
              "      <td>1.739925</td>\n",
              "      <td>1.740406</td>\n",
              "      <td>1.741383</td>\n",
              "      <td>1.742927</td>\n",
              "      <td>1.745059</td>\n",
              "      <td>1.747751</td>\n",
              "      <td>1.750910</td>\n",
              "      <td>1.754365</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.889883</td>\n",
              "      <td>1.889929</td>\n",
              "      <td>1.891485</td>\n",
              "      <td>1.895719</td>\n",
              "      <td>1.903396</td>\n",
              "      <td>1.914786</td>\n",
              "      <td>1.929653</td>\n",
              "      <td>1.947339</td>\n",
              "      <td>1.966906</td>\n",
              "      <td>1.987309</td>\n",
              "      <td>...</td>\n",
              "      <td>1.914361</td>\n",
              "      <td>1.928167</td>\n",
              "      <td>1.944398</td>\n",
              "      <td>1.962799</td>\n",
              "      <td>1.982890</td>\n",
              "      <td>2.004016</td>\n",
              "      <td>2.025433</td>\n",
              "      <td>2.046399</td>\n",
              "      <td>2.066255</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23995</th>\n",
              "      <td>1.664913</td>\n",
              "      <td>1.642295</td>\n",
              "      <td>1.622705</td>\n",
              "      <td>1.606331</td>\n",
              "      <td>1.593275</td>\n",
              "      <td>1.583516</td>\n",
              "      <td>1.576874</td>\n",
              "      <td>1.572981</td>\n",
              "      <td>1.571289</td>\n",
              "      <td>1.571116</td>\n",
              "      <td>...</td>\n",
              "      <td>1.609903</td>\n",
              "      <td>1.611269</td>\n",
              "      <td>1.612058</td>\n",
              "      <td>1.612049</td>\n",
              "      <td>1.611103</td>\n",
              "      <td>1.609213</td>\n",
              "      <td>1.606538</td>\n",
              "      <td>1.603402</td>\n",
              "      <td>1.600248</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23996</th>\n",
              "      <td>1.520984</td>\n",
              "      <td>1.516451</td>\n",
              "      <td>1.512894</td>\n",
              "      <td>1.510777</td>\n",
              "      <td>1.510324</td>\n",
              "      <td>1.511420</td>\n",
              "      <td>1.513570</td>\n",
              "      <td>1.515967</td>\n",
              "      <td>1.517666</td>\n",
              "      <td>1.517848</td>\n",
              "      <td>...</td>\n",
              "      <td>1.613789</td>\n",
              "      <td>1.614391</td>\n",
              "      <td>1.617356</td>\n",
              "      <td>1.622495</td>\n",
              "      <td>1.629054</td>\n",
              "      <td>1.636056</td>\n",
              "      <td>1.642622</td>\n",
              "      <td>1.648187</td>\n",
              "      <td>1.652595</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23997</th>\n",
              "      <td>1.656066</td>\n",
              "      <td>1.659062</td>\n",
              "      <td>1.662113</td>\n",
              "      <td>1.665637</td>\n",
              "      <td>1.669814</td>\n",
              "      <td>1.674541</td>\n",
              "      <td>1.679471</td>\n",
              "      <td>1.684122</td>\n",
              "      <td>1.688029</td>\n",
              "      <td>1.690891</td>\n",
              "      <td>...</td>\n",
              "      <td>1.600670</td>\n",
              "      <td>1.597018</td>\n",
              "      <td>1.594806</td>\n",
              "      <td>1.593269</td>\n",
              "      <td>1.591762</td>\n",
              "      <td>1.589812</td>\n",
              "      <td>1.587139</td>\n",
              "      <td>1.583680</td>\n",
              "      <td>1.579595</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23998</th>\n",
              "      <td>1.575260</td>\n",
              "      <td>1.571225</td>\n",
              "      <td>1.568130</td>\n",
              "      <td>1.566590</td>\n",
              "      <td>1.567050</td>\n",
              "      <td>1.569659</td>\n",
              "      <td>1.574187</td>\n",
              "      <td>1.580032</td>\n",
              "      <td>1.586320</td>\n",
              "      <td>1.592084</td>\n",
              "      <td>...</td>\n",
              "      <td>1.579022</td>\n",
              "      <td>1.578733</td>\n",
              "      <td>1.577374</td>\n",
              "      <td>1.575056</td>\n",
              "      <td>1.572128</td>\n",
              "      <td>1.569131</td>\n",
              "      <td>1.566724</td>\n",
              "      <td>1.565555</td>\n",
              "      <td>1.566133</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23999</th>\n",
              "      <td>1.568698</td>\n",
              "      <td>1.573141</td>\n",
              "      <td>1.578992</td>\n",
              "      <td>1.585492</td>\n",
              "      <td>1.591738</td>\n",
              "      <td>1.596858</td>\n",
              "      <td>1.600196</td>\n",
              "      <td>1.601434</td>\n",
              "      <td>1.600648</td>\n",
              "      <td>1.598281</td>\n",
              "      <td>...</td>\n",
              "      <td>1.592516</td>\n",
              "      <td>1.596574</td>\n",
              "      <td>1.599401</td>\n",
              "      <td>1.600918</td>\n",
              "      <td>1.601328</td>\n",
              "      <td>1.601033</td>\n",
              "      <td>1.600493</td>\n",
              "      <td>1.600067</td>\n",
              "      <td>1.599890</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>24000 rows × 257 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-48f6dccc-83ac-43a0-8d89-812d61aa6fd3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-48f6dccc-83ac-43a0-8d89-812d61aa6fd3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-48f6dccc-83ac-43a0-8d89-812d61aa6fd3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "arr_t = np.array([])\n",
        "\n",
        "for i in range(1,101):\n",
        "  for j in range(60):\n",
        "    arr_t = np.append(arr_t, np.array([i]))"
      ],
      "metadata": {
        "id": "Fwd-fa7oOiqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arr_v = np.array([])\n",
        "\n",
        "for i in range(1,101):\n",
        "  for j in range(60):\n",
        "    arr_v = np.append(arr_v, np.array([i]))"
      ],
      "metadata": {
        "id": "0Xt009g_OiqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_test[256]=arr_t\n",
        "data_val[256]=arr_v\n",
        "data_val"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "outputId": "6e0b3262-0deb-41d9-8142-95632afe948a",
        "id": "7GrCHXwzOiqZ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           0         1         2         3         4         5         6    \\\n",
              "0     1.757857  1.761049  1.763568  1.765083  1.765388  1.764480  1.762595   \n",
              "1     1.544946  1.546049  1.547770  1.549887  1.552157  1.554327  1.556169   \n",
              "2     1.695559  1.697523  1.699848  1.702404  1.704980  1.707305  1.709099   \n",
              "3     1.775055  1.770967  1.765108  1.757903  1.750143  1.742942  1.737608   \n",
              "4     1.724734  1.725498  1.725171  1.724080  1.722647  1.721315  1.720477   \n",
              "...        ...       ...       ...       ...       ...       ...       ...   \n",
              "5995  1.528888  1.529840  1.528139  1.524104  1.518376  1.511774  1.505117   \n",
              "5996  1.515056  1.511174  1.509395  1.510004  1.512908  1.517656  1.523524   \n",
              "5997  1.565344  1.564102  1.561757  1.559405  1.558051  1.558382  1.560646   \n",
              "5998  1.597548  1.595698  1.594912  1.595167  1.596213  1.597637  1.598970   \n",
              "5999  1.599834  1.599558  1.598638  1.596714  1.593627  1.589487  1.584661   \n",
              "\n",
              "           7         8         9    ...       247       248       249  \\\n",
              "0     1.760185  1.757859  1.756279  ...  1.742644  1.741366  1.740599   \n",
              "1     1.557550  1.558479  1.559129  ...  1.553053  1.556314  1.560194   \n",
              "2     1.710126  1.710271  1.709585  ...  1.678932  1.678790  1.679747   \n",
              "3     1.735426  1.737369  1.743771  ...  1.681760  1.655004  1.641810   \n",
              "4     1.720418  1.721269  1.722978  ...  1.698035  1.696608  1.695459   \n",
              "...        ...       ...       ...  ...       ...       ...       ...   \n",
              "5995  1.499054  1.493960  1.489913  ...  1.550977  1.583197  1.605661   \n",
              "5996  1.529653  1.535207  1.539537  ...  1.693020  1.696392  1.698900   \n",
              "5997  1.564660  1.569916  1.575756  ...  1.535411  1.536617  1.537876   \n",
              "5998  1.599797  1.599838  1.598980  ...  1.617056  1.620175  1.621880   \n",
              "5999  1.579703  1.575233  1.571814  ...  1.668857  1.717732  1.769180   \n",
              "\n",
              "           250       251       252       253       254       255    256  \n",
              "0     1.740582  1.741420  1.743086  1.745437  1.748252  1.751255    1.0  \n",
              "1     1.564465  1.568940  1.573478  1.577984  1.582383  1.586600    1.0  \n",
              "2     1.681958  1.685344  1.689624  1.694374  1.699106  1.703341    1.0  \n",
              "3     1.651567  1.692000  1.767089  1.875369  2.009146  2.154947    1.0  \n",
              "4     1.694902  1.695130  1.696178  1.697926  1.700150  1.702592    1.0  \n",
              "...        ...       ...       ...       ...       ...       ...    ...  \n",
              "5995  1.618126  1.622195  1.620631  1.616599  1.612957  1.611750  100.0  \n",
              "5996  1.700400  1.700869  1.700424  1.699334  1.697991  1.696852  100.0  \n",
              "5997  1.539222  1.540659  1.542157  1.543669  1.545152  1.546585  100.0  \n",
              "5998  1.622224  1.621475  1.620038  1.618362  1.616840  1.615746  100.0  \n",
              "5999  1.815834  1.849687  1.863721  1.853540  1.818573  1.762504  100.0  \n",
              "\n",
              "[6000 rows x 257 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-25cfc8d3-8be4-4440-9304-33f0c90ca935\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>247</th>\n",
              "      <th>248</th>\n",
              "      <th>249</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "      <th>256</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.757857</td>\n",
              "      <td>1.761049</td>\n",
              "      <td>1.763568</td>\n",
              "      <td>1.765083</td>\n",
              "      <td>1.765388</td>\n",
              "      <td>1.764480</td>\n",
              "      <td>1.762595</td>\n",
              "      <td>1.760185</td>\n",
              "      <td>1.757859</td>\n",
              "      <td>1.756279</td>\n",
              "      <td>...</td>\n",
              "      <td>1.742644</td>\n",
              "      <td>1.741366</td>\n",
              "      <td>1.740599</td>\n",
              "      <td>1.740582</td>\n",
              "      <td>1.741420</td>\n",
              "      <td>1.743086</td>\n",
              "      <td>1.745437</td>\n",
              "      <td>1.748252</td>\n",
              "      <td>1.751255</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.544946</td>\n",
              "      <td>1.546049</td>\n",
              "      <td>1.547770</td>\n",
              "      <td>1.549887</td>\n",
              "      <td>1.552157</td>\n",
              "      <td>1.554327</td>\n",
              "      <td>1.556169</td>\n",
              "      <td>1.557550</td>\n",
              "      <td>1.558479</td>\n",
              "      <td>1.559129</td>\n",
              "      <td>...</td>\n",
              "      <td>1.553053</td>\n",
              "      <td>1.556314</td>\n",
              "      <td>1.560194</td>\n",
              "      <td>1.564465</td>\n",
              "      <td>1.568940</td>\n",
              "      <td>1.573478</td>\n",
              "      <td>1.577984</td>\n",
              "      <td>1.582383</td>\n",
              "      <td>1.586600</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.695559</td>\n",
              "      <td>1.697523</td>\n",
              "      <td>1.699848</td>\n",
              "      <td>1.702404</td>\n",
              "      <td>1.704980</td>\n",
              "      <td>1.707305</td>\n",
              "      <td>1.709099</td>\n",
              "      <td>1.710126</td>\n",
              "      <td>1.710271</td>\n",
              "      <td>1.709585</td>\n",
              "      <td>...</td>\n",
              "      <td>1.678932</td>\n",
              "      <td>1.678790</td>\n",
              "      <td>1.679747</td>\n",
              "      <td>1.681958</td>\n",
              "      <td>1.685344</td>\n",
              "      <td>1.689624</td>\n",
              "      <td>1.694374</td>\n",
              "      <td>1.699106</td>\n",
              "      <td>1.703341</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.775055</td>\n",
              "      <td>1.770967</td>\n",
              "      <td>1.765108</td>\n",
              "      <td>1.757903</td>\n",
              "      <td>1.750143</td>\n",
              "      <td>1.742942</td>\n",
              "      <td>1.737608</td>\n",
              "      <td>1.735426</td>\n",
              "      <td>1.737369</td>\n",
              "      <td>1.743771</td>\n",
              "      <td>...</td>\n",
              "      <td>1.681760</td>\n",
              "      <td>1.655004</td>\n",
              "      <td>1.641810</td>\n",
              "      <td>1.651567</td>\n",
              "      <td>1.692000</td>\n",
              "      <td>1.767089</td>\n",
              "      <td>1.875369</td>\n",
              "      <td>2.009146</td>\n",
              "      <td>2.154947</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.724734</td>\n",
              "      <td>1.725498</td>\n",
              "      <td>1.725171</td>\n",
              "      <td>1.724080</td>\n",
              "      <td>1.722647</td>\n",
              "      <td>1.721315</td>\n",
              "      <td>1.720477</td>\n",
              "      <td>1.720418</td>\n",
              "      <td>1.721269</td>\n",
              "      <td>1.722978</td>\n",
              "      <td>...</td>\n",
              "      <td>1.698035</td>\n",
              "      <td>1.696608</td>\n",
              "      <td>1.695459</td>\n",
              "      <td>1.694902</td>\n",
              "      <td>1.695130</td>\n",
              "      <td>1.696178</td>\n",
              "      <td>1.697926</td>\n",
              "      <td>1.700150</td>\n",
              "      <td>1.702592</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5995</th>\n",
              "      <td>1.528888</td>\n",
              "      <td>1.529840</td>\n",
              "      <td>1.528139</td>\n",
              "      <td>1.524104</td>\n",
              "      <td>1.518376</td>\n",
              "      <td>1.511774</td>\n",
              "      <td>1.505117</td>\n",
              "      <td>1.499054</td>\n",
              "      <td>1.493960</td>\n",
              "      <td>1.489913</td>\n",
              "      <td>...</td>\n",
              "      <td>1.550977</td>\n",
              "      <td>1.583197</td>\n",
              "      <td>1.605661</td>\n",
              "      <td>1.618126</td>\n",
              "      <td>1.622195</td>\n",
              "      <td>1.620631</td>\n",
              "      <td>1.616599</td>\n",
              "      <td>1.612957</td>\n",
              "      <td>1.611750</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5996</th>\n",
              "      <td>1.515056</td>\n",
              "      <td>1.511174</td>\n",
              "      <td>1.509395</td>\n",
              "      <td>1.510004</td>\n",
              "      <td>1.512908</td>\n",
              "      <td>1.517656</td>\n",
              "      <td>1.523524</td>\n",
              "      <td>1.529653</td>\n",
              "      <td>1.535207</td>\n",
              "      <td>1.539537</td>\n",
              "      <td>...</td>\n",
              "      <td>1.693020</td>\n",
              "      <td>1.696392</td>\n",
              "      <td>1.698900</td>\n",
              "      <td>1.700400</td>\n",
              "      <td>1.700869</td>\n",
              "      <td>1.700424</td>\n",
              "      <td>1.699334</td>\n",
              "      <td>1.697991</td>\n",
              "      <td>1.696852</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5997</th>\n",
              "      <td>1.565344</td>\n",
              "      <td>1.564102</td>\n",
              "      <td>1.561757</td>\n",
              "      <td>1.559405</td>\n",
              "      <td>1.558051</td>\n",
              "      <td>1.558382</td>\n",
              "      <td>1.560646</td>\n",
              "      <td>1.564660</td>\n",
              "      <td>1.569916</td>\n",
              "      <td>1.575756</td>\n",
              "      <td>...</td>\n",
              "      <td>1.535411</td>\n",
              "      <td>1.536617</td>\n",
              "      <td>1.537876</td>\n",
              "      <td>1.539222</td>\n",
              "      <td>1.540659</td>\n",
              "      <td>1.542157</td>\n",
              "      <td>1.543669</td>\n",
              "      <td>1.545152</td>\n",
              "      <td>1.546585</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5998</th>\n",
              "      <td>1.597548</td>\n",
              "      <td>1.595698</td>\n",
              "      <td>1.594912</td>\n",
              "      <td>1.595167</td>\n",
              "      <td>1.596213</td>\n",
              "      <td>1.597637</td>\n",
              "      <td>1.598970</td>\n",
              "      <td>1.599797</td>\n",
              "      <td>1.599838</td>\n",
              "      <td>1.598980</td>\n",
              "      <td>...</td>\n",
              "      <td>1.617056</td>\n",
              "      <td>1.620175</td>\n",
              "      <td>1.621880</td>\n",
              "      <td>1.622224</td>\n",
              "      <td>1.621475</td>\n",
              "      <td>1.620038</td>\n",
              "      <td>1.618362</td>\n",
              "      <td>1.616840</td>\n",
              "      <td>1.615746</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5999</th>\n",
              "      <td>1.599834</td>\n",
              "      <td>1.599558</td>\n",
              "      <td>1.598638</td>\n",
              "      <td>1.596714</td>\n",
              "      <td>1.593627</td>\n",
              "      <td>1.589487</td>\n",
              "      <td>1.584661</td>\n",
              "      <td>1.579703</td>\n",
              "      <td>1.575233</td>\n",
              "      <td>1.571814</td>\n",
              "      <td>...</td>\n",
              "      <td>1.668857</td>\n",
              "      <td>1.717732</td>\n",
              "      <td>1.769180</td>\n",
              "      <td>1.815834</td>\n",
              "      <td>1.849687</td>\n",
              "      <td>1.863721</td>\n",
              "      <td>1.853540</td>\n",
              "      <td>1.818573</td>\n",
              "      <td>1.762504</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6000 rows × 257 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-25cfc8d3-8be4-4440-9304-33f0c90ca935')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-25cfc8d3-8be4-4440-9304-33f0c90ca935 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-25cfc8d3-8be4-4440-9304-33f0c90ca935');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# y_train = arr\n",
        "# y_test = arr_t\n",
        "# y_val = arr_v"
      ],
      "metadata": {
        "id": "gWJKvYGYOiqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터 전체 섞기"
      ],
      "metadata": {
        "id": "ruynxE4-OiqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_total=pd.concat([data_train, data_test, data_val])\n",
        "data_total"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "outputId": "4e310ce3-4508-4e6c-96f6-dec0a0e68a9a",
        "id": "DeXXgNCLOiqZ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           0         1         2         3         4         5         6    \\\n",
              "0     1.697478  1.713703  1.728594  1.741144  1.750717  1.757105  1.760490   \n",
              "1     1.748610  1.747925  1.746924  1.746097  1.745874  1.746535  1.748158   \n",
              "2     1.709674  1.710317  1.711171  1.712183  1.713227  1.714166  1.714897   \n",
              "3     1.726979  1.726187  1.724966  1.723451  1.721925  1.720776  1.720402   \n",
              "4     1.889883  1.889929  1.891485  1.895719  1.903396  1.914786  1.929653   \n",
              "...        ...       ...       ...       ...       ...       ...       ...   \n",
              "5995  1.528888  1.529840  1.528139  1.524104  1.518376  1.511774  1.505117   \n",
              "5996  1.515056  1.511174  1.509395  1.510004  1.512908  1.517656  1.523524   \n",
              "5997  1.565344  1.564102  1.561757  1.559405  1.558051  1.558382  1.560646   \n",
              "5998  1.597548  1.595698  1.594912  1.595167  1.596213  1.597637  1.598970   \n",
              "5999  1.599834  1.599558  1.598638  1.596714  1.593627  1.589487  1.584661   \n",
              "\n",
              "           7         8         9    ...       247       248       249  \\\n",
              "0     1.761353  1.760349  1.758191  ...  1.733012  1.733423  1.734708   \n",
              "1     1.750626  1.753678  1.756978  ...  1.694451  1.696899  1.699745   \n",
              "2     1.715389  1.715680  1.715858  ...  1.722441  1.724729  1.726115   \n",
              "3     1.721109  1.723015  1.726001  ...  1.739826  1.739925  1.740406   \n",
              "4     1.947339  1.966906  1.987309  ...  1.914361  1.928167  1.944398   \n",
              "...        ...       ...       ...  ...       ...       ...       ...   \n",
              "5995  1.499054  1.493960  1.489913  ...  1.550977  1.583197  1.605661   \n",
              "5996  1.529653  1.535207  1.539537  ...  1.693020  1.696392  1.698900   \n",
              "5997  1.564660  1.569916  1.575756  ...  1.535411  1.536617  1.537876   \n",
              "5998  1.599797  1.599838  1.598980  ...  1.617056  1.620175  1.621880   \n",
              "5999  1.579703  1.575233  1.571814  ...  1.668857  1.717732  1.769180   \n",
              "\n",
              "           250       251       252       253       254       255    256  \n",
              "0     1.736879  1.739700  1.742739  1.745491  1.747515  1.748556    1.0  \n",
              "1     1.702543  1.704941  1.706748  1.707952  1.708690  1.709187    1.0  \n",
              "2     1.726820  1.727122  1.727259  1.727362  1.727430  1.727353    1.0  \n",
              "3     1.741383  1.742927  1.745059  1.747751  1.750910  1.754365    1.0  \n",
              "4     1.962799  1.982890  2.004016  2.025433  2.046399  2.066255    1.0  \n",
              "...        ...       ...       ...       ...       ...       ...    ...  \n",
              "5995  1.618126  1.622195  1.620631  1.616599  1.612957  1.611750  100.0  \n",
              "5996  1.700400  1.700869  1.700424  1.699334  1.697991  1.696852  100.0  \n",
              "5997  1.539222  1.540659  1.542157  1.543669  1.545152  1.546585  100.0  \n",
              "5998  1.622224  1.621475  1.620038  1.618362  1.616840  1.615746  100.0  \n",
              "5999  1.815834  1.849687  1.863721  1.853540  1.818573  1.762504  100.0  \n",
              "\n",
              "[36000 rows x 257 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ba0eb768-e3a2-4a28-9209-dcf3f3ae9a67\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>247</th>\n",
              "      <th>248</th>\n",
              "      <th>249</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "      <th>256</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.697478</td>\n",
              "      <td>1.713703</td>\n",
              "      <td>1.728594</td>\n",
              "      <td>1.741144</td>\n",
              "      <td>1.750717</td>\n",
              "      <td>1.757105</td>\n",
              "      <td>1.760490</td>\n",
              "      <td>1.761353</td>\n",
              "      <td>1.760349</td>\n",
              "      <td>1.758191</td>\n",
              "      <td>...</td>\n",
              "      <td>1.733012</td>\n",
              "      <td>1.733423</td>\n",
              "      <td>1.734708</td>\n",
              "      <td>1.736879</td>\n",
              "      <td>1.739700</td>\n",
              "      <td>1.742739</td>\n",
              "      <td>1.745491</td>\n",
              "      <td>1.747515</td>\n",
              "      <td>1.748556</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.748610</td>\n",
              "      <td>1.747925</td>\n",
              "      <td>1.746924</td>\n",
              "      <td>1.746097</td>\n",
              "      <td>1.745874</td>\n",
              "      <td>1.746535</td>\n",
              "      <td>1.748158</td>\n",
              "      <td>1.750626</td>\n",
              "      <td>1.753678</td>\n",
              "      <td>1.756978</td>\n",
              "      <td>...</td>\n",
              "      <td>1.694451</td>\n",
              "      <td>1.696899</td>\n",
              "      <td>1.699745</td>\n",
              "      <td>1.702543</td>\n",
              "      <td>1.704941</td>\n",
              "      <td>1.706748</td>\n",
              "      <td>1.707952</td>\n",
              "      <td>1.708690</td>\n",
              "      <td>1.709187</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.709674</td>\n",
              "      <td>1.710317</td>\n",
              "      <td>1.711171</td>\n",
              "      <td>1.712183</td>\n",
              "      <td>1.713227</td>\n",
              "      <td>1.714166</td>\n",
              "      <td>1.714897</td>\n",
              "      <td>1.715389</td>\n",
              "      <td>1.715680</td>\n",
              "      <td>1.715858</td>\n",
              "      <td>...</td>\n",
              "      <td>1.722441</td>\n",
              "      <td>1.724729</td>\n",
              "      <td>1.726115</td>\n",
              "      <td>1.726820</td>\n",
              "      <td>1.727122</td>\n",
              "      <td>1.727259</td>\n",
              "      <td>1.727362</td>\n",
              "      <td>1.727430</td>\n",
              "      <td>1.727353</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.726979</td>\n",
              "      <td>1.726187</td>\n",
              "      <td>1.724966</td>\n",
              "      <td>1.723451</td>\n",
              "      <td>1.721925</td>\n",
              "      <td>1.720776</td>\n",
              "      <td>1.720402</td>\n",
              "      <td>1.721109</td>\n",
              "      <td>1.723015</td>\n",
              "      <td>1.726001</td>\n",
              "      <td>...</td>\n",
              "      <td>1.739826</td>\n",
              "      <td>1.739925</td>\n",
              "      <td>1.740406</td>\n",
              "      <td>1.741383</td>\n",
              "      <td>1.742927</td>\n",
              "      <td>1.745059</td>\n",
              "      <td>1.747751</td>\n",
              "      <td>1.750910</td>\n",
              "      <td>1.754365</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.889883</td>\n",
              "      <td>1.889929</td>\n",
              "      <td>1.891485</td>\n",
              "      <td>1.895719</td>\n",
              "      <td>1.903396</td>\n",
              "      <td>1.914786</td>\n",
              "      <td>1.929653</td>\n",
              "      <td>1.947339</td>\n",
              "      <td>1.966906</td>\n",
              "      <td>1.987309</td>\n",
              "      <td>...</td>\n",
              "      <td>1.914361</td>\n",
              "      <td>1.928167</td>\n",
              "      <td>1.944398</td>\n",
              "      <td>1.962799</td>\n",
              "      <td>1.982890</td>\n",
              "      <td>2.004016</td>\n",
              "      <td>2.025433</td>\n",
              "      <td>2.046399</td>\n",
              "      <td>2.066255</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5995</th>\n",
              "      <td>1.528888</td>\n",
              "      <td>1.529840</td>\n",
              "      <td>1.528139</td>\n",
              "      <td>1.524104</td>\n",
              "      <td>1.518376</td>\n",
              "      <td>1.511774</td>\n",
              "      <td>1.505117</td>\n",
              "      <td>1.499054</td>\n",
              "      <td>1.493960</td>\n",
              "      <td>1.489913</td>\n",
              "      <td>...</td>\n",
              "      <td>1.550977</td>\n",
              "      <td>1.583197</td>\n",
              "      <td>1.605661</td>\n",
              "      <td>1.618126</td>\n",
              "      <td>1.622195</td>\n",
              "      <td>1.620631</td>\n",
              "      <td>1.616599</td>\n",
              "      <td>1.612957</td>\n",
              "      <td>1.611750</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5996</th>\n",
              "      <td>1.515056</td>\n",
              "      <td>1.511174</td>\n",
              "      <td>1.509395</td>\n",
              "      <td>1.510004</td>\n",
              "      <td>1.512908</td>\n",
              "      <td>1.517656</td>\n",
              "      <td>1.523524</td>\n",
              "      <td>1.529653</td>\n",
              "      <td>1.535207</td>\n",
              "      <td>1.539537</td>\n",
              "      <td>...</td>\n",
              "      <td>1.693020</td>\n",
              "      <td>1.696392</td>\n",
              "      <td>1.698900</td>\n",
              "      <td>1.700400</td>\n",
              "      <td>1.700869</td>\n",
              "      <td>1.700424</td>\n",
              "      <td>1.699334</td>\n",
              "      <td>1.697991</td>\n",
              "      <td>1.696852</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5997</th>\n",
              "      <td>1.565344</td>\n",
              "      <td>1.564102</td>\n",
              "      <td>1.561757</td>\n",
              "      <td>1.559405</td>\n",
              "      <td>1.558051</td>\n",
              "      <td>1.558382</td>\n",
              "      <td>1.560646</td>\n",
              "      <td>1.564660</td>\n",
              "      <td>1.569916</td>\n",
              "      <td>1.575756</td>\n",
              "      <td>...</td>\n",
              "      <td>1.535411</td>\n",
              "      <td>1.536617</td>\n",
              "      <td>1.537876</td>\n",
              "      <td>1.539222</td>\n",
              "      <td>1.540659</td>\n",
              "      <td>1.542157</td>\n",
              "      <td>1.543669</td>\n",
              "      <td>1.545152</td>\n",
              "      <td>1.546585</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5998</th>\n",
              "      <td>1.597548</td>\n",
              "      <td>1.595698</td>\n",
              "      <td>1.594912</td>\n",
              "      <td>1.595167</td>\n",
              "      <td>1.596213</td>\n",
              "      <td>1.597637</td>\n",
              "      <td>1.598970</td>\n",
              "      <td>1.599797</td>\n",
              "      <td>1.599838</td>\n",
              "      <td>1.598980</td>\n",
              "      <td>...</td>\n",
              "      <td>1.617056</td>\n",
              "      <td>1.620175</td>\n",
              "      <td>1.621880</td>\n",
              "      <td>1.622224</td>\n",
              "      <td>1.621475</td>\n",
              "      <td>1.620038</td>\n",
              "      <td>1.618362</td>\n",
              "      <td>1.616840</td>\n",
              "      <td>1.615746</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5999</th>\n",
              "      <td>1.599834</td>\n",
              "      <td>1.599558</td>\n",
              "      <td>1.598638</td>\n",
              "      <td>1.596714</td>\n",
              "      <td>1.593627</td>\n",
              "      <td>1.589487</td>\n",
              "      <td>1.584661</td>\n",
              "      <td>1.579703</td>\n",
              "      <td>1.575233</td>\n",
              "      <td>1.571814</td>\n",
              "      <td>...</td>\n",
              "      <td>1.668857</td>\n",
              "      <td>1.717732</td>\n",
              "      <td>1.769180</td>\n",
              "      <td>1.815834</td>\n",
              "      <td>1.849687</td>\n",
              "      <td>1.863721</td>\n",
              "      <td>1.853540</td>\n",
              "      <td>1.818573</td>\n",
              "      <td>1.762504</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>36000 rows × 257 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ba0eb768-e3a2-4a28-9209-dcf3f3ae9a67')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ba0eb768-e3a2-4a28-9209-dcf3f3ae9a67 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ba0eb768-e3a2-4a28-9209-dcf3f3ae9a67');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_total=data_total.sample(frac=1)"
      ],
      "metadata": {
        "id": "jyie6GiXOiqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train=data_total.iloc[0:24000, :]\n",
        "data_val=data_total.iloc[24000:30000, :]\n",
        "data_test=data_total.iloc[30000:36000,:]"
      ],
      "metadata": {
        "id": "ZutkohbCOiqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "outputId": "0b6e5415-8a26-45d2-d0df-37395ab5bf20",
        "id": "Ccuz9w6EOiqZ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            0         1         2         3         4         5         6    \\\n",
              "14797  1.575498  1.563810  1.554748  1.551273  1.554579  1.564238  1.578608   \n",
              "4442   1.770798  1.776173  1.783383  1.791552  1.799648  1.806729  1.812172   \n",
              "19385  1.735351  1.746334  1.752386  1.757505  1.764550  1.774997  1.789012   \n",
              "1667   1.701525  1.681062  1.662269  1.645431  1.630848  1.618785  1.609411   \n",
              "19969  1.616300  1.620086  1.621910  1.621008  1.616957  1.609821  1.600242   \n",
              "...         ...       ...       ...       ...       ...       ...       ...   \n",
              "2564   1.569072  1.567932  1.567012  1.566737  1.567516  1.569643  1.573193   \n",
              "17925  1.733440  1.741777  1.750718  1.760091  1.769664  1.779155  1.788254   \n",
              "1922   1.625553  1.627044  1.628636  1.630064  1.631115  1.631655  1.631648   \n",
              "19182  1.829093  1.808695  1.786890  1.764763  1.743234  1.722943  1.704218   \n",
              "2007   1.929730  1.870354  1.800231  1.728651  1.664408  1.614277  1.582014   \n",
              "\n",
              "            7         8         9    ...       247       248       249  \\\n",
              "14797  1.595385  1.612176  1.626981  ...  1.496063  1.497101  1.499070   \n",
              "4442   1.815827  1.818039  1.819563  ...  1.857962  1.857110  1.857146   \n",
              "19385  1.805769  1.823886  1.841890  ...  1.584464  1.588439  1.592877   \n",
              "1667   1.602731  1.598562  1.596527  ...  1.593327  1.591406  1.589981   \n",
              "19969  1.589441  1.579115  1.571215  ...  1.652794  1.656397  1.661812   \n",
              "...         ...       ...       ...  ...       ...       ...       ...   \n",
              "2564   1.577953  1.583408  1.588808  ...  1.605560  1.596799  1.587624   \n",
              "17925  1.796650  1.804062  1.810267  ...  1.694811  1.708601  1.722008   \n",
              "1922   1.631146  1.630273  1.629189  ...  1.575915  1.575933  1.575032   \n",
              "19182  1.687110  1.671490  1.657168  ...  1.604341  1.608744  1.611719   \n",
              "2007   1.568048  1.569857  1.582870  ...  1.742550  1.741831  1.740769   \n",
              "\n",
              "            250       251       252       253       254       255   256  \n",
              "14797  1.501751  1.504783  1.507710  1.510087  1.511590  1.512108  62.0  \n",
              "4442   1.858595  1.861911  1.867374  1.874999  1.884480  1.895171  75.0  \n",
              "19385  1.597272  1.601050  1.603653  1.604633  1.603749  1.601042  81.0  \n",
              "1667   1.589109  1.588789  1.588973  1.589581  1.590503  1.591591   7.0  \n",
              "19969  1.669728  1.680872  1.695850  1.714960  1.738014  1.764247  84.0  \n",
              "...         ...       ...       ...       ...       ...       ...   ...  \n",
              "2564   1.578812  1.571024  1.564695  1.559992  1.556846  1.555039  43.0  \n",
              "17925  1.732289  1.738204  1.739854  1.738288  1.735035  1.731660  75.0  \n",
              "1922   1.573324  1.571086  1.568699  1.566545  1.564915  1.563942   9.0  \n",
              "19182  1.612854  1.612013  1.609360  1.605316  1.600460  1.595409  80.0  \n",
              "2007   1.739280  1.737119  1.733914  1.729231  1.722660  1.713918  34.0  \n",
              "\n",
              "[24000 rows x 257 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-88b119ff-d8f1-40bf-b791-06e8562fce55\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>247</th>\n",
              "      <th>248</th>\n",
              "      <th>249</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "      <th>256</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>14797</th>\n",
              "      <td>1.575498</td>\n",
              "      <td>1.563810</td>\n",
              "      <td>1.554748</td>\n",
              "      <td>1.551273</td>\n",
              "      <td>1.554579</td>\n",
              "      <td>1.564238</td>\n",
              "      <td>1.578608</td>\n",
              "      <td>1.595385</td>\n",
              "      <td>1.612176</td>\n",
              "      <td>1.626981</td>\n",
              "      <td>...</td>\n",
              "      <td>1.496063</td>\n",
              "      <td>1.497101</td>\n",
              "      <td>1.499070</td>\n",
              "      <td>1.501751</td>\n",
              "      <td>1.504783</td>\n",
              "      <td>1.507710</td>\n",
              "      <td>1.510087</td>\n",
              "      <td>1.511590</td>\n",
              "      <td>1.512108</td>\n",
              "      <td>62.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4442</th>\n",
              "      <td>1.770798</td>\n",
              "      <td>1.776173</td>\n",
              "      <td>1.783383</td>\n",
              "      <td>1.791552</td>\n",
              "      <td>1.799648</td>\n",
              "      <td>1.806729</td>\n",
              "      <td>1.812172</td>\n",
              "      <td>1.815827</td>\n",
              "      <td>1.818039</td>\n",
              "      <td>1.819563</td>\n",
              "      <td>...</td>\n",
              "      <td>1.857962</td>\n",
              "      <td>1.857110</td>\n",
              "      <td>1.857146</td>\n",
              "      <td>1.858595</td>\n",
              "      <td>1.861911</td>\n",
              "      <td>1.867374</td>\n",
              "      <td>1.874999</td>\n",
              "      <td>1.884480</td>\n",
              "      <td>1.895171</td>\n",
              "      <td>75.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19385</th>\n",
              "      <td>1.735351</td>\n",
              "      <td>1.746334</td>\n",
              "      <td>1.752386</td>\n",
              "      <td>1.757505</td>\n",
              "      <td>1.764550</td>\n",
              "      <td>1.774997</td>\n",
              "      <td>1.789012</td>\n",
              "      <td>1.805769</td>\n",
              "      <td>1.823886</td>\n",
              "      <td>1.841890</td>\n",
              "      <td>...</td>\n",
              "      <td>1.584464</td>\n",
              "      <td>1.588439</td>\n",
              "      <td>1.592877</td>\n",
              "      <td>1.597272</td>\n",
              "      <td>1.601050</td>\n",
              "      <td>1.603653</td>\n",
              "      <td>1.604633</td>\n",
              "      <td>1.603749</td>\n",
              "      <td>1.601042</td>\n",
              "      <td>81.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1667</th>\n",
              "      <td>1.701525</td>\n",
              "      <td>1.681062</td>\n",
              "      <td>1.662269</td>\n",
              "      <td>1.645431</td>\n",
              "      <td>1.630848</td>\n",
              "      <td>1.618785</td>\n",
              "      <td>1.609411</td>\n",
              "      <td>1.602731</td>\n",
              "      <td>1.598562</td>\n",
              "      <td>1.596527</td>\n",
              "      <td>...</td>\n",
              "      <td>1.593327</td>\n",
              "      <td>1.591406</td>\n",
              "      <td>1.589981</td>\n",
              "      <td>1.589109</td>\n",
              "      <td>1.588789</td>\n",
              "      <td>1.588973</td>\n",
              "      <td>1.589581</td>\n",
              "      <td>1.590503</td>\n",
              "      <td>1.591591</td>\n",
              "      <td>7.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19969</th>\n",
              "      <td>1.616300</td>\n",
              "      <td>1.620086</td>\n",
              "      <td>1.621910</td>\n",
              "      <td>1.621008</td>\n",
              "      <td>1.616957</td>\n",
              "      <td>1.609821</td>\n",
              "      <td>1.600242</td>\n",
              "      <td>1.589441</td>\n",
              "      <td>1.579115</td>\n",
              "      <td>1.571215</td>\n",
              "      <td>...</td>\n",
              "      <td>1.652794</td>\n",
              "      <td>1.656397</td>\n",
              "      <td>1.661812</td>\n",
              "      <td>1.669728</td>\n",
              "      <td>1.680872</td>\n",
              "      <td>1.695850</td>\n",
              "      <td>1.714960</td>\n",
              "      <td>1.738014</td>\n",
              "      <td>1.764247</td>\n",
              "      <td>84.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2564</th>\n",
              "      <td>1.569072</td>\n",
              "      <td>1.567932</td>\n",
              "      <td>1.567012</td>\n",
              "      <td>1.566737</td>\n",
              "      <td>1.567516</td>\n",
              "      <td>1.569643</td>\n",
              "      <td>1.573193</td>\n",
              "      <td>1.577953</td>\n",
              "      <td>1.583408</td>\n",
              "      <td>1.588808</td>\n",
              "      <td>...</td>\n",
              "      <td>1.605560</td>\n",
              "      <td>1.596799</td>\n",
              "      <td>1.587624</td>\n",
              "      <td>1.578812</td>\n",
              "      <td>1.571024</td>\n",
              "      <td>1.564695</td>\n",
              "      <td>1.559992</td>\n",
              "      <td>1.556846</td>\n",
              "      <td>1.555039</td>\n",
              "      <td>43.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17925</th>\n",
              "      <td>1.733440</td>\n",
              "      <td>1.741777</td>\n",
              "      <td>1.750718</td>\n",
              "      <td>1.760091</td>\n",
              "      <td>1.769664</td>\n",
              "      <td>1.779155</td>\n",
              "      <td>1.788254</td>\n",
              "      <td>1.796650</td>\n",
              "      <td>1.804062</td>\n",
              "      <td>1.810267</td>\n",
              "      <td>...</td>\n",
              "      <td>1.694811</td>\n",
              "      <td>1.708601</td>\n",
              "      <td>1.722008</td>\n",
              "      <td>1.732289</td>\n",
              "      <td>1.738204</td>\n",
              "      <td>1.739854</td>\n",
              "      <td>1.738288</td>\n",
              "      <td>1.735035</td>\n",
              "      <td>1.731660</td>\n",
              "      <td>75.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1922</th>\n",
              "      <td>1.625553</td>\n",
              "      <td>1.627044</td>\n",
              "      <td>1.628636</td>\n",
              "      <td>1.630064</td>\n",
              "      <td>1.631115</td>\n",
              "      <td>1.631655</td>\n",
              "      <td>1.631648</td>\n",
              "      <td>1.631146</td>\n",
              "      <td>1.630273</td>\n",
              "      <td>1.629189</td>\n",
              "      <td>...</td>\n",
              "      <td>1.575915</td>\n",
              "      <td>1.575933</td>\n",
              "      <td>1.575032</td>\n",
              "      <td>1.573324</td>\n",
              "      <td>1.571086</td>\n",
              "      <td>1.568699</td>\n",
              "      <td>1.566545</td>\n",
              "      <td>1.564915</td>\n",
              "      <td>1.563942</td>\n",
              "      <td>9.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19182</th>\n",
              "      <td>1.829093</td>\n",
              "      <td>1.808695</td>\n",
              "      <td>1.786890</td>\n",
              "      <td>1.764763</td>\n",
              "      <td>1.743234</td>\n",
              "      <td>1.722943</td>\n",
              "      <td>1.704218</td>\n",
              "      <td>1.687110</td>\n",
              "      <td>1.671490</td>\n",
              "      <td>1.657168</td>\n",
              "      <td>...</td>\n",
              "      <td>1.604341</td>\n",
              "      <td>1.608744</td>\n",
              "      <td>1.611719</td>\n",
              "      <td>1.612854</td>\n",
              "      <td>1.612013</td>\n",
              "      <td>1.609360</td>\n",
              "      <td>1.605316</td>\n",
              "      <td>1.600460</td>\n",
              "      <td>1.595409</td>\n",
              "      <td>80.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2007</th>\n",
              "      <td>1.929730</td>\n",
              "      <td>1.870354</td>\n",
              "      <td>1.800231</td>\n",
              "      <td>1.728651</td>\n",
              "      <td>1.664408</td>\n",
              "      <td>1.614277</td>\n",
              "      <td>1.582014</td>\n",
              "      <td>1.568048</td>\n",
              "      <td>1.569857</td>\n",
              "      <td>1.582870</td>\n",
              "      <td>...</td>\n",
              "      <td>1.742550</td>\n",
              "      <td>1.741831</td>\n",
              "      <td>1.740769</td>\n",
              "      <td>1.739280</td>\n",
              "      <td>1.737119</td>\n",
              "      <td>1.733914</td>\n",
              "      <td>1.729231</td>\n",
              "      <td>1.722660</td>\n",
              "      <td>1.713918</td>\n",
              "      <td>34.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>24000 rows × 257 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-88b119ff-d8f1-40bf-b791-06e8562fce55')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-88b119ff-d8f1-40bf-b791-06e8562fce55 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-88b119ff-d8f1-40bf-b791-06e8562fce55');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 전처리"
      ],
      "metadata": {
        "id": "FbtYLTXsOiqa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = to_categorical(data_train[256])\n",
        "y_test = to_categorical(data_test[256])\n",
        "y_val = to_categorical(data_val[256])\n",
        "y_train[23999]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a1bdabf-3601-4f3e-c3b3-607de465d832",
        "id": "GfabuFB_Oiqa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_train.drop([256], axis=1, inplace=True)\n",
        "data_test.drop([256], axis=1, inplace=True)\n",
        "data_val.drop([256], axis=1, inplace=True)\n",
        "data_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "outputId": "435eefd0-ae69-4f52-cd99-634f0e9d9431",
        "id": "eBZ1tR7fOiqa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py:4906: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  return super().drop(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            0         1         2         3         4         5         6    \\\n",
              "14797  1.575498  1.563810  1.554748  1.551273  1.554579  1.564238  1.578608   \n",
              "4442   1.770798  1.776173  1.783383  1.791552  1.799648  1.806729  1.812172   \n",
              "19385  1.735351  1.746334  1.752386  1.757505  1.764550  1.774997  1.789012   \n",
              "1667   1.701525  1.681062  1.662269  1.645431  1.630848  1.618785  1.609411   \n",
              "19969  1.616300  1.620086  1.621910  1.621008  1.616957  1.609821  1.600242   \n",
              "...         ...       ...       ...       ...       ...       ...       ...   \n",
              "2564   1.569072  1.567932  1.567012  1.566737  1.567516  1.569643  1.573193   \n",
              "17925  1.733440  1.741777  1.750718  1.760091  1.769664  1.779155  1.788254   \n",
              "1922   1.625553  1.627044  1.628636  1.630064  1.631115  1.631655  1.631648   \n",
              "19182  1.829093  1.808695  1.786890  1.764763  1.743234  1.722943  1.704218   \n",
              "2007   1.929730  1.870354  1.800231  1.728651  1.664408  1.614277  1.582014   \n",
              "\n",
              "            7         8         9    ...       246       247       248  \\\n",
              "14797  1.595385  1.612176  1.626981  ...  1.496059  1.496063  1.497101   \n",
              "4442   1.815827  1.818039  1.819563  ...  1.859191  1.857962  1.857110   \n",
              "19385  1.805769  1.823886  1.841890  ...  1.581357  1.584464  1.588439   \n",
              "1667   1.602731  1.598562  1.596527  ...  1.595596  1.593327  1.591406   \n",
              "19969  1.589441  1.579115  1.571215  ...  1.650422  1.652794  1.656397   \n",
              "...         ...       ...       ...  ...       ...       ...       ...   \n",
              "2564   1.577953  1.583408  1.588808  ...  1.613151  1.605560  1.596799   \n",
              "17925  1.796650  1.804062  1.810267  ...  1.684728  1.694811  1.708601   \n",
              "1922   1.631146  1.630273  1.629189  ...  1.575051  1.575915  1.575933   \n",
              "19182  1.687110  1.671490  1.657168  ...  1.599130  1.604341  1.608744   \n",
              "2007   1.568048  1.569857  1.582870  ...  1.742859  1.742550  1.741831   \n",
              "\n",
              "            249       250       251       252       253       254       255  \n",
              "14797  1.499070  1.501751  1.504783  1.507710  1.510087  1.511590  1.512108  \n",
              "4442   1.857146  1.858595  1.861911  1.867374  1.874999  1.884480  1.895171  \n",
              "19385  1.592877  1.597272  1.601050  1.603653  1.604633  1.603749  1.601042  \n",
              "1667   1.589981  1.589109  1.588789  1.588973  1.589581  1.590503  1.591591  \n",
              "19969  1.661812  1.669728  1.680872  1.695850  1.714960  1.738014  1.764247  \n",
              "...         ...       ...       ...       ...       ...       ...       ...  \n",
              "2564   1.587624  1.578812  1.571024  1.564695  1.559992  1.556846  1.555039  \n",
              "17925  1.722008  1.732289  1.738204  1.739854  1.738288  1.735035  1.731660  \n",
              "1922   1.575032  1.573324  1.571086  1.568699  1.566545  1.564915  1.563942  \n",
              "19182  1.611719  1.612854  1.612013  1.609360  1.605316  1.600460  1.595409  \n",
              "2007   1.740769  1.739280  1.737119  1.733914  1.729231  1.722660  1.713918  \n",
              "\n",
              "[24000 rows x 256 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-22967a7b-083d-4717-a982-64d254a5fa14\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>246</th>\n",
              "      <th>247</th>\n",
              "      <th>248</th>\n",
              "      <th>249</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>14797</th>\n",
              "      <td>1.575498</td>\n",
              "      <td>1.563810</td>\n",
              "      <td>1.554748</td>\n",
              "      <td>1.551273</td>\n",
              "      <td>1.554579</td>\n",
              "      <td>1.564238</td>\n",
              "      <td>1.578608</td>\n",
              "      <td>1.595385</td>\n",
              "      <td>1.612176</td>\n",
              "      <td>1.626981</td>\n",
              "      <td>...</td>\n",
              "      <td>1.496059</td>\n",
              "      <td>1.496063</td>\n",
              "      <td>1.497101</td>\n",
              "      <td>1.499070</td>\n",
              "      <td>1.501751</td>\n",
              "      <td>1.504783</td>\n",
              "      <td>1.507710</td>\n",
              "      <td>1.510087</td>\n",
              "      <td>1.511590</td>\n",
              "      <td>1.512108</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4442</th>\n",
              "      <td>1.770798</td>\n",
              "      <td>1.776173</td>\n",
              "      <td>1.783383</td>\n",
              "      <td>1.791552</td>\n",
              "      <td>1.799648</td>\n",
              "      <td>1.806729</td>\n",
              "      <td>1.812172</td>\n",
              "      <td>1.815827</td>\n",
              "      <td>1.818039</td>\n",
              "      <td>1.819563</td>\n",
              "      <td>...</td>\n",
              "      <td>1.859191</td>\n",
              "      <td>1.857962</td>\n",
              "      <td>1.857110</td>\n",
              "      <td>1.857146</td>\n",
              "      <td>1.858595</td>\n",
              "      <td>1.861911</td>\n",
              "      <td>1.867374</td>\n",
              "      <td>1.874999</td>\n",
              "      <td>1.884480</td>\n",
              "      <td>1.895171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19385</th>\n",
              "      <td>1.735351</td>\n",
              "      <td>1.746334</td>\n",
              "      <td>1.752386</td>\n",
              "      <td>1.757505</td>\n",
              "      <td>1.764550</td>\n",
              "      <td>1.774997</td>\n",
              "      <td>1.789012</td>\n",
              "      <td>1.805769</td>\n",
              "      <td>1.823886</td>\n",
              "      <td>1.841890</td>\n",
              "      <td>...</td>\n",
              "      <td>1.581357</td>\n",
              "      <td>1.584464</td>\n",
              "      <td>1.588439</td>\n",
              "      <td>1.592877</td>\n",
              "      <td>1.597272</td>\n",
              "      <td>1.601050</td>\n",
              "      <td>1.603653</td>\n",
              "      <td>1.604633</td>\n",
              "      <td>1.603749</td>\n",
              "      <td>1.601042</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1667</th>\n",
              "      <td>1.701525</td>\n",
              "      <td>1.681062</td>\n",
              "      <td>1.662269</td>\n",
              "      <td>1.645431</td>\n",
              "      <td>1.630848</td>\n",
              "      <td>1.618785</td>\n",
              "      <td>1.609411</td>\n",
              "      <td>1.602731</td>\n",
              "      <td>1.598562</td>\n",
              "      <td>1.596527</td>\n",
              "      <td>...</td>\n",
              "      <td>1.595596</td>\n",
              "      <td>1.593327</td>\n",
              "      <td>1.591406</td>\n",
              "      <td>1.589981</td>\n",
              "      <td>1.589109</td>\n",
              "      <td>1.588789</td>\n",
              "      <td>1.588973</td>\n",
              "      <td>1.589581</td>\n",
              "      <td>1.590503</td>\n",
              "      <td>1.591591</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19969</th>\n",
              "      <td>1.616300</td>\n",
              "      <td>1.620086</td>\n",
              "      <td>1.621910</td>\n",
              "      <td>1.621008</td>\n",
              "      <td>1.616957</td>\n",
              "      <td>1.609821</td>\n",
              "      <td>1.600242</td>\n",
              "      <td>1.589441</td>\n",
              "      <td>1.579115</td>\n",
              "      <td>1.571215</td>\n",
              "      <td>...</td>\n",
              "      <td>1.650422</td>\n",
              "      <td>1.652794</td>\n",
              "      <td>1.656397</td>\n",
              "      <td>1.661812</td>\n",
              "      <td>1.669728</td>\n",
              "      <td>1.680872</td>\n",
              "      <td>1.695850</td>\n",
              "      <td>1.714960</td>\n",
              "      <td>1.738014</td>\n",
              "      <td>1.764247</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2564</th>\n",
              "      <td>1.569072</td>\n",
              "      <td>1.567932</td>\n",
              "      <td>1.567012</td>\n",
              "      <td>1.566737</td>\n",
              "      <td>1.567516</td>\n",
              "      <td>1.569643</td>\n",
              "      <td>1.573193</td>\n",
              "      <td>1.577953</td>\n",
              "      <td>1.583408</td>\n",
              "      <td>1.588808</td>\n",
              "      <td>...</td>\n",
              "      <td>1.613151</td>\n",
              "      <td>1.605560</td>\n",
              "      <td>1.596799</td>\n",
              "      <td>1.587624</td>\n",
              "      <td>1.578812</td>\n",
              "      <td>1.571024</td>\n",
              "      <td>1.564695</td>\n",
              "      <td>1.559992</td>\n",
              "      <td>1.556846</td>\n",
              "      <td>1.555039</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17925</th>\n",
              "      <td>1.733440</td>\n",
              "      <td>1.741777</td>\n",
              "      <td>1.750718</td>\n",
              "      <td>1.760091</td>\n",
              "      <td>1.769664</td>\n",
              "      <td>1.779155</td>\n",
              "      <td>1.788254</td>\n",
              "      <td>1.796650</td>\n",
              "      <td>1.804062</td>\n",
              "      <td>1.810267</td>\n",
              "      <td>...</td>\n",
              "      <td>1.684728</td>\n",
              "      <td>1.694811</td>\n",
              "      <td>1.708601</td>\n",
              "      <td>1.722008</td>\n",
              "      <td>1.732289</td>\n",
              "      <td>1.738204</td>\n",
              "      <td>1.739854</td>\n",
              "      <td>1.738288</td>\n",
              "      <td>1.735035</td>\n",
              "      <td>1.731660</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1922</th>\n",
              "      <td>1.625553</td>\n",
              "      <td>1.627044</td>\n",
              "      <td>1.628636</td>\n",
              "      <td>1.630064</td>\n",
              "      <td>1.631115</td>\n",
              "      <td>1.631655</td>\n",
              "      <td>1.631648</td>\n",
              "      <td>1.631146</td>\n",
              "      <td>1.630273</td>\n",
              "      <td>1.629189</td>\n",
              "      <td>...</td>\n",
              "      <td>1.575051</td>\n",
              "      <td>1.575915</td>\n",
              "      <td>1.575933</td>\n",
              "      <td>1.575032</td>\n",
              "      <td>1.573324</td>\n",
              "      <td>1.571086</td>\n",
              "      <td>1.568699</td>\n",
              "      <td>1.566545</td>\n",
              "      <td>1.564915</td>\n",
              "      <td>1.563942</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19182</th>\n",
              "      <td>1.829093</td>\n",
              "      <td>1.808695</td>\n",
              "      <td>1.786890</td>\n",
              "      <td>1.764763</td>\n",
              "      <td>1.743234</td>\n",
              "      <td>1.722943</td>\n",
              "      <td>1.704218</td>\n",
              "      <td>1.687110</td>\n",
              "      <td>1.671490</td>\n",
              "      <td>1.657168</td>\n",
              "      <td>...</td>\n",
              "      <td>1.599130</td>\n",
              "      <td>1.604341</td>\n",
              "      <td>1.608744</td>\n",
              "      <td>1.611719</td>\n",
              "      <td>1.612854</td>\n",
              "      <td>1.612013</td>\n",
              "      <td>1.609360</td>\n",
              "      <td>1.605316</td>\n",
              "      <td>1.600460</td>\n",
              "      <td>1.595409</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2007</th>\n",
              "      <td>1.929730</td>\n",
              "      <td>1.870354</td>\n",
              "      <td>1.800231</td>\n",
              "      <td>1.728651</td>\n",
              "      <td>1.664408</td>\n",
              "      <td>1.614277</td>\n",
              "      <td>1.582014</td>\n",
              "      <td>1.568048</td>\n",
              "      <td>1.569857</td>\n",
              "      <td>1.582870</td>\n",
              "      <td>...</td>\n",
              "      <td>1.742859</td>\n",
              "      <td>1.742550</td>\n",
              "      <td>1.741831</td>\n",
              "      <td>1.740769</td>\n",
              "      <td>1.739280</td>\n",
              "      <td>1.737119</td>\n",
              "      <td>1.733914</td>\n",
              "      <td>1.729231</td>\n",
              "      <td>1.722660</td>\n",
              "      <td>1.713918</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>24000 rows × 256 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-22967a7b-083d-4717-a982-64d254a5fa14')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-22967a7b-083d-4717-a982-64d254a5fa14 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-22967a7b-083d-4717-a982-64d254a5fa14');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train=data_train.values\n",
        "X_test=data_test.values\n",
        "X_val=data_val.values"
      ],
      "metadata": {
        "id": "Rr80PXBHOiqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(X_val.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)\n",
        "print(y_val.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7ce4319-c349-489b-9569-7e7fecdd6c93",
        "id": "2OCVhzMEOiqa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(24000, 256)\n",
            "(6000, 256)\n",
            "(6000, 256)\n",
            "(24000, 101)\n",
            "(6000, 101)\n",
            "(6000, 101)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X_train = X_train.reshape(24000, 256, 1)\n",
        "X_test = X_test.reshape(6000, 256, 1)\n",
        "X_val = X_val.reshape(6000, 256, 1)\n",
        "X_train.shape, X_test.shape, X_val.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3024b6b6-e4c5-43b3-9b2a-a41d51b62e58",
        "id": "wyQIyKnkOiqa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((24000, 256, 1), (6000, 256, 1), (6000, 256, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***gru모델(이 모델을 사용하였습니다)***"
      ],
      "metadata": {
        "id": "mnDw7nHbOiqb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(GRU(units=100, return_sequences=True, input_shape=(256,1))) #shape[1]=열\n",
        "model.add(GRU(units=50, return_sequences=True))\n",
        "model.add(GRU(units=50, return_sequences=True))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Dense(101, activation='softmax'))\n",
        "model.compile(optimizer='adam', loss='mse',metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60136cb5-1a96-431e-dc97-9ca7f73002f6",
        "id": "DzBk-L37Oiqb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " gru_37 (GRU)                (None, 256, 100)          30900     \n",
            "                                                                 \n",
            " gru_38 (GRU)                (None, 256, 50)           22800     \n",
            "                                                                 \n",
            " gru_39 (GRU)                (None, 256, 50)           15300     \n",
            "                                                                 \n",
            " flatten_8 (Flatten)         (None, 12800)             0         \n",
            "                                                                 \n",
            " dropout_16 (Dropout)        (None, 12800)             0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 101)               1292901   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,361,901\n",
            "Trainable params: 1,361,901\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## cnn"
      ],
      "metadata": {
        "id": "GpFsZuLgOiqa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model():\n",
        "    model = Sequential()\n",
        "    model.add(Conv1D(filters=64, kernel_size=6, activation='relu', \n",
        "                    padding='same', input_shape=(256, 1)))\n",
        "    model.add(BatchNormalization())\n",
        "    \n",
        "    # adding a pooling layer\n",
        "    model.add(MaxPooling1D(pool_size=(3), strides=2, padding='same'))\n",
        "    \n",
        "    model.add(Conv1D(filters=64, kernel_size=6, activation='relu', \n",
        "                    padding='same', input_shape=(256, 1)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling1D(pool_size=(3), strides=2, padding='same'))\n",
        "    \n",
        "    model.add(Conv1D(filters=64, kernel_size=6, activation='relu', \n",
        "                    padding='same', input_shape=(256, 1)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling1D(pool_size=(3), strides=2, padding='same'))\n",
        "    \n",
        "    model.add(Flatten())\n",
        "    #model.add(Dense(64, activation='relu'))\n",
        "    #model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(101, activation='softmax'))\n",
        "    \n",
        "    model.compile(optimizer='adam', loss='mse',metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "NS-hyS2qOiqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model()\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2397946-5a7a-4f1f-e09b-f35229db7ab7",
        "id": "ZAw124CwOiqa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_18\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_17 (Conv1D)          (None, 256, 64)           448       \n",
            "                                                                 \n",
            " batch_normalization_12 (Bat  (None, 256, 64)          256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling1d_17 (MaxPoolin  (None, 128, 64)          0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_18 (Conv1D)          (None, 128, 64)           24640     \n",
            "                                                                 \n",
            " batch_normalization_13 (Bat  (None, 128, 64)          256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling1d_18 (MaxPoolin  (None, 64, 64)           0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_19 (Conv1D)          (None, 64, 64)            24640     \n",
            "                                                                 \n",
            " batch_normalization_14 (Bat  (None, 64, 64)           256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling1d_19 (MaxPoolin  (None, 32, 64)           0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " flatten_13 (Flatten)        (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_24 (Dense)            (None, 100)               204900    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 255,396\n",
            "Trainable params: 255,012\n",
            "Non-trainable params: 384\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## cnn-gru"
      ],
      "metadata": {
        "id": "y3EL6TkMOiqb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Input, Embedding, GRU, LSTM, MaxPooling1D, GlobalMaxPool1D, CuDNNGRU\n",
        "from keras.layers import Dropout, Dense, Activation, Flatten, Conv1D, SpatialDropout1D\n",
        "from keras.models import Sequential\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv1D(filters=64, kernel_size=6, activation='relu', padding='same', input_shape=(256, 1)))\n",
        "model.add(SpatialDropout1D(0.2))\n",
        "model.add(MaxPooling1D(4))\n",
        "model.add(CuDNNGRU(64))\n",
        "\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Dense(101, activation='softmax'))\n",
        "model.compile(optimizer='adam', loss='mse',metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33f77877-e599-4ded-8400-373f5cec14bc",
        "id": "P1tvQQbcOiqb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_13 (Conv1D)          (None, 256, 64)           448       \n",
            "                                                                 \n",
            " spatial_dropout1d_4 (Spatia  (None, 256, 64)          0         \n",
            " lDropout1D)                                                     \n",
            "                                                                 \n",
            " max_pooling1d_13 (MaxPoolin  (None, 64, 64)           0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " cu_dnngru_4 (CuDNNGRU)      (None, 64)                24960     \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 101)               6565      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 31,973\n",
            "Trainable params: 31,973\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSTM"
      ],
      "metadata": {
        "id": "zKaqqdRXOiqb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(units=100, return_sequences=True, input_shape=(256,1))) #shape[1]=열\n",
        "model.add(LSTM(units=50, return_sequences=True))\n",
        "model.add(LSTM(units=50, return_sequences=True))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Dense(101, activation='softmax'))\n",
        "model.compile(optimizer='adam', loss='mse',metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8055881e-f18e-4941-e7d3-1d853b39d2e3",
        "id": "FMLEPXX0Oiqb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_3 (LSTM)               (None, 256, 100)          40800     \n",
            "                                                                 \n",
            " lstm_4 (LSTM)               (None, 256, 50)           30200     \n",
            "                                                                 \n",
            " lstm_5 (LSTM)               (None, 256, 50)           20200     \n",
            "                                                                 \n",
            " flatten_6 (Flatten)         (None, 12800)             0         \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 12800)             0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 101)               1292901   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,384,101\n",
            "Trainable params: 1,384,101\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RNN"
      ],
      "metadata": {
        "id": "EdnsP_BpOiqb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Input, Embedding, GRU, LSTM, SimpleRNN, MaxPooling1D, GlobalMaxPool1D, CuDNNGRU\n",
        "from keras.layers import Dropout, Dense, Activation, Flatten, Conv1D, SpatialDropout1D\n",
        "from keras.models import Sequential\n",
        "\n",
        "model = Sequential()\n",
        "model.add(SimpleRNN(units=100, return_sequences=True, input_shape=(256,1))) #shape[1]=열\n",
        "model.add(SimpleRNN(units=50, return_sequences=True))\n",
        "model.add(SimpleRNN(units=50, return_sequences=True))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(101, activation='softmax'))\n",
        "model.compile(optimizer='adam', loss='mse',metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f56aa04-4c34-4860-fc44-299596d4f96f",
        "id": "HPYhoYU4Oiqb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " simple_rnn_12 (SimpleRNN)   (None, 256, 100)          10200     \n",
            "                                                                 \n",
            " simple_rnn_13 (SimpleRNN)   (None, 256, 50)           7550      \n",
            "                                                                 \n",
            " simple_rnn_14 (SimpleRNN)   (None, 256, 50)           5050      \n",
            "                                                                 \n",
            " flatten_4 (Flatten)         (None, 12800)             0         \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 12800)             0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 101)               1292901   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,315,701\n",
            "Trainable params: 1,315,701\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델학습/평가"
      ],
      "metadata": {
        "id": "brTFW1xjOiqb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train, y_train, batch_size = 512, epochs = 1000, verbose = 1, validation_data=(X_val, y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a84bc478-6ee0-437c-9452-60ab1c4ffbdd",
        "id": "t-st2iE7Oiqb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "47/47 [==============================] - 9s 99ms/step - loss: 0.0098 - accuracy: 0.0089 - val_loss: 0.0098 - val_accuracy: 0.0112\n",
            "Epoch 2/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 0.0098 - accuracy: 0.0105 - val_loss: 0.0098 - val_accuracy: 0.0092\n",
            "Epoch 3/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 0.0098 - accuracy: 0.0109 - val_loss: 0.0098 - val_accuracy: 0.0137\n",
            "Epoch 4/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 0.0098 - accuracy: 0.0138 - val_loss: 0.0098 - val_accuracy: 0.0095\n",
            "Epoch 5/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 0.0098 - accuracy: 0.0145 - val_loss: 0.0098 - val_accuracy: 0.0097\n",
            "Epoch 6/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 0.0098 - accuracy: 0.0105 - val_loss: 0.0098 - val_accuracy: 0.0122\n",
            "Epoch 7/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 0.0098 - accuracy: 0.0125 - val_loss: 0.0098 - val_accuracy: 0.0097\n",
            "Epoch 8/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 0.0098 - accuracy: 0.0170 - val_loss: 0.0098 - val_accuracy: 0.0205\n",
            "Epoch 9/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0098 - accuracy: 0.0192 - val_loss: 0.0098 - val_accuracy: 0.0193\n",
            "Epoch 10/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0098 - accuracy: 0.0225 - val_loss: 0.0098 - val_accuracy: 0.0247\n",
            "Epoch 11/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0098 - accuracy: 0.0300 - val_loss: 0.0098 - val_accuracy: 0.0248\n",
            "Epoch 12/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0097 - accuracy: 0.0345 - val_loss: 0.0098 - val_accuracy: 0.0342\n",
            "Epoch 13/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0097 - accuracy: 0.0412 - val_loss: 0.0098 - val_accuracy: 0.0363\n",
            "Epoch 14/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0097 - accuracy: 0.0497 - val_loss: 0.0097 - val_accuracy: 0.0438\n",
            "Epoch 15/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0097 - accuracy: 0.0560 - val_loss: 0.0097 - val_accuracy: 0.0438\n",
            "Epoch 16/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0097 - accuracy: 0.0617 - val_loss: 0.0097 - val_accuracy: 0.0475\n",
            "Epoch 17/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0097 - accuracy: 0.0741 - val_loss: 0.0097 - val_accuracy: 0.0475\n",
            "Epoch 18/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0096 - accuracy: 0.0758 - val_loss: 0.0097 - val_accuracy: 0.0565\n",
            "Epoch 19/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0096 - accuracy: 0.0871 - val_loss: 0.0097 - val_accuracy: 0.0632\n",
            "Epoch 20/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0096 - accuracy: 0.0963 - val_loss: 0.0097 - val_accuracy: 0.0810\n",
            "Epoch 21/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0096 - accuracy: 0.1046 - val_loss: 0.0097 - val_accuracy: 0.0850\n",
            "Epoch 22/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0095 - accuracy: 0.1145 - val_loss: 0.0097 - val_accuracy: 0.0887\n",
            "Epoch 23/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0094 - accuracy: 0.1278 - val_loss: 0.0096 - val_accuracy: 0.1043\n",
            "Epoch 24/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0094 - accuracy: 0.1459 - val_loss: 0.0095 - val_accuracy: 0.1125\n",
            "Epoch 25/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0093 - accuracy: 0.1616 - val_loss: 0.0095 - val_accuracy: 0.1348\n",
            "Epoch 26/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0092 - accuracy: 0.1793 - val_loss: 0.0095 - val_accuracy: 0.1345\n",
            "Epoch 27/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0091 - accuracy: 0.1939 - val_loss: 0.0094 - val_accuracy: 0.1553\n",
            "Epoch 28/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0090 - accuracy: 0.2077 - val_loss: 0.0093 - val_accuracy: 0.1588\n",
            "Epoch 29/1000\n",
            "47/47 [==============================] - 3s 75ms/step - loss: 0.0089 - accuracy: 0.2162 - val_loss: 0.0093 - val_accuracy: 0.1713\n",
            "Epoch 30/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0088 - accuracy: 0.2330 - val_loss: 0.0093 - val_accuracy: 0.1848\n",
            "Epoch 31/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0087 - accuracy: 0.2488 - val_loss: 0.0091 - val_accuracy: 0.2008\n",
            "Epoch 32/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 0.0086 - accuracy: 0.2620 - val_loss: 0.0091 - val_accuracy: 0.2065\n",
            "Epoch 33/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0085 - accuracy: 0.2813 - val_loss: 0.0090 - val_accuracy: 0.2310\n",
            "Epoch 34/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0083 - accuracy: 0.2993 - val_loss: 0.0088 - val_accuracy: 0.2517\n",
            "Epoch 35/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0082 - accuracy: 0.3160 - val_loss: 0.0088 - val_accuracy: 0.2592\n",
            "Epoch 36/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0080 - accuracy: 0.3355 - val_loss: 0.0087 - val_accuracy: 0.2810\n",
            "Epoch 37/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0079 - accuracy: 0.3517 - val_loss: 0.0086 - val_accuracy: 0.2935\n",
            "Epoch 38/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0077 - accuracy: 0.3726 - val_loss: 0.0085 - val_accuracy: 0.3032\n",
            "Epoch 39/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0076 - accuracy: 0.3892 - val_loss: 0.0083 - val_accuracy: 0.3188\n",
            "Epoch 40/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0073 - accuracy: 0.4132 - val_loss: 0.0082 - val_accuracy: 0.3438\n",
            "Epoch 41/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0072 - accuracy: 0.4312 - val_loss: 0.0080 - val_accuracy: 0.3567\n",
            "Epoch 42/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0070 - accuracy: 0.4546 - val_loss: 0.0080 - val_accuracy: 0.3723\n",
            "Epoch 43/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0068 - accuracy: 0.4780 - val_loss: 0.0077 - val_accuracy: 0.4018\n",
            "Epoch 44/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0065 - accuracy: 0.5049 - val_loss: 0.0077 - val_accuracy: 0.4025\n",
            "Epoch 45/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0064 - accuracy: 0.5167 - val_loss: 0.0075 - val_accuracy: 0.4115\n",
            "Epoch 46/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0060 - accuracy: 0.5410 - val_loss: 0.0073 - val_accuracy: 0.4457\n",
            "Epoch 47/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0058 - accuracy: 0.5600 - val_loss: 0.0071 - val_accuracy: 0.4612\n",
            "Epoch 48/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0056 - accuracy: 0.5818 - val_loss: 0.0069 - val_accuracy: 0.4860\n",
            "Epoch 49/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0053 - accuracy: 0.6052 - val_loss: 0.0067 - val_accuracy: 0.5023\n",
            "Epoch 50/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0052 - accuracy: 0.6186 - val_loss: 0.0065 - val_accuracy: 0.5138\n",
            "Epoch 51/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0049 - accuracy: 0.6396 - val_loss: 0.0064 - val_accuracy: 0.5308\n",
            "Epoch 52/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0047 - accuracy: 0.6575 - val_loss: 0.0062 - val_accuracy: 0.5470\n",
            "Epoch 53/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0045 - accuracy: 0.6751 - val_loss: 0.0059 - val_accuracy: 0.5683\n",
            "Epoch 54/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0043 - accuracy: 0.6929 - val_loss: 0.0055 - val_accuracy: 0.6000\n",
            "Epoch 55/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0040 - accuracy: 0.7143 - val_loss: 0.0055 - val_accuracy: 0.6085\n",
            "Epoch 56/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0038 - accuracy: 0.7316 - val_loss: 0.0053 - val_accuracy: 0.6200\n",
            "Epoch 57/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0036 - accuracy: 0.7453 - val_loss: 0.0053 - val_accuracy: 0.6228\n",
            "Epoch 58/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0035 - accuracy: 0.7519 - val_loss: 0.0048 - val_accuracy: 0.6607\n",
            "Epoch 59/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0033 - accuracy: 0.7668 - val_loss: 0.0047 - val_accuracy: 0.6613\n",
            "Epoch 60/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0031 - accuracy: 0.7835 - val_loss: 0.0046 - val_accuracy: 0.6732\n",
            "Epoch 61/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0029 - accuracy: 0.7958 - val_loss: 0.0045 - val_accuracy: 0.6827\n",
            "Epoch 62/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0029 - accuracy: 0.7993 - val_loss: 0.0045 - val_accuracy: 0.6863\n",
            "Epoch 63/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0027 - accuracy: 0.8110 - val_loss: 0.0043 - val_accuracy: 0.7000\n",
            "Epoch 64/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0026 - accuracy: 0.8183 - val_loss: 0.0043 - val_accuracy: 0.6975\n",
            "Epoch 65/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0025 - accuracy: 0.8258 - val_loss: 0.0041 - val_accuracy: 0.7142\n",
            "Epoch 66/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0024 - accuracy: 0.8350 - val_loss: 0.0039 - val_accuracy: 0.7272\n",
            "Epoch 67/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0023 - accuracy: 0.8440 - val_loss: 0.0038 - val_accuracy: 0.7323\n",
            "Epoch 68/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0022 - accuracy: 0.8500 - val_loss: 0.0038 - val_accuracy: 0.7352\n",
            "Epoch 69/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0021 - accuracy: 0.8563 - val_loss: 0.0037 - val_accuracy: 0.7463\n",
            "Epoch 70/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0020 - accuracy: 0.8617 - val_loss: 0.0036 - val_accuracy: 0.7565\n",
            "Epoch 71/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0020 - accuracy: 0.8643 - val_loss: 0.0036 - val_accuracy: 0.7508\n",
            "Epoch 72/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0019 - accuracy: 0.8720 - val_loss: 0.0035 - val_accuracy: 0.7607\n",
            "Epoch 73/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0018 - accuracy: 0.8767 - val_loss: 0.0034 - val_accuracy: 0.7657\n",
            "Epoch 74/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0018 - accuracy: 0.8802 - val_loss: 0.0035 - val_accuracy: 0.7620\n",
            "Epoch 75/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0017 - accuracy: 0.8831 - val_loss: 0.0034 - val_accuracy: 0.7688\n",
            "Epoch 76/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 0.0016 - accuracy: 0.8886 - val_loss: 0.0035 - val_accuracy: 0.7612\n",
            "Epoch 77/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0016 - accuracy: 0.8878 - val_loss: 0.0033 - val_accuracy: 0.7748\n",
            "Epoch 78/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0016 - accuracy: 0.8926 - val_loss: 0.0033 - val_accuracy: 0.7795\n",
            "Epoch 79/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0015 - accuracy: 0.8976 - val_loss: 0.0031 - val_accuracy: 0.7870\n",
            "Epoch 80/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0014 - accuracy: 0.9018 - val_loss: 0.0032 - val_accuracy: 0.7822\n",
            "Epoch 81/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0014 - accuracy: 0.9043 - val_loss: 0.0031 - val_accuracy: 0.7880\n",
            "Epoch 82/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0013 - accuracy: 0.9078 - val_loss: 0.0031 - val_accuracy: 0.7915\n",
            "Epoch 83/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0013 - accuracy: 0.9100 - val_loss: 0.0032 - val_accuracy: 0.7873\n",
            "Epoch 84/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0013 - accuracy: 0.9101 - val_loss: 0.0031 - val_accuracy: 0.7947\n",
            "Epoch 85/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0013 - accuracy: 0.9116 - val_loss: 0.0032 - val_accuracy: 0.7828\n",
            "Epoch 86/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0013 - accuracy: 0.9148 - val_loss: 0.0030 - val_accuracy: 0.7940\n",
            "Epoch 87/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0012 - accuracy: 0.9187 - val_loss: 0.0030 - val_accuracy: 0.8028\n",
            "Epoch 88/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0012 - accuracy: 0.9215 - val_loss: 0.0029 - val_accuracy: 0.8050\n",
            "Epoch 89/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0011 - accuracy: 0.9258 - val_loss: 0.0028 - val_accuracy: 0.8143\n",
            "Epoch 90/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0010 - accuracy: 0.9288 - val_loss: 0.0029 - val_accuracy: 0.8048\n",
            "Epoch 91/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0010 - accuracy: 0.9300 - val_loss: 0.0027 - val_accuracy: 0.8165\n",
            "Epoch 92/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0010 - accuracy: 0.9314 - val_loss: 0.0029 - val_accuracy: 0.8108\n",
            "Epoch 93/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 0.0010 - accuracy: 0.9321 - val_loss: 0.0028 - val_accuracy: 0.8142\n",
            "Epoch 94/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.8530e-04 - accuracy: 0.9339 - val_loss: 0.0027 - val_accuracy: 0.8175\n",
            "Epoch 95/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.6037e-04 - accuracy: 0.9348 - val_loss: 0.0026 - val_accuracy: 0.8248\n",
            "Epoch 96/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.3638e-04 - accuracy: 0.9376 - val_loss: 0.0027 - val_accuracy: 0.8227\n",
            "Epoch 97/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.1995e-04 - accuracy: 0.9388 - val_loss: 0.0027 - val_accuracy: 0.8197\n",
            "Epoch 98/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.1754e-04 - accuracy: 0.9389 - val_loss: 0.0028 - val_accuracy: 0.8162\n",
            "Epoch 99/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.7136e-04 - accuracy: 0.9420 - val_loss: 0.0025 - val_accuracy: 0.8302\n",
            "Epoch 100/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.9106e-04 - accuracy: 0.9473 - val_loss: 0.0026 - val_accuracy: 0.8325\n",
            "Epoch 101/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.9243e-04 - accuracy: 0.9470 - val_loss: 0.0025 - val_accuracy: 0.8337\n",
            "Epoch 102/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.3716e-04 - accuracy: 0.9502 - val_loss: 0.0025 - val_accuracy: 0.8327\n",
            "Epoch 103/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.1944e-04 - accuracy: 0.9512 - val_loss: 0.0025 - val_accuracy: 0.8352\n",
            "Epoch 104/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.3837e-04 - accuracy: 0.9502 - val_loss: 0.0024 - val_accuracy: 0.8388\n",
            "Epoch 105/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.0659e-04 - accuracy: 0.9523 - val_loss: 0.0024 - val_accuracy: 0.8390\n",
            "Epoch 106/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 6.7221e-04 - accuracy: 0.9546 - val_loss: 0.0024 - val_accuracy: 0.8382\n",
            "Epoch 107/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.8317e-04 - accuracy: 0.9541 - val_loss: 0.0024 - val_accuracy: 0.8402\n",
            "Epoch 108/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.7168e-04 - accuracy: 0.9546 - val_loss: 0.0024 - val_accuracy: 0.8440\n",
            "Epoch 109/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.6043e-04 - accuracy: 0.9558 - val_loss: 0.0024 - val_accuracy: 0.8443\n",
            "Epoch 110/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.7339e-04 - accuracy: 0.9547 - val_loss: 0.0026 - val_accuracy: 0.8302\n",
            "Epoch 111/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.4527e-04 - accuracy: 0.9502 - val_loss: 0.0025 - val_accuracy: 0.8343\n",
            "Epoch 112/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.0148e-04 - accuracy: 0.9531 - val_loss: 0.0025 - val_accuracy: 0.8328\n",
            "Epoch 113/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.7353e-04 - accuracy: 0.9554 - val_loss: 0.0025 - val_accuracy: 0.8360\n",
            "Epoch 114/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.4132e-04 - accuracy: 0.9573 - val_loss: 0.0024 - val_accuracy: 0.8417\n",
            "Epoch 115/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.0448e-04 - accuracy: 0.9600 - val_loss: 0.0022 - val_accuracy: 0.8535\n",
            "Epoch 116/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.6624e-04 - accuracy: 0.9627 - val_loss: 0.0023 - val_accuracy: 0.8522\n",
            "Epoch 117/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.5607e-04 - accuracy: 0.9629 - val_loss: 0.0022 - val_accuracy: 0.8575\n",
            "Epoch 118/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.2401e-04 - accuracy: 0.9650 - val_loss: 0.0023 - val_accuracy: 0.8555\n",
            "Epoch 119/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.3566e-04 - accuracy: 0.9643 - val_loss: 0.0023 - val_accuracy: 0.8453\n",
            "Epoch 120/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.4579e-04 - accuracy: 0.9644 - val_loss: 0.0023 - val_accuracy: 0.8517\n",
            "Epoch 121/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.9285e-04 - accuracy: 0.9671 - val_loss: 0.0022 - val_accuracy: 0.8567\n",
            "Epoch 122/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.0639e-04 - accuracy: 0.9663 - val_loss: 0.0023 - val_accuracy: 0.8482\n",
            "Epoch 123/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.0124e-04 - accuracy: 0.9670 - val_loss: 0.0021 - val_accuracy: 0.8610\n",
            "Epoch 124/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.8784e-04 - accuracy: 0.9673 - val_loss: 0.0021 - val_accuracy: 0.8628\n",
            "Epoch 125/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.6374e-04 - accuracy: 0.9690 - val_loss: 0.0021 - val_accuracy: 0.8610\n",
            "Epoch 126/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.9002e-04 - accuracy: 0.9677 - val_loss: 0.0023 - val_accuracy: 0.8513\n",
            "Epoch 127/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.3811e-04 - accuracy: 0.9640 - val_loss: 0.0022 - val_accuracy: 0.8632\n",
            "Epoch 128/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.7448e-04 - accuracy: 0.9683 - val_loss: 0.0024 - val_accuracy: 0.8452\n",
            "Epoch 129/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.0849e-04 - accuracy: 0.9674 - val_loss: 0.0022 - val_accuracy: 0.8563\n",
            "Epoch 130/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.6832e-04 - accuracy: 0.9695 - val_loss: 0.0022 - val_accuracy: 0.8567\n",
            "Epoch 131/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.7121e-04 - accuracy: 0.9694 - val_loss: 0.0023 - val_accuracy: 0.8555\n",
            "Epoch 132/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.9900e-04 - accuracy: 0.9670 - val_loss: 0.0022 - val_accuracy: 0.8565\n",
            "Epoch 133/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.8753e-04 - accuracy: 0.9680 - val_loss: 0.0023 - val_accuracy: 0.8540\n",
            "Epoch 134/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.3735e-04 - accuracy: 0.9712 - val_loss: 0.0022 - val_accuracy: 0.8575\n",
            "Epoch 135/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.0359e-04 - accuracy: 0.9733 - val_loss: 0.0021 - val_accuracy: 0.8667\n",
            "Epoch 136/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.2251e-04 - accuracy: 0.9723 - val_loss: 0.0020 - val_accuracy: 0.8710\n",
            "Epoch 137/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.0121e-04 - accuracy: 0.9737 - val_loss: 0.0021 - val_accuracy: 0.8693\n",
            "Epoch 138/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.1016e-04 - accuracy: 0.9723 - val_loss: 0.0021 - val_accuracy: 0.8683\n",
            "Epoch 139/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.1006e-04 - accuracy: 0.9728 - val_loss: 0.0021 - val_accuracy: 0.8695\n",
            "Epoch 140/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.4258e-04 - accuracy: 0.9709 - val_loss: 0.0021 - val_accuracy: 0.8672\n",
            "Epoch 141/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.1841e-04 - accuracy: 0.9725 - val_loss: 0.0021 - val_accuracy: 0.8620\n",
            "Epoch 142/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.9058e-04 - accuracy: 0.9745 - val_loss: 0.0021 - val_accuracy: 0.8638\n",
            "Epoch 143/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.3957e-04 - accuracy: 0.9713 - val_loss: 0.0021 - val_accuracy: 0.8647\n",
            "Epoch 144/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.0425e-04 - accuracy: 0.9743 - val_loss: 0.0021 - val_accuracy: 0.8710\n",
            "Epoch 145/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.2505e-04 - accuracy: 0.9726 - val_loss: 0.0021 - val_accuracy: 0.8688\n",
            "Epoch 146/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.9461e-04 - accuracy: 0.9745 - val_loss: 0.0020 - val_accuracy: 0.8748\n",
            "Epoch 147/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.6591e-04 - accuracy: 0.9759 - val_loss: 0.0021 - val_accuracy: 0.8663\n",
            "Epoch 148/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.7667e-04 - accuracy: 0.9752 - val_loss: 0.0019 - val_accuracy: 0.8735\n",
            "Epoch 149/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.5264e-04 - accuracy: 0.9766 - val_loss: 0.0019 - val_accuracy: 0.8767\n",
            "Epoch 150/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 3.6154e-04 - accuracy: 0.9763 - val_loss: 0.0021 - val_accuracy: 0.8657\n",
            "Epoch 151/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.5757e-04 - accuracy: 0.9765 - val_loss: 0.0020 - val_accuracy: 0.8748\n",
            "Epoch 152/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.4223e-04 - accuracy: 0.9775 - val_loss: 0.0019 - val_accuracy: 0.8807\n",
            "Epoch 153/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.3548e-04 - accuracy: 0.9779 - val_loss: 0.0018 - val_accuracy: 0.8860\n",
            "Epoch 154/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.4029e-04 - accuracy: 0.9773 - val_loss: 0.0019 - val_accuracy: 0.8753\n",
            "Epoch 155/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.4848e-04 - accuracy: 0.9769 - val_loss: 0.0019 - val_accuracy: 0.8763\n",
            "Epoch 156/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.8151e-04 - accuracy: 0.9751 - val_loss: 0.0022 - val_accuracy: 0.8608\n",
            "Epoch 157/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.1859e-04 - accuracy: 0.9728 - val_loss: 0.0022 - val_accuracy: 0.8623\n",
            "Epoch 158/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.1186e-04 - accuracy: 0.9735 - val_loss: 0.0020 - val_accuracy: 0.8742\n",
            "Epoch 159/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.6662e-04 - accuracy: 0.9762 - val_loss: 0.0020 - val_accuracy: 0.8728\n",
            "Epoch 160/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.7469e-04 - accuracy: 0.9756 - val_loss: 0.0020 - val_accuracy: 0.8710\n",
            "Epoch 161/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.5508e-04 - accuracy: 0.9771 - val_loss: 0.0019 - val_accuracy: 0.8805\n",
            "Epoch 162/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.4416e-04 - accuracy: 0.9777 - val_loss: 0.0019 - val_accuracy: 0.8760\n",
            "Epoch 163/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.5647e-04 - accuracy: 0.9762 - val_loss: 0.0019 - val_accuracy: 0.8795\n",
            "Epoch 164/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.2158e-04 - accuracy: 0.9727 - val_loss: 0.0020 - val_accuracy: 0.8697\n",
            "Epoch 165/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.7422e-04 - accuracy: 0.9760 - val_loss: 0.0020 - val_accuracy: 0.8727\n",
            "Epoch 166/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.8776e-04 - accuracy: 0.9747 - val_loss: 0.0021 - val_accuracy: 0.8703\n",
            "Epoch 167/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.6791e-04 - accuracy: 0.9765 - val_loss: 0.0020 - val_accuracy: 0.8755\n",
            "Epoch 168/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.3217e-04 - accuracy: 0.9790 - val_loss: 0.0020 - val_accuracy: 0.8732\n",
            "Epoch 169/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.3497e-04 - accuracy: 0.9784 - val_loss: 0.0020 - val_accuracy: 0.8750\n",
            "Epoch 170/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.5662e-04 - accuracy: 0.9767 - val_loss: 0.0020 - val_accuracy: 0.8697\n",
            "Epoch 171/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.5634e-04 - accuracy: 0.9768 - val_loss: 0.0020 - val_accuracy: 0.8742\n",
            "Epoch 172/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.6275e-04 - accuracy: 0.9766 - val_loss: 0.0019 - val_accuracy: 0.8813\n",
            "Epoch 173/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.3599e-04 - accuracy: 0.9784 - val_loss: 0.0019 - val_accuracy: 0.8807\n",
            "Epoch 174/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.6232e-04 - accuracy: 0.9771 - val_loss: 0.0019 - val_accuracy: 0.8780\n",
            "Epoch 175/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.2441e-04 - accuracy: 0.9787 - val_loss: 0.0018 - val_accuracy: 0.8858\n",
            "Epoch 176/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.9663e-04 - accuracy: 0.9806 - val_loss: 0.0019 - val_accuracy: 0.8783\n",
            "Epoch 177/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.1961e-04 - accuracy: 0.9797 - val_loss: 0.0019 - val_accuracy: 0.8797\n",
            "Epoch 178/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.9877e-04 - accuracy: 0.9808 - val_loss: 0.0019 - val_accuracy: 0.8792\n",
            "Epoch 179/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.9609e-04 - accuracy: 0.9805 - val_loss: 0.0018 - val_accuracy: 0.8848\n",
            "Epoch 180/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.0448e-04 - accuracy: 0.9804 - val_loss: 0.0019 - val_accuracy: 0.8805\n",
            "Epoch 181/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 2.8955e-04 - accuracy: 0.9818 - val_loss: 0.0018 - val_accuracy: 0.8848\n",
            "Epoch 182/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 3.1941e-04 - accuracy: 0.9797 - val_loss: 0.0019 - val_accuracy: 0.8832\n",
            "Epoch 183/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.9092e-04 - accuracy: 0.9818 - val_loss: 0.0017 - val_accuracy: 0.8933\n",
            "Epoch 184/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.6479e-04 - accuracy: 0.9834 - val_loss: 0.0017 - val_accuracy: 0.8930\n",
            "Epoch 185/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.2105e-04 - accuracy: 0.9797 - val_loss: 0.0019 - val_accuracy: 0.8825\n",
            "Epoch 186/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.0577e-04 - accuracy: 0.9803 - val_loss: 0.0018 - val_accuracy: 0.8840\n",
            "Epoch 187/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.8259e-04 - accuracy: 0.9821 - val_loss: 0.0018 - val_accuracy: 0.8848\n",
            "Epoch 188/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.4593e-04 - accuracy: 0.9845 - val_loss: 0.0018 - val_accuracy: 0.8905\n",
            "Epoch 189/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 2.5664e-04 - accuracy: 0.9841 - val_loss: 0.0019 - val_accuracy: 0.8795\n",
            "Epoch 190/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.6548e-04 - accuracy: 0.9835 - val_loss: 0.0017 - val_accuracy: 0.8913\n",
            "Epoch 191/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.6040e-04 - accuracy: 0.9837 - val_loss: 0.0018 - val_accuracy: 0.8873\n",
            "Epoch 192/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.5778e-04 - accuracy: 0.9835 - val_loss: 0.0017 - val_accuracy: 0.8903\n",
            "Epoch 193/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.5127e-04 - accuracy: 0.9843 - val_loss: 0.0017 - val_accuracy: 0.8937\n",
            "Epoch 194/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.5107e-04 - accuracy: 0.9838 - val_loss: 0.0017 - val_accuracy: 0.8942\n",
            "Epoch 195/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.5094e-04 - accuracy: 0.9847 - val_loss: 0.0020 - val_accuracy: 0.8798\n",
            "Epoch 196/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.6697e-04 - accuracy: 0.9835 - val_loss: 0.0017 - val_accuracy: 0.8920\n",
            "Epoch 197/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3551e-04 - accuracy: 0.9855 - val_loss: 0.0017 - val_accuracy: 0.8945\n",
            "Epoch 198/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3858e-04 - accuracy: 0.9852 - val_loss: 0.0016 - val_accuracy: 0.8998\n",
            "Epoch 199/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.2851e-04 - accuracy: 0.9860 - val_loss: 0.0017 - val_accuracy: 0.8948\n",
            "Epoch 200/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3888e-04 - accuracy: 0.9850 - val_loss: 0.0017 - val_accuracy: 0.8917\n",
            "Epoch 201/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.4135e-04 - accuracy: 0.9852 - val_loss: 0.0019 - val_accuracy: 0.8848\n",
            "Epoch 202/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.4203e-04 - accuracy: 0.9850 - val_loss: 0.0016 - val_accuracy: 0.8967\n",
            "Epoch 203/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.5230e-04 - accuracy: 0.9840 - val_loss: 0.0019 - val_accuracy: 0.8820\n",
            "Epoch 204/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.4368e-04 - accuracy: 0.9847 - val_loss: 0.0017 - val_accuracy: 0.8970\n",
            "Epoch 205/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3397e-04 - accuracy: 0.9857 - val_loss: 0.0016 - val_accuracy: 0.8967\n",
            "Epoch 206/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.1547e-04 - accuracy: 0.9808 - val_loss: 0.0019 - val_accuracy: 0.8833\n",
            "Epoch 207/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.7720e-04 - accuracy: 0.9824 - val_loss: 0.0018 - val_accuracy: 0.8883\n",
            "Epoch 208/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.4352e-04 - accuracy: 0.9846 - val_loss: 0.0019 - val_accuracy: 0.8838\n",
            "Epoch 209/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3590e-04 - accuracy: 0.9851 - val_loss: 0.0016 - val_accuracy: 0.8967\n",
            "Epoch 210/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.4241e-04 - accuracy: 0.9848 - val_loss: 0.0019 - val_accuracy: 0.8875\n",
            "Epoch 211/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.8273e-04 - accuracy: 0.9822 - val_loss: 0.0017 - val_accuracy: 0.8952\n",
            "Epoch 212/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.7011e-04 - accuracy: 0.9827 - val_loss: 0.0018 - val_accuracy: 0.8925\n",
            "Epoch 213/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.7202e-04 - accuracy: 0.9830 - val_loss: 0.0019 - val_accuracy: 0.8840\n",
            "Epoch 214/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.4548e-04 - accuracy: 0.9782 - val_loss: 0.0019 - val_accuracy: 0.8800\n",
            "Epoch 215/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.9440e-04 - accuracy: 0.9812 - val_loss: 0.0018 - val_accuracy: 0.8892\n",
            "Epoch 216/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.8235e-04 - accuracy: 0.9824 - val_loss: 0.0018 - val_accuracy: 0.8912\n",
            "Epoch 217/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.5654e-04 - accuracy: 0.9839 - val_loss: 0.0018 - val_accuracy: 0.8927\n",
            "Epoch 218/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.4466e-04 - accuracy: 0.9848 - val_loss: 0.0016 - val_accuracy: 0.9022\n",
            "Epoch 219/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3129e-04 - accuracy: 0.9856 - val_loss: 0.0016 - val_accuracy: 0.9028\n",
            "Epoch 220/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3081e-04 - accuracy: 0.9860 - val_loss: 0.0016 - val_accuracy: 0.8988\n",
            "Epoch 221/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.2650e-04 - accuracy: 0.9862 - val_loss: 0.0017 - val_accuracy: 0.8970\n",
            "Epoch 222/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 2.1425e-04 - accuracy: 0.9869 - val_loss: 0.0015 - val_accuracy: 0.9048\n",
            "Epoch 223/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 2.0847e-04 - accuracy: 0.9875 - val_loss: 0.0016 - val_accuracy: 0.9042\n",
            "Epoch 224/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.1172e-04 - accuracy: 0.9870 - val_loss: 0.0016 - val_accuracy: 0.9023\n",
            "Epoch 225/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.1094e-04 - accuracy: 0.9868 - val_loss: 0.0016 - val_accuracy: 0.8998\n",
            "Epoch 226/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.8153e-04 - accuracy: 0.9824 - val_loss: 0.0021 - val_accuracy: 0.8685\n",
            "Epoch 227/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.9846e-04 - accuracy: 0.9810 - val_loss: 0.0017 - val_accuracy: 0.8957\n",
            "Epoch 228/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3543e-04 - accuracy: 0.9856 - val_loss: 0.0017 - val_accuracy: 0.8920\n",
            "Epoch 229/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.1990e-04 - accuracy: 0.9865 - val_loss: 0.0017 - val_accuracy: 0.8923\n",
            "Epoch 230/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.1376e-04 - accuracy: 0.9869 - val_loss: 0.0016 - val_accuracy: 0.9012\n",
            "Epoch 231/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.2995e-04 - accuracy: 0.9858 - val_loss: 0.0018 - val_accuracy: 0.8932\n",
            "Epoch 232/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.5614e-04 - accuracy: 0.9839 - val_loss: 0.0017 - val_accuracy: 0.8973\n",
            "Epoch 233/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3604e-04 - accuracy: 0.9853 - val_loss: 0.0017 - val_accuracy: 0.8932\n",
            "Epoch 234/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3142e-04 - accuracy: 0.9854 - val_loss: 0.0017 - val_accuracy: 0.8968\n",
            "Epoch 235/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.2967e-04 - accuracy: 0.9860 - val_loss: 0.0016 - val_accuracy: 0.9018\n",
            "Epoch 236/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.2283e-04 - accuracy: 0.9861 - val_loss: 0.0016 - val_accuracy: 0.9032\n",
            "Epoch 237/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.2276e-04 - accuracy: 0.9863 - val_loss: 0.0017 - val_accuracy: 0.8977\n",
            "Epoch 238/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.2789e-04 - accuracy: 0.9859 - val_loss: 0.0017 - val_accuracy: 0.8968\n",
            "Epoch 239/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 2.5838e-04 - accuracy: 0.9843 - val_loss: 0.0016 - val_accuracy: 0.9000\n",
            "Epoch 240/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.5298e-04 - accuracy: 0.9845 - val_loss: 0.0016 - val_accuracy: 0.9023\n",
            "Epoch 241/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3149e-04 - accuracy: 0.9860 - val_loss: 0.0015 - val_accuracy: 0.9057\n",
            "Epoch 242/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.6319e-04 - accuracy: 0.9841 - val_loss: 0.0017 - val_accuracy: 0.8978\n",
            "Epoch 243/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.0986e-04 - accuracy: 0.9810 - val_loss: 0.0017 - val_accuracy: 0.8975\n",
            "Epoch 244/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3203e-04 - accuracy: 0.9861 - val_loss: 0.0018 - val_accuracy: 0.8883\n",
            "Epoch 245/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 2.3101e-04 - accuracy: 0.9858 - val_loss: 0.0018 - val_accuracy: 0.8912\n",
            "Epoch 246/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.1981e-04 - accuracy: 0.9865 - val_loss: 0.0016 - val_accuracy: 0.9020\n",
            "Epoch 247/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.5738e-04 - accuracy: 0.9840 - val_loss: 0.0018 - val_accuracy: 0.8950\n",
            "Epoch 248/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.5225e-04 - accuracy: 0.9845 - val_loss: 0.0017 - val_accuracy: 0.8958\n",
            "Epoch 249/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.8230e-04 - accuracy: 0.9827 - val_loss: 0.0016 - val_accuracy: 0.9040\n",
            "Epoch 250/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3666e-04 - accuracy: 0.9856 - val_loss: 0.0017 - val_accuracy: 0.8947\n",
            "Epoch 251/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 2.7106e-04 - accuracy: 0.9828 - val_loss: 0.0017 - val_accuracy: 0.8937\n",
            "Epoch 252/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 2.3333e-04 - accuracy: 0.9856 - val_loss: 0.0018 - val_accuracy: 0.8928\n",
            "Epoch 253/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 2.0621e-04 - accuracy: 0.9875 - val_loss: 0.0018 - val_accuracy: 0.8940\n",
            "Epoch 254/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 2.2688e-04 - accuracy: 0.9859 - val_loss: 0.0017 - val_accuracy: 0.8970\n",
            "Epoch 255/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 2.7564e-04 - accuracy: 0.9830 - val_loss: 0.0020 - val_accuracy: 0.8820\n",
            "Epoch 256/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.8962e-04 - accuracy: 0.9818 - val_loss: 0.0018 - val_accuracy: 0.8903\n",
            "Epoch 257/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.5788e-04 - accuracy: 0.9843 - val_loss: 0.0016 - val_accuracy: 0.9020\n",
            "Epoch 258/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.2197e-04 - accuracy: 0.9865 - val_loss: 0.0017 - val_accuracy: 0.8993\n",
            "Epoch 259/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.1309e-04 - accuracy: 0.9871 - val_loss: 0.0016 - val_accuracy: 0.9045\n",
            "Epoch 260/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.0096e-04 - accuracy: 0.9880 - val_loss: 0.0016 - val_accuracy: 0.9055\n",
            "Epoch 261/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.0373e-04 - accuracy: 0.9877 - val_loss: 0.0016 - val_accuracy: 0.9032\n",
            "Epoch 262/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.0906e-04 - accuracy: 0.9874 - val_loss: 0.0015 - val_accuracy: 0.9082\n",
            "Epoch 263/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.0801e-04 - accuracy: 0.9875 - val_loss: 0.0015 - val_accuracy: 0.9102\n",
            "Epoch 264/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3843e-04 - accuracy: 0.9853 - val_loss: 0.0017 - val_accuracy: 0.8972\n",
            "Epoch 265/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.2784e-04 - accuracy: 0.9860 - val_loss: 0.0016 - val_accuracy: 0.9047\n",
            "Epoch 266/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.4002e-04 - accuracy: 0.9855 - val_loss: 0.0017 - val_accuracy: 0.8995\n",
            "Epoch 267/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.7639e-04 - accuracy: 0.9823 - val_loss: 0.0015 - val_accuracy: 0.9098\n",
            "Epoch 268/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.5447e-04 - accuracy: 0.9845 - val_loss: 0.0018 - val_accuracy: 0.8928\n",
            "Epoch 269/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3284e-04 - accuracy: 0.9856 - val_loss: 0.0017 - val_accuracy: 0.8975\n",
            "Epoch 270/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.4523e-04 - accuracy: 0.9848 - val_loss: 0.0016 - val_accuracy: 0.9030\n",
            "Epoch 271/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.4544e-04 - accuracy: 0.9850 - val_loss: 0.0016 - val_accuracy: 0.9062\n",
            "Epoch 272/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3729e-04 - accuracy: 0.9856 - val_loss: 0.0019 - val_accuracy: 0.8912\n",
            "Epoch 273/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3188e-04 - accuracy: 0.9859 - val_loss: 0.0015 - val_accuracy: 0.9082\n",
            "Epoch 274/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 2.1121e-04 - accuracy: 0.9874 - val_loss: 0.0015 - val_accuracy: 0.9085\n",
            "Epoch 275/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.1262e-04 - accuracy: 0.9872 - val_loss: 0.0016 - val_accuracy: 0.9033\n",
            "Epoch 276/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.9576e-04 - accuracy: 0.9819 - val_loss: 0.0017 - val_accuracy: 0.8970\n",
            "Epoch 277/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.4366e-04 - accuracy: 0.9858 - val_loss: 0.0017 - val_accuracy: 0.9015\n",
            "Epoch 278/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.0922e-04 - accuracy: 0.9875 - val_loss: 0.0017 - val_accuracy: 0.9005\n",
            "Epoch 279/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.0660e-04 - accuracy: 0.9876 - val_loss: 0.0015 - val_accuracy: 0.9118\n",
            "Epoch 280/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 2.0704e-04 - accuracy: 0.9874 - val_loss: 0.0015 - val_accuracy: 0.9085\n",
            "Epoch 281/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 2.0929e-04 - accuracy: 0.9873 - val_loss: 0.0017 - val_accuracy: 0.8962\n",
            "Epoch 282/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 2.1040e-04 - accuracy: 0.9876 - val_loss: 0.0015 - val_accuracy: 0.9127\n",
            "Epoch 283/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 2.0395e-04 - accuracy: 0.9877 - val_loss: 0.0016 - val_accuracy: 0.9057\n",
            "Epoch 284/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 2.0218e-04 - accuracy: 0.9878 - val_loss: 0.0016 - val_accuracy: 0.9033\n",
            "Epoch 285/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.0378e-04 - accuracy: 0.9875 - val_loss: 0.0014 - val_accuracy: 0.9138\n",
            "Epoch 286/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.9631e-04 - accuracy: 0.9880 - val_loss: 0.0015 - val_accuracy: 0.9118\n",
            "Epoch 287/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.9821e-04 - accuracy: 0.9880 - val_loss: 0.0016 - val_accuracy: 0.9063\n",
            "Epoch 288/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.1305e-04 - accuracy: 0.9870 - val_loss: 0.0018 - val_accuracy: 0.8883\n",
            "Epoch 289/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.4351e-04 - accuracy: 0.9851 - val_loss: 0.0015 - val_accuracy: 0.9102\n",
            "Epoch 290/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.5311e-04 - accuracy: 0.9845 - val_loss: 0.0016 - val_accuracy: 0.9057\n",
            "Epoch 291/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.4411e-04 - accuracy: 0.9847 - val_loss: 0.0016 - val_accuracy: 0.9038\n",
            "Epoch 292/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.6230e-04 - accuracy: 0.9836 - val_loss: 0.0019 - val_accuracy: 0.8880\n",
            "Epoch 293/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3455e-04 - accuracy: 0.9858 - val_loss: 0.0016 - val_accuracy: 0.9078\n",
            "Epoch 294/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.1188e-04 - accuracy: 0.9870 - val_loss: 0.0016 - val_accuracy: 0.9053\n",
            "Epoch 295/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 2.3233e-04 - accuracy: 0.9860 - val_loss: 0.0016 - val_accuracy: 0.9013\n",
            "Epoch 296/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.2515e-04 - accuracy: 0.9864 - val_loss: 0.0016 - val_accuracy: 0.9022\n",
            "Epoch 297/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.1057e-04 - accuracy: 0.9871 - val_loss: 0.0017 - val_accuracy: 0.8998\n",
            "Epoch 298/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3214e-04 - accuracy: 0.9859 - val_loss: 0.0017 - val_accuracy: 0.9012\n",
            "Epoch 299/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 2.2669e-04 - accuracy: 0.9866 - val_loss: 0.0014 - val_accuracy: 0.9153\n",
            "Epoch 300/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 2.0929e-04 - accuracy: 0.9876 - val_loss: 0.0015 - val_accuracy: 0.9082\n",
            "Epoch 301/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 2.1352e-04 - accuracy: 0.9873 - val_loss: 0.0016 - val_accuracy: 0.9047\n",
            "Epoch 302/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.9753e-04 - accuracy: 0.9883 - val_loss: 0.0016 - val_accuracy: 0.9060\n",
            "Epoch 303/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.8843e-04 - accuracy: 0.9891 - val_loss: 0.0017 - val_accuracy: 0.9028\n",
            "Epoch 304/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.0847e-04 - accuracy: 0.9875 - val_loss: 0.0015 - val_accuracy: 0.9103\n",
            "Epoch 305/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.0406e-04 - accuracy: 0.9880 - val_loss: 0.0016 - val_accuracy: 0.9043\n",
            "Epoch 306/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.2131e-04 - accuracy: 0.9868 - val_loss: 0.0015 - val_accuracy: 0.9105\n",
            "Epoch 307/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.9709e-04 - accuracy: 0.9880 - val_loss: 0.0016 - val_accuracy: 0.9055\n",
            "Epoch 308/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.0132e-04 - accuracy: 0.9877 - val_loss: 0.0016 - val_accuracy: 0.9047\n",
            "Epoch 309/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.9408e-04 - accuracy: 0.9885 - val_loss: 0.0016 - val_accuracy: 0.9028\n",
            "Epoch 310/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.0147e-04 - accuracy: 0.9879 - val_loss: 0.0018 - val_accuracy: 0.8928\n",
            "Epoch 311/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.0710e-04 - accuracy: 0.9877 - val_loss: 0.0015 - val_accuracy: 0.9128\n",
            "Epoch 312/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.2232e-04 - accuracy: 0.9865 - val_loss: 0.0020 - val_accuracy: 0.8817\n",
            "Epoch 313/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.4653e-04 - accuracy: 0.9850 - val_loss: 0.0018 - val_accuracy: 0.8898\n",
            "Epoch 314/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.9114e-04 - accuracy: 0.9887 - val_loss: 0.0015 - val_accuracy: 0.9093\n",
            "Epoch 315/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.8193e-04 - accuracy: 0.9892 - val_loss: 0.0015 - val_accuracy: 0.9110\n",
            "Epoch 316/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.9734e-04 - accuracy: 0.9885 - val_loss: 0.0016 - val_accuracy: 0.9078\n",
            "Epoch 317/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.9180e-04 - accuracy: 0.9888 - val_loss: 0.0014 - val_accuracy: 0.9167\n",
            "Epoch 318/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.9750e-04 - accuracy: 0.9883 - val_loss: 0.0015 - val_accuracy: 0.9087\n",
            "Epoch 319/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.8793e-04 - accuracy: 0.9889 - val_loss: 0.0015 - val_accuracy: 0.9093\n",
            "Epoch 320/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.9741e-04 - accuracy: 0.9885 - val_loss: 0.0017 - val_accuracy: 0.9020\n",
            "Epoch 321/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.0375e-04 - accuracy: 0.9877 - val_loss: 0.0015 - val_accuracy: 0.9100\n",
            "Epoch 322/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.0248e-04 - accuracy: 0.9878 - val_loss: 0.0019 - val_accuracy: 0.8893\n",
            "Epoch 323/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 2.3338e-04 - accuracy: 0.9860 - val_loss: 0.0016 - val_accuracy: 0.9007\n",
            "Epoch 324/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 2.4404e-04 - accuracy: 0.9854 - val_loss: 0.0016 - val_accuracy: 0.9072\n",
            "Epoch 325/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 2.0189e-04 - accuracy: 0.9880 - val_loss: 0.0017 - val_accuracy: 0.8975\n",
            "Epoch 326/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 2.3337e-04 - accuracy: 0.9860 - val_loss: 0.0018 - val_accuracy: 0.8952\n",
            "Epoch 327/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 3.0421e-04 - accuracy: 0.9815 - val_loss: 0.0017 - val_accuracy: 0.8997\n",
            "Epoch 328/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 2.4815e-04 - accuracy: 0.9850 - val_loss: 0.0016 - val_accuracy: 0.9082\n",
            "Epoch 329/1000\n",
            "47/47 [==============================] - 4s 76ms/step - loss: 2.1749e-04 - accuracy: 0.9870 - val_loss: 0.0015 - val_accuracy: 0.9118\n",
            "Epoch 330/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.9451e-04 - accuracy: 0.9886 - val_loss: 0.0014 - val_accuracy: 0.9137\n",
            "Epoch 331/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.8316e-04 - accuracy: 0.9890 - val_loss: 0.0015 - val_accuracy: 0.9088\n",
            "Epoch 332/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 2.0898e-04 - accuracy: 0.9871 - val_loss: 0.0016 - val_accuracy: 0.9065\n",
            "Epoch 333/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 2.0856e-04 - accuracy: 0.9873 - val_loss: 0.0016 - val_accuracy: 0.9042\n",
            "Epoch 334/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.9964e-04 - accuracy: 0.9882 - val_loss: 0.0016 - val_accuracy: 0.9063\n",
            "Epoch 335/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 2.3528e-04 - accuracy: 0.9854 - val_loss: 0.0017 - val_accuracy: 0.9017\n",
            "Epoch 336/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 2.4597e-04 - accuracy: 0.9849 - val_loss: 0.0016 - val_accuracy: 0.9035\n",
            "Epoch 337/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 2.2225e-04 - accuracy: 0.9866 - val_loss: 0.0015 - val_accuracy: 0.9095\n",
            "Epoch 338/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 2.0245e-04 - accuracy: 0.9880 - val_loss: 0.0015 - val_accuracy: 0.9095\n",
            "Epoch 339/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.8160e-04 - accuracy: 0.9896 - val_loss: 0.0014 - val_accuracy: 0.9155\n",
            "Epoch 340/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.8430e-04 - accuracy: 0.9894 - val_loss: 0.0016 - val_accuracy: 0.9040\n",
            "Epoch 341/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.9187e-04 - accuracy: 0.9890 - val_loss: 0.0015 - val_accuracy: 0.9095\n",
            "Epoch 342/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.8471e-04 - accuracy: 0.9894 - val_loss: 0.0016 - val_accuracy: 0.9048\n",
            "Epoch 343/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.7879e-04 - accuracy: 0.9893 - val_loss: 0.0015 - val_accuracy: 0.9083\n",
            "Epoch 344/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.7982e-04 - accuracy: 0.9895 - val_loss: 0.0015 - val_accuracy: 0.9118\n",
            "Epoch 345/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.8850e-04 - accuracy: 0.9893 - val_loss: 0.0016 - val_accuracy: 0.9028\n",
            "Epoch 346/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.8338e-04 - accuracy: 0.9894 - val_loss: 0.0016 - val_accuracy: 0.9088\n",
            "Epoch 347/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.6904e-04 - accuracy: 0.9900 - val_loss: 0.0015 - val_accuracy: 0.9115\n",
            "Epoch 348/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.8192e-04 - accuracy: 0.9896 - val_loss: 0.0015 - val_accuracy: 0.9108\n",
            "Epoch 349/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3071e-04 - accuracy: 0.9862 - val_loss: 0.0017 - val_accuracy: 0.9017\n",
            "Epoch 350/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 2.1710e-04 - accuracy: 0.9872 - val_loss: 0.0015 - val_accuracy: 0.9078\n",
            "Epoch 351/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.0173e-04 - accuracy: 0.9882 - val_loss: 0.0014 - val_accuracy: 0.9152\n",
            "Epoch 352/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.8382e-04 - accuracy: 0.9890 - val_loss: 0.0014 - val_accuracy: 0.9180\n",
            "Epoch 353/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.7557e-04 - accuracy: 0.9898 - val_loss: 0.0014 - val_accuracy: 0.9180\n",
            "Epoch 354/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.9991e-04 - accuracy: 0.9883 - val_loss: 0.0014 - val_accuracy: 0.9163\n",
            "Epoch 355/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.8504e-04 - accuracy: 0.9893 - val_loss: 0.0015 - val_accuracy: 0.9140\n",
            "Epoch 356/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.8408e-04 - accuracy: 0.9894 - val_loss: 0.0015 - val_accuracy: 0.9105\n",
            "Epoch 357/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.8106e-04 - accuracy: 0.9897 - val_loss: 0.0014 - val_accuracy: 0.9160\n",
            "Epoch 358/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.7470e-04 - accuracy: 0.9900 - val_loss: 0.0014 - val_accuracy: 0.9142\n",
            "Epoch 359/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.6577e-04 - accuracy: 0.9903 - val_loss: 0.0014 - val_accuracy: 0.9170\n",
            "Epoch 360/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.5901e-04 - accuracy: 0.9911 - val_loss: 0.0014 - val_accuracy: 0.9183\n",
            "Epoch 361/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.6815e-04 - accuracy: 0.9905 - val_loss: 0.0013 - val_accuracy: 0.9238\n",
            "Epoch 362/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.7319e-04 - accuracy: 0.9900 - val_loss: 0.0013 - val_accuracy: 0.9225\n",
            "Epoch 363/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.6706e-04 - accuracy: 0.9905 - val_loss: 0.0014 - val_accuracy: 0.9160\n",
            "Epoch 364/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.6618e-04 - accuracy: 0.9905 - val_loss: 0.0014 - val_accuracy: 0.9152\n",
            "Epoch 365/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.7445e-04 - accuracy: 0.9899 - val_loss: 0.0015 - val_accuracy: 0.9118\n",
            "Epoch 366/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.7420e-04 - accuracy: 0.9897 - val_loss: 0.0015 - val_accuracy: 0.9085\n",
            "Epoch 367/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.9168e-04 - accuracy: 0.9888 - val_loss: 0.0015 - val_accuracy: 0.9117\n",
            "Epoch 368/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.9297e-04 - accuracy: 0.9889 - val_loss: 0.0015 - val_accuracy: 0.9108\n",
            "Epoch 369/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.1346e-04 - accuracy: 0.9872 - val_loss: 0.0015 - val_accuracy: 0.9133\n",
            "Epoch 370/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.2091e-04 - accuracy: 0.9868 - val_loss: 0.0016 - val_accuracy: 0.9087\n",
            "Epoch 371/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.0587e-04 - accuracy: 0.9877 - val_loss: 0.0015 - val_accuracy: 0.9098\n",
            "Epoch 372/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.6218e-04 - accuracy: 0.9840 - val_loss: 0.0019 - val_accuracy: 0.8908\n",
            "Epoch 373/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.9341e-04 - accuracy: 0.9885 - val_loss: 0.0014 - val_accuracy: 0.9152\n",
            "Epoch 374/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.8246e-04 - accuracy: 0.9893 - val_loss: 0.0017 - val_accuracy: 0.9002\n",
            "Epoch 375/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 2.0762e-04 - accuracy: 0.9879 - val_loss: 0.0014 - val_accuracy: 0.9135\n",
            "Epoch 376/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.9584e-04 - accuracy: 0.9885 - val_loss: 0.0015 - val_accuracy: 0.9130\n",
            "Epoch 377/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.7723e-04 - accuracy: 0.9895 - val_loss: 0.0015 - val_accuracy: 0.9130\n",
            "Epoch 378/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.7253e-04 - accuracy: 0.9898 - val_loss: 0.0013 - val_accuracy: 0.9225\n",
            "Epoch 379/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.6687e-04 - accuracy: 0.9906 - val_loss: 0.0016 - val_accuracy: 0.9062\n",
            "Epoch 380/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.7148e-04 - accuracy: 0.9900 - val_loss: 0.0014 - val_accuracy: 0.9203\n",
            "Epoch 381/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.7557e-04 - accuracy: 0.9900 - val_loss: 0.0017 - val_accuracy: 0.9007\n",
            "Epoch 382/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 2.2569e-04 - accuracy: 0.9865 - val_loss: 0.0018 - val_accuracy: 0.8930\n",
            "Epoch 383/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 2.1039e-04 - accuracy: 0.9874 - val_loss: 0.0015 - val_accuracy: 0.9137\n",
            "Epoch 384/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.0144e-04 - accuracy: 0.9882 - val_loss: 0.0012 - val_accuracy: 0.9278\n",
            "Epoch 385/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.8513e-04 - accuracy: 0.9891 - val_loss: 0.0014 - val_accuracy: 0.9167\n",
            "Epoch 386/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.7851e-04 - accuracy: 0.9896 - val_loss: 0.0015 - val_accuracy: 0.9118\n",
            "Epoch 387/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.6560e-04 - accuracy: 0.9905 - val_loss: 0.0014 - val_accuracy: 0.9205\n",
            "Epoch 388/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.7447e-04 - accuracy: 0.9898 - val_loss: 0.0014 - val_accuracy: 0.9173\n",
            "Epoch 389/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.6890e-04 - accuracy: 0.9903 - val_loss: 0.0014 - val_accuracy: 0.9188\n",
            "Epoch 390/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.6793e-04 - accuracy: 0.9902 - val_loss: 0.0014 - val_accuracy: 0.9177\n",
            "Epoch 391/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.9007e-04 - accuracy: 0.9890 - val_loss: 0.0016 - val_accuracy: 0.9060\n",
            "Epoch 392/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.9308e-04 - accuracy: 0.9889 - val_loss: 0.0014 - val_accuracy: 0.9160\n",
            "Epoch 393/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.7896e-04 - accuracy: 0.9898 - val_loss: 0.0014 - val_accuracy: 0.9202\n",
            "Epoch 394/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.6107e-04 - accuracy: 0.9907 - val_loss: 0.0014 - val_accuracy: 0.9195\n",
            "Epoch 395/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.6285e-04 - accuracy: 0.9905 - val_loss: 0.0013 - val_accuracy: 0.9240\n",
            "Epoch 396/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.5332e-04 - accuracy: 0.9912 - val_loss: 0.0013 - val_accuracy: 0.9222\n",
            "Epoch 397/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.6101e-04 - accuracy: 0.9907 - val_loss: 0.0014 - val_accuracy: 0.9203\n",
            "Epoch 398/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.5849e-04 - accuracy: 0.9909 - val_loss: 0.0014 - val_accuracy: 0.9203\n",
            "Epoch 399/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.5825e-04 - accuracy: 0.9911 - val_loss: 0.0013 - val_accuracy: 0.9262\n",
            "Epoch 400/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.6329e-04 - accuracy: 0.9906 - val_loss: 0.0013 - val_accuracy: 0.9237\n",
            "Epoch 401/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.5869e-04 - accuracy: 0.9909 - val_loss: 0.0013 - val_accuracy: 0.9213\n",
            "Epoch 402/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.7692e-04 - accuracy: 0.9899 - val_loss: 0.0013 - val_accuracy: 0.9227\n",
            "Epoch 403/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.7880e-04 - accuracy: 0.9899 - val_loss: 0.0013 - val_accuracy: 0.9245\n",
            "Epoch 404/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.6161e-04 - accuracy: 0.9910 - val_loss: 0.0013 - val_accuracy: 0.9222\n",
            "Epoch 405/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.7992e-04 - accuracy: 0.9895 - val_loss: 0.0013 - val_accuracy: 0.9253\n",
            "Epoch 406/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.8641e-04 - accuracy: 0.9889 - val_loss: 0.0014 - val_accuracy: 0.9200\n",
            "Epoch 407/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 2.1955e-04 - accuracy: 0.9870 - val_loss: 0.0015 - val_accuracy: 0.9122\n",
            "Epoch 408/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.4596e-04 - accuracy: 0.9857 - val_loss: 0.0020 - val_accuracy: 0.8838\n",
            "Epoch 409/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 2.0712e-04 - accuracy: 0.9877 - val_loss: 0.0015 - val_accuracy: 0.9148\n",
            "Epoch 410/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.8100e-04 - accuracy: 0.9893 - val_loss: 0.0015 - val_accuracy: 0.9148\n",
            "Epoch 411/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.1986e-04 - accuracy: 0.9869 - val_loss: 0.0020 - val_accuracy: 0.8840\n",
            "Epoch 412/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.0764e-04 - accuracy: 0.9878 - val_loss: 0.0014 - val_accuracy: 0.9227\n",
            "Epoch 413/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.2336e-04 - accuracy: 0.9868 - val_loss: 0.0014 - val_accuracy: 0.9217\n",
            "Epoch 414/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.9342e-04 - accuracy: 0.9886 - val_loss: 0.0014 - val_accuracy: 0.9220\n",
            "Epoch 415/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.8022e-04 - accuracy: 0.9896 - val_loss: 0.0013 - val_accuracy: 0.9225\n",
            "Epoch 416/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.0609e-04 - accuracy: 0.9878 - val_loss: 0.0016 - val_accuracy: 0.9080\n",
            "Epoch 417/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.0243e-04 - accuracy: 0.9881 - val_loss: 0.0013 - val_accuracy: 0.9223\n",
            "Epoch 418/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.8040e-04 - accuracy: 0.9896 - val_loss: 0.0013 - val_accuracy: 0.9240\n",
            "Epoch 419/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.5379e-04 - accuracy: 0.9913 - val_loss: 0.0014 - val_accuracy: 0.9205\n",
            "Epoch 420/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.5195e-04 - accuracy: 0.9914 - val_loss: 0.0012 - val_accuracy: 0.9282\n",
            "Epoch 421/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.5463e-04 - accuracy: 0.9911 - val_loss: 0.0014 - val_accuracy: 0.9192\n",
            "Epoch 422/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.6640e-04 - accuracy: 0.9846 - val_loss: 0.0016 - val_accuracy: 0.9090\n",
            "Epoch 423/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.2279e-04 - accuracy: 0.9871 - val_loss: 0.0014 - val_accuracy: 0.9185\n",
            "Epoch 424/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.8670e-04 - accuracy: 0.9895 - val_loss: 0.0016 - val_accuracy: 0.9085\n",
            "Epoch 425/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.0572e-04 - accuracy: 0.9880 - val_loss: 0.0017 - val_accuracy: 0.9015\n",
            "Epoch 426/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.9847e-04 - accuracy: 0.9884 - val_loss: 0.0014 - val_accuracy: 0.9217\n",
            "Epoch 427/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.7448e-04 - accuracy: 0.9899 - val_loss: 0.0014 - val_accuracy: 0.9200\n",
            "Epoch 428/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.6366e-04 - accuracy: 0.9908 - val_loss: 0.0013 - val_accuracy: 0.9257\n",
            "Epoch 429/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.4459e-04 - accuracy: 0.9918 - val_loss: 0.0012 - val_accuracy: 0.9270\n",
            "Epoch 430/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.5671e-04 - accuracy: 0.9912 - val_loss: 0.0013 - val_accuracy: 0.9230\n",
            "Epoch 431/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.4869e-04 - accuracy: 0.9917 - val_loss: 0.0013 - val_accuracy: 0.9253\n",
            "Epoch 432/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.4681e-04 - accuracy: 0.9916 - val_loss: 0.0013 - val_accuracy: 0.9238\n",
            "Epoch 433/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.4913e-04 - accuracy: 0.9915 - val_loss: 0.0014 - val_accuracy: 0.9212\n",
            "Epoch 434/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.6608e-04 - accuracy: 0.9905 - val_loss: 0.0013 - val_accuracy: 0.9243\n",
            "Epoch 435/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.5977e-04 - accuracy: 0.9910 - val_loss: 0.0013 - val_accuracy: 0.9270\n",
            "Epoch 436/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.5638e-04 - accuracy: 0.9913 - val_loss: 0.0012 - val_accuracy: 0.9283\n",
            "Epoch 437/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.5224e-04 - accuracy: 0.9915 - val_loss: 0.0012 - val_accuracy: 0.9293\n",
            "Epoch 438/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.4430e-04 - accuracy: 0.9920 - val_loss: 0.0012 - val_accuracy: 0.9303\n",
            "Epoch 439/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.5412e-04 - accuracy: 0.9913 - val_loss: 0.0013 - val_accuracy: 0.9235\n",
            "Epoch 440/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.7686e-04 - accuracy: 0.9899 - val_loss: 0.0013 - val_accuracy: 0.9238\n",
            "Epoch 441/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.7075e-04 - accuracy: 0.9904 - val_loss: 0.0012 - val_accuracy: 0.9327\n",
            "Epoch 442/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.7061e-04 - accuracy: 0.9902 - val_loss: 0.0014 - val_accuracy: 0.9210\n",
            "Epoch 443/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.7072e-04 - accuracy: 0.9904 - val_loss: 0.0013 - val_accuracy: 0.9285\n",
            "Epoch 444/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.5313e-04 - accuracy: 0.9914 - val_loss: 0.0013 - val_accuracy: 0.9267\n",
            "Epoch 445/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.7139e-04 - accuracy: 0.9904 - val_loss: 0.0013 - val_accuracy: 0.9252\n",
            "Epoch 446/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.8174e-04 - accuracy: 0.9897 - val_loss: 0.0014 - val_accuracy: 0.9180\n",
            "Epoch 447/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.7716e-04 - accuracy: 0.9902 - val_loss: 0.0013 - val_accuracy: 0.9247\n",
            "Epoch 448/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.6056e-04 - accuracy: 0.9911 - val_loss: 0.0014 - val_accuracy: 0.9183\n",
            "Epoch 449/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.7256e-04 - accuracy: 0.9903 - val_loss: 0.0014 - val_accuracy: 0.9207\n",
            "Epoch 450/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 2.2549e-04 - accuracy: 0.9869 - val_loss: 0.0014 - val_accuracy: 0.9182\n",
            "Epoch 451/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 2.2020e-04 - accuracy: 0.9869 - val_loss: 0.0017 - val_accuracy: 0.9028\n",
            "Epoch 452/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.9259e-04 - accuracy: 0.9887 - val_loss: 0.0015 - val_accuracy: 0.9172\n",
            "Epoch 453/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.8614e-04 - accuracy: 0.9893 - val_loss: 0.0013 - val_accuracy: 0.9252\n",
            "Epoch 454/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.7648e-04 - accuracy: 0.9899 - val_loss: 0.0015 - val_accuracy: 0.9102\n",
            "Epoch 455/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.7713e-04 - accuracy: 0.9898 - val_loss: 0.0013 - val_accuracy: 0.9228\n",
            "Epoch 456/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.7775e-04 - accuracy: 0.9897 - val_loss: 0.0013 - val_accuracy: 0.9235\n",
            "Epoch 457/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.7735e-04 - accuracy: 0.9898 - val_loss: 0.0013 - val_accuracy: 0.9223\n",
            "Epoch 458/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.8757e-04 - accuracy: 0.9893 - val_loss: 0.0016 - val_accuracy: 0.9083\n",
            "Epoch 459/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.0904e-04 - accuracy: 0.9879 - val_loss: 0.0013 - val_accuracy: 0.9255\n",
            "Epoch 460/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.6753e-04 - accuracy: 0.9903 - val_loss: 0.0012 - val_accuracy: 0.9292\n",
            "Epoch 461/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.5330e-04 - accuracy: 0.9912 - val_loss: 0.0013 - val_accuracy: 0.9295\n",
            "Epoch 462/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.4887e-04 - accuracy: 0.9916 - val_loss: 0.0014 - val_accuracy: 0.9210\n",
            "Epoch 463/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.6385e-04 - accuracy: 0.9908 - val_loss: 0.0015 - val_accuracy: 0.9162\n",
            "Epoch 464/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.6895e-04 - accuracy: 0.9904 - val_loss: 0.0016 - val_accuracy: 0.9093\n",
            "Epoch 465/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.9222e-04 - accuracy: 0.9890 - val_loss: 0.0016 - val_accuracy: 0.9092\n",
            "Epoch 466/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.8539e-04 - accuracy: 0.9895 - val_loss: 0.0014 - val_accuracy: 0.9207\n",
            "Epoch 467/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.4960e-04 - accuracy: 0.9918 - val_loss: 0.0015 - val_accuracy: 0.9138\n",
            "Epoch 468/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.7137e-04 - accuracy: 0.9905 - val_loss: 0.0014 - val_accuracy: 0.9207\n",
            "Epoch 469/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.7324e-04 - accuracy: 0.9899 - val_loss: 0.0014 - val_accuracy: 0.9185\n",
            "Epoch 470/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.7234e-04 - accuracy: 0.9901 - val_loss: 0.0013 - val_accuracy: 0.9272\n",
            "Epoch 471/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.4541e-04 - accuracy: 0.9918 - val_loss: 0.0012 - val_accuracy: 0.9295\n",
            "Epoch 472/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.6112e-04 - accuracy: 0.9909 - val_loss: 0.0013 - val_accuracy: 0.9285\n",
            "Epoch 473/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.0341e-04 - accuracy: 0.9885 - val_loss: 0.0015 - val_accuracy: 0.9125\n",
            "Epoch 474/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.5354e-04 - accuracy: 0.9915 - val_loss: 0.0014 - val_accuracy: 0.9168\n",
            "Epoch 475/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.5564e-04 - accuracy: 0.9912 - val_loss: 0.0013 - val_accuracy: 0.9268\n",
            "Epoch 476/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.6397e-04 - accuracy: 0.9905 - val_loss: 0.0013 - val_accuracy: 0.9243\n",
            "Epoch 477/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.4157e-04 - accuracy: 0.9920 - val_loss: 0.0013 - val_accuracy: 0.9228\n",
            "Epoch 478/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.6097e-04 - accuracy: 0.9910 - val_loss: 0.0014 - val_accuracy: 0.9210\n",
            "Epoch 479/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.5258e-04 - accuracy: 0.9913 - val_loss: 0.0014 - val_accuracy: 0.9218\n",
            "Epoch 480/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.5073e-04 - accuracy: 0.9915 - val_loss: 0.0012 - val_accuracy: 0.9342\n",
            "Epoch 481/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.4133e-04 - accuracy: 0.9920 - val_loss: 0.0013 - val_accuracy: 0.9268\n",
            "Epoch 482/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.3749e-04 - accuracy: 0.9922 - val_loss: 0.0013 - val_accuracy: 0.9280\n",
            "Epoch 483/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.4530e-04 - accuracy: 0.9919 - val_loss: 0.0014 - val_accuracy: 0.9212\n",
            "Epoch 484/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.3940e-04 - accuracy: 0.9920 - val_loss: 0.0013 - val_accuracy: 0.9260\n",
            "Epoch 485/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.3979e-04 - accuracy: 0.9921 - val_loss: 0.0012 - val_accuracy: 0.9330\n",
            "Epoch 486/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.4186e-04 - accuracy: 0.9920 - val_loss: 0.0013 - val_accuracy: 0.9250\n",
            "Epoch 487/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.7837e-04 - accuracy: 0.9896 - val_loss: 0.0015 - val_accuracy: 0.9107\n",
            "Epoch 488/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.5073e-04 - accuracy: 0.9913 - val_loss: 0.0014 - val_accuracy: 0.9215\n",
            "Epoch 489/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.3857e-04 - accuracy: 0.9922 - val_loss: 0.0013 - val_accuracy: 0.9270\n",
            "Epoch 490/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.4585e-04 - accuracy: 0.9917 - val_loss: 0.0013 - val_accuracy: 0.9217\n",
            "Epoch 491/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.5732e-04 - accuracy: 0.9908 - val_loss: 0.0014 - val_accuracy: 0.9208\n",
            "Epoch 492/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.5137e-04 - accuracy: 0.9914 - val_loss: 0.0014 - val_accuracy: 0.9192\n",
            "Epoch 493/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.2256e-04 - accuracy: 0.9931 - val_loss: 0.0014 - val_accuracy: 0.9180\n",
            "Epoch 494/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.1459e-04 - accuracy: 0.9936 - val_loss: 0.0013 - val_accuracy: 0.9253\n",
            "Epoch 495/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.1274e-04 - accuracy: 0.9935 - val_loss: 0.0012 - val_accuracy: 0.9288\n",
            "Epoch 496/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.0524e-04 - accuracy: 0.9940 - val_loss: 0.0012 - val_accuracy: 0.9332\n",
            "Epoch 497/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.7421e-05 - accuracy: 0.9945 - val_loss: 0.0013 - val_accuracy: 0.9258\n",
            "Epoch 498/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.1340e-04 - accuracy: 0.9937 - val_loss: 0.0012 - val_accuracy: 0.9328\n",
            "Epoch 499/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.2126e-04 - accuracy: 0.9931 - val_loss: 0.0013 - val_accuracy: 0.9275\n",
            "Epoch 500/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.3592e-04 - accuracy: 0.9924 - val_loss: 0.0012 - val_accuracy: 0.9297\n",
            "Epoch 501/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.1679e-04 - accuracy: 0.9934 - val_loss: 0.0014 - val_accuracy: 0.9163\n",
            "Epoch 502/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.0924e-04 - accuracy: 0.9939 - val_loss: 0.0011 - val_accuracy: 0.9382\n",
            "Epoch 503/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.5908e-05 - accuracy: 0.9948 - val_loss: 0.0012 - val_accuracy: 0.9307\n",
            "Epoch 504/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.8602e-05 - accuracy: 0.9945 - val_loss: 0.0012 - val_accuracy: 0.9333\n",
            "Epoch 505/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.8212e-05 - accuracy: 0.9952 - val_loss: 0.0011 - val_accuracy: 0.9352\n",
            "Epoch 506/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.6054e-05 - accuracy: 0.9948 - val_loss: 0.0012 - val_accuracy: 0.9277\n",
            "Epoch 507/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.1118e-04 - accuracy: 0.9938 - val_loss: 0.0012 - val_accuracy: 0.9322\n",
            "Epoch 508/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.0640e-04 - accuracy: 0.9940 - val_loss: 0.0012 - val_accuracy: 0.9290\n",
            "Epoch 509/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.0582e-04 - accuracy: 0.9942 - val_loss: 0.0014 - val_accuracy: 0.9205\n",
            "Epoch 510/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.2501e-04 - accuracy: 0.9927 - val_loss: 0.0013 - val_accuracy: 0.9260\n",
            "Epoch 511/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.3125e-04 - accuracy: 0.9925 - val_loss: 0.0012 - val_accuracy: 0.9307\n",
            "Epoch 512/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.0804e-04 - accuracy: 0.9940 - val_loss: 0.0013 - val_accuracy: 0.9275\n",
            "Epoch 513/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.0816e-04 - accuracy: 0.9939 - val_loss: 0.0011 - val_accuracy: 0.9343\n",
            "Epoch 514/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.9831e-05 - accuracy: 0.9942 - val_loss: 0.0012 - val_accuracy: 0.9333\n",
            "Epoch 515/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.1337e-04 - accuracy: 0.9935 - val_loss: 0.0012 - val_accuracy: 0.9300\n",
            "Epoch 516/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.0631e-04 - accuracy: 0.9940 - val_loss: 0.0012 - val_accuracy: 0.9318\n",
            "Epoch 517/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.3143e-05 - accuracy: 0.9947 - val_loss: 0.0011 - val_accuracy: 0.9332\n",
            "Epoch 518/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.0014e-04 - accuracy: 0.9944 - val_loss: 0.0014 - val_accuracy: 0.9185\n",
            "Epoch 519/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.2093e-04 - accuracy: 0.9930 - val_loss: 0.0013 - val_accuracy: 0.9240\n",
            "Epoch 520/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.1184e-04 - accuracy: 0.9934 - val_loss: 0.0011 - val_accuracy: 0.9360\n",
            "Epoch 521/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.2304e-04 - accuracy: 0.9929 - val_loss: 0.0013 - val_accuracy: 0.9257\n",
            "Epoch 522/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.0121e-04 - accuracy: 0.9881 - val_loss: 0.0013 - val_accuracy: 0.9268\n",
            "Epoch 523/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.3609e-04 - accuracy: 0.9921 - val_loss: 0.0015 - val_accuracy: 0.9148\n",
            "Epoch 524/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.1763e-04 - accuracy: 0.9933 - val_loss: 0.0013 - val_accuracy: 0.9230\n",
            "Epoch 525/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.0427e-04 - accuracy: 0.9940 - val_loss: 0.0011 - val_accuracy: 0.9327\n",
            "Epoch 526/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.1578e-04 - accuracy: 0.9933 - val_loss: 0.0014 - val_accuracy: 0.9182\n",
            "Epoch 527/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.7508e-04 - accuracy: 0.9898 - val_loss: 0.0014 - val_accuracy: 0.9203\n",
            "Epoch 528/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.2967e-04 - accuracy: 0.9923 - val_loss: 0.0013 - val_accuracy: 0.9227\n",
            "Epoch 529/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.1848e-04 - accuracy: 0.9931 - val_loss: 0.0013 - val_accuracy: 0.9262\n",
            "Epoch 530/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.7733e-05 - accuracy: 0.9944 - val_loss: 0.0012 - val_accuracy: 0.9307\n",
            "Epoch 531/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.7776e-05 - accuracy: 0.9951 - val_loss: 0.0012 - val_accuracy: 0.9328\n",
            "Epoch 532/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.6362e-05 - accuracy: 0.9952 - val_loss: 0.0012 - val_accuracy: 0.9348\n",
            "Epoch 533/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.3632e-04 - accuracy: 0.9918 - val_loss: 0.0013 - val_accuracy: 0.9275\n",
            "Epoch 534/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.3099e-04 - accuracy: 0.9923 - val_loss: 0.0013 - val_accuracy: 0.9280\n",
            "Epoch 535/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.7449e-05 - accuracy: 0.9945 - val_loss: 0.0013 - val_accuracy: 0.9278\n",
            "Epoch 536/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.2235e-05 - accuracy: 0.9948 - val_loss: 0.0012 - val_accuracy: 0.9318\n",
            "Epoch 537/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 8.4635e-05 - accuracy: 0.9953 - val_loss: 0.0012 - val_accuracy: 0.9322\n",
            "Epoch 538/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.1821e-05 - accuracy: 0.9956 - val_loss: 0.0012 - val_accuracy: 0.9313\n",
            "Epoch 539/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.6814e-05 - accuracy: 0.9958 - val_loss: 0.0012 - val_accuracy: 0.9328\n",
            "Epoch 540/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.9070e-05 - accuracy: 0.9957 - val_loss: 0.0011 - val_accuracy: 0.9352\n",
            "Epoch 541/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.8983e-05 - accuracy: 0.9956 - val_loss: 0.0011 - val_accuracy: 0.9368\n",
            "Epoch 542/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.7955e-05 - accuracy: 0.9956 - val_loss: 0.0011 - val_accuracy: 0.9350\n",
            "Epoch 543/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.9400e-05 - accuracy: 0.9957 - val_loss: 0.0011 - val_accuracy: 0.9373\n",
            "Epoch 544/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.3483e-05 - accuracy: 0.9952 - val_loss: 0.0012 - val_accuracy: 0.9302\n",
            "Epoch 545/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.7933e-05 - accuracy: 0.9955 - val_loss: 0.0013 - val_accuracy: 0.9280\n",
            "Epoch 546/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.6197e-05 - accuracy: 0.9946 - val_loss: 0.0011 - val_accuracy: 0.9367\n",
            "Epoch 547/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.1836e-05 - accuracy: 0.9949 - val_loss: 0.0011 - val_accuracy: 0.9368\n",
            "Epoch 548/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.5455e-05 - accuracy: 0.9952 - val_loss: 0.0011 - val_accuracy: 0.9358\n",
            "Epoch 549/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.4215e-05 - accuracy: 0.9954 - val_loss: 0.0012 - val_accuracy: 0.9323\n",
            "Epoch 550/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.0214e-04 - accuracy: 0.9943 - val_loss: 0.0015 - val_accuracy: 0.9177\n",
            "Epoch 551/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.2041e-04 - accuracy: 0.9933 - val_loss: 0.0013 - val_accuracy: 0.9258\n",
            "Epoch 552/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.3861e-04 - accuracy: 0.9918 - val_loss: 0.0013 - val_accuracy: 0.9232\n",
            "Epoch 553/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.1353e-04 - accuracy: 0.9934 - val_loss: 0.0013 - val_accuracy: 0.9250\n",
            "Epoch 554/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.0584e-04 - accuracy: 0.9940 - val_loss: 0.0013 - val_accuracy: 0.9255\n",
            "Epoch 555/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.2343e-04 - accuracy: 0.9928 - val_loss: 0.0012 - val_accuracy: 0.9313\n",
            "Epoch 556/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.7942e-05 - accuracy: 0.9950 - val_loss: 0.0012 - val_accuracy: 0.9293\n",
            "Epoch 557/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.5381e-05 - accuracy: 0.9952 - val_loss: 0.0011 - val_accuracy: 0.9397\n",
            "Epoch 558/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.8088e-05 - accuracy: 0.9955 - val_loss: 0.0013 - val_accuracy: 0.9245\n",
            "Epoch 559/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.9101e-05 - accuracy: 0.9949 - val_loss: 0.0013 - val_accuracy: 0.9258\n",
            "Epoch 560/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.5525e-05 - accuracy: 0.9958 - val_loss: 0.0012 - val_accuracy: 0.9305\n",
            "Epoch 561/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.4129e-05 - accuracy: 0.9958 - val_loss: 0.0011 - val_accuracy: 0.9362\n",
            "Epoch 562/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.8985e-05 - accuracy: 0.9955 - val_loss: 0.0013 - val_accuracy: 0.9237\n",
            "Epoch 563/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.2215e-04 - accuracy: 0.9929 - val_loss: 0.0014 - val_accuracy: 0.9215\n",
            "Epoch 564/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.0763e-04 - accuracy: 0.9935 - val_loss: 0.0012 - val_accuracy: 0.9292\n",
            "Epoch 565/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.3382e-05 - accuracy: 0.9951 - val_loss: 0.0013 - val_accuracy: 0.9278\n",
            "Epoch 566/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.6037e-05 - accuracy: 0.9945 - val_loss: 0.0013 - val_accuracy: 0.9247\n",
            "Epoch 567/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.9887e-05 - accuracy: 0.9948 - val_loss: 0.0012 - val_accuracy: 0.9303\n",
            "Epoch 568/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.2365e-05 - accuracy: 0.9960 - val_loss: 0.0012 - val_accuracy: 0.9310\n",
            "Epoch 569/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.8133e-05 - accuracy: 0.9962 - val_loss: 0.0012 - val_accuracy: 0.9297\n",
            "Epoch 570/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.1243e-05 - accuracy: 0.9967 - val_loss: 0.0011 - val_accuracy: 0.9340\n",
            "Epoch 571/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.7341e-05 - accuracy: 0.9963 - val_loss: 0.0011 - val_accuracy: 0.9368\n",
            "Epoch 572/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.5824e-05 - accuracy: 0.9963 - val_loss: 0.0011 - val_accuracy: 0.9355\n",
            "Epoch 573/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.4612e-05 - accuracy: 0.9958 - val_loss: 0.0013 - val_accuracy: 0.9277\n",
            "Epoch 574/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.7452e-05 - accuracy: 0.9962 - val_loss: 0.0011 - val_accuracy: 0.9338\n",
            "Epoch 575/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.2048e-05 - accuracy: 0.9961 - val_loss: 0.0011 - val_accuracy: 0.9362\n",
            "Epoch 576/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.1708e-05 - accuracy: 0.9953 - val_loss: 0.0012 - val_accuracy: 0.9327\n",
            "Epoch 577/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.4098e-05 - accuracy: 0.9960 - val_loss: 0.0012 - val_accuracy: 0.9345\n",
            "Epoch 578/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.2964e-05 - accuracy: 0.9959 - val_loss: 0.0013 - val_accuracy: 0.9247\n",
            "Epoch 579/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.2008e-04 - accuracy: 0.9930 - val_loss: 0.0013 - val_accuracy: 0.9260\n",
            "Epoch 580/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.5284e-05 - accuracy: 0.9951 - val_loss: 0.0013 - val_accuracy: 0.9267\n",
            "Epoch 581/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.3354e-05 - accuracy: 0.9952 - val_loss: 0.0013 - val_accuracy: 0.9292\n",
            "Epoch 582/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 8.9298e-05 - accuracy: 0.9948 - val_loss: 0.0012 - val_accuracy: 0.9297\n",
            "Epoch 583/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.0824e-05 - accuracy: 0.9946 - val_loss: 0.0014 - val_accuracy: 0.9207\n",
            "Epoch 584/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.4924e-05 - accuracy: 0.9951 - val_loss: 0.0012 - val_accuracy: 0.9320\n",
            "Epoch 585/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.1951e-05 - accuracy: 0.9955 - val_loss: 0.0012 - val_accuracy: 0.9310\n",
            "Epoch 586/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.9880e-05 - accuracy: 0.9955 - val_loss: 0.0013 - val_accuracy: 0.9277\n",
            "Epoch 587/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.6789e-05 - accuracy: 0.9941 - val_loss: 0.0012 - val_accuracy: 0.9295\n",
            "Epoch 588/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.9502e-05 - accuracy: 0.9960 - val_loss: 0.0011 - val_accuracy: 0.9388\n",
            "Epoch 589/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.0515e-05 - accuracy: 0.9960 - val_loss: 0.0011 - val_accuracy: 0.9352\n",
            "Epoch 590/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.2283e-05 - accuracy: 0.9952 - val_loss: 0.0011 - val_accuracy: 0.9382\n",
            "Epoch 591/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.1506e-05 - accuracy: 0.9954 - val_loss: 0.0015 - val_accuracy: 0.9170\n",
            "Epoch 592/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.1870e-05 - accuracy: 0.9958 - val_loss: 0.0012 - val_accuracy: 0.9337\n",
            "Epoch 593/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.1888e-05 - accuracy: 0.9960 - val_loss: 0.0011 - val_accuracy: 0.9395\n",
            "Epoch 594/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.5984e-05 - accuracy: 0.9963 - val_loss: 0.0012 - val_accuracy: 0.9322\n",
            "Epoch 595/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.2728e-05 - accuracy: 0.9966 - val_loss: 0.0011 - val_accuracy: 0.9375\n",
            "Epoch 596/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.2113e-05 - accuracy: 0.9952 - val_loss: 0.0013 - val_accuracy: 0.9265\n",
            "Epoch 597/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.3522e-05 - accuracy: 0.9944 - val_loss: 0.0012 - val_accuracy: 0.9290\n",
            "Epoch 598/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.8021e-05 - accuracy: 0.9948 - val_loss: 0.0013 - val_accuracy: 0.9240\n",
            "Epoch 599/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.8433e-05 - accuracy: 0.9941 - val_loss: 0.0011 - val_accuracy: 0.9348\n",
            "Epoch 600/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.4721e-05 - accuracy: 0.9965 - val_loss: 0.0012 - val_accuracy: 0.9318\n",
            "Epoch 601/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.1052e-05 - accuracy: 0.9970 - val_loss: 0.0010 - val_accuracy: 0.9412\n",
            "Epoch 602/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.4622e-05 - accuracy: 0.9975 - val_loss: 0.0010 - val_accuracy: 0.9423\n",
            "Epoch 603/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.1702e-05 - accuracy: 0.9964 - val_loss: 0.0013 - val_accuracy: 0.9253\n",
            "Epoch 604/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.9905e-05 - accuracy: 0.9966 - val_loss: 0.0013 - val_accuracy: 0.9257\n",
            "Epoch 605/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.0439e-05 - accuracy: 0.9973 - val_loss: 0.0010 - val_accuracy: 0.9427\n",
            "Epoch 606/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.0130e-05 - accuracy: 0.9977 - val_loss: 0.0011 - val_accuracy: 0.9393\n",
            "Epoch 607/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.0891e-05 - accuracy: 0.9978 - val_loss: 0.0011 - val_accuracy: 0.9372\n",
            "Epoch 608/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.1351e-05 - accuracy: 0.9977 - val_loss: 0.0012 - val_accuracy: 0.9328\n",
            "Epoch 609/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.4009e-05 - accuracy: 0.9976 - val_loss: 0.0011 - val_accuracy: 0.9385\n",
            "Epoch 610/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.8992e-05 - accuracy: 0.9967 - val_loss: 0.0011 - val_accuracy: 0.9385\n",
            "Epoch 611/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.8840e-05 - accuracy: 0.9967 - val_loss: 0.0012 - val_accuracy: 0.9323\n",
            "Epoch 612/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 4.4343e-05 - accuracy: 0.9975 - val_loss: 0.0012 - val_accuracy: 0.9323\n",
            "Epoch 613/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 4.1690e-05 - accuracy: 0.9977 - val_loss: 0.0011 - val_accuracy: 0.9402\n",
            "Epoch 614/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.8941e-05 - accuracy: 0.9979 - val_loss: 0.0010 - val_accuracy: 0.9427\n",
            "Epoch 615/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.6263e-05 - accuracy: 0.9981 - val_loss: 0.0010 - val_accuracy: 0.9430\n",
            "Epoch 616/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.1489e-05 - accuracy: 0.9977 - val_loss: 0.0011 - val_accuracy: 0.9385\n",
            "Epoch 617/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.0220e-05 - accuracy: 0.9966 - val_loss: 0.0012 - val_accuracy: 0.9322\n",
            "Epoch 618/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.7290e-05 - accuracy: 0.9955 - val_loss: 0.0011 - val_accuracy: 0.9363\n",
            "Epoch 619/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.5473e-05 - accuracy: 0.9967 - val_loss: 0.0012 - val_accuracy: 0.9330\n",
            "Epoch 620/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.6897e-05 - accuracy: 0.9967 - val_loss: 0.0011 - val_accuracy: 0.9368\n",
            "Epoch 621/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.8516e-05 - accuracy: 0.9960 - val_loss: 0.0011 - val_accuracy: 0.9393\n",
            "Epoch 622/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.3115e-05 - accuracy: 0.9975 - val_loss: 9.7231e-04 - val_accuracy: 0.9448\n",
            "Epoch 623/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.9202e-05 - accuracy: 0.9965 - val_loss: 0.0012 - val_accuracy: 0.9280\n",
            "Epoch 624/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.0071e-05 - accuracy: 0.9953 - val_loss: 0.0012 - val_accuracy: 0.9317\n",
            "Epoch 625/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.6767e-05 - accuracy: 0.9963 - val_loss: 0.0012 - val_accuracy: 0.9320\n",
            "Epoch 626/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.6151e-05 - accuracy: 0.9963 - val_loss: 0.0012 - val_accuracy: 0.9330\n",
            "Epoch 627/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.1834e-05 - accuracy: 0.9971 - val_loss: 0.0012 - val_accuracy: 0.9343\n",
            "Epoch 628/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.2001e-05 - accuracy: 0.9969 - val_loss: 0.0012 - val_accuracy: 0.9328\n",
            "Epoch 629/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.9859e-05 - accuracy: 0.9965 - val_loss: 0.0013 - val_accuracy: 0.9275\n",
            "Epoch 630/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.6756e-05 - accuracy: 0.9973 - val_loss: 0.0011 - val_accuracy: 0.9388\n",
            "Epoch 631/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.9620e-05 - accuracy: 0.9966 - val_loss: 0.0011 - val_accuracy: 0.9362\n",
            "Epoch 632/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.0296e-05 - accuracy: 0.9967 - val_loss: 0.0011 - val_accuracy: 0.9352\n",
            "Epoch 633/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.5323e-05 - accuracy: 0.9980 - val_loss: 0.0013 - val_accuracy: 0.9243\n",
            "Epoch 634/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.8540e-05 - accuracy: 0.9960 - val_loss: 0.0013 - val_accuracy: 0.9240\n",
            "Epoch 635/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.1779e-05 - accuracy: 0.9964 - val_loss: 0.0011 - val_accuracy: 0.9357\n",
            "Epoch 636/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.7897e-05 - accuracy: 0.9973 - val_loss: 0.0012 - val_accuracy: 0.9320\n",
            "Epoch 637/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.1378e-05 - accuracy: 0.9977 - val_loss: 0.0011 - val_accuracy: 0.9385\n",
            "Epoch 638/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.2381e-05 - accuracy: 0.9977 - val_loss: 0.0011 - val_accuracy: 0.9338\n",
            "Epoch 639/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.2228e-04 - accuracy: 0.9927 - val_loss: 0.0013 - val_accuracy: 0.9253\n",
            "Epoch 640/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.0174e-05 - accuracy: 0.9957 - val_loss: 0.0011 - val_accuracy: 0.9353\n",
            "Epoch 641/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.6610e-05 - accuracy: 0.9943 - val_loss: 0.0013 - val_accuracy: 0.9228\n",
            "Epoch 642/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.1495e-05 - accuracy: 0.9951 - val_loss: 0.0014 - val_accuracy: 0.9233\n",
            "Epoch 643/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.6603e-05 - accuracy: 0.9974 - val_loss: 0.0011 - val_accuracy: 0.9368\n",
            "Epoch 644/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.7414e-05 - accuracy: 0.9974 - val_loss: 0.0012 - val_accuracy: 0.9353\n",
            "Epoch 645/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.7645e-05 - accuracy: 0.9972 - val_loss: 0.0011 - val_accuracy: 0.9342\n",
            "Epoch 646/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.3190e-05 - accuracy: 0.9976 - val_loss: 0.0011 - val_accuracy: 0.9372\n",
            "Epoch 647/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.1313e-05 - accuracy: 0.9978 - val_loss: 0.0010 - val_accuracy: 0.9418\n",
            "Epoch 648/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.3376e-05 - accuracy: 0.9981 - val_loss: 0.0011 - val_accuracy: 0.9397\n",
            "Epoch 649/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.8316e-05 - accuracy: 0.9978 - val_loss: 0.0010 - val_accuracy: 0.9422\n",
            "Epoch 650/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.2753e-05 - accuracy: 0.9976 - val_loss: 0.0011 - val_accuracy: 0.9398\n",
            "Epoch 651/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.6300e-05 - accuracy: 0.9979 - val_loss: 0.0011 - val_accuracy: 0.9360\n",
            "Epoch 652/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.6004e-05 - accuracy: 0.9980 - val_loss: 0.0011 - val_accuracy: 0.9362\n",
            "Epoch 653/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.5321e-05 - accuracy: 0.9982 - val_loss: 0.0010 - val_accuracy: 0.9413\n",
            "Epoch 654/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.0000e-05 - accuracy: 0.9952 - val_loss: 0.0010 - val_accuracy: 0.9430\n",
            "Epoch 655/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.4106e-05 - accuracy: 0.9963 - val_loss: 0.0012 - val_accuracy: 0.9317\n",
            "Epoch 656/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.1889e-05 - accuracy: 0.9952 - val_loss: 0.0011 - val_accuracy: 0.9347\n",
            "Epoch 657/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.7475e-05 - accuracy: 0.9949 - val_loss: 0.0013 - val_accuracy: 0.9285\n",
            "Epoch 658/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.4148e-04 - accuracy: 0.9919 - val_loss: 0.0011 - val_accuracy: 0.9370\n",
            "Epoch 659/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.7688e-05 - accuracy: 0.9955 - val_loss: 0.0011 - val_accuracy: 0.9358\n",
            "Epoch 660/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.6881e-05 - accuracy: 0.9968 - val_loss: 0.0010 - val_accuracy: 0.9407\n",
            "Epoch 661/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.7151e-05 - accuracy: 0.9957 - val_loss: 0.0011 - val_accuracy: 0.9372\n",
            "Epoch 662/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.8079e-05 - accuracy: 0.9968 - val_loss: 0.0011 - val_accuracy: 0.9372\n",
            "Epoch 663/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.4831e-05 - accuracy: 0.9975 - val_loss: 0.0011 - val_accuracy: 0.9377\n",
            "Epoch 664/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.8780e-05 - accuracy: 0.9961 - val_loss: 0.0015 - val_accuracy: 0.9152\n",
            "Epoch 665/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.2144e-04 - accuracy: 0.9931 - val_loss: 0.0012 - val_accuracy: 0.9328\n",
            "Epoch 666/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.8181e-05 - accuracy: 0.9966 - val_loss: 0.0012 - val_accuracy: 0.9348\n",
            "Epoch 667/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.2966e-05 - accuracy: 0.9970 - val_loss: 0.0011 - val_accuracy: 0.9377\n",
            "Epoch 668/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.5284e-05 - accuracy: 0.9973 - val_loss: 0.0014 - val_accuracy: 0.9233\n",
            "Epoch 669/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.6197e-05 - accuracy: 0.9975 - val_loss: 0.0011 - val_accuracy: 0.9362\n",
            "Epoch 670/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.0967e-05 - accuracy: 0.9977 - val_loss: 0.0011 - val_accuracy: 0.9397\n",
            "Epoch 671/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.0324e-05 - accuracy: 0.9979 - val_loss: 0.0010 - val_accuracy: 0.9408\n",
            "Epoch 672/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.8341e-05 - accuracy: 0.9979 - val_loss: 0.0010 - val_accuracy: 0.9418\n",
            "Epoch 673/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.1238e-05 - accuracy: 0.9977 - val_loss: 0.0011 - val_accuracy: 0.9353\n",
            "Epoch 674/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.8021e-05 - accuracy: 0.9973 - val_loss: 9.8343e-04 - val_accuracy: 0.9443\n",
            "Epoch 675/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.4385e-05 - accuracy: 0.9974 - val_loss: 9.8307e-04 - val_accuracy: 0.9445\n",
            "Epoch 676/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.6429e-05 - accuracy: 0.9980 - val_loss: 0.0011 - val_accuracy: 0.9393\n",
            "Epoch 677/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.4957e-05 - accuracy: 0.9980 - val_loss: 0.0010 - val_accuracy: 0.9417\n",
            "Epoch 678/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.8383e-05 - accuracy: 0.9978 - val_loss: 0.0010 - val_accuracy: 0.9428\n",
            "Epoch 679/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.2837e-05 - accuracy: 0.9970 - val_loss: 0.0010 - val_accuracy: 0.9420\n",
            "Epoch 680/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.9624e-05 - accuracy: 0.9967 - val_loss: 0.0011 - val_accuracy: 0.9395\n",
            "Epoch 681/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.0403e-05 - accuracy: 0.9967 - val_loss: 0.0010 - val_accuracy: 0.9413\n",
            "Epoch 682/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.7464e-05 - accuracy: 0.9979 - val_loss: 0.0011 - val_accuracy: 0.9410\n",
            "Epoch 683/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.8939e-05 - accuracy: 0.9979 - val_loss: 0.0010 - val_accuracy: 0.9410\n",
            "Epoch 684/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.4731e-05 - accuracy: 0.9981 - val_loss: 0.0011 - val_accuracy: 0.9393\n",
            "Epoch 685/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.1357e-05 - accuracy: 0.9977 - val_loss: 9.7767e-04 - val_accuracy: 0.9452\n",
            "Epoch 686/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.2104e-05 - accuracy: 0.9972 - val_loss: 0.0016 - val_accuracy: 0.9113\n",
            "Epoch 687/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.0510e-05 - accuracy: 0.9955 - val_loss: 0.0012 - val_accuracy: 0.9315\n",
            "Epoch 688/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.8809e-05 - accuracy: 0.9949 - val_loss: 0.0015 - val_accuracy: 0.9172\n",
            "Epoch 689/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.4410e-05 - accuracy: 0.9947 - val_loss: 0.0011 - val_accuracy: 0.9395\n",
            "Epoch 690/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.5128e-05 - accuracy: 0.9968 - val_loss: 0.0011 - val_accuracy: 0.9368\n",
            "Epoch 691/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.2201e-05 - accuracy: 0.9970 - val_loss: 0.0011 - val_accuracy: 0.9368\n",
            "Epoch 692/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.0864e-05 - accuracy: 0.9966 - val_loss: 0.0012 - val_accuracy: 0.9332\n",
            "Epoch 693/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.7576e-05 - accuracy: 0.9968 - val_loss: 0.0012 - val_accuracy: 0.9343\n",
            "Epoch 694/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.3572e-05 - accuracy: 0.9969 - val_loss: 0.0011 - val_accuracy: 0.9385\n",
            "Epoch 695/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.2194e-05 - accuracy: 0.9964 - val_loss: 0.0014 - val_accuracy: 0.9218\n",
            "Epoch 696/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.6503e-05 - accuracy: 0.9968 - val_loss: 0.0010 - val_accuracy: 0.9415\n",
            "Epoch 697/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.7072e-05 - accuracy: 0.9967 - val_loss: 0.0012 - val_accuracy: 0.9342\n",
            "Epoch 698/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.7409e-05 - accuracy: 0.9966 - val_loss: 0.0010 - val_accuracy: 0.9420\n",
            "Epoch 699/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.8052e-05 - accuracy: 0.9973 - val_loss: 9.7889e-04 - val_accuracy: 0.9450\n",
            "Epoch 700/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.0089e-05 - accuracy: 0.9969 - val_loss: 0.0010 - val_accuracy: 0.9422\n",
            "Epoch 701/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 4.4015e-05 - accuracy: 0.9976 - val_loss: 0.0011 - val_accuracy: 0.9367\n",
            "Epoch 702/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.5249e-05 - accuracy: 0.9981 - val_loss: 0.0010 - val_accuracy: 0.9412\n",
            "Epoch 703/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.7421e-05 - accuracy: 0.9979 - val_loss: 0.0010 - val_accuracy: 0.9422\n",
            "Epoch 704/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.9309e-05 - accuracy: 0.9977 - val_loss: 9.0465e-04 - val_accuracy: 0.9497\n",
            "Epoch 705/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.7882e-05 - accuracy: 0.9975 - val_loss: 0.0010 - val_accuracy: 0.9415\n",
            "Epoch 706/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.5510e-05 - accuracy: 0.9980 - val_loss: 0.0010 - val_accuracy: 0.9437\n",
            "Epoch 707/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.2644e-05 - accuracy: 0.9976 - val_loss: 0.0010 - val_accuracy: 0.9427\n",
            "Epoch 708/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.1858e-05 - accuracy: 0.9977 - val_loss: 0.0011 - val_accuracy: 0.9382\n",
            "Epoch 709/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.7387e-05 - accuracy: 0.9980 - val_loss: 0.0010 - val_accuracy: 0.9432\n",
            "Epoch 710/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.0983e-05 - accuracy: 0.9977 - val_loss: 0.0010 - val_accuracy: 0.9417\n",
            "Epoch 711/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.0552e-05 - accuracy: 0.9984 - val_loss: 0.0011 - val_accuracy: 0.9393\n",
            "Epoch 712/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.6037e-05 - accuracy: 0.9975 - val_loss: 0.0012 - val_accuracy: 0.9320\n",
            "Epoch 713/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.3740e-05 - accuracy: 0.9975 - val_loss: 0.0011 - val_accuracy: 0.9360\n",
            "Epoch 714/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.6512e-05 - accuracy: 0.9962 - val_loss: 0.0010 - val_accuracy: 0.9423\n",
            "Epoch 715/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.5009e-05 - accuracy: 0.9963 - val_loss: 0.0011 - val_accuracy: 0.9383\n",
            "Epoch 716/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.2442e-05 - accuracy: 0.9971 - val_loss: 0.0012 - val_accuracy: 0.9317\n",
            "Epoch 717/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.6960e-05 - accuracy: 0.9968 - val_loss: 0.0012 - val_accuracy: 0.9307\n",
            "Epoch 718/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.6880e-05 - accuracy: 0.9975 - val_loss: 0.0013 - val_accuracy: 0.9268\n",
            "Epoch 719/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.2975e-05 - accuracy: 0.9965 - val_loss: 0.0012 - val_accuracy: 0.9363\n",
            "Epoch 720/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.1433e-05 - accuracy: 0.9969 - val_loss: 0.0011 - val_accuracy: 0.9385\n",
            "Epoch 721/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.1211e-05 - accuracy: 0.9971 - val_loss: 0.0010 - val_accuracy: 0.9437\n",
            "Epoch 722/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.8120e-05 - accuracy: 0.9979 - val_loss: 9.9966e-04 - val_accuracy: 0.9432\n",
            "Epoch 723/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.0249e-05 - accuracy: 0.9972 - val_loss: 9.7714e-04 - val_accuracy: 0.9455\n",
            "Epoch 724/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.7931e-05 - accuracy: 0.9979 - val_loss: 0.0011 - val_accuracy: 0.9372\n",
            "Epoch 725/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.5971e-05 - accuracy: 0.9972 - val_loss: 0.0013 - val_accuracy: 0.9240\n",
            "Epoch 726/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.2036e-05 - accuracy: 0.9964 - val_loss: 0.0011 - val_accuracy: 0.9412\n",
            "Epoch 727/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.7508e-05 - accuracy: 0.9972 - val_loss: 0.0011 - val_accuracy: 0.9392\n",
            "Epoch 728/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.8383e-05 - accuracy: 0.9961 - val_loss: 0.0011 - val_accuracy: 0.9372\n",
            "Epoch 729/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.4991e-05 - accuracy: 0.9975 - val_loss: 9.9750e-04 - val_accuracy: 0.9442\n",
            "Epoch 730/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.6286e-05 - accuracy: 0.9975 - val_loss: 0.0010 - val_accuracy: 0.9425\n",
            "Epoch 731/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.8813e-05 - accuracy: 0.9972 - val_loss: 0.0012 - val_accuracy: 0.9322\n",
            "Epoch 732/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.8917e-05 - accuracy: 0.9966 - val_loss: 0.0011 - val_accuracy: 0.9347\n",
            "Epoch 733/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.0789e-05 - accuracy: 0.9972 - val_loss: 0.0011 - val_accuracy: 0.9390\n",
            "Epoch 734/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.6034e-05 - accuracy: 0.9981 - val_loss: 0.0010 - val_accuracy: 0.9413\n",
            "Epoch 735/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.9710e-05 - accuracy: 0.9978 - val_loss: 0.0012 - val_accuracy: 0.9315\n",
            "Epoch 736/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.4267e-05 - accuracy: 0.9976 - val_loss: 9.4494e-04 - val_accuracy: 0.9478\n",
            "Epoch 737/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.9964e-05 - accuracy: 0.9978 - val_loss: 0.0010 - val_accuracy: 0.9432\n",
            "Epoch 738/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.3641e-05 - accuracy: 0.9982 - val_loss: 0.0011 - val_accuracy: 0.9408\n",
            "Epoch 739/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.9521e-05 - accuracy: 0.9970 - val_loss: 0.0011 - val_accuracy: 0.9378\n",
            "Epoch 740/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.8676e-05 - accuracy: 0.9967 - val_loss: 0.0012 - val_accuracy: 0.9323\n",
            "Epoch 741/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.3599e-05 - accuracy: 0.9965 - val_loss: 0.0011 - val_accuracy: 0.9367\n",
            "Epoch 742/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.5590e-05 - accuracy: 0.9969 - val_loss: 0.0011 - val_accuracy: 0.9400\n",
            "Epoch 743/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.4985e-05 - accuracy: 0.9968 - val_loss: 0.0011 - val_accuracy: 0.9383\n",
            "Epoch 744/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.5777e-05 - accuracy: 0.9968 - val_loss: 0.0011 - val_accuracy: 0.9407\n",
            "Epoch 745/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.2842e-05 - accuracy: 0.9970 - val_loss: 0.0010 - val_accuracy: 0.9430\n",
            "Epoch 746/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.1283e-05 - accuracy: 0.9978 - val_loss: 0.0010 - val_accuracy: 0.9432\n",
            "Epoch 747/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.5347e-05 - accuracy: 0.9982 - val_loss: 0.0011 - val_accuracy: 0.9390\n",
            "Epoch 748/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.0258e-05 - accuracy: 0.9973 - val_loss: 0.0012 - val_accuracy: 0.9347\n",
            "Epoch 749/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 5.7478e-05 - accuracy: 0.9967 - val_loss: 0.0012 - val_accuracy: 0.9335\n",
            "Epoch 750/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.7311e-05 - accuracy: 0.9968 - val_loss: 0.0011 - val_accuracy: 0.9398\n",
            "Epoch 751/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.9680e-05 - accuracy: 0.9978 - val_loss: 0.0011 - val_accuracy: 0.9387\n",
            "Epoch 752/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.8349e-05 - accuracy: 0.9979 - val_loss: 0.0011 - val_accuracy: 0.9363\n",
            "Epoch 753/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.4002e-05 - accuracy: 0.9970 - val_loss: 0.0012 - val_accuracy: 0.9337\n",
            "Epoch 754/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.7525e-05 - accuracy: 0.9974 - val_loss: 0.0014 - val_accuracy: 0.9220\n",
            "Epoch 755/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.6031e-05 - accuracy: 0.9968 - val_loss: 0.0011 - val_accuracy: 0.9370\n",
            "Epoch 756/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.5814e-05 - accuracy: 0.9968 - val_loss: 0.0013 - val_accuracy: 0.9275\n",
            "Epoch 757/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.7857e-05 - accuracy: 0.9950 - val_loss: 0.0011 - val_accuracy: 0.9380\n",
            "Epoch 758/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.4168e-04 - accuracy: 0.9917 - val_loss: 0.0012 - val_accuracy: 0.9365\n",
            "Epoch 759/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.5879e-05 - accuracy: 0.9962 - val_loss: 0.0011 - val_accuracy: 0.9380\n",
            "Epoch 760/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.1275e-05 - accuracy: 0.9958 - val_loss: 0.0011 - val_accuracy: 0.9387\n",
            "Epoch 761/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.4725e-05 - accuracy: 0.9982 - val_loss: 9.4092e-04 - val_accuracy: 0.9473\n",
            "Epoch 762/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.0744e-05 - accuracy: 0.9977 - val_loss: 0.0012 - val_accuracy: 0.9358\n",
            "Epoch 763/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.1272e-05 - accuracy: 0.9972 - val_loss: 0.0010 - val_accuracy: 0.9428\n",
            "Epoch 764/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.2959e-05 - accuracy: 0.9976 - val_loss: 0.0011 - val_accuracy: 0.9400\n",
            "Epoch 765/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.6460e-05 - accuracy: 0.9980 - val_loss: 0.0010 - val_accuracy: 0.9428\n",
            "Epoch 766/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.3877e-05 - accuracy: 0.9975 - val_loss: 0.0012 - val_accuracy: 0.9343\n",
            "Epoch 767/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.9050e-05 - accuracy: 0.9979 - val_loss: 0.0010 - val_accuracy: 0.9437\n",
            "Epoch 768/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.3678e-05 - accuracy: 0.9976 - val_loss: 0.0015 - val_accuracy: 0.9178\n",
            "Epoch 769/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.5708e-05 - accuracy: 0.9969 - val_loss: 0.0011 - val_accuracy: 0.9388\n",
            "Epoch 770/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.4097e-04 - accuracy: 0.9918 - val_loss: 0.0013 - val_accuracy: 0.9257\n",
            "Epoch 771/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.0193e-04 - accuracy: 0.9939 - val_loss: 0.0013 - val_accuracy: 0.9263\n",
            "Epoch 772/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.5787e-05 - accuracy: 0.9951 - val_loss: 0.0011 - val_accuracy: 0.9363\n",
            "Epoch 773/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.8043e-05 - accuracy: 0.9973 - val_loss: 0.0010 - val_accuracy: 0.9443\n",
            "Epoch 774/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.0748e-05 - accuracy: 0.9977 - val_loss: 0.0013 - val_accuracy: 0.9265\n",
            "Epoch 775/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.5976e-05 - accuracy: 0.9980 - val_loss: 9.8216e-04 - val_accuracy: 0.9455\n",
            "Epoch 776/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.3528e-05 - accuracy: 0.9983 - val_loss: 9.4516e-04 - val_accuracy: 0.9477\n",
            "Epoch 777/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.9045e-05 - accuracy: 0.9985 - val_loss: 9.1797e-04 - val_accuracy: 0.9480\n",
            "Epoch 778/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.8418e-05 - accuracy: 0.9985 - val_loss: 9.7369e-04 - val_accuracy: 0.9452\n",
            "Epoch 779/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.0983e-05 - accuracy: 0.9984 - val_loss: 0.0012 - val_accuracy: 0.9352\n",
            "Epoch 780/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.1559e-05 - accuracy: 0.9965 - val_loss: 0.0011 - val_accuracy: 0.9372\n",
            "Epoch 781/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.7004e-05 - accuracy: 0.9979 - val_loss: 0.0013 - val_accuracy: 0.9283\n",
            "Epoch 782/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.2299e-05 - accuracy: 0.9969 - val_loss: 0.0012 - val_accuracy: 0.9338\n",
            "Epoch 783/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.1516e-05 - accuracy: 0.9972 - val_loss: 0.0011 - val_accuracy: 0.9407\n",
            "Epoch 784/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.9767e-05 - accuracy: 0.9977 - val_loss: 0.0011 - val_accuracy: 0.9408\n",
            "Epoch 785/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.5305e-05 - accuracy: 0.9981 - val_loss: 0.0012 - val_accuracy: 0.9318\n",
            "Epoch 786/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.7782e-05 - accuracy: 0.9980 - val_loss: 0.0010 - val_accuracy: 0.9440\n",
            "Epoch 787/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.2257e-05 - accuracy: 0.9983 - val_loss: 9.9698e-04 - val_accuracy: 0.9453\n",
            "Epoch 788/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.6929e-05 - accuracy: 0.9970 - val_loss: 0.0012 - val_accuracy: 0.9307\n",
            "Epoch 789/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.1166e-05 - accuracy: 0.9959 - val_loss: 0.0011 - val_accuracy: 0.9363\n",
            "Epoch 790/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.0419e-04 - accuracy: 0.9942 - val_loss: 0.0011 - val_accuracy: 0.9372\n",
            "Epoch 791/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.1236e-05 - accuracy: 0.9959 - val_loss: 0.0013 - val_accuracy: 0.9297\n",
            "Epoch 792/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.7932e-05 - accuracy: 0.9974 - val_loss: 0.0013 - val_accuracy: 0.9292\n",
            "Epoch 793/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.9113e-05 - accuracy: 0.9961 - val_loss: 0.0011 - val_accuracy: 0.9392\n",
            "Epoch 794/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.9641e-05 - accuracy: 0.9972 - val_loss: 0.0010 - val_accuracy: 0.9422\n",
            "Epoch 795/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.7032e-05 - accuracy: 0.9974 - val_loss: 0.0011 - val_accuracy: 0.9372\n",
            "Epoch 796/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.1019e-05 - accuracy: 0.9977 - val_loss: 0.0010 - val_accuracy: 0.9422\n",
            "Epoch 797/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 5.8347e-05 - accuracy: 0.9966 - val_loss: 0.0013 - val_accuracy: 0.9297\n",
            "Epoch 798/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.2773e-05 - accuracy: 0.9970 - val_loss: 0.0011 - val_accuracy: 0.9397\n",
            "Epoch 799/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.5166e-05 - accuracy: 0.9981 - val_loss: 0.0010 - val_accuracy: 0.9422\n",
            "Epoch 800/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.9416e-05 - accuracy: 0.9985 - val_loss: 0.0010 - val_accuracy: 0.9428\n",
            "Epoch 801/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.6224e-05 - accuracy: 0.9980 - val_loss: 0.0010 - val_accuracy: 0.9425\n",
            "Epoch 802/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.3893e-05 - accuracy: 0.9981 - val_loss: 0.0011 - val_accuracy: 0.9392\n",
            "Epoch 803/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.7977e-05 - accuracy: 0.9978 - val_loss: 0.0012 - val_accuracy: 0.9305\n",
            "Epoch 804/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.2382e-05 - accuracy: 0.9977 - val_loss: 0.0013 - val_accuracy: 0.9262\n",
            "Epoch 805/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.7205e-05 - accuracy: 0.9973 - val_loss: 0.0011 - val_accuracy: 0.9377\n",
            "Epoch 806/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.1102e-05 - accuracy: 0.9947 - val_loss: 0.0013 - val_accuracy: 0.9277\n",
            "Epoch 807/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.5765e-05 - accuracy: 0.9975 - val_loss: 0.0011 - val_accuracy: 0.9410\n",
            "Epoch 808/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.3967e-05 - accuracy: 0.9974 - val_loss: 0.0015 - val_accuracy: 0.9162\n",
            "Epoch 809/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.9188e-05 - accuracy: 0.9949 - val_loss: 0.0012 - val_accuracy: 0.9315\n",
            "Epoch 810/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.7527e-05 - accuracy: 0.9960 - val_loss: 0.0011 - val_accuracy: 0.9403\n",
            "Epoch 811/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.3949e-05 - accuracy: 0.9969 - val_loss: 0.0012 - val_accuracy: 0.9360\n",
            "Epoch 812/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.8706e-05 - accuracy: 0.9966 - val_loss: 0.0010 - val_accuracy: 0.9430\n",
            "Epoch 813/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.1684e-05 - accuracy: 0.9977 - val_loss: 0.0010 - val_accuracy: 0.9415\n",
            "Epoch 814/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.0018e-05 - accuracy: 0.9978 - val_loss: 0.0011 - val_accuracy: 0.9403\n",
            "Epoch 815/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.4725e-05 - accuracy: 0.9982 - val_loss: 9.9461e-04 - val_accuracy: 0.9452\n",
            "Epoch 816/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.6366e-05 - accuracy: 0.9980 - val_loss: 9.4478e-04 - val_accuracy: 0.9470\n",
            "Epoch 817/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.2692e-05 - accuracy: 0.9983 - val_loss: 9.8515e-04 - val_accuracy: 0.9442\n",
            "Epoch 818/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.6769e-05 - accuracy: 0.9986 - val_loss: 0.0010 - val_accuracy: 0.9432\n",
            "Epoch 819/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.2335e-05 - accuracy: 0.9982 - val_loss: 0.0010 - val_accuracy: 0.9420\n",
            "Epoch 820/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.6550e-05 - accuracy: 0.9972 - val_loss: 0.0010 - val_accuracy: 0.9415\n",
            "Epoch 821/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.7454e-05 - accuracy: 0.9979 - val_loss: 9.8409e-04 - val_accuracy: 0.9457\n",
            "Epoch 822/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.3912e-05 - accuracy: 0.9975 - val_loss: 9.7000e-04 - val_accuracy: 0.9475\n",
            "Epoch 823/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.8078e-05 - accuracy: 0.9967 - val_loss: 0.0011 - val_accuracy: 0.9383\n",
            "Epoch 824/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.8943e-05 - accuracy: 0.9954 - val_loss: 0.0011 - val_accuracy: 0.9388\n",
            "Epoch 825/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.8035e-05 - accuracy: 0.9972 - val_loss: 0.0011 - val_accuracy: 0.9385\n",
            "Epoch 826/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.5188e-05 - accuracy: 0.9981 - val_loss: 9.6904e-04 - val_accuracy: 0.9462\n",
            "Epoch 827/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.0382e-05 - accuracy: 0.9983 - val_loss: 0.0011 - val_accuracy: 0.9392\n",
            "Epoch 828/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.7713e-05 - accuracy: 0.9974 - val_loss: 0.0011 - val_accuracy: 0.9400\n",
            "Epoch 829/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.1780e-05 - accuracy: 0.9983 - val_loss: 0.0011 - val_accuracy: 0.9407\n",
            "Epoch 830/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.4599e-05 - accuracy: 0.9974 - val_loss: 9.6986e-04 - val_accuracy: 0.9448\n",
            "Epoch 831/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.6477e-05 - accuracy: 0.9971 - val_loss: 0.0011 - val_accuracy: 0.9368\n",
            "Epoch 832/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.3109e-05 - accuracy: 0.9971 - val_loss: 0.0011 - val_accuracy: 0.9365\n",
            "Epoch 833/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.3412e-05 - accuracy: 0.9964 - val_loss: 0.0013 - val_accuracy: 0.9272\n",
            "Epoch 834/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.6261e-05 - accuracy: 0.9962 - val_loss: 0.0010 - val_accuracy: 0.9417\n",
            "Epoch 835/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.3121e-05 - accuracy: 0.9957 - val_loss: 0.0011 - val_accuracy: 0.9347\n",
            "Epoch 836/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.6252e-05 - accuracy: 0.9974 - val_loss: 9.6605e-04 - val_accuracy: 0.9465\n",
            "Epoch 837/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.8100e-05 - accuracy: 0.9973 - val_loss: 0.0011 - val_accuracy: 0.9405\n",
            "Epoch 838/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.0091e-05 - accuracy: 0.9977 - val_loss: 0.0011 - val_accuracy: 0.9408\n",
            "Epoch 839/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.0109e-05 - accuracy: 0.9972 - val_loss: 0.0013 - val_accuracy: 0.9298\n",
            "Epoch 840/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.4484e-05 - accuracy: 0.9975 - val_loss: 0.0011 - val_accuracy: 0.9380\n",
            "Epoch 841/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.3827e-05 - accuracy: 0.9970 - val_loss: 0.0013 - val_accuracy: 0.9262\n",
            "Epoch 842/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.4944e-05 - accuracy: 0.9981 - val_loss: 9.7347e-04 - val_accuracy: 0.9463\n",
            "Epoch 843/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.9223e-05 - accuracy: 0.9985 - val_loss: 0.0010 - val_accuracy: 0.9448\n",
            "Epoch 844/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.5578e-05 - accuracy: 0.9981 - val_loss: 0.0011 - val_accuracy: 0.9373\n",
            "Epoch 845/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.3044e-05 - accuracy: 0.9982 - val_loss: 0.0012 - val_accuracy: 0.9328\n",
            "Epoch 846/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.5863e-05 - accuracy: 0.9980 - val_loss: 0.0011 - val_accuracy: 0.9363\n",
            "Epoch 847/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.2574e-04 - accuracy: 0.9926 - val_loss: 0.0014 - val_accuracy: 0.9215\n",
            "Epoch 848/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 9.9113e-05 - accuracy: 0.9940 - val_loss: 0.0011 - val_accuracy: 0.9368\n",
            "Epoch 849/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.5328e-05 - accuracy: 0.9958 - val_loss: 9.5385e-04 - val_accuracy: 0.9467\n",
            "Epoch 850/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.7563e-05 - accuracy: 0.9980 - val_loss: 0.0010 - val_accuracy: 0.9427\n",
            "Epoch 851/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.6247e-05 - accuracy: 0.9980 - val_loss: 0.0010 - val_accuracy: 0.9437\n",
            "Epoch 852/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.3272e-05 - accuracy: 0.9983 - val_loss: 9.2135e-04 - val_accuracy: 0.9487\n",
            "Epoch 853/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.6260e-05 - accuracy: 0.9987 - val_loss: 9.1465e-04 - val_accuracy: 0.9488\n",
            "Epoch 854/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.7439e-05 - accuracy: 0.9986 - val_loss: 8.6384e-04 - val_accuracy: 0.9532\n",
            "Epoch 855/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.2882e-05 - accuracy: 0.9988 - val_loss: 9.4744e-04 - val_accuracy: 0.9480\n",
            "Epoch 856/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3309e-05 - accuracy: 0.9988 - val_loss: 8.8539e-04 - val_accuracy: 0.9510\n",
            "Epoch 857/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3033e-05 - accuracy: 0.9988 - val_loss: 8.7205e-04 - val_accuracy: 0.9525\n",
            "Epoch 858/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3148e-05 - accuracy: 0.9988 - val_loss: 9.3413e-04 - val_accuracy: 0.9482\n",
            "Epoch 859/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3245e-05 - accuracy: 0.9988 - val_loss: 8.3571e-04 - val_accuracy: 0.9547\n",
            "Epoch 860/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3119e-05 - accuracy: 0.9988 - val_loss: 8.2863e-04 - val_accuracy: 0.9550\n",
            "Epoch 861/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3048e-05 - accuracy: 0.9988 - val_loss: 8.2228e-04 - val_accuracy: 0.9557\n",
            "Epoch 862/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3119e-05 - accuracy: 0.9988 - val_loss: 8.2163e-04 - val_accuracy: 0.9550\n",
            "Epoch 863/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3101e-05 - accuracy: 0.9988 - val_loss: 8.2185e-04 - val_accuracy: 0.9548\n",
            "Epoch 864/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3111e-05 - accuracy: 0.9988 - val_loss: 8.2069e-04 - val_accuracy: 0.9550\n",
            "Epoch 865/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3101e-05 - accuracy: 0.9988 - val_loss: 8.1999e-04 - val_accuracy: 0.9552\n",
            "Epoch 866/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3098e-05 - accuracy: 0.9988 - val_loss: 8.2075e-04 - val_accuracy: 0.9553\n",
            "Epoch 867/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.2821e-05 - accuracy: 0.9988 - val_loss: 8.7419e-04 - val_accuracy: 0.9517\n",
            "Epoch 868/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.4147e-05 - accuracy: 0.9988 - val_loss: 9.0634e-04 - val_accuracy: 0.9495\n",
            "Epoch 869/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3246e-05 - accuracy: 0.9988 - val_loss: 9.0296e-04 - val_accuracy: 0.9495\n",
            "Epoch 870/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3802e-05 - accuracy: 0.9988 - val_loss: 8.7295e-04 - val_accuracy: 0.9520\n",
            "Epoch 871/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3281e-05 - accuracy: 0.9988 - val_loss: 8.4781e-04 - val_accuracy: 0.9533\n",
            "Epoch 872/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3771e-05 - accuracy: 0.9988 - val_loss: 8.4619e-04 - val_accuracy: 0.9532\n",
            "Epoch 873/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3137e-05 - accuracy: 0.9988 - val_loss: 8.5238e-04 - val_accuracy: 0.9532\n",
            "Epoch 874/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3088e-05 - accuracy: 0.9988 - val_loss: 8.5175e-04 - val_accuracy: 0.9528\n",
            "Epoch 875/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3095e-05 - accuracy: 0.9988 - val_loss: 8.5271e-04 - val_accuracy: 0.9527\n",
            "Epoch 876/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3015e-05 - accuracy: 0.9988 - val_loss: 8.6339e-04 - val_accuracy: 0.9515\n",
            "Epoch 877/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3142e-05 - accuracy: 0.9988 - val_loss: 8.4451e-04 - val_accuracy: 0.9520\n",
            "Epoch 878/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.2954e-05 - accuracy: 0.9988 - val_loss: 8.4641e-04 - val_accuracy: 0.9520\n",
            "Epoch 879/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.3457e-05 - accuracy: 0.9988 - val_loss: 8.4113e-04 - val_accuracy: 0.9533\n",
            "Epoch 880/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.8327e-05 - accuracy: 0.9985 - val_loss: 0.0011 - val_accuracy: 0.9390\n",
            "Epoch 881/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 5.2739e-05 - accuracy: 0.9970 - val_loss: 0.0012 - val_accuracy: 0.9348\n",
            "Epoch 882/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.4228e-04 - accuracy: 0.9920 - val_loss: 0.0014 - val_accuracy: 0.9227\n",
            "Epoch 883/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.4648e-04 - accuracy: 0.9916 - val_loss: 0.0013 - val_accuracy: 0.9290\n",
            "Epoch 884/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.5913e-04 - accuracy: 0.9906 - val_loss: 0.0014 - val_accuracy: 0.9230\n",
            "Epoch 885/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.2725e-04 - accuracy: 0.9927 - val_loss: 0.0011 - val_accuracy: 0.9375\n",
            "Epoch 886/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.9562e-05 - accuracy: 0.9961 - val_loss: 0.0013 - val_accuracy: 0.9313\n",
            "Epoch 887/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.7527e-05 - accuracy: 0.9950 - val_loss: 0.0014 - val_accuracy: 0.9237\n",
            "Epoch 888/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.2200e-05 - accuracy: 0.9955 - val_loss: 0.0011 - val_accuracy: 0.9405\n",
            "Epoch 889/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.4515e-05 - accuracy: 0.9970 - val_loss: 0.0010 - val_accuracy: 0.9447\n",
            "Epoch 890/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.5851e-05 - accuracy: 0.9975 - val_loss: 0.0010 - val_accuracy: 0.9418\n",
            "Epoch 891/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.9275e-05 - accuracy: 0.9976 - val_loss: 0.0011 - val_accuracy: 0.9397\n",
            "Epoch 892/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.8814e-05 - accuracy: 0.9972 - val_loss: 0.0010 - val_accuracy: 0.9450\n",
            "Epoch 893/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.3709e-05 - accuracy: 0.9981 - val_loss: 0.0010 - val_accuracy: 0.9417\n",
            "Epoch 894/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.3587e-05 - accuracy: 0.9981 - val_loss: 9.1138e-04 - val_accuracy: 0.9495\n",
            "Epoch 895/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.1307e-05 - accuracy: 0.9983 - val_loss: 0.0010 - val_accuracy: 0.9423\n",
            "Epoch 896/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.6739e-05 - accuracy: 0.9979 - val_loss: 0.0011 - val_accuracy: 0.9420\n",
            "Epoch 897/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.0975e-05 - accuracy: 0.9971 - val_loss: 0.0012 - val_accuracy: 0.9358\n",
            "Epoch 898/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.8054e-05 - accuracy: 0.9967 - val_loss: 9.8545e-04 - val_accuracy: 0.9453\n",
            "Epoch 899/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.7022e-05 - accuracy: 0.9980 - val_loss: 0.0010 - val_accuracy: 0.9450\n",
            "Epoch 900/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.6979e-05 - accuracy: 0.9975 - val_loss: 0.0011 - val_accuracy: 0.9405\n",
            "Epoch 901/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 3.7960e-05 - accuracy: 0.9980 - val_loss: 0.0011 - val_accuracy: 0.9403\n",
            "Epoch 902/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.9672e-05 - accuracy: 0.9977 - val_loss: 0.0011 - val_accuracy: 0.9377\n",
            "Epoch 903/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.1005e-05 - accuracy: 0.9977 - val_loss: 0.0011 - val_accuracy: 0.9402\n",
            "Epoch 904/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.4916e-05 - accuracy: 0.9980 - val_loss: 0.0010 - val_accuracy: 0.9415\n",
            "Epoch 905/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 3.7127e-05 - accuracy: 0.9978 - val_loss: 9.7983e-04 - val_accuracy: 0.9442\n",
            "Epoch 906/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.1333e-05 - accuracy: 0.9982 - val_loss: 9.2107e-04 - val_accuracy: 0.9478\n",
            "Epoch 907/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.7665e-05 - accuracy: 0.9985 - val_loss: 8.8048e-04 - val_accuracy: 0.9498\n",
            "Epoch 908/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.4645e-05 - accuracy: 0.9987 - val_loss: 9.2506e-04 - val_accuracy: 0.9492\n",
            "Epoch 909/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.6967e-05 - accuracy: 0.9986 - val_loss: 9.3922e-04 - val_accuracy: 0.9480\n",
            "Epoch 910/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.7762e-05 - accuracy: 0.9973 - val_loss: 0.0012 - val_accuracy: 0.9327\n",
            "Epoch 911/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.5917e-05 - accuracy: 0.9973 - val_loss: 9.1630e-04 - val_accuracy: 0.9498\n",
            "Epoch 912/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.0112e-05 - accuracy: 0.9977 - val_loss: 0.0010 - val_accuracy: 0.9417\n",
            "Epoch 913/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.4971e-05 - accuracy: 0.9967 - val_loss: 0.0012 - val_accuracy: 0.9357\n",
            "Epoch 914/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.3630e-05 - accuracy: 0.9958 - val_loss: 0.0010 - val_accuracy: 0.9432\n",
            "Epoch 915/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.8037e-05 - accuracy: 0.9974 - val_loss: 8.9811e-04 - val_accuracy: 0.9498\n",
            "Epoch 916/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.1884e-05 - accuracy: 0.9977 - val_loss: 0.0010 - val_accuracy: 0.9415\n",
            "Epoch 917/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.3318e-05 - accuracy: 0.9977 - val_loss: 9.6285e-04 - val_accuracy: 0.9477\n",
            "Epoch 918/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.6705e-05 - accuracy: 0.9981 - val_loss: 0.0011 - val_accuracy: 0.9418\n",
            "Epoch 919/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.9910e-05 - accuracy: 0.9978 - val_loss: 0.0011 - val_accuracy: 0.9415\n",
            "Epoch 920/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.0656e-05 - accuracy: 0.9973 - val_loss: 9.5155e-04 - val_accuracy: 0.9463\n",
            "Epoch 921/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.7187e-05 - accuracy: 0.9979 - val_loss: 8.8377e-04 - val_accuracy: 0.9517\n",
            "Epoch 922/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 3.0596e-05 - accuracy: 0.9984 - val_loss: 8.8639e-04 - val_accuracy: 0.9502\n",
            "Epoch 923/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.2286e-05 - accuracy: 0.9983 - val_loss: 8.6760e-04 - val_accuracy: 0.9515\n",
            "Epoch 924/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 3.0286e-05 - accuracy: 0.9984 - val_loss: 9.3801e-04 - val_accuracy: 0.9475\n",
            "Epoch 925/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.6477e-05 - accuracy: 0.9980 - val_loss: 0.0012 - val_accuracy: 0.9350\n",
            "Epoch 926/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 6.4678e-05 - accuracy: 0.9964 - val_loss: 0.0011 - val_accuracy: 0.9405\n",
            "Epoch 927/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.2123e-05 - accuracy: 0.9955 - val_loss: 0.0016 - val_accuracy: 0.9137\n",
            "Epoch 928/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.2850e-05 - accuracy: 0.9952 - val_loss: 0.0011 - val_accuracy: 0.9402\n",
            "Epoch 929/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.9464e-05 - accuracy: 0.9962 - val_loss: 0.0014 - val_accuracy: 0.9243\n",
            "Epoch 930/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.4980e-05 - accuracy: 0.9968 - val_loss: 9.7936e-04 - val_accuracy: 0.9462\n",
            "Epoch 931/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 5.4860e-05 - accuracy: 0.9970 - val_loss: 0.0012 - val_accuracy: 0.9345\n",
            "Epoch 932/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 4.3999e-05 - accuracy: 0.9975 - val_loss: 0.0010 - val_accuracy: 0.9435\n",
            "Epoch 933/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.5907e-05 - accuracy: 0.9975 - val_loss: 9.8348e-04 - val_accuracy: 0.9452\n",
            "Epoch 934/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 3.6037e-05 - accuracy: 0.9980 - val_loss: 0.0010 - val_accuracy: 0.9445\n",
            "Epoch 935/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.9121e-05 - accuracy: 0.9984 - val_loss: 8.8806e-04 - val_accuracy: 0.9512\n",
            "Epoch 936/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.3558e-05 - accuracy: 0.9975 - val_loss: 9.9807e-04 - val_accuracy: 0.9453\n",
            "Epoch 937/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.1737e-05 - accuracy: 0.9965 - val_loss: 0.0011 - val_accuracy: 0.9372\n",
            "Epoch 938/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 6.0842e-05 - accuracy: 0.9964 - val_loss: 0.0012 - val_accuracy: 0.9322\n",
            "Epoch 939/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.5004e-05 - accuracy: 0.9975 - val_loss: 0.0011 - val_accuracy: 0.9387\n",
            "Epoch 940/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 3.6728e-05 - accuracy: 0.9980 - val_loss: 0.0012 - val_accuracy: 0.9330\n",
            "Epoch 941/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.0287e-05 - accuracy: 0.9977 - val_loss: 0.0012 - val_accuracy: 0.9350\n",
            "Epoch 942/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.0744e-05 - accuracy: 0.9978 - val_loss: 9.6405e-04 - val_accuracy: 0.9472\n",
            "Epoch 943/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.7200e-05 - accuracy: 0.9986 - val_loss: 9.2243e-04 - val_accuracy: 0.9502\n",
            "Epoch 944/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.3263e-05 - accuracy: 0.9982 - val_loss: 9.4763e-04 - val_accuracy: 0.9473\n",
            "Epoch 945/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.9857e-05 - accuracy: 0.9977 - val_loss: 0.0010 - val_accuracy: 0.9445\n",
            "Epoch 946/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.2324e-05 - accuracy: 0.9977 - val_loss: 9.2171e-04 - val_accuracy: 0.9490\n",
            "Epoch 947/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 2.9716e-05 - accuracy: 0.9985 - val_loss: 9.5418e-04 - val_accuracy: 0.9477\n",
            "Epoch 948/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 2.4065e-05 - accuracy: 0.9988 - val_loss: 8.9727e-04 - val_accuracy: 0.9508\n",
            "Epoch 949/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 2.3093e-05 - accuracy: 0.9988 - val_loss: 8.9699e-04 - val_accuracy: 0.9507\n",
            "Epoch 950/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 2.3103e-05 - accuracy: 0.9988 - val_loss: 8.9756e-04 - val_accuracy: 0.9507\n",
            "Epoch 951/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 2.5140e-05 - accuracy: 0.9987 - val_loss: 9.5281e-04 - val_accuracy: 0.9477\n",
            "Epoch 952/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 2.4944e-05 - accuracy: 0.9987 - val_loss: 8.7457e-04 - val_accuracy: 0.9512\n",
            "Epoch 953/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.6422e-05 - accuracy: 0.9986 - val_loss: 9.1052e-04 - val_accuracy: 0.9497\n",
            "Epoch 954/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.4746e-05 - accuracy: 0.9987 - val_loss: 9.5131e-04 - val_accuracy: 0.9478\n",
            "Epoch 955/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.1602e-05 - accuracy: 0.9966 - val_loss: 0.0010 - val_accuracy: 0.9422\n",
            "Epoch 956/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 6.5538e-05 - accuracy: 0.9963 - val_loss: 0.0013 - val_accuracy: 0.9272\n",
            "Epoch 957/1000\n",
            "47/47 [==============================] - 3s 74ms/step - loss: 1.1941e-04 - accuracy: 0.9933 - val_loss: 0.0013 - val_accuracy: 0.9272\n",
            "Epoch 958/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 7.0256e-05 - accuracy: 0.9961 - val_loss: 0.0010 - val_accuracy: 0.9440\n",
            "Epoch 959/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.2583e-05 - accuracy: 0.9983 - val_loss: 0.0010 - val_accuracy: 0.9447\n",
            "Epoch 960/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.1637e-05 - accuracy: 0.9982 - val_loss: 0.0010 - val_accuracy: 0.9435\n",
            "Epoch 961/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.3567e-05 - accuracy: 0.9982 - val_loss: 8.9539e-04 - val_accuracy: 0.9502\n",
            "Epoch 962/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 3.0119e-05 - accuracy: 0.9984 - val_loss: 9.6518e-04 - val_accuracy: 0.9458\n",
            "Epoch 963/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.9448e-05 - accuracy: 0.9985 - val_loss: 0.0010 - val_accuracy: 0.9458\n",
            "Epoch 964/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.4581e-05 - accuracy: 0.9981 - val_loss: 9.1865e-04 - val_accuracy: 0.9502\n",
            "Epoch 965/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.8789e-05 - accuracy: 0.9979 - val_loss: 0.0015 - val_accuracy: 0.9145\n",
            "Epoch 966/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 1.0757e-04 - accuracy: 0.9936 - val_loss: 0.0013 - val_accuracy: 0.9305\n",
            "Epoch 967/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.1467e-05 - accuracy: 0.9953 - val_loss: 0.0011 - val_accuracy: 0.9402\n",
            "Epoch 968/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.5851e-05 - accuracy: 0.9981 - val_loss: 0.0011 - val_accuracy: 0.9398\n",
            "Epoch 969/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 9.4068e-05 - accuracy: 0.9947 - val_loss: 0.0018 - val_accuracy: 0.9035\n",
            "Epoch 970/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.7182e-05 - accuracy: 0.9949 - val_loss: 0.0011 - val_accuracy: 0.9403\n",
            "Epoch 971/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.9076e-05 - accuracy: 0.9973 - val_loss: 0.0011 - val_accuracy: 0.9420\n",
            "Epoch 972/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.1757e-05 - accuracy: 0.9975 - val_loss: 9.4334e-04 - val_accuracy: 0.9475\n",
            "Epoch 973/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 2.7136e-05 - accuracy: 0.9986 - val_loss: 9.2836e-04 - val_accuracy: 0.9482\n",
            "Epoch 974/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 2.8312e-05 - accuracy: 0.9985 - val_loss: 9.0936e-04 - val_accuracy: 0.9497\n",
            "Epoch 975/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.3990e-05 - accuracy: 0.9981 - val_loss: 9.1127e-04 - val_accuracy: 0.9497\n",
            "Epoch 976/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.1927e-05 - accuracy: 0.9977 - val_loss: 0.0010 - val_accuracy: 0.9432\n",
            "Epoch 977/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.5418e-05 - accuracy: 0.9973 - val_loss: 0.0012 - val_accuracy: 0.9332\n",
            "Epoch 978/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.6733e-05 - accuracy: 0.9968 - val_loss: 0.0011 - val_accuracy: 0.9395\n",
            "Epoch 979/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 8.8828e-05 - accuracy: 0.9950 - val_loss: 0.0013 - val_accuracy: 0.9283\n",
            "Epoch 980/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.0255e-05 - accuracy: 0.9973 - val_loss: 0.0012 - val_accuracy: 0.9347\n",
            "Epoch 981/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 3.9101e-05 - accuracy: 0.9978 - val_loss: 0.0011 - val_accuracy: 0.9397\n",
            "Epoch 982/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 3.8757e-05 - accuracy: 0.9979 - val_loss: 9.7921e-04 - val_accuracy: 0.9463\n",
            "Epoch 983/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.4044e-05 - accuracy: 0.9982 - val_loss: 9.6120e-04 - val_accuracy: 0.9475\n",
            "Epoch 984/1000\n",
            "47/47 [==============================] - 3s 72ms/step - loss: 3.7827e-05 - accuracy: 0.9980 - val_loss: 0.0011 - val_accuracy: 0.9370\n",
            "Epoch 985/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.7880e-05 - accuracy: 0.9980 - val_loss: 9.9317e-04 - val_accuracy: 0.9455\n",
            "Epoch 986/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.2685e-05 - accuracy: 0.9982 - val_loss: 0.0010 - val_accuracy: 0.9450\n",
            "Epoch 987/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.1957e-05 - accuracy: 0.9977 - val_loss: 9.3875e-04 - val_accuracy: 0.9478\n",
            "Epoch 988/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.1365e-05 - accuracy: 0.9983 - val_loss: 0.0011 - val_accuracy: 0.9410\n",
            "Epoch 989/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.0489e-05 - accuracy: 0.9977 - val_loss: 0.0010 - val_accuracy: 0.9447\n",
            "Epoch 990/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.3817e-05 - accuracy: 0.9981 - val_loss: 0.0011 - val_accuracy: 0.9397\n",
            "Epoch 991/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 5.4880e-05 - accuracy: 0.9969 - val_loss: 0.0013 - val_accuracy: 0.9275\n",
            "Epoch 992/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.6603e-05 - accuracy: 0.9973 - val_loss: 0.0010 - val_accuracy: 0.9435\n",
            "Epoch 993/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.8893e-05 - accuracy: 0.9979 - val_loss: 9.1736e-04 - val_accuracy: 0.9500\n",
            "Epoch 994/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.0473e-05 - accuracy: 0.9983 - val_loss: 0.0010 - val_accuracy: 0.9430\n",
            "Epoch 995/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.5711e-05 - accuracy: 0.9981 - val_loss: 0.0011 - val_accuracy: 0.9392\n",
            "Epoch 996/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.6832e-05 - accuracy: 0.9980 - val_loss: 0.0011 - val_accuracy: 0.9398\n",
            "Epoch 997/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 3.0882e-05 - accuracy: 0.9984 - val_loss: 9.5843e-04 - val_accuracy: 0.9473\n",
            "Epoch 998/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.2086e-05 - accuracy: 0.9977 - val_loss: 9.8889e-04 - val_accuracy: 0.9460\n",
            "Epoch 999/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.0516e-05 - accuracy: 0.9978 - val_loss: 0.0011 - val_accuracy: 0.9415\n",
            "Epoch 1000/1000\n",
            "47/47 [==============================] - 3s 73ms/step - loss: 4.0768e-05 - accuracy: 0.9976 - val_loss: 0.0011 - val_accuracy: 0.9368\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7708c09-3e74-4fce-acd1-f7077df418aa",
        "id": "XmPbTOUqOiqb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "188/188 [==============================] - 2s 9ms/step - loss: 0.0011 - accuracy: 0.9410\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.0010692388750612736, 0.9409999847412109]"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('lowpass_1s.h5')"
      ],
      "metadata": {
        "id": "2HyH71npFn2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 그래프 분석"
      ],
      "metadata": {
        "id": "hY4i-LmaOiqc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history=history.history\n",
        "epochs = range(1, len(history['loss']) + 1)\n",
        "acc = history['accuracy']\n",
        "loss = history['loss']\n",
        "val_acc = history['val_accuracy']\n",
        "val_loss = history['val_loss']\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.plot(epochs, acc, label='accuracy')\n",
        "plt.plot(epochs, val_acc, label='val_acc')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.plot(epochs, loss, label='loss', color='g')\n",
        "plt.plot(epochs, val_loss, label='val_loss', color='r')\n",
        "plt.legend()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 701
        },
        "outputId": "a11fd5d5-1d19-44a4-9349-9d2845878798",
        "id": "bo9BenMoOiqc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f11c467f370>"
            ]
          },
          "metadata": {},
          "execution_count": 108
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmEAAAFNCAYAAABIc7ibAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdd3ib1fXA8e+VZMl7j8SJnU1CQnYgCXvvWcoquxQoFFra0s2vUFo6gLaUQlsoZZW9V1llj7IJZO/pON57SNa4vz+OFMmO7TjBshznfJ5Hj/ROHb1y8h6de9/7GmstSimllFJqYDkSHYBSSiml1O5IkzCllFJKqQTQJEwppZRSKgE0CVNKKaWUSgBNwpRSSimlEkCTMKWUUkqpBNAkTKkhxBjzkjHm/P5eN5GMMeuNMYfHYb9vGWO+FX59tjHm1b6suxPvU2qMaTHGOHc2VqXU0KRJmFIJFj5BRx4hY0x7zPTZO7Iva+0x1tr7+nvdwcgY81NjzDvdzM83xnQYY/bq676stQ9aa4/sp7g6JY3W2o3W2nRrbbA/9t/N+xljzFpjzNJ47F8pFT+ahCmVYOETdLq1Nh3YCJwQM+/ByHrGGFfiohyUHgD2NcaM6TL/TGCRtXZxAmJKhAOBQmCsMWbvgXxj/ZtU6qvRJEypQcoYc7AxpswY8xNjTAVwjzEmxxjzgjGm2hhTH349Mmab2Ca2C4wx7xljbg6vu84Yc8xOrjvGGPOOMabZGPOaMeZ2Y8wDPcTdlxh/bYx5P7y/V40x+THLzzXGbDDG1BpjftHT8bHWlgFvAOd2WXQecP/24ugS8wXGmPdipo8wxiw3xjQaY24DTMyyccaYN8Lx1RhjHjTGZIeX/RsoBZ4PVzJ/bIwZbYyxkYTFGFNsjHnOGFNnjFltjLk4Zt/XGWMeM8bcHz42S4wxc3o6BmHnA88CL4Zfx36uKcaY/4bfq9IY8/PwfKcx5ufGmDXh9/nMGFPSNdbwul3/Tt43xvzZGFMLXNfb8QhvU2KMeSr8PdQaY24zxrjDMU2NWa/QGNNmjCnYzudVasjQJEypwW0YkAuMAi5B/s3eE54uBdqB23rZfi6wAsgHbgT+ZYwxO7HuQ8DHQB5wHdsmPrH6EuM3gAuRCo4buBrAGDMZ+Ht4/8Xh9+s2cQq7LzYWY8xEYEY43h09VpF95ANPAdcgx2INsF/sKsDvwvHtCZQgxwRr7bl0rmbe2M1bPAKUhbf/OvBbY8yhMctPDK+TDTzXW8zGmNTwPh4MP840xrjDyzKA14CXw+81Hng9vOkPgLOAY4FM4JtAW68HJmousBYoAm7o7XgY6Qf3ArABGA2MAB6x1naEP+M5Mfs9C3jdWlvdxziU2vVZa/WhD30MkgewHjg8/PpgoANI7mX9GUB9zPRbwLfCry8AVscsSwUsMGxH1kUSmACQGrP8AeCBPn6m7mK8Jmb6cuDl8OtfIifpyLK08DE4vId9pwJNwL7h6RuAZ3fyWL0Xfn0e8GHMegZJmr7Vw35PBhZ09x2Gp0eHj6ULSVCCQEbM8t8B94ZfXwe8FrNsMtDey7E9B6gO7zsZaAROCS87KzauLtutAE7qZv7WWHs5Thu3831vPR7A/Eh83aw3F0lYTXj6U+D0RP7704c+BvqhlTClBrdqa603MmGMSTXG3BFurmsC3gGyTc9X3lVEXlhrI5WO9B1ctxioi5kHsKmngPsYY0XM67aYmIpj922tbQVqe3qvcEyPA+eFq3ZnA/fvQBzd6RqDjZ02xhQZYx4xxmwO7/cBpGLWF5Fj2RwzbwNSIYroemySTc99r84HHrPWBsJ/J08SbZIsQap43elt2fZ0+u63czxKgA3W2kDXnVhrP0I+38HGmElIpe65nYxJqV2SJmFKDW62y/QPgYnAXGttJtIpG2L6LMXBFiA33PQVUdLL+l8lxi2x+w6/Z952trkPOB04AsgAnv+KcXSNwdD58/4W+V6mhvd7Tpd9dv3OYpUjxzIjZl4psHk7MW0j3L/tUOAcY0yFkX6DXweODTepbgLG9rD5JmBcN/Nbw8+x3/WwLut0/Xy9HY9NQGkvSeR94fXPBZ6I/cGh1O5AkzCldi0ZSN+mBmNMLnBtvN/QWrsBaSq6Ltyhej5wQpxifAI43hizf7hv0/Vs//+pd4EG4E6i/Y2+Shz/AaYYY74WTh6+S+dEJANoARqNMSOAH3XZvpIekh9r7Sbgf8DvjDHJxphpwEVI9WhHnQusRBLNGeHHHkjT6VlIX6zhxpirjDEeY0yGMWZueNu7gF8bYyYYMc0Yk2elP9ZmJLFzGmO+SffJWqzejsfHSFL7e2NMWvgzx/avewA4BUnE7t+JY6DULk2TMKV2LbcAKUAN8CHS6XognI3076kFfgM8Cvh6WHenY7TWLgG+g3Ss3wLUI0lFb9tY5AQ+is4n8p2Kw1pbA5wG/B75vBOA92NW+RUwC+l/9R+kE3+s3wHXGGMajDFXd/MWZyF9r8qBp4FrrbWv9SW2Ls4H/matrYh9AP8Azg83eR6BJMwVwCrgkPC2fwIeA15F+tT9CzlWABcjiVQtMAVJGnvT4/GwMjbaCUhT40bkuzwjZvkm4HOkkvbujh8CpXZtkQ6RSinVZ8aYR4Hl1tq4V+LU0GaMuRsot9Zek+hYlBpomoQppbbLyCCgdcA64EjgGWC+tXZBQgNTuzRjzGjgC2CmtXZdYqNRauBpc6RSqi+GIUMVtAC3ApdpAqa+CmPMr4HFwE2agKndlVbClFJKKaUSQCthSimllFIJoEmYUkoppVQC9DSA3qCVn59vR48enegwlFJKKaW267PPPqux1nZ7Y/pdLgkbPXo0n376aaLDUEoppZTaLmPMhp6WaXOkUkoppVQCaBKmlFJKKZUAmoQppZRSSiWAJmFKKaWUUgmgSZhSSimlVAJoEqaUUkoplQBxS8KMMXcbY6qMMYt7WG6MMbcaY1YbYxYaY2bFKxallFJKqcEmnpWwe4Gje1l+DDAh/LgE+HscY1FKKaWUGlTiloRZa98B6npZ5STgfis+BLKNMcPjFY9SSiml1GCSyBHzRwCbYqbLwvO2JCYcpZQaGkIhi8NhqG/toN0fJCfVDUB5YzuhkGV8YTrGGPzBEC6HwRizddtgyFLd7KMo00Nlk4+CDA9Oh8HrD9LqC+ALhAiGrDysZVxBercxWGtp7QhS39pBMGRJdTupbe2gIMNDdkoSG+racDsdFGen4HSYbrf3BUKsq2klOzWJto4gaW4X9W0dpCQ5Kc1NxeEwhEIWgKpmH8ZAQboHY8DrD5Gc5MDrD1HZ5KWtI8i4wjQ8LicAjW1+alt9jM5LY1N9G8OyknE7HRhjCIYsq6taGJaVTHWzD4/LwcicFIwxtHcEWVPdQnWLj2SXk8nFmaR7XJQ3tFPX2sFeI7KIfJy2jiBOh2FjXRt1rR1kpyZRkpOK2+XAAC6ng2avn7rWDto6gnj9QfLSPOSkJZGc5GRhWSNOh6EkJ4W8dA8AgWCIZm+ANI+LJeWN+AIh8tM9jC9MZ11NK5vq2qhq9uEPhjCAwxgw8uwIPxsDJjxtkGenwzB3TB5ZqUmd/haavX7K6ttxOQ0jc1Kpb+3AFwjhMFDf1kF1sw+HMew9OpdUjxNroay+DadDajzlDe1kpSQxeXgmDofBWktlk4+NdW1MGp6BwxgqGr1UNHpxOgzJSQ5WVbVAOPbCDA8HTMinqtnH2upW/MEQqW4n00Zm09juJz/dzYrKZrY0eBlbkEZpbiodwRAba9to7Qhu/VtKcTsZkZ3CiopmCB/74VnJZCS7SHUnLhXaJW5bZIy5BGmypLS0NMHRKKWGgvaOIJVNXqpbfMwoyaa2pYMP1tZQ3+pnSnEmuWluCjOT2VjbRorbQWO7n5KcVAoy5GRY3uhleGYyFU1eXly0hfGF6bidDlZWNlPe6KWp3Y/DYXA7HYzKS6WpPcBhexYyvjCd9bWtvL6sitqWDlp9AcYWpDEmP43NDe18sKaWFl8Aa8HlNGSmJJHkMIzOTyMn1U2LL8Da6lYa2jooykqmvSPI+tpWijKSmVaSxYuLtrBsSzOFGR62NHoB8LgchKzFH5SEJTfNTarbSVl9O6PyUslOSWJTfTsha8Mn3gDZqUk0tPkBOGXmCF5YWL51+1h5aW4uOXAsCzY2sGRLI9XNPgxyMq0Pb99VhsdFsy8AwLDMZPYdn8d7q2pwuxzY8Fs0e/00h49DdwoyPHQEQjR5/XhckmwBpCQ5afcHt75Pa0eAcJ5GZrKLjOQkmr1+WnwyP83t3Hqyzk93M6s0h+UVzWysa+v0fiW5KZw2u4Tb3lhNRzDU49/V7FE5JDkNH66VhiBj6PEzjMhOYUtj+9b4IhyGTvOMgZE5KeSmullc3kSw6wbACdOLef7L8h7j6ouROSn89/sH8be3VrO2upXXllXiC/T8WbtyOQyBbmIDmDsmlzSPiw/X1tIWPt7pHheBUGjrd9eTEdkpbG5o73aZ02G6PR599ZOjJ3HZweN2evuvytie/jr6Y+fGjAZesNbu1c2yO4C3rLUPh6dXAAdba3uthM2ZM8fqvSOVEqGQpa6tg+QkJ26ng3Z/kBZfgBHZKQTCJwpvIMSG2la+2NRAfroHhzFMKExnVF5qpwoIQFWzl0VljexRlEFJbipNXj+LNzdiMIwvTKe8oZ2lW5rw+YOELGQku8hP9+BxOdh3fD4gvzqNkcqJwxge/WQjLb4gX589krw0Nxvr2njo4420+AI4jWFUXirlDV4a2jqYPy6PdI+LjXVt7DUii8317Rw+uYjslCQ21rWR6nGSneLm3VXVfLK+nkMmFlDb2sHGujYykl3yi91hCFlLe0cIt8tBXauPpvYAQWtxGPC4nLgchme+2Lz1RJeR7KLVF9jmZNgdl8OQkezammBEqi2x3C4HWSlJWGtp8QV6PMkYI4lCkzewdd6wzGRG5qTQEQwRCFrK6ts6LY9wOgwuhyE/3UNJbgpLypukQuJ2cvy0Ypq8fqaNzCY7NYml5U0kJzmYUpxFk9fPIx9vIictiVZfkNpWHzmpbsYXpOML/82sqWqhNDeVpVuaKKuXk9+s0mxOmjECj8uBMeByOHhp8RZeW1a1NaYR2SlkJLtYXtHMlOJMTpxeTE6aG6cxNLb7Kcz0sKmunVVVzcwoycbpMNzz/npWV7UwvSSb0XmpBEIWVzh5zUv3sOfwDJq8AdxOQ8hCktNBTYuP91fXkJfmxuNy4nQaRuel4vOHWFnVQn1rB9NGZlHf1kFhRjKlual4A0EWb26kvSNIdqqb3DQ32alJLNncxPjCdGpafSwqa+R/a2oBOHXWSMYWpFGQ7qHJ6+c3/1kGwJxROXxz/zEUZSbzyfo6lm9pYlhWCqPyUgkEQ9z86ko59iOyOHAP+fscm5/GuMJ0Wn0Bnv58M2+sqMJhDJOGZbDPmFz2Ks4i1S0Vus0N7aytaeW9VTVccch4vIEg9a1+VlU1s7qqhXEF6UwuzsQXCFGam0phhoerHv2CutYODp1UyMUHjKU4O5kkp3xPIcvWamHIWpm2Fmvl32rIgsXyxcYGfvrUok5J6XFThzNlRCbDMpNxuxwsLW/C43IyMicFhwNy0zwUpHsoq29jXU0rC8sacbscHLhHPl6/VEzHF6azYGMD93+wnpC1HDVlGBMK08lN8/DmiiqcxrDPmFyyUpLwJDlo9QWYNCwTZ/jf8YMfbeTOd9YCcPcFc8hITuLVJRX88911W/89fvugcew/IZ+11a089PEGfP4Qlx08juzUJDoCFl9AKrL1bX6mjsjC6TA0hRPxGSXZTCnO2v4//K/AGPOZtXZOt8sSmIQdB1wBHAvMBW611u6zvX1qErZ7sdZS3+bH7XKQ7nHhD58kkpzbdmcMhizra1tZX9NKWX0700ZmMSI7hffX1DBnVC5ZqUmU1bXjDQSZUpxJszdAMGRZV9PKHkUZZCS7qGr28cyCzZw7fxTVzT4e+mgjNS0+irNTOGavYUwenolFyuRvLq9iTXULY/LTeHtlNb5AiI7wI2QtJ88cwbFTh7OmuoXFmxvx+oO0+IKsrmrhk/V15KW5yUh20eILUJSZTElOKo99ugkTThTSPS5mj8ohI9lFaW4qY/LTuOaZxeSmuSnI8HDanBJ+9+IylofL67EmFKbT7A1Q0eTt8djmpCZxyMRCalo7WFreBEBtqw9r5QR/6KRC3l5R3euv/lizSrNxGMPCzY1MKc5kUVkjyUlOWsIVj8IMD83eAP5giEDIkuZ2YoyhxRcgOcmB2+noNtlwuxxkJruoaenoUxyxkpyG3DQ3WSlJrKluJRh+33Z/kFNnjWTWqByyUpJ4a0UVRZnJHL5nEblpbpaUN7F0SxMVje3sOy4fXyBImsfForJG7gifEA7fs4jXllVywIR8rjx0An94eTkzS7K59KBx5Ke7tya4HYEQNS0+UpKc3P7mapZuaWLy8ExOmF7MnsMzSXIaHvhwA06Hg33G5DI6LxVXl7/vlxdvIdXtYtKwDDwuid8YOaaR92ny+llX3UpumpuS3NQdPlbdqWnx8cjHGzlispw4HT00G/7trTW0+AL85OhJ+IMhnvq8jKOnDO/UtNWbxnY/mcmubX4UDDR/MMRVj37BYZMK+dqskZ2W/fqFpSQnOfj+4Xts8/3EqmvtwOsPUpyd0u1yay3t/iAuh4Mkp+mXz/zSoi1UNfs4d96obr+jvrDWcvXjC3nuy83MG5vHXefP2dp0m2hvr6xmeFYyexRlAPI9vbuqmn3G5BEM2k5/Z5HEsrsm7kRJSBJmjHkYOBjIByqBa4EkAGvtP4z85d2GXEHZBlxord1udqVJ2NAS6Q/xzqoaRuel8tc3VpOb6sbhgPIGL19sati6bobHRchaRuen4Q+GaGoPcMqsERy+ZyHVzT5uemUFa6pb+/S+2ythZyS78IWrF0VZ8us9ltvloCOmTJ+c5CA31Y3b5cDtctDiDVDe6CUvzU1ta+fkwe10MCwrmRZfgMZ2P6PyUqlr7aChzY/TYThu6nDa/UEa2/x8vL7ztS3pHhcdwdDW985JTeLbB43DGDnZOxyGFxdtYfHmJuaOySXF7WT6yGzGFaYzdUQW1c0+XE7Dks2NvLmimjeWR6sYc8fkMn9cHvPG5vHDx76ksd3PqbNGMH9cHi6Hg0831DMiJ4W5Y3LJTE7C4YDN9e1sbmhnZWULt76+qlOsI3NSyEmVpqqnF2ze+l6nzR7Jj46aSGFmMoFgiIomL8OzUvD6gyzY2EBGsouslCSWbmmivKGd37+0nEnDMzhn7ij8IcuCDfXkpbv57mETeHHRFhra/MwZnUOrL0izN8DmhjZOmF5MXpqn00muutnHyspm5o/NwxsI7nQ/kDvfWQPAxQeMZcGmBqaNyOr1pKzUrqSxzU96smtQJTG7uoRVwuJBk7DBJxSybG5opyhcsgbpPGqRitXmhnb+t7qGY6YOZ3N9O++srOaJz8rwBYJsqGvr1F+iOCuZFLeTto4gqW4ngZDlkImFJCc5KW9oZ21NC4s3NzE8Kxlga58XgPGF6Vy0/5jwryXLqX//AIBbz5rJsws2M74onekjs2n2+vnv0sqtzSizR+VQ0ejd2ufgnHmlPPDhRooyPTx52b6MzEnlo7W1vLBwC//+cAMH7VHA+MJ0po3M4vrnl1Lb2sHC644kMzn6a8wfDHHnO2v5aF0dHYEgFx8wloIMD/6gZWZJ9tYOqsDWDtJPfFbG5OGZTC/J3rqftdUt/G9NLdc8I8PtffHLI8hMTmJxeSOvL6vi9L1LGNHlF3erL0Bda0efqiGtvgC/en4JB0wo4ITpxVvnN3n9OI0hzdP3RGVNdQtN7X4mF2cSCkGKO/oretmWJt5aUc2lB47d4V/qzV4/aW7XTv/CV0qpRNIkTH0lkauc0j0uKpu81LVKZ+LHPy1jyZZG6lo6KG/0kup28uOjJvLRujreW1VD0FrG5KexqrKFjmCoU9+ZlCQn88flUVbfRorbRSAYYnhWMr8/dRr54auAutPeEeRf763l1NkjyUl189qySq54aAFnzy3lVydO6VSR+GR9HZvr2zl55ohu97W5oZ1Xl1Rwwb6jt1715HIakpwOlpQ3kp3q3ibBqWj0UpQZbQKKXHU1Jj/tqx7mXi0tb8LpMEwclhHX91FKKdW/NAlTOywYsnQEQry2rJIrH14AdN8BeVReKhOLMpg/Lo9XllRsvSJo+sgsvixr3LreVYdPYFNdO62+AHNG53DBvqP7rQlnS2M7wzKTE96fRCmllOqqtyRslxiiQsVXfWsHz31ZTkWTl/8s3EJGsosl4Y7aIMnX3qNzWVLehNcv/ZsmDcvgH+fMZnRMBeisfUr525urmT06l4P2KGDx5kY+XFvLWfuU7lCz1o4antV9B1illFJqMNMkbDf22YZ6rn1uMSsrWrq9Ai4/3UNmiosHLppLcXYKoZDF0vNVJ8lJTn5w5MSt03uNyGKvEfG99FcppZTaVWkStpv6bEMdZ9zxIYGQZfaoHE6eOYKjpwyjvq2Dm15ZwU1fn0ZmctLWkZUB7RitlFJK9SNNwnYTizc3srS8iXW1rRw2qZDLH/yc4uwUHv/2fIoyk7euV5Dh4Z/nddt0rZRSalfSUgWf3QsdrZCSI8+zL4CsERAKgmNwjAO2O9MkbIhbsLGeO95ey8tLKrbO+/tba8hNc3PX+XM6JWBKKTUo+dshSft+9kntGrjnGMgeBWUfh2caIHwR3ob35Viufg08WZBdIvdVCnbA5R+Cs5u0IBQi3Cyy7bKmLZAxDLwNkui11UFqLgR88NbvwNsEB/8U0gs7b7foCRhzYHR+0xZIKwB/Gyx7DpJS5fW0M8AZHv6nuRKSkqGtFnLHdt5fSzUkZ4Kr56vr8Xtln6m52zuKA0aTsCHKWsutr6/mz6+tBGBGSTa/OG7Prbd7uO2smVtHH1ZKqUFr0RPw5EVw5eeQ18s9/kJyq51tqjuhELxxPcw6b9sTd0/a6mDps1I16i7x+PzfsO5tOPIGyCjq2z5707gZtnwBk46D6hWw/l2YfSE0bOgccygImz+Hkr1lumoZ5E8EhwPq10PDRnjgVEmoWiplnamnw0m3QSgAn94Dr/5C5u/9LTAO2PABVC0Ox7Gx+2P05ykwen/42p2SsIVvzs17f4bXrouut/8P4L0/QeEUmHmOLAcIeOHkv0klztcsidmTF0HJPLjoFahaDn+b2/2xWfgonPMUOFzwxz2i86/4DPLHyz5f+AEsfATGHgznPSvLlzwNTg+E/FIR3OdiePRsST6/8zFs+gimnxVN8BJEk7AhatHmRv782kqMgUcvmc/0kiw8LiezS+W+Z3pFoVJDlLVQuRhS86C1BoZPiy5b9ZqcoCcdKyf0+06EuZfA5JP6vn9vE7TVQM4YKF8ATjcM2+bOdFGBDlj8JHz0D9j7IkjNh/GHybIbx8GxN8KMb3Texu+Fh8+EEbNg7dsy7x/7wzE3SpIwal/Y8iUMny5J0oqX4KlLYcRMOQkvfRbKPoFD/0+Sk/f+DJ/cDT/bKPtq2ASLn4C9ToXs0m1jfvpSWPVqNAlqKocDfyTPdx8FTZuj6556lzx/di+8+n9wwX/kmPu90FIBrmRoLIO6dTD5xM6VmpZqSbheuAq8jTD/CvjgNlm2/n1Y8hSceBt88k/5XF8+Ai//FM58CFJy4Z6jYcopcNq9cMeBsg+AUftB/h5S9TnoJ+H39MD870DOaKkYjTkwGseGD2RftWujSdiG/8Fbv4eKhdBeD4seCyd6G+Bbr0ml640bICkN/OE7lbz3J3muWgKv/Ey+66IpsPwFsLfDQ2fI543Y9KEkyV880Pn4H3szrH8P6tbCunfg43/C+7d0Xqe1SpKwz++XBAzkb6WlSo714xd0Xn/tW5KAAdx7HLRWQ3MFHPTjbb//AaTjhA1Ba6tb+N4jX7CqqpkPf3YY2anuRIek1ODXtEVOTu5+HHjX2u4rKQAf3SEnwsI9d3y/oSCseBHyJsiJvqMVJh4r7/Xyz+HD26PrXvSarONrgWe+LfOu/FxOcA9+HTKGww+Xy/wN/4OsEmmiCvqh/Ito1QVgxcvw8Bnyeq9TJblKK4AfrYZ3/ygn6SNvkOPY0SqJ0OMXyEm8J3kTYN8roHgWFEyUysqHf+v9808+GZY+AyfcKgncnybLSRnAnQ4dLfJ69gWSHEWc/m9J2L58SKYnHQ/FMyF3jHyeUEiO1Z+6+U4O/DFULZWEAqSKU78eLn4d7j8ZasO37TrmJkk2b5kGTWWd9zH7QjjiV3DrLNjz+M6xJWeHk6gezslTT5dEKGLviyU5A7imGn5TEP3MJ/ylpyPXveZKqTIdc5Mk5aEg3HEQVC7qfbuUXPjmK3D73t0vP+J6SUJf+nH480VvQ0f+HlCzEo77Iyx8TP5eppwiCXrxTFkn6Iebxke3yy6F8YfDp3dHk8iNH0qid9yf4J+HQNFUGDY1+h33JjUfvr9EmjjjSAdr3Y0sLGvg9Ds+wOsP8cvjJ/PN/cckOiSlEqdunfzH3ZcOyNdlyX/Kx/xBTsjGyEl5yxdScXE4YcGDUlWY+vXO29aukSaXoikyXbEY/n2y/Nqecgps+liej/yN7Le1Bm4aBxnFsNfXYMKRMPYg2TbQAY+eI/1W0oskqZh9AaQXRN9v2fOyTqxh0+D0++DWmT1/RlcKBDrfB5VR+8M3HpUT4j8PkXnnPQf3nwRY2Pe7clJ77xapcHRn/OHRKgPAAT+UpKyrknkwco4ke7ljJImbehoserznmAEueFGSzkiVKPYz73EUvHMTzL0MPvp7z/tIK5DvI2LcYbDm9ej0lK9J5amvjvg1/Pf/ul+WMRyat0SnU/Olerfsue7X3/e7cPivoH6dJMfPfy9abSuYBNXL+xbTcX+UZsYdZS3cvId8z1NOgeeukPlFe8nf4T6XwIs/huZy+Vtd9ao0ZX7rdalWXhczFNG4Q2G/q+TzTz5Z+qA98LXO75eSA5e+C7fEVFCnnhatKsa6+2jY+IH87f5iC9Suhtu65DNnPSrv+5uYfyPuDOhohlnny7+jd26U+WMPkWbced+WhP/E22DWuTt+zHaADtGUCvsAACAASURBVNa6G2jy+rn1tVXc+7/15KW7efl78zsNpKrUoOZrliaij/4OJXOlP0lvAj5wJEX7poD8erehaB+Pliq4dQbMuxzyxsuJ/sKXopUpf7v82q9ZIdUbkGa2Jy+C9/8C5z4Nr/wi2tSRXSp9bgBK58MTF8LX75b3iSQvZz8h2276GII+mbfkaXn+4DbpszLhCHj1GpnXXC7zFz0OV6+UvjFPXyJNbbE2vAfnPiOVl3dvhvoN2x6TioVwz3Gd5znd0vwYMeMb0lTXViPTqXmy77/OivYhArj/xOjr/93aeZ9fuwtK58Kz35HqWvnn0QRs2hnShyc2ATvxNvjgdjj6dzDukM77qlzacwJWui/MPFuO++j9JImZfaFUg97+Q/QzVyyUE/CBV0eTsIN/LsfVFx10mpNuh4dOl2Py/SXSx+jG8I/UnNGdE7BT7oTNn0nC+NTF3cc36biek7BIAjZsmsSXVgDTz+w+CTv/BRhzgLzOGyePS9+R2LNK5O/5tV9Fm/rGHChNdN0ZsZNXthsDc74Jb/++c1PrnG9KVQ+kcvXKz+HUf8nnS0rZtim3ZJ40Jcb23Svu5kfB/O9ItTVW6bzuY8sZLUlY1kiJ05O57Tr5E8DVpcXnxL9I377pZ4InI5qEnfw3yCyWxLN+vfwYSCBNwoaA8oZ2Tr79fapbfJwxp4QfHjmRgoxerhBRqi8WPCC/ICcc0f3yho1ykjCm92a3NW/AmjfhyF/LdCgoiUDJXLlU/ouHo81kIH086tfDwT/rvoK1+TO451j5RfuNR6L7fOBUqVqdepf0U4kkQbFNW+velr46E4+GT+6SBAyinZUjKhZKparr54146hLp2LvgAVjyTHT+g+EKWem+8p/9uzfLOjPOkeRh6TNyzL58uPO+WyrhD6OjzXbDpkJFTFPQ2rfgzoM6J2e546BuTef9NJfLc+TKsvnfkSajtAJpRptzoVQ6Prhdqn1LnoYV/4G0wmgSllEc3U8sd7okB5ET7PnPR5uxQPpMHfwzScJilcztudIQe8XjsTdLleOjf8j0OU90bho2RvoAzTofKpfA/t+Hu8J9y0rmSUIZccAPpJl382dSldr0kVS+9rlUEvzIFXnfWwiNm+Rk/VhMjNPPkAdIEtG18gJyHI65UZrappwiPySaK2Oa8Iz8Ld6+jyROpfO33ceRN0QTsFhp+fKIOPBH0STs3Gfg+pir+06/Hx47T17njN52X301MvwZY6tusUlWwUQ450l5ndwlETr7SbBBqUp2lZobPU4glb0DrpbX578AH98p203/xrbbgvQ9hOh31vW9IZqYXfIW3HmwvN7jGHCnbrtu+jB5NmbHm23jQJOwIeA3/1lKiy/A05fvx4yS7ESHowar8gXS/HPA1ZDSw99JzSppKkjLl0oHwHXRe4DSWgNv/BqGz5DOxNPOkD42viY5OYw7RE5EFYvkqvimLdGmjf2ukn2/+yd48zcyz5Mp26YXda7EvHOTPI76Hcy/XCo87fXSoXzFS9L0t+oVqYi5PNIvZO2bsu0Dp8pzdye9+8Md0K/4TJrXujr1X5KofX5/dN5h10oice+x0Xkb3pPnJU/LSevI30inaYcLTrxVmi9BmpiCfkmGVr8mCdmCcCfktMJoPyaIJmAH/yxclfiDJBLWwotXd07ASubB4dfKUAQAF78B/zxUXu97pXSwXvEf6WQ9K3yCnnFWdPvIiT+7VE5qJ9wqn+mtP8Co+dKRfcxB8v6RpCizeNurEzOK4GebZSiCeZd3nzT3VmkI+KKvCydL1SLyfj31zcsaAWc+KK/Pe1a+04lHd/4R4EySTvCTwxW9eeEk/9gbO+8rZ5Q8amOS2VPu6LLO6G1jyJsgz9POkKatQ/8P0vLkx0AoKM11ngxJXC58SSpULrf8u9nyRXQ/fb1a050q3/mIWXKMr1oEt0yF5KzOF1X09O+6LwombTsvc0Tftp1weO/LYxPk1PzodzXmgO6T0Fil86TqPTZcRXV103/LE77Sv3gmnHaf/FvqmoAd/QepqMVWzwcBTcJ2cUvKG3lpcQWXHzxOEzDVWcNGSQoyi2X6lWvkRBsKStNQV9ZGf/Gf+Nfo/DsPhj1PlE7MXTvgxlY93v+LJGFv3gCf37ft/j+7W/p1RBIwkAQsJQe++4U0wy17vvM2r/xMrsaKJD2RPkmRZrYVL8Hr129bEQL5DzetQCo49es6L7tttjyfcoec/O85Rjpz54wO9/cy8hlOuBVmn9952+Ss6FVo1culI/C8yyX56SotXy7rh85J5jlPQlapVMSKpkgTaNFecpwPvFpOtMeFm/TaG6TJc9oZ0bgveqXz+4yYDd/7Ev57Lcy5SJplJx0r/Zx6UzpXHiBNeuMOlUS8YhHMPDda2fjoH9LxvzuedDjqhm3n73eVJBy9DQEQuaoOJBGPnEzzJ3a/fldjD4Zf1kWTv1Pu6Hyc+yqSaI07VBLBWLHxX/qOxGbCJ/KUbDghJpl3OOWx5/HReaP2jb7+5suSIEQ6/u9I5Sr2O88uhR+v69/BVrNGRl9PPE5+6OSN7599p+REX590W8/rdWfsQfDLmuh0dxX32CtOp5zc/X7mfTuajA8imoTtwoIhy8+fXkxemptLDuhl/Bw1sLxNcjLp7j+LLx+Rqkpv1YEXfiC/Sudesu2yjrbOv/DWvSsJQex/+iCXt993giRh8y7rfHn35/dL5928cdLx3NsAq1/vPJTBczEJRfkCebz+K5mecKRUsErmSlPDqP2kb8yyF6Tjetd+L5Fmszdikq8L/iPbrXhRkhB3KpzxgJygljwjydl/fynrRiojsWaeK32JHo9JkNKHSadzbyMceo38+h4xR76L6hXRq/oi2wa8kmSk5MgJqHp59GRx8E8l2Yo9IeeOlbGWZp4riWbEmQ/27WQ4al/ppPzzLdHv8PBr5Tl7lHwf3Q0imZINh4abS894sOdBS3NGS8f8iN7G1OpN/oRosxPIhQpH3tD9IJ69OeJX21+noy36OnLhwZWf79hgmrHHvmsCtSP7+N7Czk2AXSWlRSucOysppfP391WaD2OP0ZWfR8dJ21nGyFWWDlf/V4vc6fJcPCs+fbB66gqxC9AkbBfV1hHg72+t4ctNDdxyxgyyUhM74NyQFgpJ36GCSZIkdD1BBDokCUlKlQrC6v9K1eiI6+U/2chJwtcs4w+l5MAPlktVx1rpi3HPcdKkkl0Kn/5L1p92mpThPeH/wN77s1R9zn9eBk4MBeG+cPL1043w5m+lHN+wUU727jSpgkUSsClfk6rOw2fB3/eV2LxN8nkqF/d+DLJKpO9MZEyirqZ8TRKbZy+X6ZF7y/AEAGc/Lh2/I4ZNlfhBOjfHSsmRfkuhkCR7zRVylWFX2SXyize22fB7X8DvSqLxxCYhsf/x7/99SSxaqqJJ15kPyXABkf4nmcXRPmwRl30gz9XLJAk7+0k53ml59MkZD8h4Ud31Uynp4RL/rrom2996I9r3LZ52JAHb55Lod789/vCVmsf/WZJe2Pnk8avKGdXzsu8t7N+hS775Cqz6b/d/Czujv45Z187t/SWS3I6Y3T/7cySFm5FX98/+EkiTsF1Qk9fPKbe/z5rqVo6YXMRJM4oTHdKux1pJirrr5Bkr6IdnLpdO1YdeI9Wc856T/5Bfv14SionHblutWf6CPCafJCf2PU+Mnsja6+GRb0Qvj7/8Q2luizS5RfxhtGy733elmSQyMvWbv4Wjfit9piJ+382Ak3Mvk1Gi/zoL5n5btnE4pYntqW9FO+B21wk7YvYFcoVX7Wrp4J6/R/frdR2sM9KsBJJYxl6yH+kY2xuHQ/ojZQzvPH/U/nKcHC7peBtJwr63UCoM5zwpVbfuTkrzviPjZ+WMkf3HJmZ547ZNurqKjCVUPBOubdjxX9+puf1/u5SR/XRS60/H3rT9dSKO/5P8Xc86f7urJlRvCdrOKJ3X89WAQ1HeOBmvLrba/lX8rEyahGOHpNhFaRK2C7r5lRWsrWnlj6dN5+SZIzC7cCm2X236WJqiXr9eTuDH/7nzr9egX5oDxx4EH/5dkoqfbJD5rVUyKvMxN8qvQWulie7hM6QJCqLNae/cFB31ed3bkhD0ZGn4Fhrv3yIVmIjY8YnuOKjzNnkTogM/1q+DF74vfV8geluQO7ts052JR8t/fj/ZIM2HkSaGqV+Xy9BH7ScnwNjkr9PYRwaOv0WSjUVPyKyRPVRsUrokF65k+Mbj0i/LmSSdk1/6sXRYbqvdfuxb9xvTz/GqRVItef3XkhwmpcoAjYV7Rk+SYw+KjrfV1VE3SBNZfzS16L+5/jHtdHmooa+v1d6+iPPgqgNJk7BdzGcb6rj/gw1cuN9oTp09cvsb7OpaqmXsmkP/r/crf+rXw79ihlKoWCjJ08l/h5smwJHXy/3Z3rmxc8fqFS/CM5dFt5tyipzEV74st02BbQdejL3tBmx7O42Jx8p+u4rcRy1SkfFkwdxLo+PXgCRLV34qSeLLP43OX/uWDB1w2C/lUu3Isov+Kx3i0/LlMvuyT6X5smalDJMA2x43Y2D/q+T1odfI7Uoi5l8hx+bdm4GYYSf2OlU6jhd2cwUVbNtUY0Owx5HyAKk6nfhX+OPEzh2A+yJnjPTHilwuf9zN0WWRMYz6wpiE3ydOKdWPZl/Queq+C9IkbBfzr/fWkZWSxI+P6uFkuCsKheQ5tkLha5bEasGDMp6T3wvLn5cEYe635dLp5S/IcAt7HClDFMSacTZ88aBcAu9rlBGoI7yNcmVa48bOCRhIgvXmb+WeZiBXeB3xq84jQndn3KEyHtaeJ0jfn67rz71MOrdP/bo0ER7562hfsff/Eu3XE2l+m/ttSbxWvhzdx/jDJJGYd5m8T8Anla7I5fogFbzWGhmMsy/9OzJjmvvOfVq+g4nHhpOwGMb0nIBFlsfqrpNwxjDpC1PQxyvfIr73xfbXUUrtfgbBOF9flSZhu5Cy+jZeXlzBJQeOI8Xdj5cmJ9o/D5bR0q/8TAZVrFgsiUlHc3Sd2Bu8xva/eug0uPhNWPyUXIFz/vNSEdqyQJZH7vMWcc6Tsv8JR0rCFRlAMCK2n1XehOgVXhe/KX0QXrwaqldKYheZ31YngzG+9OPOTY6x9r2icwUo9oqukXOkIz1EkyJjopeHj5gjieO+341u01M1yRi5yiy9j30lYvtcjQuPM1U8o2/b9ibShNvV7tQPRimltkOTsF3E5oZ29v+DDEZ53vx+7iTaHxo2QubIvvW3aauTErIzKZwwhQehvPsoaUbr6sS/dh4yoavILWMOu1YGM4TO40JNPR1K9pFmvPGHywPkCrtIEvazzZI4LHpcKmHtdVA0ObqPyH4Pu1aqVkuelqbSETFX/Z3QpVkyVnpRz8tOvx/+Ml3GqYodHDFyRdH+V0nlKx4i4+vsHXNrFodTqmJphTu/X/sVL5dXSqndgCZhu4iHP5Jbply432iKs3sYJyhRyr+QjuLH3ixNbb0J+OR+bVNPl0EV170dXRabgM2/Inqz3hln95yERQbtnHCk3KokIjL0QEounPrP7rf1ZEgfq+HTo8NA7HOx3BLls3tkdOuuIqM7RxK5nnzzFWl+LJwsnfB764uUli/DLSx4QPpdRcy/Qqphk47vedv+EDsifkSkKrazrP1q2yul1G5gcI3fr7q1uaGdu99fx+F7FnLtCVMSHc62FoevnNsYHkcp0AH/PkUedWuj6316D/wmXF1Z9FjnBCx3bOdRlWNH4HY4ZViIiNQ8qRhdtVg6tsO2QydExhwy2/kTP+CH2yZUkcuoR+3X+7a9KZ0nfbfGHiTjhW1PRniYkdjRtZ1JUgHbVa7E+9pdcjxnnD0k+moopVS8aSVsF3DbG6sIBC3XnTiIErDqlVC1VIZLiDQnLn5SOqA3lUkndYA3bpBhDdpqYOlz2+5nn0tl/KpQQKpaL/4QCsK39Djs2ui4SmMPktt0WNt5cMwDroam8m2vkksOXxG4M7f1mH0hjD5ARg4fKAdeDXsc3T/9sRJl2mmJjkAppXYpxu5izQZz5syxn376aaLDGDDlDe0ceOObfGNuKdeftNf2N4iHsk+lyW/4DBnhuXKJjLgeq2gqVC7a/r7mXyGXFafm9f/AlbGqV8p9DjOGww+Xx+99lFJKqV4YYz6z1s7pbplWwga515ZVEghZLtwvDvfb6ouONrjrsOj0SbfDf3647XrfeBTuOFAqXiBNeZnF0tE91t7fis+9w7qK3A4ktY+3lFFKKaUGmCZhg5i1lv8uraQkN4Ux+f1437KerH1LhnnY8qU0yTkcsPo1WTb6ABnS4dnvdL9t1ggZTLOtRpoYj71Rbs+z75XR+/HVrRmYBAykz9jhv5LBV5VSSqlBSJOwQeztldW8u6qGHx+9g4Nb7oiAT4YpWP8e3H9SdH7uGLlCbtNH4PTAOU/J7W3+fYokZGc+CMGA9OXqaJFtIs2LI8NV15Sczp3ti2fG73N0FTsqvFJKKTUIaRI2iD37RTlZKUl8a/+x/b9za2HBv+W+hAf+CN76Xefl/z5Fxv1qKoOSuTL6+rhD4bsLIHt0l/HAwmNgHXkDTD0N9vp6/8erlFJKDTGahA1SXn+QV5dUcML0YtyuOIwksuSp6NhbXROwiKYyeZ4Xc2uf3F4SwsJJvd/aRimllFJbaRI2SL2xvIrWjiAnTC+OzxusC9+E2umJ3rcQYNqZsPAReX1tAzRsgJzR8YlBKaWU2o3pYK2D1AsLy8lP9zBvbD9d3RcKSdPjI2fL6+oVUDJPrlYEGRT07Cfga3dEtzFGEzCllFIqTrQSNgiFQpb3V9dy1JQinI5+GC192Qvw+f2w6hWZrl8HlYth6tfh0F/AuENgwhHR9b/1htzSRymllFJxo0nYILS2ppXGdj+zR+Vsf+XeBP2w+nV49GyZNg6wIXjim+BrgonHgjutcwIGMHL2V3tfpZRSSm2XJmGD0Ocb6wG+WhLWXAn3nQA1KyCrBM55Um4LdOsM2PKFzBt7SD9FrJRSSqkdpUnYILRgYz2ZyS7G5qfv/E7+dyvUrpJBV+ddBgUTpS9YxDlPglO/fqWUUipR9Cw8CH2yvp6ZpTk4dqY/2Kd3w8LHwN8ORXvBCbdElzkccPVqSMkGZ1L/BayUUkqpHaZJ2CCzpbGd1VUtnDGnZOd28NbvoaVSXnd3y570gp0PTimllFL9RpOwQebdVXID7AP2yO/7Ri3V8MFfYd07koAlpYK/DZIG4H6TSimllNopcU3CjDFHA38BnMBd1trfd1leCtwHZIfX+am19sV4xjTYvbuqhoIMDxOLdmCIiH/sDy0V0envLoD3/wIzz+3/AJVSSinVL+KWhBljnMDtwBFAGfCJMeY5a+3SmNWuAR6z1v7dGDMZeBEYHa+YdgWfrq9j/tg8jNmB/mCxCdgpd0DGMDi6h1sRKaWUUmpQiOeI+fsAq621a621HcAjwEld1rFAZvh1FlAex3gGvcY2P1savUwuztz+yhHr34++nv4NmH5m/wemlFJKqX4Xz+bIEcCmmOkyYG6Xda4DXjXGXAmkAYfHMZ5Bb0VlM8CONUW+/xdIK4T5l8twFEoppZTaJST63pFnAfdaa0cCxwL/NsZsE5Mx5hJjzKfGmE+rq6sHPMiB8vDHG/G4HEwbmbX9la2FZc/Dxg9h/OGw//dl6AmllFJK7RLiWQnbDMSOszAyPC/WRcDRANbaD4wxyUA+UBW7krX2TuBOgDlz5th4BZxob6+s5sTpxeSle3pf0dcC9x4nI98DFM+Mf3BKKaWU6lfxrIR9AkwwxowxxriBM4HnuqyzETgMwBizJ5AMDN1SVy+avH7qWjsYV7idUfLLPoP7T4wmYAAjZsU3OKWUUkr1u7hVwqy1AWPMFcAryPATd1trlxhjrgc+tdY+B/wQ+Kcx5vtIJ/0LrLVDttLVm421bQCMzkvtfcW7Dt12XtGUOESklFJKqXiK6zhh4TG/Xuwy75cxr5cC+8Uzhl3F+tpWAEbl9TLAaktMkfC0e2HtW7D5c0hKiWtsSimllOp/OmL+ILEhXAkrze2lEvblQ/J87jMw7pDub0uklFJKqV1Coq+OVGEbalspyPCQ5uklL17+IhTPkgRMKaWUUrs0TcIGifW1bT33B/M2weMXwKYPYdheAxqXUkoppeJDmyMHiY21bew3voebdv9jf2jYIK+zSgcuKKWUUkrFjVbCBgGvP0hFk7f7SljF4mgCBpBRNHCBKaWUUiputBI2CGysk075o/K7XBlZtxb+Eb549MKXoK0WJh47wNEppZRSKh40CRsE1tWEh6foemXkwseir0vngzEDGJVSSiml4kmbIweBD9fW4nE5mFDUZbT8xjJ5nnWeJmBKKaXUEKNJ2CDw1opq9hufT6q7S2GyYiGMPRhO/GsiwlJKKaVUHGkSlmC+QJD1ta3sNSKr84KgH6qWwbBpiQlMKaWUUnGlSViCbaxtw1oY27VTfvVyCHbA8OmJCUwppZRScaVJWIKtDXfKH9M1CXvnZnB6oHReAqJSSimlVLxpEpZgZfXtQJd7RjaWwdJnYf53IGtkgiJTSimlVDxpEpZglU1e3C4H2alJ0Znr3wcsTD0tYXEppZRSKr40CUuwikYvwzKTMbFDUNSvAwzkjUtYXEoppZSKL03CEqyiSZKwTurWQuYIcHkSE5RSSiml4k6TsASrbPJSlNU1CVsHuWMSE5BSSimlBoQmYQlkrQ03R3apeNVrEqaUUkoNdZqEJVBjux9fIERRbHOkrxlaqyFHkzCllFJqKNMkLIEqm3wAnZOwpy6RZ62EKaWUUkOaJmEJVNHkBWBYbJ+wFS/Kc/7EBESklFJKqYGiSVgCVTaGk7BIJaytTp4nnwxFkxMUlVJKKaUGgiZhCVTe2I4xUBjpmF+7Rp6nn5W4oJRSSik1IDQJS6AtDV7y0z14XE6ZseI/8ly4Z+KCUkoppdSA0CQsgcob2ymO7Q/2xUMw6XjIGZW4oJRSSik1IDQJS6DyhnaKs1NkorkCWiph1H6JDUoppZRSA0KTsASx1lLe4GV4VjgJ27JQnodPS1xQSimllBowmoQlSGO7n3Z/kOLscHNkRTgJGzY1cUEppZRSasBoEpYg5Q0yPMXW5sjyBZAzGpKzEheUUkoppQaMJmEJUt7QDoSTsPXvw/IXYPiMBEellFJKqYGiSViCbGmUJGx4VjJUL5OZB/4ogREppZRSaiBpEpYgFU1enA5DfrpHbtoNkDs2sUEppZRSasBoEpYgFY0+CjM8OB1GkjDjgKSURIellFJKqQGiSViCVDZ5KYrcM9LXDJ4MMCaxQSmllFJqwGgSliAVTd7ojbt9LeDJTGxASimllBpQmoQlSGWjl2GRWxb5mqQSppRSSqndhiZhCdDqC9DsC1CY6ZEZkeZIpZRSSu02NAlLgMomGah1WGyfMHd6AiNSSiml1EDTJCwBKrpLwrQSppRSSu1WNAlLgEglrCgrGTZ9DLWroGBigqNSSiml1EDSJCwBNtfHjJa/+nUZI2zfKxMclVJKKaUGUlyTMGPM0caYFcaY1caYn/awzunGmKXGmCXGmIfiGc9gsba6leFZyaS6XVC5WEbK1+ZIpZRSarfiiteOjTFO4HbgCKAM+MQY85y1dmnMOhOAnwH7WWvrjTGF8YpnMFlb08qY/DSZqFwCw6YmNiCllFJKDbh4VsL2AVZba9daazuAR4CTuqxzMXC7tbYewFpbFcd4Bo31teEkrKUK6tfByDmJDkkppZRSAyyeSdgIYFPMdFl4Xqw9gD2MMe8bYz40xhwdx3gGBa8/SEObX66M3PiBzCzdN7FBKaWUUmrAxa05cgfefwJwMDASeMcYM9Va2xC7kjHmEuASgNLS0oGOsV/VtnYAkJ/hgcYymZk/IYERKaWUUioR4lkJ2wyUxEyPDM+LVQY8Z631W2vXASuRpKwTa+2d1to51to5BQUFcQt4INQ0+wDIT/dAewNg9L6RSiml1G4onknYJ8AEY8wYY4wbOBN4rss6zyBVMIwx+Ujz5No4xpRwNS2RJMwN3kZIzgSHjhSilFJK7W62e/Y3xpxgjNnhLMFaGwCuAF4BlgGPWWuXGGOuN8acGF7tFaDWGLMUeBP4kbW2dkffa1cSTcI84SQsK8ERKaWUUioR+tIn7AzgFmPMk8Dd1trlfd25tfZF4MUu834Z89oCPwg/dgs1LdInrCDDA94GSM5OcERKKaWUSoTtVristecAM4E1wL3GmA+MMZcYY3R00Z1Q3ewj3eMiOcmplTCllFJqN9anZkZrbRPwBDLW13DgFOBzY4zea2cH1bT4pD8YSBKWopUwpZRSanfUlz5hJxpjngbeApKAfay1xwDTgR/GN7yhR5Iwj0y0N2glTCmllNpN9aVP2KnAn62178TOtNa2GWMuik9YQ1dNSwfjC9IhFITWKkgvSnRISimllEqAvjRHXgd8HJkwxqQYY0YDWGtfj0tUQ1hNi4/8DLfcsigUgMyuNxFQSiml1O6gL0nY40AoZjoYnqd2UEcgREObn4L0ZGgKj1urSZhSSim1W+pLEuYK34AbgPBrd/xCGrpqW2WMsMLMmFsWZWkSppRSSu2O+pKEVccMroox5iSgJn4hDV3V4VsWFaR7oKlcZmolTCmllNot9aVj/reBB40xtwEG2AScF9eohqitSViGBzZtBlcKpOQkOCqllFJKJcJ2kzBr7RpgnjEmPTzdEveohqhOSVhjmTRFGpPgqJRSSimVCH2phGGMOQ6YAiSbcNJgrb0+jnENSZEkLC/dLc2R2hSplFJK7bb6MljrP5D7R16JNEeeBoyKc1xDUlWzj+zUJDwup1wdqUmYUkoptdvqS8f8fa215wH11tpfAfOBPeIb1tBU3eyTTvnBADRv0SsjlVJKqd1YX5Iwb/i5zRhTDPiR+0eqHVTd4pP+YC2Vk7QS/AAAHU9JREFUYENaCVNKKaV2Y31Jwp43xmQDNwGfA+uBh+IZ1FBV3RxOwnSgVqWUUmq312vHfGOMA3jdWtsAPGmMeQFIttY2Dkh0Q4i1lqpmrzRHNq6TmdocqZRSSu22eq2EWWtDwO0x0z5NwHbO5oZ2vP4QYwrSoGGjzMwamdiglFJKKZUwfWmOfN0Yc6oxOqDVV7GyshmAiUUZULUUMoohOSvBUSmllFIqUfqShF2K3LDbZ4xpMsY0G2Oa4hzXkLOqUsa4nVCUAZVLoWhygiNSSimlVCL1ZcT8jIEIZKirbvaRnOQgy+OEmpUw9qBEh6SUUkqpBNpuEmaMObC7+dbad/o/nKGrvs1PbqobWqsg6IOc0YkOSSmllFIJ1JfbFv0o5nUysA/wGXBoXCIaohraOshOdUc75WfrTQeUUkqp3VlfmiNPiJ02xpQAt8QtoiGqvq2DnLSkmCSsNLEBKaWUUiqh+tIxv6syYM/+DmSoa2jzk5PqhsYymaFjhCmllFK7tb70CfsrYMOTDmAGMnK+2gF1bR2ShLXVgCsZ3OmJDkkppZRSCdSXPmGfxrwOAA9ba9+PUzxDUiAYorHdT06aG1pqITUfdNg1pZRSarfWlyTsCcBrrQ0CGGOcxphUa21bfEMbOmpbO7AWCjM8UFUDaXmJDkkppZRSCdanEfOBlJjpFOC1+IQzNFU1+YBwEtZaI5UwpZRSSu3W+pKEJVtrWyIT4dep8Qtp6Klu8QJQkOGRPmFpmoQppZRSu7u+JGGtxphZkQljzGygPX4hDT3bVMLSChIckVJKKaUSrS99wq4CHjfGlAMGGAacEdeohpiqZknC8h1N4G+DrJIER6SUUkqpROvLYK2fGGMmARPDs1ZYa/3xDWtoqWr2kp2ahKc5PEaY3rJIKaWU2u1ttznSGPMdIM1au9hauxhIN8ZcHv/Qho7qZp80RdavlxmahCmllFK7vb70CbvYWtsQmbDW1gMXxy+koaeq2UdhRjI0bpIZ2docqZRSSu3u+pKEOY2JjixqjHEC7viFNPRUNYUrYd5GcCRBkl5cqpRSSu3u+tIx/2XgUWPMHeHpS4GX4hfS0GKtpbrZJ8NTeBshOVNHy1dKKaVUn5KwnwCXAN8OTy9ErpBUfdDUHqAjGJIkrLIJkrMSHZJSSimlBoHtNkdaa0PAR8B6YB/gUGBZfMMaOqqaZaDWwsxk8DWBJzPBESmllFJqMOixEmaM2QM4K/yoAR4FsNYeMjChDQ2RMcIKY5sjlVJKKbXb660Sthypeh1vrd3fWvtXIDgwYQ0dkUqY9AnTSpj6//buPTqr+s73+PubCwkhXEISromQVlq5hBihSpk5SqW2OFVRRwatepRqu5jWYy/T41BbW6dlZuzUtqM9HFeZVlvUlrFYOp4ZR49IPHTVa1CPKOARESRckpAbCSQhCd/zx97EgAQS8uzsJ+HzWovFs3/Pfvbz3dlrhw+//du/LSIiEjhZCLsa2AuUmdm/mNl8ghnzpRequ/aEtWpMmIiIiAS6DWHu/gd3vxY4BygjeHzRGDN7wMw+05ONm9kCM3vbzLaZ2bKTrPeXZuZmNru3O5Dsqg60MjQ9leyMtOBypHrCREREhJ4NzD/o7r9x98uBAuA1gjsmTyqcT2wFcCkwDbjOzKadYL3hwFcJBv8POlWNrYwZkYEd2A2Hm2B0UdwliYiISBLoyWStndy9zt1Xuvv8Hqx+PrDN3be7+2FgNbDwBOv9APgh0NKbWgaKqsYW8rMzoPzBoKHw/HgLEhERkaTQqxDWSxOBXV2WK8K2TmZ2HlDo7v8RYR2xqmpsZezwdPjT/cF4sLEz4i5JREREkkCUIeykzCwF+AnwNz1Y90tmVm5m5dXV1dEXl0DVja0UZrXDkTa48A5ITY+7JBEREUkCUYaw3UDXJ1UXhG1HDQdmAM+Z2Q5gDvDEiQbnh5dAZ7v77Pz8/AhLTqyWtg4aW9qZmBleac3KjbcgERERSRpRhrBXgClmVmRmQ4BrgSeOvunuDe6e5+6T3X0y8CJwhbuXR1hTv6o6EExPMW5Ic9AwdFSM1YiIiEgyiSyEuXs7cBvwNMFjjh5z97fM7PtmdkVU35tMOh9ZlHY0hOXEWI2IiIgkk548wPu0ufuTwJPHtX23m3XnRVlLHI5O1JqbcihoUAgTERGRUGwD888ER58bOdIOBg2ZuhwpIiIiAYWwCFU1tpCaYgzrOBA0aEyYiIiIhBTCIlR1oJW87CGkNOyEYWMgLSPukkRERCRJKIRFqLqplTHDM6H2Pcj9aNzliIiISBJRCItQ1YFW8odnQM27MPojcZcjIiIiSUQhLEJVja1MHHYEmvYphImIiMgxIp2i4kzW3nGEmoOtTDk6R5hCmIiIiHShnrCIVDa24g5FKVVBg8aEiYiISBcKYRHZVRtM0Frge4MG9YSJiIhIFwphEamoCy5Djj5SCxkjIGN4zBWJiIhIMlEIi0hFXdATlt1RD1m5MVcjIiIiyUYhLCJ76pvJH55BanMNDMuLuxwRERFJMgphEdnb0MKEkZlwsEY9YSIiIvIhCmER2dfQwriRmXBoP2SpJ0xERESOpRAWkb0NLYwfkQmHamCYesJERETkWAphEWhsaaOptZ2irBboOAzZ4+IuSURERJKMQlgE9jW0ADA5bX/QkDM5vmJEREQkKSmERWBvGMIKCGfLz5kUYzUiIiKSjBTCIrC3IZioNbctnC1/lEKYiIiIHEshLAJ7G1owg+ENb8OIiZCRHXdJIiIikmQUwiKwr6GFvOwMUve8ChPPi7scERERSUIKYRHY09DC2cPboe49mFAadzkiIiKShBTCIrCvoZmSzMpgYcy0eIsRERGRpKQQFoG9DS1MTQsH5ed/PN5iREREJCkphCVYU2s7jS3tnMUeSB2iOyNFRETkhBTCEmxfOD1F3pHaYKb8lNSYKxIREZFkpBCWYEcnah3RUQvZY2KuRkRERJKVQliCHQ1hQw/XQPbYmKsRERGRZKUQlmB764MQlt68Xz1hIiIi0i2FsASrqDvE+OxU7JB6wkRERKR7CmEJtn3/QeaP2gM45H8s7nJEREQkSSmEJdi71U1cmLYZMCiaF3c5IiIikqQUwhKo7uBh6g+1McmqYPh4GJYbd0kiIiKSpBTCEmhX3SEARnsdZOfHXI2IiIgkM4WwBKqoCyZqzW6vg2G6M1JERES6pxCWQBVhT1hGi6anEBERkZNTCEug3XXNDM9MJeXQfhimy5EiIiLSPYWwBKqoa+ackUfgSJt6wkREROSkFMISqKKumXOGB+PCNFGriIiInIxCWIK4OxV1h/hoVjAuTJcjRURE5GQUwhKk5uBhDh7u4KwhTUGDLkeKiIjISUQawsxsgZm9bWbbzGzZCd7/hpltNrM3zOxZM5sUZT1RenVnHUCXnjCFMBEREeleZCHMzFKBFcClwDTgOjObdtxqrwGz3X0msAb4p6jqiVr5zjqGpKYwMa0JLBWG5sRdkoiIiCSxKHvCzge2uft2dz8MrAYWdl3B3cvcPew64kWgIMJ6IvVOZSNnj8kmbfdLkDcFUnSlV0RERLoXZVKYCOzqslwRtnXnFuA/I6wnUjtqDjE9pwN2/gmmXx13OSIiIpLkkqK7xsxuAGYDP+rm/S+ZWbmZlVdXV/dvcT3Q3nGEXbWHKM6qDRrGFcdbkIiIiCS9KEPYbqCwy3JB2HYMM/s08G3gCndvPdGG3H2lu89299n5+ck39cPu+mbajzhnD6kJGkadFW9BIiIikvSiDGGvAFPMrMjMhgDXAk90XcHMSoGfEwSwqghridR7+w8CUGhhL13OgL3JU0RERPpJZCHM3duB24CngS3AY+7+lpl938yuCFf7EZAN/M7MXjezJ7rZXFLbEYawvLa9MHQ0ZAyPuSIRERFJdmlRbtzdnwSePK7tu11efzrK7+8vO2oOkZ2RRuaB7cGdkSIiIiKnkBQD8we6irpDFOQMxfa/oxAmIiIiPaIQlgC761uYMqIDmioh72NxlyMiIiIDgEJYAuypb6Y4szJYUAgTERGRHlAI66Om1nYamtuYkrI3aFAIExERkR5QCOujPfXNABS2vw8p6TBK01OIiIjIqSmE9dHu+mYyaWXyrt9D0YWQGukNpyIiIjJIKIT10Z76ZiZZJWmt9VB6fdzliIiIyAChENZHu+uaGZ/SECwMHx9vMSIiIjJgKIT10Z76ZqYMawoWssfGW4yIiIgMGAphfbSnvoWijDCEDR8XbzEiIiIyYGgUeR9V1h3g860PBXdGDhkWdzkiIiIyQKgnrA/aO46Q1fResJD70XiLERERkQFFIawPKhtbGe31wcLnfhJvMSIiIjKgKIT1wY79B8kjvDNSg/JFRESkFxTC+uDd6iby7WgIy4+3GBERERlQFML6YHv1QSakHcBTMyBjRNzliIiIyACiuyP74K09DVyceQAbOhbM4i5HRETktLW1tVFRUUFLS0vcpQxImZmZFBQUkJ6e3uPPKISdpsPtR3ijop6SrK0wcW7c5YiIiPRJRUUFw4cPZ/LkyZg6FnrF3ampqaGiooKioqIef06XI0/Tlr0HyG2vZuThSpj0Z3GXIyIi0ictLS3k5uYqgJ0GMyM3N7fXvYgKYadp4846JqfsCxbyPx5vMSIiIgmgAHb6TudnpxB2ml7bVc+MrLpgYdSkeIsRERGRAUch7DRtqqinNLseLBVGTIy7HBEREemh9vb2uEsAFMJOy4GWNnbUHKLY/x/kTIZU3d8gIiKSCFdeeSWzZs1i+vTprFy5EoCnnnqK8847j5KSEubPnw9AU1MTS5Ysobi4mJkzZ/L4448DkJ2d3bmtNWvWcPPNNwNw8803s3TpUi644ALuuOMOXn75ZT75yU9SWlrK3LlzefvttwHo6Ojgm9/8JjNmzGDmzJn87Gc/Y/369Vx55ZWd233mmWe46qqr+ryvSg+n4a3dB5ib8iYFDRvhs/8QdzkiIiIJ9Xf/6y027zmQ0G1OmzCC710+/ZTrPfjgg4wePZrm5mY+8YlPsHDhQr74xS+yYcMGioqKqK2tBeAHP/gBI0eOZNOmTQDU1dWdctsVFRU8//zzpKamcuDAAf74xz+SlpbGunXruPPOO3n88cdZuXIlO3bs4PXXXyctLY3a2lpycnL48pe/THV1Nfn5+Tz00EN84Qtf6NsPBIWw0/Lm7gY+n/osR7LySZl9S9zliIiIDBr3338/a9euBWDXrl2sXLmSCy+8sHPqh9GjRwOwbt06Vq9e3fm5nJycU2570aJFpKamAtDQ0MBNN93EO++8g5nR1tbWud2lS5eSlpZ2zPfdeOONPPLIIyxZsoQXXniBVatW9XlfFcJ66XD7ER5/tYKfpu0nZXwxpGfGXZKIiEhC9aTHKgrPPfcc69at44UXXiArK4t58+Zx7rnnsnXr1h5vo+tdisdPGTFs2LDO13fddRef+tSnWLt2LTt27GDevHkn3e6SJUu4/PLLyczMZNGiRZ0hrS80JqyXynfWsnVfI5PS6zUgX0REJIEaGhrIyckhKyuLrVu38uKLL9LS0sKGDRt47733ADovR15yySWsWLGi87NHL0eOHTuWLVu2cOTIkc4ete6+a+LE4N/xX/3qV53tl1xyCT//+c87B+8f/b4JEyYwYcIEli9fzpIlSxKyvwphvbStqokhtJF1uAZGFsZdjoiIyKCxYMEC2tvbmTp1KsuWLWPOnDnk5+ezcuVKrr76akpKSli8eDEA3/nOd6irq2PGjBmUlJRQVlYGwD333MNll13G3LlzGT9+fLffdccdd/Ctb32L0tLSY+6WvPXWWznrrLOYOXMmJSUl/OY3v+l87/rrr6ewsJCpU6cmZH/N3ROyof4ye/ZsLy8vj+377/rDm2x8bSNP2u2wcAWU3hBbLSIiIomyZcuWhIWLweq2226jtLSUW2458XjwE/0MzWyju88+0foaE9ZLr+yo5apR70IDML4k7nJERESkH8yaNYthw4bx4x//OGHbVAjrhW1VjWzd18iCwjeCS5FjZ8RdkoiIiPSDjRs3JnybGhPWC//xxj7MnMKmN2Dyn4OesSUiIiKnSSGsF9ZvreR/jvotKc01UHh+3OWIiIjIAKbLkT10sLWdz1fey6WpZTBxNky/Ou6SREREZABTCOuhV96t4tKUl2jOnsTQm/8d0ofGXZKIiIgMYLoc2UO1f3qQEXaItM/erQAmIiIifaYQ1gONLW3MqPhXdmV8jPRpl8ddjoiIyBkvOzs77hL6TCGsB17c+Cofs/dJmfoXkJoedzkiIiIyCGhM2Km0NlH63M0AjCu+ON5aRERE+sN/LoN9mxK7zXHFcOk93b69bNkyCgsL+cpXvgLA3XffTVpaGmVlZdTV1dHW1sby5ctZuHDhKb+qqamJhQsXnvBzq1at4t5778XMmDlzJg8//DCVlZUsXbqU7du3A/DAAw8wd+7cBOz0ySmEnULjg1eR17aH53OvYe5HLoy7HBERkUFp8eLFfO1rX+sMYY899hhPP/00t99+OyNGjGD//v3MmTOHK664AjvFPJ2ZmZmsXbv2Q5/bvHkzy5cv5/nnnycvL6/z4dy33347F110EWvXrqWjo4OmpqbI9xcUwk7sUC1byh5lVeMn+MfKlwGYesOPNDmriIicGU7SYxWV0tJSqqqq2LNnD9XV1eTk5DBu3Di+/vWvs2HDBlJSUti9ezeVlZWMGzfupNtyd+68884PfW79+vUsWrSIvLw8AEaPHg3A+vXrWbVqFQCpqamMHDky2p0NRRrCzGwBcB+QCvzC3e857v0MYBUwC6gBFrv7jihrOpX2bc+R9shCpgL/GLZtOu/7FOeMjrMsERGRQW/RokWsWbOGffv2sXjxYh599FGqq6vZuHEj6enpTJ48mZaWllNu53Q/198iG5hvZqnACuBSYBpwnZlNO261W4A6dz8b+Cnww6jq6al1ezLY7bnHtM2Y+7mYqhERETlzLF68mNWrV7NmzRoWLVpEQ0MDY8aMIT09nbKyMnbu3Nmj7XT3uYsvvpjf/e531NTUAHRejpw/fz4PPPAAAB0dHTQ0NESwdx8W5d2R5wPb3H27ux8GVgPHj6ZbCPw6fL0GmG+nutAbsc/+lzm8f8ML8J1quKsG7m7A8s6OsyQREZEzwvTp02lsbGTixImMHz+e66+/nvLycoqLi1m1ahXnnHNOj7bT3eemT5/Ot7/9bS666CJKSkr4xje+AcB9991HWVkZxcXFzJo1i82bN0e2j12Zu0ezYbNrgAXufmu4fCNwgbvf1mWdN8N1KsLld8N19ne33dmzZ3t5eXkkNYuIiJyptmzZwtSpU+MuY0A70c/QzDa6++wTrT8g5gkzsy+ZWbmZlVdXV8ddjoiIiEifRTkwfzdQ2GW5IGw70ToVZpYGjCQYoH8Md18JrISgJyySakVERGRA2bRpEzfeeOMxbRkZGbz00ksxVdQ7UYawV4ApZlZEELauBT5/3DpPADcBLwDXAOs9quujIiIiMqgUFxfz+uuvx13GaYsshLl7u5ndBjxNMEXFg+7+lpl9Hyh39yeAXwIPm9k2oJYgqImIiEgM3P2UE6HKiZ1OH1Kk84S5+5PAk8e1fbfL6xZgUZQ1iIiIyKllZmZSU1NDbm6uglgvuTs1NTVkZmb26nOaMV9EREQoKCigoqIC3QB3ejIzMykoKOjVZxTCREREhPT0dIqKiuIu44wyIKaoEBERERlsFMJEREREYqAQJiIiIhKDyB5bFBUzqwZ69gTP05cHdPvoJImFjkly0nFJTjouyUfHJDn1x3GZ5O75J3pjwIWw/mBm5d0950nioWOSnHRckpOOS/LRMUlOcR8XXY4UERERiYFCmIiIiEgMFMJObGXcBciH6JgkJx2X5KTjknx0TJJTrMdFY8JEREREYqCeMBEREZEYKIR1YWYLzOxtM9tmZsvirudMYmaFZlZmZpvN7C0z+2rYPtrMnjGzd8K/c8J2M7P7w2P1hpmdF+8eDF5mlmpmr5nZv4fLRWb2Uviz/1czGxK2Z4TL28L3J8dZ92BmZqPMbI2ZbTWzLWb2SZ0r8TKzr4e/u940s9+aWabOlf5nZg+aWZWZvdmlrdfnhpndFK7/jpndFFW9CmEhM0sFVgCXAtOA68xsWrxVnVHagb9x92nAHOAr4c9/GfCsu08Bng2XIThOU8I/XwIe6P+SzxhfBbZ0Wf4h8FN3PxuoA24J228B6sL2n4brSTTuA55y93OAEoLjo3MlJmY2EbgdmO3uM4BU4Fp0rsThV8CC49p6dW6Y2Wjge8AFwPnA944Gt0RTCPvA+cA2d9/u7oeB1cDCmGs6Y7j7Xnd/NXzdSPCPykSCY/DrcLVfA1eGrxcCqzzwIjDKzMb3c9mDnpkVAJ8DfhEuG3AxsCZc5fhjcvRYrQHmh+tLApnZSOBC4JcA7n7Y3evRuRK3NGComaUBWcBedK70O3ffANQe19zbc+OzwDPuXuvudcAzfDjYJYRC2AcmAru6LFeEbdLPwq75UuAlYKy77w3f2geMDV/rePWPfwbuAI6Ey7lAvbu3h8tdf+6dxyR8vyFcXxKrCKgGHgovE//CzIahcyU27r4buBd4nyB8NQAb0bmSLHp7bvTbOaMQJknFzLKBx4GvufuBru95cCuvbuftJ2Z2GVDl7hvjrkWOkQacBzzg7qXAQT64vALoXOlv4aWqhQQBeQIwjIh6TqRvku3cUAj7wG6gsMtyQdgm/cTM0gkC2KPu/vuwufLopZPw76qwXccren8GXGFmOwguz19MMBZpVHjJBY79uXcek/D9kUBNfxZ8hqgAKtz9pXB5DUEo07kSn08D77l7tbu3Ab8nOH90riSH3p4b/XbOKIR94BVgSng3yxCCQZVPxFzTGSMcD/FLYIu7/6TLW08AR+9MuQn4ty7t/zW8u2UO0NClu1kSwN2/5e4F7j6Z4HxY7+7XA2XANeFqxx+To8fqmnD9pPkf52Dh7vuAXWb28bBpPrAZnStxeh+YY2ZZ4e+yo8dE50py6O258TTwGTPLCXs5PxO2JZwma+3CzP6CYAxMKvCgu/99zCWdMczsz4E/Apv4YPzRnQTjwh4DzgJ2An/l7rXhL7r/QdDlfwhY4u7l/V74GcLM5gHfdPfLzOwjBD1jo4HXgBvcvdXMMoGHCcbz1QLXuvv2uGoezMzsXIKbJYYA24ElBP+p1rkSEzP7O2AxwZ3erwG3Eowj0rnSj8zst8A8IA+oJLjL8Q/08twwsy8Q/BsE8Pfu/lAk9SqEiYiIiPQ/XY4UERERiYFCmIiIiEgMFMJEREREYqAQJiIiIhIDhTARERGRGCiEiciAZ2YdZvZ6lz/LTv2pHm97spm9majtiYgclXbqVUREkl6zu58bdxEiIr2hnjARGbTMbIeZ/ZOZbTKzl83s7LB9spmtN7M3zOxZMzsrbB9rZmvN7P+Gf+aGm0o1s38xs7fM7H+b2dBw/dvNbHO4ndUx7aaIDFAKYSIyGAw97nLk4i7vNbh7McHM2P8ctv0M+LW7zwQeBe4P2+8H/o+7lxA8j/GtsH0KsMLdpwP1wF+G7cuA0nA7S6PaOREZnDRjvogMeGbW5O7ZJ2jfAVzs7tvDB8Tvc/dcM9sPjHf3trB9r7vnmVk1UODurV22MRl4xt2nhMt/C6S7+3IzewpoIngsyh/cvSniXRWRQUQ9YSIy2Hk3r3ujtcvrDj4YT/s5YAVBr9krZqZxtiLSYwphIjLYLe7y9wvh6+eBa8PX1xM8PB7gWeCvAcws1cxGdrdRM0sBCt29DPhbYCTwod44EZHu6H9tIjIYDDWz17ssP+XuR6epyDGzNwh6s64L2/4b8JCZ/XegGlgStn8VWGlmtxD0eP01sLeb70wFHgmDmgH3u3t9wvZIRAY9jQkTkUErHBM22933x12LiMjxdDlSREREJAbqCRMRERGJgXrCRERERGKgECYiIiISA4UwERERkRgohImIiIjEQCFMREREJAYKYSIiIiIx+P++kixh9Bbn0gAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 예측해보기(predict)"
      ],
      "metadata": {
        "id": "0kZeUf4HOiqc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e06685b-4c91-4d7b-804a-f36f83b0daf8",
        "id": "Ywbc7dwdOiqc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "188/188 [==============================] - 2s 9ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[44,  0,  0, ...,  0,  0,  0],\n",
              "       [ 0, 57,  0, ...,  0,  0,  0],\n",
              "       [ 0,  0, 65, ...,  0,  0,  0],\n",
              "       ...,\n",
              "       [ 0,  0,  0, ..., 68,  0,  0],\n",
              "       [ 0,  0,  0, ...,  0, 67,  0],\n",
              "       [ 0,  0,  0, ...,  0,  0, 57]])"
            ]
          },
          "metadata": {},
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_p=pd.DataFrame(y_test)\n",
        "\n",
        "for i in range(1,101):\n",
        "  y_test_p.loc[y_test_p[i] == 1.0 , 'y_test'] = i  \n",
        "y_test_p.y_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "348ddc8c-b4b0-4b45-9b9b-a83b87cbce28",
        "id": "UznLC4h1Oiqc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       59.0\n",
              "1       68.0\n",
              "2       72.0\n",
              "3        2.0\n",
              "4       57.0\n",
              "        ... \n",
              "5995     3.0\n",
              "5996    87.0\n",
              "5997    40.0\n",
              "5998    68.0\n",
              "5999    37.0\n",
              "Name: y_test, Length: 6000, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ex=np.argmax(y_pred, axis=1)\n",
        "ex=ex.astype('float64')\n",
        "ex=pd.DataFrame(ex)\n",
        "ex"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "22f2ac13-d7ce-409c-db36-bf92c9ca0cad",
        "id": "8dvHH9kjOiqc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         0\n",
              "0     59.0\n",
              "1     68.0\n",
              "2     72.0\n",
              "3      2.0\n",
              "4     57.0\n",
              "...    ...\n",
              "5995   3.0\n",
              "5996  87.0\n",
              "5997  40.0\n",
              "5998  68.0\n",
              "5999  45.0\n",
              "\n",
              "[6000 rows x 1 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-33a2dfcf-6ed7-497f-9b45-70f7cc7db973\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>59.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>68.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>72.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>57.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5995</th>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5996</th>\n",
              "      <td>87.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5997</th>\n",
              "      <td>40.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5998</th>\n",
              "      <td>68.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5999</th>\n",
              "      <td>45.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6000 rows × 1 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-33a2dfcf-6ed7-497f-9b45-70f7cc7db973')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-33a2dfcf-6ed7-497f-9b45-70f7cc7db973 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-33a2dfcf-6ed7-497f-9b45-70f7cc7db973');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ch1 = pd.DataFrame(np.rint(y_pred))\n",
        "ch1['pred'] = ex\n",
        "data_test.reset_index(inplace=True, drop=True)\n",
        "predict = pd.concat([data_test,ch1['pred'], y_test_p['y_test']],axis = 1)"
      ],
      "metadata": {
        "id": "Ak1wLqaVOiqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict['compare'] = np.where(predict['pred']==predict['y_test'],'yes', 'no')"
      ],
      "metadata": {
        "id": "A7edBwKcOiqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_1 = predict[predict[\"compare\"]=='no']\n",
        "predict_1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "outputId": "833df983-0f21-460a-de46-668f491d18e1",
        "id": "_M8Qra9qOiqc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             0         1         2         3         4         5         6  \\\n",
              "30    0.001909  0.008317  0.014489  0.020412  0.025995  0.031137  0.035801   \n",
              "46    0.096114  0.095362  0.095047  0.095447  0.096676  0.098677  0.101247   \n",
              "54   -0.041972 -0.048778 -0.055266 -0.061636 -0.067728 -0.073065 -0.076978   \n",
              "55    0.050686  0.056807  0.061922  0.066330  0.070463  0.074831  0.079964   \n",
              "71   -0.511413 -0.507140 -0.469849 -0.408462 -0.333769 -0.256404 -0.185178   \n",
              "...        ...       ...       ...       ...       ...       ...       ...   \n",
              "5971  0.005315 -0.043084 -0.083200 -0.110473 -0.122973 -0.121490 -0.109050   \n",
              "5981 -0.005281 -0.018280 -0.031669 -0.042939 -0.050464 -0.053648 -0.052817   \n",
              "5992 -0.084734 -0.091664 -0.098196 -0.101411 -0.098148 -0.085744 -0.062829   \n",
              "5994  0.008461  0.011468  0.012707  0.012096  0.009875  0.006558  0.002841   \n",
              "5999  0.052145  0.038833  0.024907  0.010402 -0.004429 -0.019087 -0.032867   \n",
              "\n",
              "             7         8         9  ...       249       250       251  \\\n",
              "30    0.040075  0.044195  0.048538  ...  0.044631  0.050399  0.057432   \n",
              "46    0.104083  0.106844  0.109202  ... -0.004398 -0.008214 -0.013081   \n",
              "54   -0.078796 -0.078050 -0.074630  ... -0.102289 -0.108650 -0.114708   \n",
              "55    0.086349  0.094365  0.104239  ...  0.125449  0.136371  0.147691   \n",
              "71   -0.126038 -0.081724 -0.052058  ...  0.031533  0.034874  0.038518   \n",
              "...        ...       ...       ...  ...       ...       ...       ...   \n",
              "5971 -0.090022 -0.069069 -0.050204  ... -0.027012 -0.029614 -0.031356   \n",
              "5981 -0.048958 -0.043371 -0.037307  ... -0.016826 -0.013496 -0.011275   \n",
              "5992 -0.030004  0.009808  0.051498  ...  0.072150  0.081214  0.090895   \n",
              "5994 -0.000513 -0.002782 -0.003375  ... -0.048723 -0.051678 -0.052151   \n",
              "5999 -0.044991 -0.054774 -0.061789  ... -0.056943 -0.054300 -0.054324   \n",
              "\n",
              "           252       253       254       255   pred  y_test  compare  \n",
              "30    0.066046  0.076371  0.088327  0.101648   85.0    36.0       no  \n",
              "46   -0.019006 -0.025674 -0.032320 -0.037693   83.0    80.0       no  \n",
              "54   -0.119718 -0.122885 -0.123496 -0.121095   20.0    71.0       no  \n",
              "55    0.159697  0.172642  0.186692  0.201860   41.0    50.0       no  \n",
              "71    0.042180  0.045745  0.049241  0.052764    4.0    93.0       no  \n",
              "...        ...       ...       ...       ...    ...     ...      ...  \n",
              "5971 -0.033020 -0.035244 -0.038329 -0.042129   95.0    23.0       no  \n",
              "5981 -0.010594 -0.011830 -0.015208 -0.020697   52.0    40.0       no  \n",
              "5992  0.101388  0.112932  0.125713  0.139775  100.0    83.0       no  \n",
              "5994 -0.050205 -0.046318 -0.041326 -0.036264   95.0    49.0       no  \n",
              "5999 -0.058068 -0.065599 -0.075635 -0.085448   45.0    37.0       no  \n",
              "\n",
              "[545 rows x 259 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-22dffb56-3200-49eb-89c1-97f537806f07\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>249</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "      <th>pred</th>\n",
              "      <th>y_test</th>\n",
              "      <th>compare</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.001909</td>\n",
              "      <td>0.008317</td>\n",
              "      <td>0.014489</td>\n",
              "      <td>0.020412</td>\n",
              "      <td>0.025995</td>\n",
              "      <td>0.031137</td>\n",
              "      <td>0.035801</td>\n",
              "      <td>0.040075</td>\n",
              "      <td>0.044195</td>\n",
              "      <td>0.048538</td>\n",
              "      <td>...</td>\n",
              "      <td>0.044631</td>\n",
              "      <td>0.050399</td>\n",
              "      <td>0.057432</td>\n",
              "      <td>0.066046</td>\n",
              "      <td>0.076371</td>\n",
              "      <td>0.088327</td>\n",
              "      <td>0.101648</td>\n",
              "      <td>85.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>0.096114</td>\n",
              "      <td>0.095362</td>\n",
              "      <td>0.095047</td>\n",
              "      <td>0.095447</td>\n",
              "      <td>0.096676</td>\n",
              "      <td>0.098677</td>\n",
              "      <td>0.101247</td>\n",
              "      <td>0.104083</td>\n",
              "      <td>0.106844</td>\n",
              "      <td>0.109202</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.004398</td>\n",
              "      <td>-0.008214</td>\n",
              "      <td>-0.013081</td>\n",
              "      <td>-0.019006</td>\n",
              "      <td>-0.025674</td>\n",
              "      <td>-0.032320</td>\n",
              "      <td>-0.037693</td>\n",
              "      <td>83.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>-0.041972</td>\n",
              "      <td>-0.048778</td>\n",
              "      <td>-0.055266</td>\n",
              "      <td>-0.061636</td>\n",
              "      <td>-0.067728</td>\n",
              "      <td>-0.073065</td>\n",
              "      <td>-0.076978</td>\n",
              "      <td>-0.078796</td>\n",
              "      <td>-0.078050</td>\n",
              "      <td>-0.074630</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.102289</td>\n",
              "      <td>-0.108650</td>\n",
              "      <td>-0.114708</td>\n",
              "      <td>-0.119718</td>\n",
              "      <td>-0.122885</td>\n",
              "      <td>-0.123496</td>\n",
              "      <td>-0.121095</td>\n",
              "      <td>20.0</td>\n",
              "      <td>71.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>0.050686</td>\n",
              "      <td>0.056807</td>\n",
              "      <td>0.061922</td>\n",
              "      <td>0.066330</td>\n",
              "      <td>0.070463</td>\n",
              "      <td>0.074831</td>\n",
              "      <td>0.079964</td>\n",
              "      <td>0.086349</td>\n",
              "      <td>0.094365</td>\n",
              "      <td>0.104239</td>\n",
              "      <td>...</td>\n",
              "      <td>0.125449</td>\n",
              "      <td>0.136371</td>\n",
              "      <td>0.147691</td>\n",
              "      <td>0.159697</td>\n",
              "      <td>0.172642</td>\n",
              "      <td>0.186692</td>\n",
              "      <td>0.201860</td>\n",
              "      <td>41.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>-0.511413</td>\n",
              "      <td>-0.507140</td>\n",
              "      <td>-0.469849</td>\n",
              "      <td>-0.408462</td>\n",
              "      <td>-0.333769</td>\n",
              "      <td>-0.256404</td>\n",
              "      <td>-0.185178</td>\n",
              "      <td>-0.126038</td>\n",
              "      <td>-0.081724</td>\n",
              "      <td>-0.052058</td>\n",
              "      <td>...</td>\n",
              "      <td>0.031533</td>\n",
              "      <td>0.034874</td>\n",
              "      <td>0.038518</td>\n",
              "      <td>0.042180</td>\n",
              "      <td>0.045745</td>\n",
              "      <td>0.049241</td>\n",
              "      <td>0.052764</td>\n",
              "      <td>4.0</td>\n",
              "      <td>93.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5971</th>\n",
              "      <td>0.005315</td>\n",
              "      <td>-0.043084</td>\n",
              "      <td>-0.083200</td>\n",
              "      <td>-0.110473</td>\n",
              "      <td>-0.122973</td>\n",
              "      <td>-0.121490</td>\n",
              "      <td>-0.109050</td>\n",
              "      <td>-0.090022</td>\n",
              "      <td>-0.069069</td>\n",
              "      <td>-0.050204</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.027012</td>\n",
              "      <td>-0.029614</td>\n",
              "      <td>-0.031356</td>\n",
              "      <td>-0.033020</td>\n",
              "      <td>-0.035244</td>\n",
              "      <td>-0.038329</td>\n",
              "      <td>-0.042129</td>\n",
              "      <td>95.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5981</th>\n",
              "      <td>-0.005281</td>\n",
              "      <td>-0.018280</td>\n",
              "      <td>-0.031669</td>\n",
              "      <td>-0.042939</td>\n",
              "      <td>-0.050464</td>\n",
              "      <td>-0.053648</td>\n",
              "      <td>-0.052817</td>\n",
              "      <td>-0.048958</td>\n",
              "      <td>-0.043371</td>\n",
              "      <td>-0.037307</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.016826</td>\n",
              "      <td>-0.013496</td>\n",
              "      <td>-0.011275</td>\n",
              "      <td>-0.010594</td>\n",
              "      <td>-0.011830</td>\n",
              "      <td>-0.015208</td>\n",
              "      <td>-0.020697</td>\n",
              "      <td>52.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5992</th>\n",
              "      <td>-0.084734</td>\n",
              "      <td>-0.091664</td>\n",
              "      <td>-0.098196</td>\n",
              "      <td>-0.101411</td>\n",
              "      <td>-0.098148</td>\n",
              "      <td>-0.085744</td>\n",
              "      <td>-0.062829</td>\n",
              "      <td>-0.030004</td>\n",
              "      <td>0.009808</td>\n",
              "      <td>0.051498</td>\n",
              "      <td>...</td>\n",
              "      <td>0.072150</td>\n",
              "      <td>0.081214</td>\n",
              "      <td>0.090895</td>\n",
              "      <td>0.101388</td>\n",
              "      <td>0.112932</td>\n",
              "      <td>0.125713</td>\n",
              "      <td>0.139775</td>\n",
              "      <td>100.0</td>\n",
              "      <td>83.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5994</th>\n",
              "      <td>0.008461</td>\n",
              "      <td>0.011468</td>\n",
              "      <td>0.012707</td>\n",
              "      <td>0.012096</td>\n",
              "      <td>0.009875</td>\n",
              "      <td>0.006558</td>\n",
              "      <td>0.002841</td>\n",
              "      <td>-0.000513</td>\n",
              "      <td>-0.002782</td>\n",
              "      <td>-0.003375</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.048723</td>\n",
              "      <td>-0.051678</td>\n",
              "      <td>-0.052151</td>\n",
              "      <td>-0.050205</td>\n",
              "      <td>-0.046318</td>\n",
              "      <td>-0.041326</td>\n",
              "      <td>-0.036264</td>\n",
              "      <td>95.0</td>\n",
              "      <td>49.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5999</th>\n",
              "      <td>0.052145</td>\n",
              "      <td>0.038833</td>\n",
              "      <td>0.024907</td>\n",
              "      <td>0.010402</td>\n",
              "      <td>-0.004429</td>\n",
              "      <td>-0.019087</td>\n",
              "      <td>-0.032867</td>\n",
              "      <td>-0.044991</td>\n",
              "      <td>-0.054774</td>\n",
              "      <td>-0.061789</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.056943</td>\n",
              "      <td>-0.054300</td>\n",
              "      <td>-0.054324</td>\n",
              "      <td>-0.058068</td>\n",
              "      <td>-0.065599</td>\n",
              "      <td>-0.075635</td>\n",
              "      <td>-0.085448</td>\n",
              "      <td>45.0</td>\n",
              "      <td>37.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>545 rows × 259 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-22dffb56-3200-49eb-89c1-97f537806f07')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-22dffb56-3200-49eb-89c1-97f537806f07 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-22dffb56-3200-49eb-89c1-97f537806f07');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 163
        }
      ]
    }
  ]
}